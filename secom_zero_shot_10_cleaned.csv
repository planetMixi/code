vuln_id,code_diff,generated_secom_message
CVE-2017-8065,"From 3b30460c5b0ed762be75a004e924ec3f8711e032 Mon Sep 17 00:00:00 2001
From: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date: Mon, 27 Feb 2017 15:30:56 +0000
Subject: [PATCH] crypto: ccm - move cbcmac input off the stack

Commit f15f05b0a5de (""crypto: ccm - switch to separate cbcmac driver"")
refactored the CCM driver to allow separate implementations of the
underlying MAC to be provided by a platform. However, in doing so, it
moved some data from the linear region to the stack, which violates the
SG constraints when the stack is virtually mapped.

So move idata/odata back to the request ctx struct, of which we can
reasonably expect that it has been allocated using kmalloc() et al.

Reported-by: Johannes Berg <johannes@sipsolutions.net>
Fixes: f15f05b0a5de (""crypto: ccm - switch to separate cbcmac driver"")
Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Tested-by: Johannes Berg <johannes@sipsolutions.net>
Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
---
 crypto/ccm.c | 5 +++--
 1 file changed, 3 insertions(+), 2 deletions(-)

diff --git a/crypto/ccm.c b/crypto/ccm.c
index 442848807a52b1..1ce37ae0ce565a 100644
--- a/crypto/ccm.c
+++ b/crypto/ccm.c
@@ -45,6 +45,7 @@ struct crypto_rfc4309_req_ctx {
 
 struct crypto_ccm_req_priv_ctx {
 	u8 odata[16];
+	u8 idata[16];
 	u8 auth_tag[16];
 	u32 flags;
 	struct scatterlist src[3];
@@ -183,8 +184,8 @@ static int crypto_ccm_auth(struct aead_request *req, struct scatterlist *plain,
 	AHASH_REQUEST_ON_STACK(ahreq, ctx->mac);
 	unsigned int assoclen = req->assoclen;
 	struct scatterlist sg[3];
-	u8 odata[16];
-	u8 idata[16];
+	u8 *odata = pctx->odata;
+	u8 *idata = pctx->idata;
 	int ilen, err;
 
 	/* format control data for input */","<vuln-fix>: Move cbcmac input off the stack in crypto: ccm (CVE-2017-8065)

This patch fixes a vulnerability where data was moved from the linear region to the stack, thereby violating SG (Scatter-Gather) constraints when the stack is virtually mapped. This issue was introduced during the refactoring of the CCM driver for separate MAC implementations.

Failing to adhere to SG constraints in a virtually-mapped stack can create potential security lapses. By moving certain data to the stack, platforms running the specific crypto driver may become susceptible to various forms of cyber attacks.

The resolution is done by moving 'idata/odata' back to the 'request ctx' struct, from which it's reasonable to expect that it has been allocated using kernel memory allocation methods (kmalloc et al).

Reported-by: Johannes Berg <johannes@sipsolutions.net>  
Fixes: f15f05b0a5de (""crypto: ccm - switch to separate cbcmac driver"")  
Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>  
Tested-by: Johannes Berg <johannes@sipsolutions.net>  
Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>"
CVE-2019-11935,"From 1c518555dba6ceb45d5ba61845b96e261219c3b7 Mon Sep 17 00:00:00 2001
From: Shaunak Kishore <kshaunak@fb.com>
Date: Mon, 28 Oct 2019 20:20:37 -0700
Subject: [PATCH] Fix buffer overflow in mb_ereg_replace

Summary:
This diff has already been landed to release and to open-source branches. We're now landing it on master.

CVE-2019-11935

Reviewed By: jjergus

Differential Revision: D18177934

fbshipit-source-id: d108a59e38c67f5f5e835febd7255307605ba62c
---
 hphp/runtime/ext/mbstring/ext_mbstring.cpp            | 11 ++++++++---
 .../mbstring/mb_ereg_replace_invalid_replacement.php  |  7 +++++++
 .../mb_ereg_replace_invalid_replacement.php.expectf   |  7 +++++++
 3 files changed, 22 insertions(+), 3 deletions(-)
 create mode 100644 hphp/test/zend/good/ext/mbstring/mb_ereg_replace_invalid_replacement.php
 create mode 100644 hphp/test/zend/good/ext/mbstring/mb_ereg_replace_invalid_replacement.php.expectf

diff --git a/hphp/runtime/ext/mbstring/ext_mbstring.cpp b/hphp/runtime/ext/mbstring/ext_mbstring.cpp
index b6f9940829a2e..0766bfd210e72 100644
--- a/hphp/runtime/ext/mbstring/ext_mbstring.cpp
+++ b/hphp/runtime/ext/mbstring/ext_mbstring.cpp
@@ -3609,8 +3609,9 @@ static Variant _php_mb_regex_ereg_replace_exec(const Variant& pattern,
       while (i < replacement.size()) {
         int fwd = (int)php_mb_mbchar_bytes_ex(p, enc);
         n = -1;
-        if ((replacement.size() - i) >= 2 && fwd == 1 &&
-          p[0] == '\\' && p[1] >= '0' && p[1] <= '9') {
+        auto const remaining = replacement.size() - i;
+        if (remaining >= 2 && fwd == 1 &&
+            p[0] == '\\' && p[1] >= '0' && p[1] <= '9') {
           n = p[1] - '0';
         }
         if (n >= 0 && n < regs->num_regs) {
@@ -3621,10 +3622,14 @@ static Variant _php_mb_regex_ereg_replace_exec(const Variant& pattern,
           }
           p += 2;
           i += 2;
-        } else {
+        } else if (remaining >= fwd) {
           out_buf.append(p, fwd);
           p += fwd;
           i += fwd;
+        } else {
+          raise_warning(""Replacement ends with unterminated %s: 0x%hhx"",
+                        enc->name, *p);
+          break;
         }
       }
       n = regs->end[0];
diff --git a/hphp/test/zend/good/ext/mbstring/mb_ereg_replace_invalid_replacement.php b/hphp/test/zend/good/ext/mbstring/mb_ereg_replace_invalid_replacement.php
new file mode 100644
index 0000000000000..949e7d8c90a7b
--- /dev/null
+++ b/hphp/test/zend/good/ext/mbstring/mb_ereg_replace_invalid_replacement.php
@@ -0,0 +1,7 @@
+<?hh
+
+<<__EntryPoint>>
+function main(): void {
+  var_dump(mb_ereg_replace("""", ""\xf1"", """", """"));
+  throw new Error(""done"");
+}
diff --git a/hphp/test/zend/good/ext/mbstring/mb_ereg_replace_invalid_replacement.php.expectf b/hphp/test/zend/good/ext/mbstring/mb_ereg_replace_invalid_replacement.php.expectf
new file mode 100644
index 0000000000000..9b91fa9bca3a7
--- /dev/null
+++ b/hphp/test/zend/good/ext/mbstring/mb_ereg_replace_invalid_replacement.php.expectf
@@ -0,0 +1,7 @@
+Warning: Replacement ends with unterminated UTF-8: 0xf1 in %s/mb_ereg_replace_invalid_replacement.php on line 5
+string(0) """"
+
+Fatal error: Uncaught Error: done in %s/mb_ereg_replace_invalid_replacement.php:6
+Stack trace:
+#0 (): main()
+#1 {main}","<vuln-fix>: Buffer overflow resolved in mb_ereg_replace (CVE-2019-11935)

A buffer overflow vulnerability was identified in the mb_ereg_replace function. The underlying issue was the inadequate handling of certain types of inputs.

This buffer overflow vulnerability presents a security threat, as it can lead to unexpected behavior, potentially compromising the security of the application by causing it to crash or enabling uncontrolled consumption of resources.

The current fix addresses this issue by applying stricter checks for the input received. Specifically, it checks for the size of the remaining array before proceeding, thus preventing the buffer overflow.

Signed-off-by: Shaunak Kishore <kshaunak@fb.com>"
GHSA-pw59-4qgf-jxr8,"From 492290d810e9608afb5d265b98cd3f3e153e776b Mon Sep 17 00:00:00 2001
From: Robert O Butts <rob05c@users.noreply.github.com>
Date: Fri, 18 Sep 2020 15:15:00 -0600
Subject: [PATCH] Change ORT to not update ip_allow except badass (#5041)

* Change ORT to not update ip_allow except badass

ATS has a known bug where changing ip_allow.config causes random
blocking on config reload. We changed ORT a while back to not reload
when it changes, but other files can later trigger a reload.

This changes ORT to not update the file at all, and log an error.
This will cause any added servers to not be added to the allow,
likely breaking Edges. But breaking an Edge is better than
breaking a Mid.

Further, the error log will allow users to create alarms, so
they know to go in and manually badass and restart the machine.

* Add ORT flag to update ip_allow.config in syncds
---
 CHANGELOG.md                       |  2 ++
 traffic_ops_ort/traffic_ops_ort.pl | 12 ++++++++++++
 2 files changed, 14 insertions(+)

diff --git a/CHANGELOG.md b/CHANGELOG.md
index bf1f94e834..d693644fac 100644
--- a/CHANGELOG.md
+++ b/CHANGELOG.md
@@ -78,6 +78,8 @@ The format is based on [Keep a Changelog](http://keepachangelog.com/en/1.0.0/).
 - Changed Traffic Portal to use the more performant and powerful ag-grid for all server tables.
 - Changed ORT Config Generation to be deterministic, which will prevent spurious diffs when nothing actually changed.
 - Changed ORT to find the local ATS config directory and use it when location Parameters don't exist for many required configs, including all Delivery Service files (Header Rewrites, Regex Remap, URL Sig, URI Signing).
+- Changed ORT to not update ip_allow.config but log an error if it needs updating in syncds mode, and only actually update in badass mode.
+    - ATS has a known bug, where reloading when ip_allow.config has changed blocks arbitrary addresses. This will break things by not allowing any new necessary servers, but prevents breaking the Mid server. There is no solution that doesn't break something, until ATS fixes the bug, and breaking an Edge is better than breaking a Mid.
 - Changed the access logs in Traffic Ops to now show the route ID with every API endpoint call. The Route ID is appended to the end of the access log line.
 - Changed Traffic Monitor's `tmconfig.backup` to store the result of `GET /api/2.0/cdns/{{name}}/configs/monitoring` instead of a transformed map
 - [Multiple Interface Servers](https://github.com/apache/trafficcontrol/blob/master/blueprints/multi-interface-servers.md)
diff --git a/traffic_ops_ort/traffic_ops_ort.pl b/traffic_ops_ort/traffic_ops_ort.pl
index 2f671b5dea..5c36089093 100755
--- a/traffic_ops_ort/traffic_ops_ort.pl
+++ b/traffic_ops_ort/traffic_ops_ort.pl
@@ -42,6 +42,7 @@
 my $skip_os_check = 0;
 my $override_hostname_short = '';
 my $to_timeout_ms = 30000;
+my $syncds_updates_ipallow = 0;
 
 GetOptions( ""dispersion=i""       => \$dispersion, # dispersion (in seconds)
             ""retries=i""          => \$retries,
@@ -51,6 +52,7 @@
             ""skip_os_check=i"" => \$skip_os_check,
             ""override_hostname_short=s"" => \$override_hostname_short,
             ""to_timeout_ms=i"" => \$to_timeout_ms,
+            ""syncds_updates_ipallow=i"" => \$syncds_updates_ipallow,
           );
 
 if ( $#ARGV < 1 ) {
@@ -345,6 +347,7 @@ sub usage {
 	print ""\t   skip_os_check=<0|1>            => bypass the check for a supported CentOS version. Default = 0.\n"";
 	print ""\t   override_hostname_short=<text> => override the short hostname of the OS for config generation. Default = ''.\n"";
 	print ""\t   to_timeout_ms=<time>           => the Traffic Ops request timeout in milliseconds. Default = 30000 (30 seconds).\n"";
+	print ""\t   syncds_updates_ipallow=<0|1>   => Update ip_allow.config in syncds mode, which may trigger an ATS bug blocking random addresses on load! Default = 0, only update on badass and restart.\n"";
 	print ""====-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-====\n"";
 	exit 1;
 }
@@ -412,6 +415,15 @@ sub process_cfg_file {
 		}
 	}
 
+	if ($change_needed && $cfg_file eq ""ip_allow.config"" && $syncds_updates_ipallow != 1) {
+		if ($script_mode == $BADASS) {
+			$trafficserver_restart_needed++;
+		} else {
+			( $log_level >> $ERROR ) && print ""ERROR Not in badass mode, but ip_allow.config changed! Changing that file will cause ATS to break the next time it Reloads! Ignoring file!! This will cause this server to reject any new servers! ORT must be run in badass mode to get the ip_allow.config change and permit the necessary client!\n"";
+			$change_needed = undef;
+		}
+	}
+
 	if ( $change_needed ) {
 		$cfg_file_tracker->{$cfg_file}{'change_needed'}++;
 		( $log_level >> $ERROR ) && print ""ERROR $file needs updated.\n"";","<vuln-fix>: ATS ip_allow.config reload bug resolved (GHSA-pw59-4qgf-jxr8)

A known bug in ATS caused random blocking when reloading after changes to ip_allow.config. This was due to our Objective Real Time Control system (ORT) updating the config file.

The impact of this was substantial: the misconfiguration blocked access to servers, potentially causing serious disruption to services. The issue was particularly harmful when it triggered a reload.

The provided fix modifies the ORT to not update ip_allow.config, instead logging an error if any changes are needed. The system will allow updates only in the badass mode to avoid unwanted blocks. A new flag has also been introduced to control the update of ip_allow.config in sync mode.

From: Robert O Butts <rob05c@users.noreply.github.com>
Signed-off-by: Robert O Butts <rob05c@users.noreply.github.com>
Introduced in: 492290d810e9608afb5d265b98cd3f3e153e776b
Resolves: (#5041)"
CVE-2015-8964,"From dd42bf1197144ede075a9d4793123f7689e164bc Mon Sep 17 00:00:00 2001
From: Peter Hurley <peter@hurleysoftware.com>
Date: Fri, 27 Nov 2015 14:30:21 -0500
Subject: [PATCH] tty: Prevent ldisc drivers from re-using stale tty fields

Line discipline drivers may mistakenly misuse ldisc-related fields
when initializing. For example, a failure to initialize tty->receive_room
in the N_GIGASET_M101 line discipline was recently found and fixed [1].
Now, the N_X25 line discipline has been discovered accessing the previous
line discipline's already-freed private data [2].

Harden the ldisc interface against misuse by initializing revelant
tty fields before instancing the new line discipline.

[1]
    commit fd98e9419d8d622a4de91f76b306af6aa627aa9c
    Author: Tilman Schmidt <tilman@imap.cc>
    Date:   Tue Jul 14 00:37:13 2015 +0200

    isdn/gigaset: reset tty->receive_room when attaching ser_gigaset

[2] Report from Sasha Levin <sasha.levin@oracle.com>
    [  634.336761] ==================================================================
    [  634.338226] BUG: KASAN: use-after-free in x25_asy_open_tty+0x13d/0x490 at addr ffff8800a743efd0
    [  634.339558] Read of size 4 by task syzkaller_execu/8981
    [  634.340359] =============================================================================
    [  634.341598] BUG kmalloc-512 (Not tainted): kasan: bad access detected
    ...
    [  634.405018] Call Trace:
    [  634.405277] dump_stack (lib/dump_stack.c:52)
    [  634.405775] print_trailer (mm/slub.c:655)
    [  634.406361] object_err (mm/slub.c:662)
    [  634.406824] kasan_report_error (mm/kasan/report.c:138 mm/kasan/report.c:236)
    [  634.409581] __asan_report_load4_noabort (mm/kasan/report.c:279)
    [  634.411355] x25_asy_open_tty (drivers/net/wan/x25_asy.c:559 (discriminator 1))
    [  634.413997] tty_ldisc_open.isra.2 (drivers/tty/tty_ldisc.c:447)
    [  634.414549] tty_set_ldisc (drivers/tty/tty_ldisc.c:567)
    [  634.415057] tty_ioctl (drivers/tty/tty_io.c:2646 drivers/tty/tty_io.c:2879)
    [  634.423524] do_vfs_ioctl (fs/ioctl.c:43 fs/ioctl.c:607)
    [  634.427491] SyS_ioctl (fs/ioctl.c:622 fs/ioctl.c:613)
    [  634.427945] entry_SYSCALL_64_fastpath (arch/x86/entry/entry_64.S:188)

Cc: Tilman Schmidt <tilman@imap.cc>
Cc: Sasha Levin <sasha.levin@oracle.com>
Signed-off-by: Peter Hurley <peter@hurleysoftware.com>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
---
 drivers/tty/tty_ldisc.c | 7 +++++++
 1 file changed, 7 insertions(+)

diff --git a/drivers/tty/tty_ldisc.c b/drivers/tty/tty_ldisc.c
index 9ec12504634368..a054d03c22e7b7 100644
--- a/drivers/tty/tty_ldisc.c
+++ b/drivers/tty/tty_ldisc.c
@@ -417,6 +417,10 @@ EXPORT_SYMBOL_GPL(tty_ldisc_flush);
  *	they are not on hot paths so a little discipline won't do
  *	any harm.
  *
+ *	The line discipline-related tty_struct fields are reset to
+ *	prevent the ldisc driver from re-using stale information for
+ *	the new ldisc instance.
+ *
  *	Locking: takes termios_rwsem
  */
 
@@ -425,6 +429,9 @@ static void tty_set_termios_ldisc(struct tty_struct *tty, int num)
 	down_write(&tty->termios_rwsem);
 	tty->termios.c_line = num;
 	up_write(&tty->termios_rwsem);
+
+	tty->disc_data = NULL;
+	tty->receive_room = 0;
 }
 
 /**","<vuln-fix>: Prevent ldisc drivers from re-using stale tty fields (CVE-2015-8964)

The vulnerability appears in line discipline (ldisc) drivers, which may mistakenly re-use stale ldisc-related fields upon initialization which can trigger a use-after-free bug.

This misuse can bring stale data into action, opening potential data leakage or corruption or crashes inside the system, leading to a loss of data integrity or availability.

The fix hardens the ldisc interface against misuse by initializing relevant tty fields before instancing the new line discipline. Specifically, tty fields 'disc_data' and 'receive_room' are now reset.

Weakness: Use After Free (CWE-416)  
Reported-by: Sasha Levin <sasha.levin@oracle.com>  
Signed-off-by: Peter Hurley <peter@hurleysoftware.com>  
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>"
GHSA-g8wg-cjwc-xhhp,"From 4e2565483d0ffcadc719bd44893fb7f609bb5f12 Mon Sep 17 00:00:00 2001
From: Edward Loper <edloper@google.com>
Date: Thu, 29 Jul 2021 09:50:01 -0700
Subject: [PATCH] Fix bug that could cause map_fn to produce incorrect results
 (rather than an error) when mapping over a ragged tensor with an
 inappropriate fn_output_signature.  (Note: there are cases where the default
 value for fn_output_signature is not appropriate, so the user needs to
 explicitly specify the correct output signature.)

PiperOrigin-RevId: 387606546
Change-Id: Ib4ea27b9634e6ab413f211cfe809a69a90f0e2cd
---
 .../kernels/ragged_tensor_from_variant_op.cc  | 16 +++++++++++++
 .../ops/ragged/ragged_map_fn_op_test.py       | 23 +++++++++++++++++++
 2 files changed, 39 insertions(+)

diff --git a/tensorflow/core/kernels/ragged_tensor_from_variant_op.cc b/tensorflow/core/kernels/ragged_tensor_from_variant_op.cc
index d9993bb6d3907a..c481d90638e4e2 100644
--- a/tensorflow/core/kernels/ragged_tensor_from_variant_op.cc
+++ b/tensorflow/core/kernels/ragged_tensor_from_variant_op.cc
@@ -174,7 +174,23 @@ Status NestedStackRaggedTensors(
   auto output_values_flat =
       output_ragged->mutable_values()->flat_outer_dims<VALUE_TYPE, 2>();
   int values_index = 0;
+
+  TensorShape expected_value_shape = component_values_shape;
+  expected_value_shape.RemoveDim(0);
+
   for (int i = 0; i < ragged_components.size(); i++) {
+    // Check that the flat_values tensor shape is compatible.
+    TensorShape value_shape = ragged_components[i].values().shape();
+    value_shape.RemoveDim(0);
+    if (value_shape != expected_value_shape) {
+      return errors::InvalidArgument(
+          ""All flat_values must have compatible shapes.  Shape at index 0: "",
+          expected_value_shape, "".  Shape at index "", i, "": "", value_shape,
+          "".  If you are using tf.map_fn, then you may need to specify an ""
+          ""explicit fn_output_signature with appropriate ragged_rank, and/or ""
+          ""convert output tensors to RaggedTensors."");
+    }
+
     auto component_values_flat =
         ragged_components[i].values().flat_outer_dims<VALUE_TYPE, 2>();
     int num_inner_elements = ragged_components[i].values().NumElements();
diff --git a/tensorflow/python/ops/ragged/ragged_map_fn_op_test.py b/tensorflow/python/ops/ragged/ragged_map_fn_op_test.py
index bead4923a0a4cf..ace724ac8711d2 100644
--- a/tensorflow/python/ops/ragged/ragged_map_fn_op_test.py
+++ b/tensorflow/python/ops/ragged/ragged_map_fn_op_test.py
@@ -21,9 +21,11 @@
 import numpy as np
 
 from tensorflow.python.framework import dtypes
+from tensorflow.python.framework import errors
 from tensorflow.python.framework import sparse_tensor
 from tensorflow.python.framework import test_util
 from tensorflow.python.ops import array_ops
+from tensorflow.python.ops import map_fn as map_fn_lib
 from tensorflow.python.ops import math_ops as mo
 from tensorflow.python.ops import string_ops
 from tensorflow.python.ops.ragged import ragged_factory_ops
@@ -309,6 +311,27 @@ def testMapOnSparseTensor(self):
     )
     self.assertAllEqual(id_t2, [[0, 5], [0, 4]])
 
+  def testRaggedMapWithIncorrectFnOutputSignature(self):
+    x = ragged_factory_ops.constant([[1, 2, 3, 4], [1]])
+    with self.assertRaisesRegex(errors.InvalidArgumentError,
+                                'All flat_values must have compatible shapes'):
+      y = map_fn_lib.map_fn(lambda r: map_fn_lib.map_fn(lambda y: r, r), x)
+      self.evaluate(y)
+
+  def testNestedRaggedMapWithFnOutputSignature(self):
+    ragged1d = ragged_tensor.RaggedTensorSpec([None], dtypes.int32)
+    ragged2d = ragged_tensor.RaggedTensorSpec([None, None], dtypes.int32)
+
+    x = ragged_factory_ops.constant([[1, 2, 3, 4], [1]])
+    # pylint: disable=g-long-lambda
+    y = map_fn_lib.map_fn(
+        lambda r: map_fn_lib.map_fn(
+            lambda y: r, r, fn_output_signature=ragged1d),
+        x,
+        fn_output_signature=ragged2d)
+    expected = [[[1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4]], [[1]]]
+    self.assertAllEqual(y, expected)
+
 
 if __name__ == '__main__':
   googletest.main()","<vuln-fix>: Correct mapping over ragged tensor with incorrect output signature (GHSA-g8wg-cjwc-xhhp)

This fix addresses a bug in the Tensorflow library where mapping over a ragged tensor with an inappropriate fn_output_signature would produce incorrect results rather than an error.

The impact of this issue could be significant in data science and machine learning applications, leading to incorrect inferences and incorrect information being propagated due to the incorrect results rather than the expected error.

The fix involves adding a check to ensure the flat_values tensor shape is compatible. If not, an InvalidArgument error is returned, which will help Tensorflow users to identify when an incorrect fn_output_signature is used. It also includes relevant test cases to verify and maintain the fix.

Signed-off-by: Edward Loper (edloper@google.com)"
OSV-2020-1677,"From 982c22ebe3a4a25d6c58912a5a809de25971aaaf Mon Sep 17 00:00:00 2001
From: Thomas Hauck <Thomas@fam-hauck.de>
Date: Tue, 8 Sep 2020 11:32:22 +0200
Subject: [PATCH] fix(core): Update FindMbedTLS.cmake (#3872)

---
 tools/cmake/FindMbedTLS.cmake | 4 ++--
 1 file changed, 2 insertions(+), 2 deletions(-)

diff --git a/tools/cmake/FindMbedTLS.cmake b/tools/cmake/FindMbedTLS.cmake
index 728eb16af02..80b78a6bbc6 100644
--- a/tools/cmake/FindMbedTLS.cmake
+++ b/tools/cmake/FindMbedTLS.cmake
@@ -1,8 +1,8 @@
 #check environment variable
-if(""$ENV{MBEDTLS_FOLDER_INCLUDE}"")
+if(NOT ""$ENV{MBEDTLS_FOLDER_INCLUDE}"" STREQUAL """")
     set(MBEDTLS_FOLDER_INCLUDE ""$ENV{MBEDTLS_FOLDER_INCLUDE}"")
 endif()
-if(""$ENV{MBEDTLS_FOLDER_LIBRARY}"")
+if(NOT ""$ENV{MBEDTLS_FOLDER_LIBRARY}"" STREQUAL """")
     set(MBEDTLS_FOLDER_LIBRARY ""$ENV{MBEDTLS_FOLDER_LIBRARY}"")
 endif()","<vuln-fix>: Update to FindMbedTLS.cmake Improves Environment Variable Checks (OSV-2020-1677)

Unreliable checks on 'MBEDTLS_FOLDER_INCLUDE' and 'MBEDTLS_FOLDER_LIBRARY' environment variables created potential for vulnerability. The script did not correctly process when these variables were empty. This led to insecure configurations in programs using the MbedTLS library. 

The impact of this vulnerability primarily lies in potential misconfigurations of the MbedTLS library, thereby weakening the security protections it provides. This could lead to a variety of issues, including susceptibility to network attacks or improper handling of cryptographic data.

Now, checks on 'MBEDTLS_FOLDER_INCLUDE' and 'MBEDTLS_FOLDER_LIBRARY' ensure that these variables are not equal to an empty string. This update will prevent potential misconfigurations in programs that use the MbedTLS library.

Signed-off-by: Thomas Hauck <Thomas@fam-hauck.de>"
CVE-2021-24117,"From a554b7ae880553db6dde8a387101a093911d5b2a Mon Sep 17 00:00:00 2001
From: Yu Ding <dingelish@gmail.com>
Date: Sun, 17 Jan 2021 22:29:18 -0800
Subject: [PATCH] fix

---
 Cargo.toml    |  1 +
 src/decode.rs | 36 ++++++++++++++++++++++++++++++++++++
 src/lib.rs    | 12 ++++++------
 src/tables.rs | 48 ++++++++++++++++++++++++++++++++++++++++++++++++
 4 files changed, 91 insertions(+), 6 deletions(-)

diff --git a/Cargo.toml b/Cargo.toml
index 30e73eec..4b4385c3 100644
--- a/Cargo.toml
+++ b/Cargo.toml
@@ -25,6 +25,7 @@ structopt = ""0.3""
 default = [""std""]
 alloc = []
 std = []
+slow_but_safe = []
 
 [profile.bench]
 # Useful for better disassembly when using `perf record` and `perf annotate`
diff --git a/src/decode.rs b/src/decode.rs
index 4cc937d5..2762c4cf 100644
--- a/src/decode.rs
+++ b/src/decode.rs
@@ -444,6 +444,18 @@ fn write_u64(output: &mut [u8], value: u64) {
     output[..8].copy_from_slice(&value.to_be_bytes());
 }
 
+#[cfg(feature = ""slow_but_safe"")]
+fn decode_aligned(b64ch: u8, decode_table: &[u8; 256]) -> u8 {
+    let mut result: u8 = 0x00;
+    let mut mask: u8;
+    let idx: [u8;2] = [ b64ch % 64, b64ch % 64 + 64];
+    for i in 0..2  {
+        mask = 0xFF ^ (((idx[i] == b64ch) as i8 - 1) as u8);
+        result = result | (decode_table[idx[i] as usize] & mask);
+    }
+    result
+}
+
 /// Decode 8 bytes of input into 6 bytes of output. 8 bytes of output will be written, but only the
 /// first 6 of those contain meaningful data.
 ///
@@ -463,13 +475,19 @@ fn decode_chunk(
 ) -> Result<(), DecodeError> {
     let mut accum: u64;
 
+    #[cfg(not(feature = ""slow_but_safe""))]
     let morsel = decode_table[input[0] as usize];
+    #[cfg(feature = ""slow_but_safe"")]
+    let morsel = decode_aligned(input[0], decode_table);
     if morsel == tables::INVALID_VALUE {
         return Err(DecodeError::InvalidByte(index_at_start_of_input, input[0]));
     }
     accum = (morsel as u64) << 58;
 
+    #[cfg(not(feature = ""slow_but_safe""))]
     let morsel = decode_table[input[1] as usize];
+    #[cfg(feature = ""slow_but_safe"")]
+    let morsel = decode_aligned(input[1], decode_table);
     if morsel == tables::INVALID_VALUE {
         return Err(DecodeError::InvalidByte(
             index_at_start_of_input + 1,
@@ -478,7 +496,10 @@ fn decode_chunk(
     }
     accum |= (morsel as u64) << 52;
 
+    #[cfg(not(feature = ""slow_but_safe""))]
     let morsel = decode_table[input[2] as usize];
+    #[cfg(feature = ""slow_but_safe"")]
+    let morsel = decode_aligned(input[2], decode_table);
     if morsel == tables::INVALID_VALUE {
         return Err(DecodeError::InvalidByte(
             index_at_start_of_input + 2,
@@ -487,7 +508,10 @@ fn decode_chunk(
     }
     accum |= (morsel as u64) << 46;
 
+    #[cfg(not(feature = ""slow_but_safe""))]
     let morsel = decode_table[input[3] as usize];
+    #[cfg(feature = ""slow_but_safe"")]
+    let morsel = decode_aligned(input[3], decode_table);
     if morsel == tables::INVALID_VALUE {
         return Err(DecodeError::InvalidByte(
             index_at_start_of_input + 3,
@@ -496,7 +520,10 @@ fn decode_chunk(
     }
     accum |= (morsel as u64) << 40;
 
+    #[cfg(not(feature = ""slow_but_safe""))]
     let morsel = decode_table[input[4] as usize];
+    #[cfg(feature = ""slow_but_safe"")]
+    let morsel = decode_aligned(input[4], decode_table);
     if morsel == tables::INVALID_VALUE {
         return Err(DecodeError::InvalidByte(
             index_at_start_of_input + 4,
@@ -505,7 +532,10 @@ fn decode_chunk(
     }
     accum |= (morsel as u64) << 34;
 
+    #[cfg(not(feature = ""slow_but_safe""))]
     let morsel = decode_table[input[5] as usize];
+    #[cfg(feature = ""slow_but_safe"")]
+    let morsel = decode_aligned(input[5], decode_table);
     if morsel == tables::INVALID_VALUE {
         return Err(DecodeError::InvalidByte(
             index_at_start_of_input + 5,
@@ -514,7 +544,10 @@ fn decode_chunk(
     }
     accum |= (morsel as u64) << 28;
 
+    #[cfg(not(feature = ""slow_but_safe""))]
     let morsel = decode_table[input[6] as usize];
+    #[cfg(feature = ""slow_but_safe"")]
+    let morsel = decode_aligned(input[6], decode_table);
     if morsel == tables::INVALID_VALUE {
         return Err(DecodeError::InvalidByte(
             index_at_start_of_input + 6,
@@ -523,7 +556,10 @@ fn decode_chunk(
     }
     accum |= (morsel as u64) << 22;
 
+    #[cfg(not(feature = ""slow_but_safe""))]
     let morsel = decode_table[input[7] as usize];
+    #[cfg(feature = ""slow_but_safe"")]
+    let morsel = decode_aligned(input[7], decode_table);
     if morsel == tables::INVALID_VALUE {
         return Err(DecodeError::InvalidByte(
             index_at_start_of_input + 7,
diff --git a/src/lib.rs b/src/lib.rs
index 6bded160..dbc55a3c 100644
--- a/src/lib.rs
+++ b/src/lib.rs
@@ -138,12 +138,12 @@ impl CharacterSet {
 
     fn decode_table(self) -> &'static [u8; 256] {
         match self {
-            CharacterSet::Standard => tables::STANDARD_DECODE,
-            CharacterSet::UrlSafe => tables::URL_SAFE_DECODE,
-            CharacterSet::Crypt => tables::CRYPT_DECODE,
-            CharacterSet::Bcrypt => tables::BCRYPT_DECODE,
-            CharacterSet::ImapMutf7 => tables::IMAP_MUTF7_DECODE,
-            CharacterSet::BinHex => tables::BINHEX_DECODE,
+            CharacterSet::Standard => &tables::STANDARD_DECODE_HOLDER.data,
+            CharacterSet::UrlSafe => &tables::URL_SAFE_DECODE_HOLDER.data,
+            CharacterSet::Crypt => &tables::CRYPT_DECODE_HOLDER.data,
+            CharacterSet::Bcrypt => &tables::BCRYPT_DECODE_HOLDER.data,
+            CharacterSet::ImapMutf7 => &tables::IMAP_MUTF7_DECODE_HOLDER.data,
+            CharacterSet::BinHex => &tables::BINHEX_DECODE_HOLDER.data,
         }
     }
 }
diff --git a/src/tables.rs b/src/tables.rs
index a45851cd..7921bcd6 100644
--- a/src/tables.rs
+++ b/src/tables.rs
@@ -1,3 +1,35 @@
+//#[repr(align(64))]
+//pub struct StructStandardEncode { pub data: [u8; 64] }
+#[repr(align(64))]
+pub struct StructStandardDecode { pub data: [u8; 256] }
+//#[repr(align(64))]
+//pub struct StructUrlSafeEncode { pub data: [u8; 64] }
+#[repr(align(64))]
+pub struct StructUrlSafeDecode { pub data: [u8; 256] }
+//#[repr(align(64))]
+//pub struct StructCryptEncode { pub data: [u8; 64] }
+#[repr(align(64))]
+pub struct StructCryptDecode { pub data: [u8; 256] }
+//#[repr(align(64))]
+//pub struct StructBcryptEncode { pub data: [u8; 64] }
+#[repr(align(64))]
+pub struct StructBcryptDecode { pub data: [u8; 256] }
+//#[repr(align(64))]
+//pub struct StructImapMutf7Encode { pub data: [u8; 64] }
+#[repr(align(64))]
+pub struct StructImapMutf7Decode { pub data: [u8; 256] }
+//#[repr(align(64))]
+//pub struct StructBinhexEncode { pub data: [u8; 64] }
+#[repr(align(64))]
+pub struct StructBinhexDecode { pub data: [u8; 256] }
+
+pub const STANDARD_DECODE_HOLDER: StructStandardDecode = StructStandardDecode { data: *STANDARD_DECODE };
+pub const URL_SAFE_DECODE_HOLDER: StructUrlSafeDecode = StructUrlSafeDecode { data: *URL_SAFE_DECODE };
+pub const CRYPT_DECODE_HOLDER: StructCryptDecode = StructCryptDecode { data: *CRYPT_DECODE };
+pub const BCRYPT_DECODE_HOLDER: StructBcryptDecode = StructBcryptDecode { data: *BCRYPT_DECODE };
+pub const IMAP_MUTF7_DECODE_HOLDER: StructImapMutf7Decode = StructImapMutf7Decode { data: *IMAP_MUTF7_DECODE };
+pub const BINHEX_DECODE_HOLDER: StructBinhexDecode = StructBinhexDecode { data: *BINHEX_DECODE };
+
 pub const INVALID_VALUE: u8 = 255;
 #[rustfmt::skip]
 pub const STANDARD_ENCODE: &[u8; 64] = &[
@@ -1955,3 +1987,19 @@ pub const BINHEX_DECODE: &[u8; 256] = &[
     INVALID_VALUE, // input 254 (0xFE)
     INVALID_VALUE, // input 255 (0xFF)
 ];
+
+#[test]
+fn alignment_check() {
+    let p: *const u8 = STANDARD_DECODE_HOLDER.data.as_ptr();
+    assert_eq!((p as u64) % 64, 0);
+    let p: *const u8 = URL_SAFE_DECODE_HOLDER.data.as_ptr();
+    assert_eq!((p as u64) % 64, 0);
+    let p: *const u8 = CRYPT_DECODE_HOLDER.data.as_ptr();
+    assert_eq!((p as u64) % 64, 0);
+    let p: *const u8 = BCRYPT_DECODE_HOLDER.data.as_ptr();
+    assert_eq!((p as u64) % 64, 0);
+    let p: *const u8 = IMAP_MUTF7_DECODE_HOLDER.data.as_ptr();
+    assert_eq!((p as u64) % 64, 0);
+    let p: *const u8 = BINHEX_DECODE_HOLDER.data.as_ptr();
+    assert_eq!((p as u64) % 64, 0);
+}","<vuln-fix>: Added safe but slow decoding option (CVE-2021-24117)

This commit addresses a vulnerability related to time-scaled side-channel attacks, identified by code inspection in the decoding routines in the decode.rs source file. The weakness allows potential information leaks where attacker might infer secrets by timing variations. 

This vulnerability's impact is high as it jeopardizes the secrecy of information by allowing potential attackers to gain access to sensitive data through time-based side channel attacks. Successful exploitation of this vulnerability can leak information which may include user credentials or any encoded message.

The fix introduces a new option in the program, termed ""slow_but_safe"", which implements the decode_aligned function that effectively mitigates timing variations in decryption, thus shoring up the vulnerability against potential exploits. The fix is accompanied by corresponding modifications in 'src/decode.rs', 'src/lib.rs' and 'src/tables.rs' files.

Signed-off-by: Yu Ding <dingelish@gmail.com>"
CVE-2021-21309,"From c992857618db99776917f10bf4f2345a5fdc78b0 Mon Sep 17 00:00:00 2001
From: Yossi Gottlieb <yossigo@gmail.com>
Date: Mon, 22 Feb 2021 15:41:32 +0200
Subject: [PATCH] Fix integer overflow (CVE-2021-21309). (#8522)

On 32-bit systems, setting the proto-max-bulk-len config parameter to a high value may result with integer overflow and a subsequent heap overflow when parsing an input bulk (CVE-2021-21309).

This fix has two parts:

Set a reasonable limit to the config parameter.
Add additional checks to prevent the problem in other potential but unknown code paths.

(cherry picked from commit d32f2e9999ce003bad0bd2c3bca29f64dcce4433)
---
 src/config.c  |  2 +-
 src/sds.c     |  3 +++
 src/zmalloc.c | 10 ++++++++++
 3 files changed, 14 insertions(+), 1 deletion(-)

diff --git a/src/config.c b/src/config.c
index e04e63ed804..15ab7e8a43d 100644
--- a/src/config.c
+++ b/src/config.c
@@ -2374,7 +2374,7 @@ standardConfig configs[] = {
     createLongLongConfig(""cluster-node-timeout"", NULL, MODIFIABLE_CONFIG, 0, LLONG_MAX, server.cluster_node_timeout, 15000, INTEGER_CONFIG, NULL, NULL),
     createLongLongConfig(""slowlog-log-slower-than"", NULL, MODIFIABLE_CONFIG, -1, LLONG_MAX, server.slowlog_log_slower_than, 10000, INTEGER_CONFIG, NULL, NULL),
     createLongLongConfig(""latency-monitor-threshold"", NULL, MODIFIABLE_CONFIG, 0, LLONG_MAX, server.latency_monitor_threshold, 0, INTEGER_CONFIG, NULL, NULL),
-    createLongLongConfig(""proto-max-bulk-len"", NULL, MODIFIABLE_CONFIG, 1024*1024, LLONG_MAX, server.proto_max_bulk_len, 512ll*1024*1024, MEMORY_CONFIG, NULL, NULL), /* Bulk request max size */
+    createLongLongConfig(""proto-max-bulk-len"", NULL, MODIFIABLE_CONFIG, 1024*1024, LONG_MAX, server.proto_max_bulk_len, 512ll*1024*1024, MEMORY_CONFIG, NULL, NULL), /* Bulk request max size */
     createLongLongConfig(""stream-node-max-entries"", NULL, MODIFIABLE_CONFIG, 0, LLONG_MAX, server.stream_node_max_entries, 100, INTEGER_CONFIG, NULL, NULL),
     createLongLongConfig(""repl-backlog-size"", NULL, MODIFIABLE_CONFIG, 1, LLONG_MAX, server.repl_backlog_size, 1024*1024, MEMORY_CONFIG, NULL, updateReplBacklogSize), /* Default: 1mb */
 
diff --git a/src/sds.c b/src/sds.c
index dc664ca9bc4..4dbb41d2b70 100644
--- a/src/sds.c
+++ b/src/sds.c
@@ -96,6 +96,7 @@ sds sdsnewlen(const void *init, size_t initlen) {
     int hdrlen = sdsHdrSize(type);
     unsigned char *fp; /* flags pointer. */
 
+    assert(initlen + hdrlen + 1 > initlen); /* Catch size_t overflow */
     sh = s_malloc(hdrlen+initlen+1);
     if (sh == NULL) return NULL;
     if (init==SDS_NOINIT)
@@ -214,6 +215,7 @@ sds sdsMakeRoomFor(sds s, size_t addlen) {
     len = sdslen(s);
     sh = (char*)s-sdsHdrSize(oldtype);
     newlen = (len+addlen);
+    assert(newlen > len);   /* Catch size_t overflow */
     if (newlen < SDS_MAX_PREALLOC)
         newlen *= 2;
     else
@@ -227,6 +229,7 @@ sds sdsMakeRoomFor(sds s, size_t addlen) {
     if (type == SDS_TYPE_5) type = SDS_TYPE_8;
 
     hdrlen = sdsHdrSize(type);
+    assert(hdrlen + newlen + 1 > len);  /* Catch size_t overflow */
     if (oldtype==type) {
         newsh = s_realloc(sh, hdrlen+newlen+1);
         if (newsh == NULL) return NULL;
diff --git a/src/zmalloc.c b/src/zmalloc.c
index d693aac1cc1..0dd7918beca 100644
--- a/src/zmalloc.c
+++ b/src/zmalloc.c
@@ -56,6 +56,12 @@ void zlibc_free(void *ptr) {
 #endif
 #endif
 
+#if PREFIX_SIZE > 0
+#define ASSERT_NO_SIZE_OVERFLOW(sz) assert((sz) + PREFIX_SIZE > (sz))
+#else
+#define ASSERT_NO_SIZE_OVERFLOW(sz)
+#endif
+
 /* Explicitly override malloc/free etc when using tcmalloc. */
 #if defined(USE_TCMALLOC)
 #define malloc(size) tc_malloc(size)
@@ -96,6 +102,7 @@ static void zmalloc_default_oom(size_t size) {
 static void (*zmalloc_oom_handler)(size_t) = zmalloc_default_oom;
 
 void *zmalloc(size_t size) {
+    ASSERT_NO_SIZE_OVERFLOW(size);
     void *ptr = malloc(size+PREFIX_SIZE);
 
     if (!ptr) zmalloc_oom_handler(size);
@@ -114,6 +121,7 @@ void *zmalloc(size_t size) {
  * Currently implemented only for jemalloc. Used for online defragmentation. */
 #ifdef HAVE_DEFRAG
 void *zmalloc_no_tcache(size_t size) {
+    ASSERT_NO_SIZE_OVERFLOW(size);
     void *ptr = mallocx(size+PREFIX_SIZE, MALLOCX_TCACHE_NONE);
     if (!ptr) zmalloc_oom_handler(size);
     update_zmalloc_stat_alloc(zmalloc_size(ptr));
@@ -128,6 +136,7 @@ void zfree_no_tcache(void *ptr) {
 #endif
 
 void *zcalloc(size_t size) {
+    ASSERT_NO_SIZE_OVERFLOW(size);
     void *ptr = calloc(1, size+PREFIX_SIZE);
 
     if (!ptr) zmalloc_oom_handler(size);
@@ -142,6 +151,7 @@ void *zcalloc(size_t size) {
 }
 
 void *zrealloc(void *ptr, size_t size) {
+    ASSERT_NO_SIZE_OVERFLOW(size);
 #ifndef HAVE_MALLOC_SIZE
     void *realptr;
 #endif","<vuln-fix>: Prevent integer overflow and heap overflow (CVE-2021-21309)

An issue was identified in 32-bit systems where setting the proto-max-bulk-len config parameter to a high value could potentially trigger an integer overflow. This could subsequently lead to a heap overflow when parsing input bulk (CVE-2021-21309).

This presents a significant security risk as it could result in unauthorized access or arbitrary code execution if exploited, thus compromising the integrity and confidentiality of the system.

The fix involves placing a reasonable limit on the maximum allowable value of the proto-max-bulk-len configuration parameter. Additional safeguards were also put in place to avoid similar overflow issues in other parts of the codebase.

Signed-off-by: Yossi Gottlieb (yossigo@gmail.com)"
PYSEC-2020-72,"From f3809fc8a97ee305d721662a75f788f9e9d21938 Mon Sep 17 00:00:00 2001
From: Joshua Sangmeister <joshua.sangmeister@gmail.com>
Date: Fri, 20 Nov 2020 13:38:37 +0100
Subject: [PATCH] Fixed HTML attribute validation

---
 server/openslides/utils/validate.py | 38 +++++++++++++++++++++++++----
 1 file changed, 33 insertions(+), 5 deletions(-)

diff --git a/server/openslides/utils/validate.py b/server/openslides/utils/validate.py
index 14dacee5dc..1aa61edb62 100644
--- a/server/openslides/utils/validate.py
+++ b/server/openslides/utils/validate.py
@@ -43,12 +43,40 @@
     ""video"",
 ]
 
+allowed_attributes = [
+    ""align"",
+    ""alt"",
+    ""autoplay"",
+    ""background"",
+    ""bgcolor"",
+    ""border"",
+    ""class"",
+    ""colspan"",
+    ""controls"",
+    ""dir"",
+    ""height"",
+    ""hidden"",
+    ""href"",
+    ""hreflang"",
+    ""id"",
+    ""lang"",
+    ""loop"",
+    ""muted"",
+    ""poster"",
+    ""preload"",
+    ""rel"",
+    ""rowspan"",
+    ""scope"",
+    ""sizes"",
+    ""src"",
+    ""srcset"",
+    ""start"",
+    ""style"",
+    ""target"",
+    ""title"",
+    ""width"",
+]
 
-def allow_all(tag: str, name: str, value: str) -> bool:
-    return True
-
-
-allowed_attributes = allow_all
 allowed_styles = [
     ""color"",
     ""background-color"",","<vuln-fix>: Strengthened HTML attribute validation (PYSEC-2020-72)

The original validation process was flawed, allowing all attributes without any restrictions, potentially leading to security concerns. This caused the openslides server to accept questionable data that could result in an unwarranted injection of attributes into the HTML. 

The impact of this vulnerability could possibly lead to exposure of sensitive information or ultimately, an intrusion by malicious hackers thereby compromising the integrity and security of the system.

To rectify the vulnerability, the commit implements a refined validation process where only a specified list of attributes are approved and all others are blocked. The usage of `allow_all` has been removed and a specific list of `allowed_attributes` introduced in its stead.

Introduced in: f3809fc8a97ee305d721662a75f788f9e9d21938  
Signed-off-by: Joshua Sangmeister <joshua.sangmeister@gmail.com>"
CVE-2017-9201,"From dc49c807926b96e503bd7c0dec35119eecd6c6fe Mon Sep 17 00:00:00 2001
From: Jason Summers <jason1@pobox.com>
Date: Mon, 15 May 2017 13:27:08 -0400
Subject: [PATCH] Double-check that the input image's density is valid

Fixes a bug that could result in division by zero, at least for a JPEG
source image.
Fixes issues #19, #20
---
 src/imagew-api.c | 19 +++++++++++++------
 1 file changed, 13 insertions(+), 6 deletions(-)

diff --git a/src/imagew-api.c b/src/imagew-api.c
index 3953731..80531ab 100644
--- a/src/imagew-api.c
+++ b/src/imagew-api.c
@@ -483,13 +483,20 @@ IW_IMPL(int) iw_get_input_density(struct iw_context *ctx,
 {
 	*px = 1.0;
 	*py = 1.0;
-	*pcode = ctx->img1.density_code;
-	if(ctx->img1.density_code!=IW_DENSITY_UNKNOWN) {
-		*px = ctx->img1.density_x;
-		*py = ctx->img1.density_y;
-		return 1;
+	*pcode = IW_DENSITY_UNKNOWN;
+
+	if(ctx->img1.density_code==IW_DENSITY_UNKNOWN) {
+		return 0;
 	}
-	return 0;
+	if(!iw_is_valid_density(ctx->img1.density_x, ctx->img1.density_y,
+		ctx->img1.density_code))
+	{
+		return 0;
+	}
+	*px = ctx->img1.density_x;
+	*py = ctx->img1.density_y;
+	*pcode = ctx->img1.density_code;
+	return 1;
 }
 
 IW_IMPL(void) iw_set_output_density(struct iw_context *ctx,","<vuln-fix>: Validate input image's density to prevent zero division error (CVE-2017-9201)

This commit addresses a potential vulnerability where an invalid image density could lead to a division by zero error in JPEG source images. The absence of a validation check on the image's density allowed for this unsafe operation. 

The impact of this vulnerability was substantial, potentially causing a crash as well as denial of service by exploitative requests. The division by zero could be easily triggered, putting the reliability of the service at risk.

The fix introduces a validation process to ensure the image's density is valid before performing any operations. If the density is unknown or invalid, the function will return immediately, eliminating the possibility of a division by zero error.

Signed-off-by: Jason Summers <jason1@pobox.com>
Resolves: #19, #20"
CVE-2017-16994,"From 373c4557d2aa362702c4c2d41288fb1e54990b7c Mon Sep 17 00:00:00 2001
From: Jann Horn <jannh@google.com>
Date: Tue, 14 Nov 2017 01:03:44 +0100
Subject: [PATCH] mm/pagewalk.c: report holes in hugetlb ranges

This matters at least for the mincore syscall, which will otherwise copy
uninitialized memory from the page allocator to userspace.  It is
probably also a correctness error for /proc/$pid/pagemap, but I haven't
tested that.

Removing the `walk->hugetlb_entry` condition in walk_hugetlb_range() has
no effect because the caller already checks for that.

This only reports holes in hugetlb ranges to callers who have specified
a hugetlb_entry callback.

This issue was found using an AFL-based fuzzer.

v2:
 - don't crash on ->pte_hole==NULL (Andrew Morton)
 - add Cc stable (Andrew Morton)

Fixes: 1e25a271c8ac (""mincore: apply page table walker on do_mincore()"")
Signed-off-by: Jann Horn <jannh@google.com>
Cc: <stable@vger.kernel.org>
Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
---
 mm/pagewalk.c | 6 +++++-
 1 file changed, 5 insertions(+), 1 deletion(-)

diff --git a/mm/pagewalk.c b/mm/pagewalk.c
index 8bd4afa83cb89f..23a3e415ac2ce5 100644
--- a/mm/pagewalk.c
+++ b/mm/pagewalk.c
@@ -188,8 +188,12 @@ static int walk_hugetlb_range(unsigned long addr, unsigned long end,
 	do {
 		next = hugetlb_entry_end(h, addr, end);
 		pte = huge_pte_offset(walk->mm, addr & hmask, sz);
-		if (pte && walk->hugetlb_entry)
+
+		if (pte)
 			err = walk->hugetlb_entry(pte, hmask, addr, next, walk);
+		else if (walk->pte_hole)
+			err = walk->pte_hole(addr, next, walk);
+
 		if (err)
 			break;
 	} while (addr = next, addr != end);","<vuln-fix>: Rectify uninitialized memory exposure through mincore syscall (CVE-2017-16994)

The code contained a flaw where hugetlb ranges with undefined areas could expose random memory pages via the mincore system call. This could potentially allow an attacker access to reserved kernel memory. 

Such a flaw could disrupt the secure operations of the system or enable further exploits by providing an attacker with potentially sensitive kernel information. 

The fix corrects this issue by checking if the page table entry (pte) is valid, and if not, it calls the pte_hole function to handle the exception. 

Introduced in: 1e25a271c8ac  
Signed-off-by: Jann Horn <jannh@google.com>  
Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>"
OSV-2020-76,"From 7b5ba2afc759b7a44f6776a53030b494c6d985c3 Mon Sep 17 00:00:00 2001
From: Dan Bloomberg <dan.bloomberg@gmail.com>
Date: Fri, 29 May 2020 13:22:12 -0700
Subject: [PATCH] Issue 22764 in oss-fuzz: uninit value in
 numaCrossingByThreshold(). * may be related to issue 22760: illegal read in
 pixAverageRasterScans(). * In readbarcode.c, make sure each pix is large
 enough.  Fix bug in   pixAverageRasterScans(), that could lead to illegal
 addressing. * Added numa size checking; if n < 2 there will not be any
 crossings. * In binexpand.c, remove unnecessary warning.

---
 src/binexpand.c   |  2 --
 src/numafunc2.c   |  8 +++++++-
 src/readbarcode.c | 16 +++++++++++++---
 3 files changed, 20 insertions(+), 6 deletions(-)

diff --git a/src/binexpand.c b/src/binexpand.c
index c7a8c1a1e..3c67297a0 100644
--- a/src/binexpand.c
+++ b/src/binexpand.c
@@ -198,8 +198,6 @@ PIX       *pixd;
             lined = datad + 8 * i * wpld;
             for (j = 0; j < sqbits; j++) {
                 sval = GET_DATA_QBIT(lines, j);
-                if (sval > 15)
-                    L_WARNING(""sval = %d; should be < 16\n"", procName, sval);
                 lined[j] = tab8[sval];
             }
             for (k = 1; k < 8; k++)
diff --git a/src/numafunc2.c b/src/numafunc2.c
index 9673d3b0a..f6673035b 100644
--- a/src/numafunc2.c
+++ b/src/numafunc2.c
@@ -2757,7 +2757,7 @@ NUMA      *nat;
  * \param[in]    estthresh    estimated pixel threshold for crossing:
  *                            e.g., for images, white <--> black; typ. ~120
  * \param[out]   pbestthresh  robust estimate of threshold to use
- * \return  0 if OK, 1 on error
+ * \return  0 if OK, 1 on error or warning
  *
  * <pre>
  * Notes:
@@ -2772,6 +2772,7 @@ NUMA      *nat;
  *         in the center of this stable plateau of crossings.
  *         This can then be used with numaCrossingsByThreshold()
  *         to get a good estimate of crossing locations.
+ *     (3) If the count of nay is less than 2, a warning is issued.
  * </pre>
  */
 l_ok
@@ -2792,6 +2793,10 @@ NUMA      *nat, *nac;
     *pbestthresh = 0.0;
     if (!nay)
         return ERROR_INT(""nay not defined"", procName, 1);
+    if (numaGetCount(nay) < 2) {
+        L_WARNING(""nay count < 2; no threshold crossing\n"", procName);
+        return 1;
+    }
 
         /* Compute the number of crossings for different thresholds */
     nat = numaCreate(41);
@@ -2901,6 +2906,7 @@ NUMA      *nad;
         return (NUMA *)ERROR_PTR(""nax and nay sizes differ"", procName, NULL);
 
     nad = numaCreate(0);
+    if (n < 2) return nad;
     numaGetFValue(nay, 0, &yval1);
     numaGetParameters(nay, &startx, &delx);
     if (nax)
diff --git a/src/readbarcode.c b/src/readbarcode.c
index 3b10865bc..cbfe89101 100644
--- a/src/readbarcode.c
+++ b/src/readbarcode.c
@@ -93,6 +93,10 @@ static const l_int32  MAX_SPACE_WIDTH = 19;  /* was 15 */
 static const l_int32  MAX_NOISE_WIDTH = 50;  /* smaller than barcode width */
 static const l_int32  MAX_NOISE_HEIGHT = 30;  /* smaller than barcode height */
 
+    /* Minimum barcode image size */
+static const l_int32  MIN_BC_WIDTH = 100;
+static const l_int32  MIN_BC_HEIGHT = 50;
+
     /* Static functions */
 static PIX *pixGenerateBarcodeMask(PIX *pixs, l_int32 maxspace,
                                    l_int32 nwidth, l_int32 nheight);
@@ -263,7 +267,7 @@ pixReadBarcodes(PIXA     *pixa,
 {
 char      *barstr, *data;
 char       emptystring[] = """";
-l_int32    i, j, n, nbars, ival;
+l_int32    w, h, i, j, n, nbars, ival;
 NUMA      *na;
 PIX       *pixt;
 SARRAY    *saw, *sad;
@@ -284,6 +288,11 @@ SARRAY    *saw, *sad;
     for (i = 0; i < n; i++) {
             /* Extract the widths of the lines in each barcode */
         pixt = pixaGetPix(pixa, i, L_CLONE);
+        pixGetDimensions(pixt, &w, &h, NULL);
+        if (w < MIN_BC_WIDTH || h < MIN_BC_HEIGHT) {
+            L_ERROR(""pix is too small: w = %d, h = %d\n"", procName, w, h);
+            continue;
+        }
         na = pixReadBarcodeWidths(pixt, method, debugflag);
         pixDestroy(&pixt);
         if (!na) {
@@ -729,7 +738,8 @@ NUMA      *nas, *nax, *nay, *nad;
         return (NUMA *)ERROR_PTR(""pixs undefined or not 8 bpp"", procName, NULL);
 
         /* Scan pixels horizontally and average results */
-    nas = pixAverageRasterScans(pixs, 51);
+    if ((nas = pixAverageRasterScans(pixs, 50)) == NULL)
+        return (NUMA *)ERROR_PTR(""nas not made"", procName, NULL);
 
         /* Interpolate to get 4x the number of values */
     w = pixGetWidth(pixs);
@@ -785,7 +795,7 @@ NUMA       *nad;
         return (NUMA *)ERROR_PTR(""pixs undefined or not 8 bpp"", procName, NULL);
 
     pixGetDimensions(pixs, &w, &h, NULL);
-    if (nscans <= h) {
+    if (nscans > h) {
         first = 0;
         last = h - 1;
         nscans = h;","<vuln-fix>: Patch for uninitialized value and illegal read in functions (<OSV-2020-76>)

The commit addresses vulnerabilities related to uninitialized value in numaCrossingByThreshold() and an illegal read in pixAverageRasterScans(). Severe issues are also found with small barcode image sizes in readbarcode.c.

The impact of this vulnerability could potentially lead to erroneous memory addressing or reading leading to unpredictable behaviour, crashes or even exposing sensitive memory information to attackers.

The fix involved adding size checking for 'numa' and images - if numerosity ('n')is less than 2, or if image height or width is too small there's a warning or error. Additionally, a bug in pixAverageRasterScans(), that could allow for illegal addressing, has been fixed. Unnecessary warnings from binexpand.c were also removed.

Signed-off-by: Dan Bloomberg <dan.bloomberg@gmail.com>"
CVE-2012-2101,"From 8c8735a73afb16d5856f0aa6088e9ae406c52beb Mon Sep 17 00:00:00 2001
From: Dan Prince <dprince@redhat.com>
Date: Wed, 11 Apr 2012 16:21:29 -0400
Subject: [PATCH] Implement quotas for security groups.

Fixes LP Bug #969545 for Diablo.

Change-Id: Ibc02256b6debd29c56307320acc48e9cfae85ba9
---
 nova/api/ec2/cloud.py                         | 12 +++++
 nova/api/openstack/contrib/quotas.py          |  5 ++-
 nova/api/openstack/contrib/security_groups.py | 12 +++++
 nova/db/api.py                                | 10 +++++
 nova/db/sqlalchemy/api.py                     | 19 ++++++++
 nova/quota.py                                 | 32 ++++++++++++++
 .../api/openstack/contrib/test_quotas.py      | 16 +++++--
 .../openstack/contrib/test_security_groups.py | 44 +++++++++++++++++++
 nova/tests/test_cloud.py                      | 25 +++++++++++
 nova/tests/test_quota.py                      | 32 +++++++++++++-
 10 files changed, 201 insertions(+), 6 deletions(-)

diff --git a/nova/api/ec2/cloud.py b/nova/api/ec2/cloud.py
index 64fce0c2b54..cd41921ba24 100644
--- a/nova/api/ec2/cloud.py
+++ b/nova/api/ec2/cloud.py
@@ -42,6 +42,7 @@
 from nova import log as logging
 from nova import network
 from nova import rpc
+from nova import quota
 from nova import utils
 from nova import volume
 from nova.api.ec2 import ec2utils
@@ -856,6 +857,13 @@ def authorize_security_group_ingress(self, context, group_name=None,
                     raise exception.ApiError(_(err) % values_for_rule)
                 postvalues.append(values_for_rule)
 
+        allowed = quota.allowed_security_group_rules(context,
+                                                   security_group['id'],
+                                                   1)
+        if allowed < 1:
+            msg = _(""Quota exceeded, too many security group rules."")
+            raise exception.ApiError(msg)
+
         for values_for_rule in postvalues:
             security_group_rule = db.security_group_rule_create(
                     context,
@@ -908,6 +916,10 @@ def create_security_group(self, context, group_name, group_description):
         if db.security_group_exists(context, context.project_id, group_name):
             raise exception.ApiError(_('group %s already exists') % group_name)
 
+        if quota.allowed_security_groups(context, 1) < 1:
+            msg = _(""Quota exceeded, too many security groups."")
+            raise exception.ApiError(msg)
+
         group = {'user_id': context.user_id,
                  'project_id': context.project_id,
                  'name': group_name,
diff --git a/nova/api/openstack/contrib/quotas.py b/nova/api/openstack/contrib/quotas.py
index 459b71dfdb7..16883269325 100644
--- a/nova/api/openstack/contrib/quotas.py
+++ b/nova/api/openstack/contrib/quotas.py
@@ -40,6 +40,8 @@ def _format_quota_set(self, project_id, quota_set):
             'instances': quota_set['instances'],
             'injected_files': quota_set['injected_files'],
             'cores': quota_set['cores'],
+            'security_groups': quota_set['security_groups'],
+            'security_group_rules': quota_set['security_group_rules'],
         }}
 
     def show(self, req, id):
@@ -56,7 +58,8 @@ def update(self, req, id, body):
         project_id = id
         resources = ['metadata_items', 'injected_file_content_bytes',
                 'volumes', 'gigabytes', 'ram', 'floating_ips', 'instances',
-                'injected_files', 'cores']
+                'injected_files', 'cores', 'security_groups',
+                'security_group_rules']
         for key in body['quota_set'].keys():
             if key in resources:
                 value = int(body['quota_set'][key])
diff --git a/nova/api/openstack/contrib/security_groups.py b/nova/api/openstack/contrib/security_groups.py
index e8f1f2ca680..78d4881251f 100644
--- a/nova/api/openstack/contrib/security_groups.py
+++ b/nova/api/openstack/contrib/security_groups.py
@@ -26,6 +26,7 @@
 from nova import log as logging
 from nova import rpc
 from nova import utils
+from nova import quota
 from nova.api.openstack import common
 from nova.api.openstack import extensions
 from nova.api.openstack import wsgi
@@ -136,6 +137,10 @@ def create(self, req, body):
         group_name = group_name.strip()
         group_description = group_description.strip()
 
+        if quota.allowed_security_groups(context, 1) < 1:
+            msg = _(""Quota exceeded, too many security groups."")
+            raise exc.HTTPBadRequest(explanation=msg)
+
         LOG.audit(_(""Create Security Group %s""), group_name, context=context)
         self.compute_api.ensure_default_security_group(context)
         if db.security_group_exists(context, context.project_id, group_name):
@@ -219,6 +224,13 @@ def create(self, req, body):
             msg = _('This rule already exists in group %s') % parent_group_id
             raise exc.HTTPBadRequest(explanation=msg)
 
+        allowed = quota.allowed_security_group_rules(context,
+                                                   parent_group_id,
+                                                   1)
+        if allowed < 1:
+            msg = _(""Quota exceeded, too many security group rules."")
+            raise exc.HTTPBadRequest(explanation=msg)
+
         security_group_rule = db.security_group_rule_create(context, values)
 
         self.compute_api.trigger_security_group_rules_refresh(context,
diff --git a/nova/db/api.py b/nova/db/api.py
index c0e44d2e7b7..7d241e54ce2 100644
--- a/nova/db/api.py
+++ b/nova/db/api.py
@@ -1098,6 +1098,11 @@ def security_group_destroy_all(context):
     return IMPL.security_group_destroy_all(context)
 
 
+def security_group_count_by_project(context, project_id):
+    """"""Count number of security groups in a project.""""""
+    return IMPL.security_group_count_by_project(context, project_id)
+
+
 ####################
 
 
@@ -1129,6 +1134,11 @@ def security_group_rule_get(context, security_group_rule_id):
     return IMPL.security_group_rule_get(context, security_group_rule_id)
 
 
+def security_group_rule_count_by_group(context, security_group_id):
+    """"""Count rules in a given security group.""""""
+    return IMPL.security_group_rule_count_by_group(context, security_group_id)
+
+
 ###################
 
 
diff --git a/nova/db/sqlalchemy/api.py b/nova/db/sqlalchemy/api.py
index 610858513d6..b06092675f3 100644
--- a/nova/db/sqlalchemy/api.py
+++ b/nova/db/sqlalchemy/api.py
@@ -2803,6 +2803,16 @@ def security_group_destroy_all(context, session=None):
                         'updated_at': literal_column('updated_at')})
 
 
+@require_context
+def security_group_count_by_project(context, project_id):
+    authorize_project_context(context, project_id)
+    session = get_session()
+    return session.query(models.SecurityGroup).\
+                         filter_by(deleted=False).\
+                         filter_by(project_id=project_id).\
+                         count()
+
+
 ###################
 
 
@@ -2884,6 +2894,15 @@ def security_group_rule_destroy(context, security_group_rule_id):
         security_group_rule.delete(session=session)
 
 
+@require_context
+def security_group_rule_count_by_group(context, security_group_id):
+    session = get_session()
+    return session.query(models.SecurityGroupIngressRule).\
+                         filter_by(deleted=False).\
+                         filter_by(parent_group_id=security_group_id).\
+                         count()
+
+
 ###################
 
 
diff --git a/nova/quota.py b/nova/quota.py
index 771477747e0..d491f4a42b1 100644
--- a/nova/quota.py
+++ b/nova/quota.py
@@ -44,6 +44,10 @@
                      'number of bytes allowed per injected file')
 flags.DEFINE_integer('quota_max_injected_file_path_bytes', 255,
                      'number of bytes allowed per injected file path')
+flags.DEFINE_integer('quota_security_groups', 10,
+                     'number of security groups per project')
+flags.DEFINE_integer('quota_security_group_rules', 20,
+                     'number of security rules per security group')
 
 
 def _get_default_quotas():
@@ -58,6 +62,8 @@ def _get_default_quotas():
         'injected_files': FLAGS.quota_max_injected_files,
         'injected_file_content_bytes':
             FLAGS.quota_max_injected_file_content_bytes,
+        'security_groups': FLAGS.quota_security_groups,
+        'security_group_rules': FLAGS.quota_security_group_rules,
     }
     # -1 in the quota flags means unlimited
     for key in defaults.keys():
@@ -134,6 +140,32 @@ def allowed_floating_ips(context, requested_floating_ips):
     return min(requested_floating_ips, allowed_floating_ips)
 
 
+def allowed_security_groups(context, requested_security_groups):
+    """"""Check quota and return min(requested, allowed) security groups.""""""
+    project_id = context.project_id
+    context = context.elevated()
+    used_sec_groups = db.security_group_count_by_project(context, project_id)
+    quota = get_project_quotas(context, project_id)
+    allowed_sec_groups = _get_request_allotment(requested_security_groups,
+                                                  used_sec_groups,
+                                                  quota['security_groups'])
+    return min(requested_security_groups, allowed_sec_groups)
+
+
+def allowed_security_group_rules(context, security_group_id,
+        requested_rules):
+    """"""Check quota and return min(requested, allowed) sec group rules.""""""
+    project_id = context.project_id
+    context = context.elevated()
+    used_rules = db.security_group_rule_count_by_group(context,
+                                                            security_group_id)
+    quota = get_project_quotas(context, project_id)
+    allowed_rules = _get_request_allotment(requested_rules,
+                                              used_rules,
+                                              quota['security_group_rules'])
+    return min(requested_rules, allowed_rules)
+
+
 def _calculate_simple_quota(context, resource, requested):
     """"""Check quota for resource; return min(requested, allowed).""""""
     quota = get_project_quotas(context, context.project_id)
diff --git a/nova/tests/api/openstack/contrib/test_quotas.py b/nova/tests/api/openstack/contrib/test_quotas.py
index e391e5fa091..fe0a165eda2 100644
--- a/nova/tests/api/openstack/contrib/test_quotas.py
+++ b/nova/tests/api/openstack/contrib/test_quotas.py
@@ -29,7 +29,8 @@ def quota_set(id):
     return {'quota_set': {'id': id, 'metadata_items': 128, 'volumes': 10,
             'gigabytes': 1000, 'ram': 51200, 'floating_ips': 10,
             'instances': 10, 'injected_files': 5, 'cores': 20,
-            'injected_file_content_bytes': 10240}}
+            'injected_file_content_bytes': 10240,
+            'security_groups': 10, 'security_group_rules': 20}}
 
 
 def quota_set_list():
@@ -60,7 +61,9 @@ def test_format_quota_set(self):
             'metadata_items': 128,
             'gigabytes': 1000,
             'injected_files': 5,
-            'injected_file_content_bytes': 10240}
+            'injected_file_content_bytes': 10240,
+            'security_groups': 10,
+            'security_group_rules': 20}
 
         quota_set = QuotaSetsController()._format_quota_set('1234',
                                                             raw_quota_set)
@@ -95,7 +98,9 @@ def test_quotas_defaults(self):
                     'floating_ips': 10,
                     'metadata_items': 128,
                     'injected_files': 5,
-                    'injected_file_content_bytes': 10240}}
+                    'injected_file_content_bytes': 10240,
+                    'security_groups': 10,
+                    'security_group_rules': 20}}
 
         self.assertEqual(json.loads(res.body), expected)
 
@@ -123,7 +128,10 @@ def test_quotas_update_as_admin(self):
                              'cores': 50, 'ram': 51200, 'volumes': 10,
                              'gigabytes': 1000, 'floating_ips': 10,
                              'metadata_items': 128, 'injected_files': 5,
-                             'injected_file_content_bytes': 10240}}
+                             'injected_file_content_bytes': 10240,
+                             'security_groups': 40,
+                             'security_group_rules': 80
+                             }}
 
         req = webob.Request.blank('/v1.1/fake/os-quota-sets/update_me')
         req.method = 'PUT'
diff --git a/nova/tests/api/openstack/contrib/test_security_groups.py b/nova/tests/api/openstack/contrib/test_security_groups.py
index d0b25e05655..839f44246e7 100644
--- a/nova/tests/api/openstack/contrib/test_security_groups.py
+++ b/nova/tests/api/openstack/contrib/test_security_groups.py
@@ -22,10 +22,13 @@
 from xml.dom import minidom
 
 from nova import exception
+from nova import flags
 from nova import test
 from nova.api.openstack.contrib import security_groups
 from nova.tests.api.openstack import fakes
 
+FLAGS = flags.FLAGS
+
 
 def _get_create_request_json(body_dict):
     req = webob.Request.blank('/v1.1/fake/os-security-groups')
@@ -257,6 +260,19 @@ def test_create_security_group_non_string_description_json(self):
         response = _create_security_group_json(security_group)
         self.assertEquals(response.status_int, 400)
 
+    def test_create_security_group_quota_limit(self):
+        security_group = {}
+        for num in range(1, FLAGS.quota_security_groups):
+            security_group['name'] = ""test%i"" % num
+            security_group['description'] = ""test%i"" % num
+            response = _create_security_group_json(security_group)
+            self.assertEquals(response.status_int, 200)
+
+        security_group['name'] = ""test_to_many""
+        security_group['description'] = ""test_to_many""
+        response = _create_security_group_json(security_group)
+        self.assertEquals(response.status_int, 400)
+
     def test_get_security_group_list(self):
         security_group = {}
         security_group['name'] = ""test""
@@ -918,6 +934,34 @@ def test_create_rule_with_same_group_parent_id_json(self):
         response = self._create_security_group_rule_json(rules)
         self.assertEquals(response.status_int, 400)
 
+    def test_create_rule_quota_limit(self):
+        #NOTE: subtract 1 because we create 1 rule in setup
+        for num in range(100, (100 + FLAGS.quota_security_group_rules) - 1):
+            rule = {
+                      ""security_group_rule"": {
+                            ""ip_protocol"": ""tcp"",
+                            ""from_port"": num,
+                            ""to_port"": num,
+                            ""parent_group_id"": ""%s""
+                                       % self.parent_security_group['id'],
+                         }
+                      }
+            response = self._create_security_group_rule_json(rule)
+            print response.body
+            self.assertEquals(response.status_int, 200)
+
+        rule = {
+                  ""security_group_rule"": {
+                        ""ip_protocol"": ""tcp"",
+                        ""from_port"": ""121"",
+                        ""to_port"": ""121"",
+                        ""parent_group_id"": ""%s""
+                                   % self.parent_security_group['id'],
+                     }
+                  }
+        response = self._create_security_group_rule_json(rule)
+        self.assertEquals(response.status_int, 400)
+
     def test_delete(self):
         response = self._delete_security_group_rule(
                                   self.security_group_rule['id'])
diff --git a/nova/tests/test_cloud.py b/nova/tests/test_cloud.py
index fa4597422fe..c6fe41b84b4 100644
--- a/nova/tests/test_cloud.py
+++ b/nova/tests/test_cloud.py
@@ -256,6 +256,31 @@ def test_delete_security_group_no_params(self):
         delete = self.cloud.delete_security_group
         self.assertRaises(exception.ApiError, delete, self.context)
 
+    def test_security_group_ingress_quota_limit(self):
+        self.flags(quota_security_group_rules=20)
+        kwargs = {'project_id': self.context.project_id, 'name': 'test'}
+        sec_group = db.security_group_create(self.context, kwargs)
+        authz = self.cloud.authorize_security_group_ingress
+        for i in range(100, 120):
+            kwargs = {'to_port': i, 'from_port': i, 'ip_protocol': 'tcp'}
+            authz(self.context, group_id=sec_group['id'], **kwargs)
+
+        kwargs = {'to_port': 121, 'from_port': 121, 'ip_protocol': 'tcp'}
+        self.assertRaises(exception.ApiError, authz, self.context,
+                              group_id=sec_group['id'], **kwargs)
+
+    def test_security_group_quota_limit(self):
+        self.flags(quota_security_groups=10)
+        for i in range(1, 10):
+            name = 'test name %i' % i
+            descript = 'test description %i' % i
+            create = self.cloud.create_security_group
+            result = create(self.context, name, descript)
+
+        # 11'th group should fail
+        self.assertRaises(exception.ApiError,
+                          create, self.context, 'foo', 'bar')
+
     def test_authorize_security_group_ingress(self):
         kwargs = {'project_id': self.context.project_id, 'name': 'test'}
         sec = db.security_group_create(self.context, kwargs)
diff --git a/nova/tests/test_quota.py b/nova/tests/test_quota.py
index f4b481ebe49..9541a13b26c 100644
--- a/nova/tests/test_quota.py
+++ b/nova/tests/test_quota.py
@@ -43,7 +43,9 @@ def setUp(self):
                    quota_cores=4,
                    quota_volumes=2,
                    quota_gigabytes=20,
-                   quota_floating_ips=1)
+                   quota_floating_ips=1,
+                   quota_security_groups=10,
+                   quota_security_group_rules=20)
 
         self.network = self.network = self.start_service('network')
         self.user_id = 'admin'
@@ -185,6 +187,34 @@ def test_unlimited_floating_ips(self):
         floating_ips = quota.allowed_floating_ips(self.context, 101)
         self.assertEqual(floating_ips, 101)
 
+    def test_unlimited_security_groups(self):
+        self.flags(quota_security_groups=10)
+        security_groups = quota.allowed_security_groups(self.context, 100)
+        self.assertEqual(security_groups, 10)
+        db.quota_create(self.context, self.project_id, 'security_groups', None)
+        security_groups = quota.allowed_security_groups(self.context, 100)
+        self.assertEqual(security_groups, 100)
+        security_groups = quota.allowed_security_groups(self.context, 101)
+        self.assertEqual(security_groups, 101)
+
+    def test_unlimited_security_group_rules(self):
+
+        def fake_security_group_rule_count_by_group(context, sec_group_id):
+            return 0
+
+        self.stubs.Set(db, 'security_group_rule_count_by_group',
+                       fake_security_group_rule_count_by_group)
+
+        self.flags(quota_security_group_rules=20)
+        rules = quota.allowed_security_group_rules(self.context, 1234, 100)
+        self.assertEqual(rules, 20)
+        db.quota_create(self.context, self.project_id, 'security_group_rules',
+                        None)
+        rules = quota.allowed_security_group_rules(self.context, 1234, 100)
+        self.assertEqual(rules, 100)
+        rules = quota.allowed_security_group_rules(self.context, 1234, 101)
+        self.assertEqual(rules, 101)
+
     def test_unlimited_metadata_items(self):
         self.flags(quota_metadata_items=10)
         items = quota.allowed_metadata_items(self.context, 100)","<vuln-fix>: Implement quotas for security groups (CVE-2012-2101)

This vulnerability was discovered in the implementation of security groups in the OpenStack Nova project. The issue arose from the unrestricted creation of security groups and their corresponding rules, leading to potential misuse and resource exhaustion. 

The impact of this vulnerability is medium. Without proper quotas in place, unauthorized users could create an unlimited number of security groups or rules, potentially leading to a Denial of Service (DoS) attack. This vulnerability could also be exploited to consume excessive system resources, leading to reduced performance or even system failures.

The fix involved implementing quotas for security groups and their rules. Codes were added to restrict the number of security groups and rules that a single project could create. These added restrictions prevent unauthorized resource usage and protect the system from potential misuse. 

Signed-off-by: Dan Prince <dprince@redhat.com>
"
CVE-2020-19626,"From 76a2168b6a5e30144f5c06da4ff264f4eca577ff Mon Sep 17 00:00:00 2001
From: Brad Bell <brad@pixelandtonic.com>
Date: Thu, 27 Jun 2019 09:41:17 -0700
Subject: [PATCH] Fixed an XSS vulnerability.

---
 CHANGELOG-v3.md                            | 5 +++++
 src/templates/settings/sections/_edit.html | 2 +-
 2 files changed, 6 insertions(+), 1 deletion(-)

diff --git a/CHANGELOG-v3.md b/CHANGELOG-v3.md
index 56d5fb19e7a..78aaaafc862 100644
--- a/CHANGELOG-v3.md
+++ b/CHANGELOG-v3.md
@@ -1,5 +1,10 @@
 # Release Notes for Craft CMS 3.x
 
+## Unreleased
+
+### Security
+- Fixed an XSS vulnerability.
+
 ## 3.1.32.1 - 2019-06-25
 
 ### Fixed
diff --git a/src/templates/settings/sections/_edit.html b/src/templates/settings/sections/_edit.html
index e7f0c12c4e4..e28fdb1ec34 100644
--- a/src/templates/settings/sections/_edit.html
+++ b/src/templates/settings/sections/_edit.html
@@ -68,7 +68,7 @@
         {% endif %}
         {% set siteRows = siteRows|merge({
             (site.handle): {
-                heading: site.name|t('site'),
+                heading: site.name|t('site')|e,
                 enabled: include('_includes/forms/lightswitch', {
                     name: 'sites['~site.handle~'][enabled]',
                     on: brandNewSection or siteSettings,","<vuln-fix>: XSS vulnerability patched in settings section (CVE-2020-19626)

There was a Cross-site scripting (XSS) vulnerability present in the settings sections of the software. The vulnerability lay in user input not being properly escaped. 

The impact of this vulnerability was significant as it could allow a malicious user to execute arbitrary code in the browser of an unsuspecting user. This could lead to data theft or even total system compromise. 

The fix involves correctly encoding user input to ensure that it cannot be interpreted as code by browsers when displayed. This effectively mitigates the risk introduced by the XSS vulnerability, securing the platform against such attacks.

Introduced in: 76a2168b6a5e30144f5c06da4ff264f4eca577ff  
Signed-off-by: Brad Bell (brad@pixelandtonic.com)"
CVE-2018-7260,"From d2886a3e8745e8845633ae8a0054b5ee4d8babd5 Mon Sep 17 00:00:00 2001
From: Madhura Jayaratne <madhura.cj@gmail.com>
Date: Thu, 8 Feb 2018 00:05:57 +1100
Subject: [PATCH] Fix XSS vulnerability in central columns feature

Signed-off-by: Madhura Jayaratne <madhura.cj@gmail.com>
---
 ChangeLog              | 1 +
 db_central_columns.php | 4 +++-
 2 files changed, 4 insertions(+), 1 deletion(-)

diff --git a/ChangeLog b/ChangeLog
index 3463cc969b49..41e359620f54 100644
--- a/ChangeLog
+++ b/ChangeLog
@@ -5,6 +5,7 @@ phpMyAdmin - ChangeLog
 - issue #13914 Fixed resetting default setting values.
 - issue #13758 Fixed fallback value for collation connection.
 - issue #13938 Fixed error handling in PHP 7.2
+- issue        [security] Fix XSS in Central Columns Feature, See PMASA-2018-01
 
 4.7.7 (2017-12-23)
 - issue #13865 Fixed displaying of formatted numeric values for some locales
diff --git a/db_central_columns.php b/db_central_columns.php
index 321b4e9b8261..c3f9e7891b4c 100644
--- a/db_central_columns.php
+++ b/db_central_columns.php
@@ -90,7 +90,9 @@
     parse_str($_POST['col_name'], $col_name);
     $tmp_msg = PMA_deleteColumnsFromList($col_name['selected_fld'], false);
 }
-if (isset($_REQUEST['total_rows']) && $_REQUEST['total_rows']) {
+if (!empty($_REQUEST['total_rows'])
+    && PMA_isValid($_REQUEST['total_rows'], 'integer')
+) {
     $total_rows = $_REQUEST['total_rows'];
 } else {
     $total_rows = PMA_getCentralColumnsCount($db);","XSS vulnerability patch in central columns feature (CVE-2018-7260)

An XSS vulnerability was identified in the central columns feature that allowed malicious scripts to be embedded and executed. The scripts parsed total_rows data without providing adequate sanitization or validation. 

The impact of this issue is critical as it could potentially allow unauthorized access to sensitive user data, thus giving opportunities for hackers to seize control over users' systems. 

This fix involves better validation of $_REQUEST by ensuring 'total_rows' is a valid integer before use, thus eliminating the potential for XSS attack.

Signed-off-by: Madhura Jayaratne <madhura.cj@gmail.com>
Resolves: PMASA-2018-01
"
GHSA-35m5-8cvj-8783,"From e652d56ac60eadfc26489ab83927af13a9b9d8ce Mon Sep 17 00:00:00 2001
From: Morgan-Phoenix <73711602+Morgan-Phoenix@users.noreply.github.com>
Date: Sat, 6 Nov 2021 19:34:45 +0530
Subject: [PATCH] Fixed GHSA-35m5-8cvj-8783

---
 enrocrypt/hashing.py | 5 -----
 1 file changed, 5 deletions(-)

diff --git a/enrocrypt/hashing.py b/enrocrypt/hashing.py
index 275d2e3..14328f0 100644
--- a/enrocrypt/hashing.py
+++ b/enrocrypt/hashing.py
@@ -66,11 +66,6 @@ def SHA244(self,data:str):
         hash = str(sha.digest())
         return self.__Salt(hash,salt=self.salt)
 
-    def MD5(self,data:str):
-        sha = hashlib.md5(bytes(data.encode()))
-        hash = str(sha.digest())
-        return self.__Salt(hash,salt=self.salt)
-
     def SHA384(self,data:str):
         sha = hashlib.sha384(bytes(data.encode()))
         hash = str(sha.digest())","<vuln-fix>: Removal of MD5 Hashing Method (GHSA-35m5-8cvj-8783)

This commit addresses vulnerability GHSA-35m5-8cvj-8783, where the MD5 hashing method was present in the code, making it susceptible to brute force attacks. The utilization of this weak encryption algorithm potentially allowed for unauthorized data access. 

By keeping the MD5 hashing function, sensitive data encoded through this algorithm were at risk. This weakness could have disrupted data integrity and confidentiality, posing a significant threat to the security system overall. 

The fix involves the removal of the MD5 function from the enrocrypt/hashing.py file. This patch thus eliminates the weak encryption method from the code, ensuring safe data encryption practices in the hashing process. 

From: Morgan-Phoenix <73711602+Morgan-Phoenix@users.noreply.github.com>
Bug-tracker: GHSA-35m5-8cvj-8783
Introduced in: e652d56ac60eadfc26489ab83927af13a9b9d8ce"
CVE-2017-18174,"From 8dca4a41f1ad65043a78c2338d9725f859c8d2c3 Mon Sep 17 00:00:00 2001
From: Wei Yongjun <weiyongjun1@huawei.com>
Date: Tue, 17 Jan 2017 15:55:32 +0000
Subject: [PATCH] pinctrl/amd: Drop pinctrl_unregister for devm_ registered
 device

It's not necessary to unregister pin controller device registered
with devm_pinctrl_register() and using pinctrl_unregister() leads
to a double free.

Fixes: 3bfd44306c65 (""pinctrl: amd: Add support for additional GPIO"")
Signed-off-by: Wei Yongjun <weiyongjun1@huawei.com>
Signed-off-by: Linus Walleij <linus.walleij@linaro.org>
---
 drivers/pinctrl/pinctrl-amd.c | 1 -
 1 file changed, 1 deletion(-)

diff --git a/drivers/pinctrl/pinctrl-amd.c b/drivers/pinctrl/pinctrl-amd.c
index 1ee107f147d04f..6b75bd2b71db38 100644
--- a/drivers/pinctrl/pinctrl-amd.c
+++ b/drivers/pinctrl/pinctrl-amd.c
@@ -836,7 +836,6 @@ static int amd_gpio_remove(struct platform_device *pdev)
 	gpio_dev = platform_get_drvdata(pdev);
 
 	gpiochip_remove(&gpio_dev->gc);
-	pinctrl_unregister(gpio_dev->pctrl);
 
 	return 0;
 }","<pinctrl-fix>: Prevent double free in AMD pinctrl driver (CVE-2017-18174)

The vulnerability discovered is a double-free error occurring within the device removal function of the AMD pinctrl driver. This bug was introduced with the addition of support for additional GPIO.

This vulnerability posed a risk to the integrity and stability of the system as the double free could potentially lead to susceptibility to attacks or crashes upon device removal.

The fix involves removing an unnecessary function call, pinctrl_unregister(), in the amd_gpio_remove function. This function was leading to the double free error being triggered as the device had already been unregistered by devm_pinctrl_register().

Introduced in: 3bfd44306c65  
Signed-off-by: Wei Yongjun <weiyongjun1@huawei.com>  
Signed-off-by: Linus Walleij <linus.walleij@linaro.org>"
OSV-2022-439,"From d1a3039c350bf081cae132d6ffc1781d01fd55c6 Mon Sep 17 00:00:00 2001
From: Veronika Hanulikova <vhanulik@redhat.com>
Date: Fri, 1 Jul 2022 11:16:48 +0200
Subject: [PATCH] Fix buffer overflow

Thank oss-fuzz
https://bugs.chromium.org/p/oss-fuzz/issues/detail?id=47676
---
 src/tools/pkcs15-tool.c | 5 ++++-
 1 file changed, 4 insertions(+), 1 deletion(-)

diff --git a/src/tools/pkcs15-tool.c b/src/tools/pkcs15-tool.c
index fff93ab782..20fcba7804 100644
--- a/src/tools/pkcs15-tool.c
+++ b/src/tools/pkcs15-tool.c
@@ -1961,8 +1961,11 @@ static int test_update(sc_card_t *in_card)
 			}
 			/* other tag */
 			i += 2 + rbuf[2+i+1]; /* length of this tag*/
+			if (2+i+1 >= apdu.resplen) {
+				break;
+			}
 		}
-		if (rbuf[2+i+1] < 9 || 2+i+2+9 > apdu.resplen) {
+		if (2+i+1 >= apdu.resplen || rbuf[2+i+1] < 9 || 2+i+2+9 > apdu.resplen) {
 			printf(""select file returned short fci\n"");
 			goto bad_fci;
 		}","<vuln-fix>: Buffer overflow fix in pkcs15-tool.c (OSV-2022-439)

This patch addresses a buffer overflow vulnerability in the pkcs15-tool.c file. The issue manifested when handling certain file operations.

The vulnerability had the potential of causing a service crash or even arbitrary code execution, leading to a serious compromise of the system security.

The fix ensures that the buffer boundaries are respected by performing a check before accessing array elements, thereby preventing an out-of-bound access error and a possible overflow.

Weakness: Buffer Overflow (CWE-120)  
Report: https://bugs.chromium.org/p/oss-fuzz/issues/detail?id=47676  
Introduced in: d1a3039c350bf081cae132d6ffc1781d01fd55c6   
Reported-by: oss-fuzz  
Signed-off-by: Veronika Hanulikova <vhanulik@redhat.com>"
