{
    "schema":{
        "fields":[
            {
                "name":"index",
                "type":"integer"
            },
            {
                "name":"vuln_id",
                "type":"string"
            },
            {
                "name":"cwe_id",
                "type":"string"
            },
            {
                "name":"score",
                "type":"number"
            },
            {
                "name":"chain",
                "type":"string"
            },
            {
                "name":"dataset",
                "type":"string"
            },
            {
                "name":"summary",
                "type":"string"
            },
            {
                "name":"published_date",
                "type":"string"
            },
            {
                "name":"chain_len",
                "type":"integer"
            },
            {
                "name":"project",
                "type":"string"
            },
            {
                "name":"commit_href",
                "type":"string"
            },
            {
                "name":"commit_sha",
                "type":"string"
            },
            {
                "name":"patch",
                "type":"string"
            },
            {
                "name":"chain_ord",
                "type":"string"
            },
            {
                "name":"before_first_fix_commit",
                "type":"string"
            },
            {
                "name":"last_fix_commit",
                "type":"string"
            },
            {
                "name":"chain_ord_pos",
                "type":"number"
            },
            {
                "name":"commit_datetime",
                "type":"string"
            },
            {
                "name":"message",
                "type":"string"
            },
            {
                "name":"author",
                "type":"string"
            },
            {
                "name":"comments",
                "type":"string"
            },
            {
                "name":"stats",
                "type":"string"
            },
            {
                "name":"files",
                "type":"string"
            },
            {
                "name":"message_norm",
                "type":"string"
            },
            {
                "name":"language",
                "type":"string"
            },
            {
                "name":"entities",
                "type":"string"
            },
            {
                "name":"classification_level_1",
                "type":"string"
            },
            {
                "name":"classification_level_2",
                "type":"string"
            },
            {
                "name":"list_files",
                "type":"string"
            },
            {
                "name":"num_files",
                "type":"number"
            },
            {
                "name":"patch_content",
                "type":"string"
            }
        ],
        "primaryKey":[
            "index"
        ],
        "pandas_version":"1.4.0"
    },
    "data":[
        {
            "index":806,
            "vuln_id":"GHSA-h2fw-93qx-vrcq",
            "cwe_id":"{'CWE-89'}",
            "score":8.8,
            "chain":"{'https:\/\/github.com\/moodle\/moodle\/commit\/c2794752ea3cdda2d64a0651da08b2cdf730d9f1'}",
            "dataset":"osv",
            "summary":"SQL Injection in Moodle An SQL injection risk was identified in Badges code relating to configuring criteria. Access to the relevant capability was limited to teachers and managers by default.",
            "published_date":"2022-03-26",
            "chain_len":1,
            "project":"https:\/\/github.com\/moodle\/moodle",
            "commit_href":"https:\/\/github.com\/moodle\/moodle\/commit\/c2794752ea3cdda2d64a0651da08b2cdf730d9f1",
            "commit_sha":"c2794752ea3cdda2d64a0651da08b2cdf730d9f1",
            "patch":"SINGLE",
            "chain_ord":"['c2794752ea3cdda2d64a0651da08b2cdf730d9f1']",
            "before_first_fix_commit":"{'addd4f894d8173ec8ff0ae2212d51a1977e7bcad'}",
            "last_fix_commit":"c2794752ea3cdda2d64a0651da08b2cdf730d9f1",
            "chain_ord_pos":1.0,
            "commit_datetime":"03\/03\/2022, 18:02:15",
            "message":"MDL-74074 badges: Ensure profile criteria exists before completion check",
            "author":"Michael Hawkins",
            "comments":null,
            "stats":"{'additions': 23, 'deletions': 6, 'total': 29}",
            "files":"{'badges\/criteria\/award_criteria_profile.php': {'additions': 23, 'deletions': 6, 'changes': 29, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/moodle\/moodle\/raw\/c2794752ea3cdda2d64a0651da08b2cdf730d9f1\/badges%2Fcriteria%2Faward_criteria_profile.php', 'patch': '@@ -39,6 +39,26 @@ class award_criteria_profile extends award_criteria {\\n     public $required_param = \\'field\\';\\n     public $optional_params = array();\\n \\n+    \/* @var array The default profile fields allowed to be used as award criteria.\\n+     *\\n+     * Note: This is used instead of user_get_default_fields(), because it is not possible to\\n+     * determine which fields the user can modify.\\n+     *\/\\n+    protected $allowed_default_fields = [\\n+        \\'firstname\\',\\n+        \\'lastname\\',\\n+        \\'email\\',\\n+        \\'address\\',\\n+        \\'phone1\\',\\n+        \\'phone2\\',\\n+        \\'department\\',\\n+        \\'institution\\',\\n+        \\'description\\',\\n+        \\'picture\\',\\n+        \\'city\\',\\n+        \\'country\\',\\n+    ];\\n+\\n     \/**\\n      * Add appropriate new criteria options to the form\\n      *\\n@@ -50,10 +70,7 @@ public function get_options(&$mform) {\\n         $none = true;\\n         $existing = array();\\n         $missing = array();\\n-\\n-        \/\/ Note: cannot use user_get_default_fields() here because it is not possible to decide which fields user can modify.\\n-        $dfields = array(\\'firstname\\', \\'lastname\\', \\'email\\', \\'address\\', \\'phone1\\', \\'phone2\\',\\n-                         \\'department\\', \\'institution\\', \\'description\\', \\'picture\\', \\'city\\', \\'country\\');\\n+        $dfields = $this->allowed_default_fields;\\n \\n         \/\/ Get custom fields.\\n         $cfields = array_filter(profile_get_custom_fields(), function($field) {\\n@@ -230,8 +247,8 @@ public function get_completed_criteria_sql() {\\n                 $join .= \" LEFT JOIN {user_info_data} uid{$idx} ON uid{$idx}.userid = u.id AND uid{$idx}.fieldid = :fieldid{$idx} \";\\n                 $params[\"fieldid{$idx}\"] = $param[\\'field\\'];\\n                 $whereparts[] = \"uid{$idx}.id IS NOT NULL\";\\n-            } else {\\n-                \/\/ This is a field from {user} table.\\n+            } else if (in_array($param[\\'field\\'], $this->allowed_default_fields)) {\\n+                \/\/ This is a valid field from {user} table.\\n                 if ($param[\\'field\\'] == \\'picture\\') {\\n                     \/\/ The picture field is numeric and requires special handling.\\n                     $whereparts[] = \"u.{$param[\\'field\\']} != 0\";'}}",
            "message_norm":"mdl-74074 badges: ensure profile criteria exists before completion check",
            "language":"en",
            "entities":"[('ensure', 'ACTION', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['badges\/criteria\/award_criteria_profile.php'])",
            "num_files":1.0,
            "patch_content":"From c2794752ea3cdda2d64a0651da08b2cdf730d9f1 Mon Sep 17 00:00:00 2001\nFrom: Michael Hawkins <michaelh@moodle.com>\nDate: Fri, 4 Mar 2022 02:02:15 +0800\nSubject: [PATCH] MDL-74074 badges: Ensure profile criteria exists before\n completion check\n\n---\n badges\/criteria\/award_criteria_profile.php | 29 +++++++++++++++++-----\n 1 file changed, 23 insertions(+), 6 deletions(-)\n\ndiff --git a\/badges\/criteria\/award_criteria_profile.php b\/badges\/criteria\/award_criteria_profile.php\nindex b014cf5a92b00..d3ac2f25556f8 100644\n--- a\/badges\/criteria\/award_criteria_profile.php\n+++ b\/badges\/criteria\/award_criteria_profile.php\n@@ -39,6 +39,26 @@ class award_criteria_profile extends award_criteria {\n     public $required_param = 'field';\n     public $optional_params = array();\n \n+    \/* @var array The default profile fields allowed to be used as award criteria.\n+     *\n+     * Note: This is used instead of user_get_default_fields(), because it is not possible to\n+     * determine which fields the user can modify.\n+     *\/\n+    protected $allowed_default_fields = [\n+        'firstname',\n+        'lastname',\n+        'email',\n+        'address',\n+        'phone1',\n+        'phone2',\n+        'department',\n+        'institution',\n+        'description',\n+        'picture',\n+        'city',\n+        'country',\n+    ];\n+\n     \/**\n      * Add appropriate new criteria options to the form\n      *\n@@ -50,10 +70,7 @@ public function get_options(&$mform) {\n         $none = true;\n         $existing = array();\n         $missing = array();\n-\n-        \/\/ Note: cannot use user_get_default_fields() here because it is not possible to decide which fields user can modify.\n-        $dfields = array('firstname', 'lastname', 'email', 'address', 'phone1', 'phone2',\n-                         'department', 'institution', 'description', 'picture', 'city', 'country');\n+        $dfields = $this->allowed_default_fields;\n \n         \/\/ Get custom fields.\n         $cfields = array_filter(profile_get_custom_fields(), function($field) {\n@@ -230,8 +247,8 @@ public function get_completed_criteria_sql() {\n                 $join .= \" LEFT JOIN {user_info_data} uid{$idx} ON uid{$idx}.userid = u.id AND uid{$idx}.fieldid = :fieldid{$idx} \";\n                 $params[\"fieldid{$idx}\"] = $param['field'];\n                 $whereparts[] = \"uid{$idx}.id IS NOT NULL\";\n-            } else {\n-                \/\/ This is a field from {user} table.\n+            } else if (in_array($param['field'], $this->allowed_default_fields)) {\n+                \/\/ This is a valid field from {user} table.\n                 if ($param['field'] == 'picture') {\n                     \/\/ The picture field is numeric and requires special handling.\n                     $whereparts[] = \"u.{$param['field']} != 0\";"
        },
        {
            "index":820,
            "vuln_id":"GHSA-wcm2-9c89-wmfm",
            "cwe_id":"{'CWE-79'}",
            "score":0.0,
            "chain":"{'https:\/\/github.com\/jquery\/jquery-ui\/commit\/7e9060c109b928769a664dbcc2c17bd21231b6f3'}",
            "dataset":"osv",
            "summary":"Cross-site Scripting in jquery-ui Cross-site scripting (XSS) vulnerability in jquery.ui.dialog.js in the Dialog widget in jQuery UI before 1.10.0 allows remote attackers to inject arbitrary web script or HTML via the title option.",
            "published_date":"2017-10-24",
            "chain_len":1,
            "project":"https:\/\/github.com\/jquery\/jquery-ui",
            "commit_href":"https:\/\/github.com\/jquery\/jquery-ui\/commit\/7e9060c109b928769a664dbcc2c17bd21231b6f3",
            "commit_sha":"7e9060c109b928769a664dbcc2c17bd21231b6f3",
            "patch":"SINGLE",
            "chain_ord":"['7e9060c109b928769a664dbcc2c17bd21231b6f3']",
            "before_first_fix_commit":"{'60486ac632a0a1bbbb0c7449fe17bccfae11af80'}",
            "last_fix_commit":"7e9060c109b928769a664dbcc2c17bd21231b6f3",
            "chain_ord_pos":1.0,
            "commit_datetime":"11\/26\/2012, 09:14:36",
            "message":"Dialog: Extract setting the title into a _title method, use .text() to prevent XSS. Fixes #6016 - Dialog: Title XSS Vulnerability.",
            "author":"J\u00f6rn Zaefferer",
            "comments":null,
            "stats":"{'additions': 9, 'deletions': 4, 'total': 13}",
            "files":"{'ui\/jquery.ui.dialog.js': {'additions': 9, 'deletions': 4, 'changes': 13, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/jquery\/jquery-ui\/raw\/7e9060c109b928769a664dbcc2c17bd21231b6f3\/ui%2Fjquery.ui.dialog.js', 'patch': '@@ -352,14 +352,21 @@ $.widget(\"ui.dialog\", {\\n \\t\\tuiDialogTitle = $( \"<span>\" )\\n \\t\\t\\t.uniqueId()\\n \\t\\t\\t.addClass( \"ui-dialog-title\" )\\n-\\t\\t\\t.html( this.options.title || \"&#160;\" )\\n \\t\\t\\t.prependTo( this.uiDialogTitlebar );\\n+\\t\\tthis._title( uiDialogTitle );\\n \\n \\t\\tthis.uiDialog.attr({\\n \\t\\t\\t\"aria-labelledby\": uiDialogTitle.attr( \"id\" )\\n \\t\\t});\\n \\t},\\n \\n+\\t_title: function( title ) {\\n+\\t\\tif ( !this.options.title ) {\\n+\\t\\t\\ttitle.html( \"&#160;\" );\\n+\\t\\t}\\n+\\t\\ttitle.text( this.options.title );\\n+\\t},\\n+\\n \\t_createButtonPane: function() {\\n \\t\\tvar uiDialogButtonPane = ( this.uiDialogButtonPane = $( \"<div>\" ) )\\n \\t\\t\\t.addClass( \"ui-dialog-buttonpane ui-widget-content ui-helper-clearfix\" );\\n@@ -600,9 +607,7 @@ $.widget(\"ui.dialog\", {\\n \\t\\t}\\n \\n \\t\\tif ( key === \"title\" ) {\\n-\\t\\t\\t\/\/ convert whatever was passed in to a string, for html() to not throw up\\n-\\t\\t\\t$( \".ui-dialog-title\", this.uiDialogTitlebar )\\n-\\t\\t\\t\\t.html( \"\" + ( value || \"&#160;\" ) );\\n+\\t\\t\\tthis._title( this.uiDialogTitlebar.find( \".ui-dialog-title\" ) );\\n \\t\\t}\\n \\t},'}}",
            "message_norm":"dialog: extract setting the title into a _title method, use .text() to prevent xss. fixes #6016 - dialog: title xss vulnerability.",
            "language":"en",
            "entities":"[('prevent', 'ACTION', ''), ('xss', 'SECWORD', ''), ('fixes', 'ACTION', ''), ('#6016', 'ISSUE', ''), ('xss', 'SECWORD', ''), ('vulnerability', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['ui\/jquery.ui.dialog.js'])",
            "num_files":1.0,
            "patch_content":"From 7e9060c109b928769a664dbcc2c17bd21231b6f3 Mon Sep 17 00:00:00 2001\nFrom: =?UTF-8?q?J=C3=B6rn=20Zaefferer?= <joern.zaefferer@gmail.com>\nDate: Mon, 26 Nov 2012 10:14:36 +0100\nSubject: [PATCH] Dialog: Extract setting the title into a _title method, use\n .text() to prevent XSS. Fixes #6016 - Dialog: Title XSS Vulnerability.\n\n---\n ui\/jquery.ui.dialog.js | 13 +++++++++----\n 1 file changed, 9 insertions(+), 4 deletions(-)\n\ndiff --git a\/ui\/jquery.ui.dialog.js b\/ui\/jquery.ui.dialog.js\nindex 5eb3e2aca82..808d31d5b2b 100644\n--- a\/ui\/jquery.ui.dialog.js\n+++ b\/ui\/jquery.ui.dialog.js\n@@ -352,14 +352,21 @@ $.widget(\"ui.dialog\", {\n \t\tuiDialogTitle = $( \"<span>\" )\n \t\t\t.uniqueId()\n \t\t\t.addClass( \"ui-dialog-title\" )\n-\t\t\t.html( this.options.title || \"&#160;\" )\n \t\t\t.prependTo( this.uiDialogTitlebar );\n+\t\tthis._title( uiDialogTitle );\n \n \t\tthis.uiDialog.attr({\n \t\t\t\"aria-labelledby\": uiDialogTitle.attr( \"id\" )\n \t\t});\n \t},\n \n+\t_title: function( title ) {\n+\t\tif ( !this.options.title ) {\n+\t\t\ttitle.html( \"&#160;\" );\n+\t\t}\n+\t\ttitle.text( this.options.title );\n+\t},\n+\n \t_createButtonPane: function() {\n \t\tvar uiDialogButtonPane = ( this.uiDialogButtonPane = $( \"<div>\" ) )\n \t\t\t.addClass( \"ui-dialog-buttonpane ui-widget-content ui-helper-clearfix\" );\n@@ -600,9 +607,7 @@ $.widget(\"ui.dialog\", {\n \t\t}\n \n \t\tif ( key === \"title\" ) {\n-\t\t\t\/\/ convert whatever was passed in to a string, for html() to not throw up\n-\t\t\t$( \".ui-dialog-title\", this.uiDialogTitlebar )\n-\t\t\t\t.html( \"\" + ( value || \"&#160;\" ) );\n+\t\t\tthis._title( this.uiDialogTitlebar.find( \".ui-dialog-title\" ) );\n \t\t}\n \t},"
        },
        {
            "index":561,
            "vuln_id":"GHSA-c94w-c95p-phf8",
            "cwe_id":"{'CWE-190'}",
            "score":6.5,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/fcd18ce3101f245b083b30655c27b239dc72221e'}",
            "dataset":"osv",
            "summary":"Integer overflow in Tensorflow ### Impact\nThe [implementation of `OpLevelCostEstimator::CalculateTensorSize`](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/a1320ec1eac186da1d03f033109191f715b2b130\/tensorflow\/core\/grappler\/costs\/op_level_cost_estimator.cc#L1552-L1558) is vulnerable to an integer overflow if an attacker can create an operation which would involve a tensor with large enough number of elements:\n```cc\nint64_t OpLevelCostEstimator::CalculateTensorSize(\n    const OpInfo::TensorProperties& tensor, bool* found_unknown_shapes) {\n  int64_t count = CalculateTensorElementCount(tensor, found_unknown_shapes);\n  int size = DataTypeSize(BaseType(tensor.dtype()));\n  VLOG(2) << \"Count: \" << count << \" DataTypeSize: \" << size;\n  return count * size;\n}\n```\nHere, `count` and `size` can be large enough to cause `count * size` to overflow.\n\n### Patches\nWe have patched the issue in GitHub commit [fcd18ce3101f245b083b30655c27b239dc72221e](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/fcd18ce3101f245b083b30655c27b239dc72221e).\n\nThe fix will be included in TensorFlow 2.8.0. We will also cherrypick this commit on TensorFlow 2.7.1, TensorFlow 2.6.3, and TensorFlow 2.5.3, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.",
            "published_date":"2022-02-10",
            "chain_len":1,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/fcd18ce3101f245b083b30655c27b239dc72221e",
            "commit_sha":"fcd18ce3101f245b083b30655c27b239dc72221e",
            "patch":"SINGLE",
            "chain_ord":"['fcd18ce3101f245b083b30655c27b239dc72221e']",
            "before_first_fix_commit":"{'29e899868d77d8f575907515acefa012c5574246'}",
            "last_fix_commit":"fcd18ce3101f245b083b30655c27b239dc72221e",
            "chain_ord_pos":1.0,
            "commit_datetime":"11\/09\/2021, 22:54:52",
            "message":"Prevent integer overflow in `OpLevelCostEstimator::CalculateTensorSize`.\n\nIn order to not change the API, we return a negative value in case of overflow. A better fix is to change the API to return a status instead.\n\nPiperOrigin-RevId: 408713061\nChange-Id: I3771475b0c72a2844a3854086966562fd33f2da5",
            "author":"Mihai Maruseac",
            "comments":null,
            "stats":"{'additions': 7, 'deletions': 1, 'total': 8}",
            "files":"{'tensorflow\/core\/grappler\/costs\/op_level_cost_estimator.cc': {'additions': 7, 'deletions': 1, 'changes': 8, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/fcd18ce3101f245b083b30655c27b239dc72221e\/tensorflow%2Fcore%2Fgrappler%2Fcosts%2Fop_level_cost_estimator.cc', 'patch': '@@ -1555,7 +1555,13 @@ int64_t OpLevelCostEstimator::CalculateTensorSize(\\n   int64_t count = CalculateTensorElementCount(tensor, found_unknown_shapes);\\n   int size = DataTypeSize(BaseType(tensor.dtype()));\\n   VLOG(2) << \"Count: \" << count << \" DataTypeSize: \" << size;\\n-  return count * size;\\n+  int64_t tensor_size = MultiplyWithoutOverflow(count, size);\\n+  if (tensor_size < 0) {\\n+    VLOG(1) << \"Overflow encountered when computing tensor size, multiplying \"\\n+            << count << \" with \" << size;\\n+    return -1;\\n+  }\\n+  return tensor_size;\\n }\\n \\n int64_t OpLevelCostEstimator::CalculateInputSize(const OpInfo& op_info,'}}",
            "message_norm":"prevent integer overflow in `oplevelcostestimator::calculatetensorsize`.\n\nin order to not change the api, we return a negative value in case of overflow. a better fix is to change the api to return a status instead.\n\npiperorigin-revid: 408713061\nchange-id: i3771475b0c72a2844a3854086966562fd33f2da5",
            "language":"en",
            "entities":"[('prevent', 'ACTION', ''), ('integer overflow', 'SECWORD', ''), ('change', 'ACTION', ''), ('overflow', 'SECWORD', ''), ('change', 'ACTION', ''), ('408713061', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/core\/grappler\/costs\/op_level_cost_estimator.cc'])",
            "num_files":1.0,
            "patch_content":"From fcd18ce3101f245b083b30655c27b239dc72221e Mon Sep 17 00:00:00 2001\nFrom: Mihai Maruseac <mihaimaruseac@google.com>\nDate: Tue, 9 Nov 2021 14:54:52 -0800\nSubject: [PATCH] Prevent integer overflow in\n `OpLevelCostEstimator::CalculateTensorSize`.\n\nIn order to not change the API, we return a negative value in case of overflow. A better fix is to change the API to return a status instead.\n\nPiperOrigin-RevId: 408713061\nChange-Id: I3771475b0c72a2844a3854086966562fd33f2da5\n---\n tensorflow\/core\/grappler\/costs\/op_level_cost_estimator.cc | 8 +++++++-\n 1 file changed, 7 insertions(+), 1 deletion(-)\n\ndiff --git a\/tensorflow\/core\/grappler\/costs\/op_level_cost_estimator.cc b\/tensorflow\/core\/grappler\/costs\/op_level_cost_estimator.cc\nindex 5e328542ab1452..2bb62edf64532c 100644\n--- a\/tensorflow\/core\/grappler\/costs\/op_level_cost_estimator.cc\n+++ b\/tensorflow\/core\/grappler\/costs\/op_level_cost_estimator.cc\n@@ -1555,7 +1555,13 @@ int64_t OpLevelCostEstimator::CalculateTensorSize(\n   int64_t count = CalculateTensorElementCount(tensor, found_unknown_shapes);\n   int size = DataTypeSize(BaseType(tensor.dtype()));\n   VLOG(2) << \"Count: \" << count << \" DataTypeSize: \" << size;\n-  return count * size;\n+  int64_t tensor_size = MultiplyWithoutOverflow(count, size);\n+  if (tensor_size < 0) {\n+    VLOG(1) << \"Overflow encountered when computing tensor size, multiplying \"\n+            << count << \" with \" << size;\n+    return -1;\n+  }\n+  return tensor_size;\n }\n \n int64_t OpLevelCostEstimator::CalculateInputSize(const OpInfo& op_info,"
        },
        {
            "index":227,
            "vuln_id":"GHSA-vvg4-vgrv-xfr7",
            "cwe_id":"{'CWE-665'}",
            "score":6.3,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/4504a081af71514bb1828048363e6540f797005b', 'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/14607c0707040d775e06b6817325640cb4b5864c'}",
            "dataset":"osv",
            "summary":"Incomplete validation in `tf.raw_ops.CTCLoss` ### Impact \nIncomplete validation in `tf.raw_ops.CTCLoss` allows an attacker to trigger an OOB read from heap:\n\n```python\nimport tensorflow as tf\n\ninputs = tf.constant([], shape=[10, 16, 0], dtype=tf.float32)\nlabels_indices = tf.constant([], shape=[8, 0], dtype=tf.int64)\nlabels_values = tf.constant([-100] * 8, shape=[8], dtype=tf.int32)\nsequence_length = tf.constant([-100] * 16, shape=[16], dtype=tf.int32)\n  \ntf.raw_ops.CTCLoss(inputs=inputs, labels_indices=labels_indices,\n                   labels_values=labels_values, sequence_length=sequence_length,\n                   preprocess_collapse_repeated=True, ctc_merge_repeated=False,\n                   ignore_longer_outputs_than_inputs=True)\n```   \n      \nAn attacker can also trigger a heap buffer overflow:\n\n```python\nimport tensorflow as tf\n\ninputs = tf.constant([], shape=[7, 2, 0], dtype=tf.float32)\nlabels_indices = tf.constant([-100, -100], shape=[2, 1], dtype=tf.int64)\nlabels_values = tf.constant([-100, -100], shape=[2], dtype=tf.int32)\nsequence_length = tf.constant([-100, -100], shape=[2], dtype=tf.int32)\n\ntf.raw_ops.CTCLoss(inputs=inputs, labels_indices=labels_indices,\n                   labels_values=labels_values, sequence_length=sequence_length,\n                   preprocess_collapse_repeated=False, ctc_merge_repeated=False,\n                   ignore_longer_outputs_than_inputs=False)\n```\n\nFinally, an attacker can trigger a null pointer dereference:\n\n```python \nimport tensorflow as tf\n\ninputs = tf.constant([], shape=[0, 2, 11], dtype=tf.float32)\nlabels_indices = tf.constant([], shape=[0, 2], dtype=tf.int64)\nlabels_values = tf.constant([], shape=[0], dtype=tf.int32)\nsequence_length = tf.constant([-100, -100], shape=[2], dtype=tf.int32)\n\ntf.raw_ops.CTCLoss(inputs=inputs, labels_indices=labels_indices,\n                   labels_values=labels_values, sequence_length=sequence_length,\n                   preprocess_collapse_repeated=False, ctc_merge_repeated=False,\n                   ignore_longer_outputs_than_inputs=False)\n```\n\n### Patches\nWe have patched the issue in GitHub commit[14607c0707040d775e06b6817325640cb4b5864c](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/14607c0707040d775e06b6817325640cb4b5864c) followed by GitHub commit [4504a081af71514bb1828048363e6540f797005b](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/4504a081af71514bb1828048363e6540f797005b).\n\nThe fix will be included in TensorFlow 2.5.0. We will also cherrypick these commits on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by Yakun Zhang and Ying Wang of Baidu X-Team.",
            "published_date":"2021-05-21",
            "chain_len":2,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/4504a081af71514bb1828048363e6540f797005b",
            "commit_sha":"4504a081af71514bb1828048363e6540f797005b",
            "patch":"MULTI",
            "chain_ord":"['4504a081af71514bb1828048363e6540f797005b', '14607c0707040d775e06b6817325640cb4b5864c']",
            "before_first_fix_commit":"{'8410ce671b48e96965a1e4a97017f8a5bbd03d3a'}",
            "last_fix_commit":"14607c0707040d775e06b6817325640cb4b5864c",
            "chain_ord_pos":1.0,
            "commit_datetime":"05\/06\/2021, 00:33:47",
            "message":"Fix OOB read issue with `tf.raw_ops.CTCLoss`.\n\nPiperOrigin-RevId: 372242187\nChange-Id: I347228ed8c04e1d2eb9d2479ae52f51d1b512c6e",
            "author":"Amit Patankar",
            "comments":null,
            "stats":"{'additions': 4, 'deletions': 0, 'total': 4}",
            "files":"{'tensorflow\/core\/kernels\/ctc_loss_op.cc': {'additions': 4, 'deletions': 0, 'changes': 4, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/4504a081af71514bb1828048363e6540f797005b\/tensorflow%2Fcore%2Fkernels%2Fctc_loss_op.cc', 'patch': '@@ -100,6 +100,10 @@ class CTCLossOp : public OpKernel {\\n                 errors::InvalidArgument(\"sequence_length is not a vector\"));\\n     OP_REQUIRES(ctx, TensorShapeUtils::IsMatrix(labels_indices->shape()),\\n                 errors::InvalidArgument(\"labels_indices is not a matrix\"));\\n+    OP_REQUIRES(ctx, labels_indices->dim_size(1) > 1,\\n+                errors::InvalidArgument(\\n+                    \"labels_indices second dimension must be >= 1. Received \",\\n+                    labels_indices->dim_size(1)));\\n     OP_REQUIRES(ctx, TensorShapeUtils::IsVector(labels_values->shape()),\\n                 errors::InvalidArgument(\"labels_values is not a vector\"));'}}",
            "message_norm":"fix oob read issue with `tf.raw_ops.ctcloss`.\n\npiperorigin-revid: 372242187\nchange-id: i347228ed8c04e1d2eb9d2479ae52f51d1b512c6e",
            "language":"en",
            "entities":"[('fix', 'ACTION', ''), ('oob', 'SECWORD', ''), ('issue', 'FLAW', ''), ('372242187', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/core\/kernels\/ctc_loss_op.cc'])",
            "num_files":1.0,
            "patch_content":"From 4504a081af71514bb1828048363e6540f797005b Mon Sep 17 00:00:00 2001\nFrom: Amit Patankar <amitpatankar@google.com>\nDate: Wed, 5 May 2021 17:33:47 -0700\nSubject: [PATCH] Fix OOB read issue with `tf.raw_ops.CTCLoss`.\n\nPiperOrigin-RevId: 372242187\nChange-Id: I347228ed8c04e1d2eb9d2479ae52f51d1b512c6e\n---\n tensorflow\/core\/kernels\/ctc_loss_op.cc | 4 ++++\n 1 file changed, 4 insertions(+)\n\ndiff --git a\/tensorflow\/core\/kernels\/ctc_loss_op.cc b\/tensorflow\/core\/kernels\/ctc_loss_op.cc\nindex 6358e82fdda853..b0e298a0f329f0 100644\n--- a\/tensorflow\/core\/kernels\/ctc_loss_op.cc\n+++ b\/tensorflow\/core\/kernels\/ctc_loss_op.cc\n@@ -100,6 +100,10 @@ class CTCLossOp : public OpKernel {\n                 errors::InvalidArgument(\"sequence_length is not a vector\"));\n     OP_REQUIRES(ctx, TensorShapeUtils::IsMatrix(labels_indices->shape()),\n                 errors::InvalidArgument(\"labels_indices is not a matrix\"));\n+    OP_REQUIRES(ctx, labels_indices->dim_size(1) > 1,\n+                errors::InvalidArgument(\n+                    \"labels_indices second dimension must be >= 1. Received \",\n+                    labels_indices->dim_size(1)));\n     OP_REQUIRES(ctx, TensorShapeUtils::IsVector(labels_values->shape()),\n                 errors::InvalidArgument(\"labels_values is not a vector\"));"
        },
        {
            "index":186,
            "vuln_id":"GHSA-9rpc-5v9q-5r7f",
            "cwe_id":"{'CWE-665', 'CWE-20'}",
            "score":3.6,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/1d04d7d93f4ed3854abf75d6b712d72c3f70d6b6'}",
            "dataset":"osv",
            "summary":"Incomplete validation in `SparseReshape` ### Impact\nIncomplete validation in `SparseReshape` results in a denial of service based on a `CHECK`-failure.\n\n```python\nimport tensorflow as tf\n\ninput_indices = tf.constant(41, shape=[1, 1], dtype=tf.int64)\ninput_shape = tf.zeros([11], dtype=tf.int64)\nnew_shape = tf.zeros([1], dtype=tf.int64)\n\ntf.raw_ops.SparseReshape(input_indices=input_indices,\n    input_shape=input_shape,\n    new_shape=new_shape)\n``` \n    \nThe [implementation](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/e87b51ce05c3eb172065a6ea5f48415854223285\/tensorflow\/core\/kernels\/sparse_reshape_op.cc#L40) has no validation that the input arguments specify a valid sparse tensor.\n\n### Patches \nWe have patched the issue in GitHub commit [1d04d7d93f4ed3854abf75d6b712d72c3f70d6b6](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/1d04d7d93f4ed3854abf75d6b712d72c3f70d6b6).\n\nThe fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2 and TensorFlow 2.3.3, as these are the only affected versions.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution \nThis vulnerability has been reported by Ying Wang and Yakun Zhang of Baidu X-Team.",
            "published_date":"2021-05-21",
            "chain_len":1,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/1d04d7d93f4ed3854abf75d6b712d72c3f70d6b6",
            "commit_sha":"1d04d7d93f4ed3854abf75d6b712d72c3f70d6b6",
            "patch":"SINGLE",
            "chain_ord":"['1d04d7d93f4ed3854abf75d6b712d72c3f70d6b6']",
            "before_first_fix_commit":"{'8d78df9997a8537a2f389adc2cfdc36e71da0665'}",
            "last_fix_commit":"1d04d7d93f4ed3854abf75d6b712d72c3f70d6b6",
            "chain_ord_pos":1.0,
            "commit_datetime":"04\/29\/2021, 22:30:30",
            "message":"Fix heap-buffer-overflow issue with `tf.raw_ops.SparseReshape`.\n\nPiperOrigin-RevId: 371218558\nChange-Id: I6a6dc5bf15b50a1d05bdd95e9ba347cb39f40f45",
            "author":"Amit Patankar",
            "comments":null,
            "stats":"{'additions': 12, 'deletions': 0, 'total': 12}",
            "files":"{'tensorflow\/core\/kernels\/sparse_reshape_op.cc': {'additions': 12, 'deletions': 0, 'changes': 12, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/1d04d7d93f4ed3854abf75d6b712d72c3f70d6b6\/tensorflow%2Fcore%2Fkernels%2Fsparse_reshape_op.cc', 'patch': '@@ -26,6 +26,7 @@ limitations under the License.\\n #include \"tensorflow\/core\/framework\/types.h\"\\n #include \"tensorflow\/core\/kernels\/reshape_util.h\"\\n #include \"tensorflow\/core\/lib\/gtl\/inlined_vector.h\"\\n+#include \"tensorflow\/core\/platform\/errors.h\"\\n \\n namespace tensorflow {\\n \\n@@ -38,6 +39,17 @@ class SparseReshapeOp : public OpKernel {\\n   explicit SparseReshapeOp(OpKernelConstruction* context) : OpKernel(context) {}\\n \\n   void Compute(OpKernelContext* context) override {\\n+    const Tensor& input_indices_in = context->input(0);\\n+    const Tensor& input_shape_in = context->input(1);\\n+\\n+    OP_REQUIRES(context, TensorShapeUtils::IsMatrix(input_indices_in.shape()),\\n+                errors::InvalidArgument(\"Input must be a matrix.\"));\\n+    OP_REQUIRES(context, TensorShapeUtils::IsVector(input_shape_in.shape()),\\n+                errors::InvalidArgument(\"Input shape must be a vector.\"));\\n+    OP_REQUIRES(context,\\n+                input_indices_in.dim_size(1) == input_shape_in.dim_size(0),\\n+                errors::InvalidArgument(\\n+                    \"Input tensor rank must match input shape length.\"));\\n     ReshapeSparseTensor<Device>(context, context->input(0), context->input(1),\\n                                 context->input(2), 0 \/* output indices index *\/,\\n                                 1 \/* output shape index *\/);'}}",
            "message_norm":"fix heap-buffer-overflow issue with `tf.raw_ops.sparsereshape`.\n\npiperorigin-revid: 371218558\nchange-id: i6a6dc5bf15b50a1d05bdd95e9ba347cb39f40f45",
            "language":"en",
            "entities":"[('fix', 'ACTION', ''), ('overflow', 'SECWORD', ''), ('issue', 'FLAW', ''), ('371218558', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/core\/kernels\/sparse_reshape_op.cc'])",
            "num_files":1.0,
            "patch_content":"From 1d04d7d93f4ed3854abf75d6b712d72c3f70d6b6 Mon Sep 17 00:00:00 2001\nFrom: Amit Patankar <amitpatankar@google.com>\nDate: Thu, 29 Apr 2021 15:30:30 -0700\nSubject: [PATCH] Fix heap-buffer-overflow issue with\n `tf.raw_ops.SparseReshape`.\n\nPiperOrigin-RevId: 371218558\nChange-Id: I6a6dc5bf15b50a1d05bdd95e9ba347cb39f40f45\n---\n tensorflow\/core\/kernels\/sparse_reshape_op.cc | 12 ++++++++++++\n 1 file changed, 12 insertions(+)\n\ndiff --git a\/tensorflow\/core\/kernels\/sparse_reshape_op.cc b\/tensorflow\/core\/kernels\/sparse_reshape_op.cc\nindex 13e9010dcbb243..d8120788db9bfe 100644\n--- a\/tensorflow\/core\/kernels\/sparse_reshape_op.cc\n+++ b\/tensorflow\/core\/kernels\/sparse_reshape_op.cc\n@@ -26,6 +26,7 @@ limitations under the License.\n #include \"tensorflow\/core\/framework\/types.h\"\n #include \"tensorflow\/core\/kernels\/reshape_util.h\"\n #include \"tensorflow\/core\/lib\/gtl\/inlined_vector.h\"\n+#include \"tensorflow\/core\/platform\/errors.h\"\n \n namespace tensorflow {\n \n@@ -38,6 +39,17 @@ class SparseReshapeOp : public OpKernel {\n   explicit SparseReshapeOp(OpKernelConstruction* context) : OpKernel(context) {}\n \n   void Compute(OpKernelContext* context) override {\n+    const Tensor& input_indices_in = context->input(0);\n+    const Tensor& input_shape_in = context->input(1);\n+\n+    OP_REQUIRES(context, TensorShapeUtils::IsMatrix(input_indices_in.shape()),\n+                errors::InvalidArgument(\"Input must be a matrix.\"));\n+    OP_REQUIRES(context, TensorShapeUtils::IsVector(input_shape_in.shape()),\n+                errors::InvalidArgument(\"Input shape must be a vector.\"));\n+    OP_REQUIRES(context,\n+                input_indices_in.dim_size(1) == input_shape_in.dim_size(0),\n+                errors::InvalidArgument(\n+                    \"Input tensor rank must match input shape length.\"));\n     ReshapeSparseTensor<Device>(context, context->input(0), context->input(1),\n                                 context->input(2), 0 \/* output indices index *\/,\n                                 1 \/* output shape index *\/);"
        },
        {
            "index":289,
            "vuln_id":"GHSA-8c6g-4xc5-w96c",
            "cwe_id":"{'CWE-908'}",
            "score":6.5,
            "chain":"{'https:\/\/github.com\/ruuda\/claxon\/commit\/8f28ec275e412dd3af4f3cda460605512faf332c'}",
            "dataset":"osv",
            "summary":"Uninitialized memory exposure in claxon An issue was discovered in the claxon crate before 0.4.1 for Rust. Uninitialized memory can be exposed because certain decode buffer sizes are mishandled.",
            "published_date":"2021-08-25",
            "chain_len":1,
            "project":"https:\/\/github.com\/ruuda\/claxon",
            "commit_href":"https:\/\/github.com\/ruuda\/claxon\/commit\/8f28ec275e412dd3af4f3cda460605512faf332c",
            "commit_sha":"8f28ec275e412dd3af4f3cda460605512faf332c",
            "patch":"SINGLE",
            "chain_ord":"['8f28ec275e412dd3af4f3cda460605512faf332c']",
            "before_first_fix_commit":"{'cd82be35f413940ba446d2a19f10d74b86466487'}",
            "last_fix_commit":"8f28ec275e412dd3af4f3cda460605512faf332c",
            "chain_ord_pos":1.0,
            "commit_datetime":"08\/23\/2018, 18:01:40",
            "message":"Fix bug in decoding residuals\n\nA partition order could occur, such that the block size was not a\nmultiple of 2^order. Computation of the number of samples per partition\ndid not account for this case, rounding down due to the bit shift. This\nmeant that we would not fill the entire decode buffer.\n\nClaxon does not zero the decode buffer because it is (should be)\noverwritten anyway, and in the case of a format error, where the buffer\nmight be only partially full, the buffer is not exposed again.\nFurthermore, the way decoding works in most places, is that we fill the\nentire buffer, just by looping to fill it. If the input bitstream does\nnot contain enough data to fill the buffer, then that's a format error.\nIn a few places though, we need to slice up the buffer before decoding\ninto it: for decoding individual channels, and also for decoding\nresiduals, which are split into partitions.\n\nThis particular format error was especially nasty because it did not\ncause a format error down the line. Instead, it caused the buffer to be\nsliced in a way where the slices together did not cover the entire\nbuffer, and so parts of uninitialized memory could remain in the buffer.\n\nThanks a lot to Sergey \"Shnatsel\" Davidoff for reporting this bug,\ntogether with elaborate steps to reproduce that allowed me to pinpoint\nthe cause quickly.",
            "author":"Ruud van Asseldonk",
            "comments":null,
            "stats":"{'additions': 19, 'deletions': 6, 'total': 25}",
            "files":"{'src\/subframe.rs': {'additions': 19, 'deletions': 6, 'changes': 25, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/ruuda\/claxon\/raw\/8f28ec275e412dd3af4f3cda460605512faf332c\/src%2Fsubframe.rs', 'patch': '@@ -254,35 +254,48 @@ fn decode_residual<R: ReadBytes>(input: &mut Bitstream<R>,\\n     \/\/ most 2^16 - 1 samples in the block. No values have been marked as\\n     \/\/ invalid by the specification though.\\n     let n_partitions = 1u32 << order;\\n-    let n_samples = block_size >> order;\\n+    let n_samples_per_partition = block_size >> order;\\n+\\n+    \/\/ The partitions together must fill the block. If the block size is not a\\n+    \/\/ multiple of 2^order; if we shifted off some bits, then we would not fill\\n+    \/\/ the entire block. Such a partition order is invalid for this block size.\\n+    if block_size & (n_partitions - 1) as u16 != 0 {\\n+        return fmt_err(\"invalid partition order\")\\n+    }\\n+\\n+    \/\/ NOTE: the check above checks that block_size is a multiple of n_partitions\\n+    \/\/ (this works because n_partitions is a power of 2). The check below is\\n+    \/\/ equivalent but more expensive.\\n+    debug_assert_eq!(n_partitions * n_samples_per_partition as u32, block_size as u32);\\n+\\n     let n_warm_up = block_size - buffer.len() as u16;\\n \\n     \/\/ The partition size must be at least as big as the number of warm-up\\n     \/\/ samples, otherwise the size of the first partition is negative.\\n-    if n_warm_up > n_samples {\\n+    if n_warm_up > n_samples_per_partition {\\n         return fmt_err(\"invalid residual\");\\n     }\\n \\n     \/\/ Finally decode the partitions themselves.\\n     match partition_type {\\n         RicePartitionType::Rice => {\\n             let mut start = 0;\\n-            let mut len = n_samples - n_warm_up;\\n+            let mut len = n_samples_per_partition - n_warm_up;\\n             for _ in 0..n_partitions {\\n                 let slice = &mut buffer[start..start + len as usize];\\n                 try!(decode_rice_partition(input, slice));\\n                 start = start + len as usize;\\n-                len = n_samples;\\n+                len = n_samples_per_partition;\\n             }\\n         }\\n         RicePartitionType::Rice2 => {\\n             let mut start = 0;\\n-            let mut len = n_samples - n_warm_up;\\n+            let mut len = n_samples_per_partition - n_warm_up;\\n             for _ in 0..n_partitions {\\n                 let slice = &mut buffer[start..start + len as usize];\\n                 try!(decode_rice2_partition(input, slice));\\n                 start = start + len as usize;\\n-                len = n_samples;\\n+                len = n_samples_per_partition;\\n             }\\n         }\\n     }'}}",
            "message_norm":"fix bug in decoding residuals\n\na partition order could occur, such that the block size was not a\nmultiple of 2^order. computation of the number of samples per partition\ndid not account for this case, rounding down due to the bit shift. this\nmeant that we would not fill the entire decode buffer.\n\nclaxon does not zero the decode buffer because it is (should be)\noverwritten anyway, and in the case of a format error, where the buffer\nmight be only partially full, the buffer is not exposed again.\nfurthermore, the way decoding works in most places, is that we fill the\nentire buffer, just by looping to fill it. if the input bitstream does\nnot contain enough data to fill the buffer, then that's a format error.\nin a few places though, we need to slice up the buffer before decoding\ninto it: for decoding individual channels, and also for decoding\nresiduals, which are split into partitions.\n\nthis particular format error was especially nasty because it did not\ncause a format error down the line. instead, it caused the buffer to be\nsliced in a way where the slices together did not cover the entire\nbuffer, and so parts of uninitialized memory could remain in the buffer.\n\nthanks a lot to sergey \"shnatsel\" davidoff for reporting this bug,\ntogether with elaborate steps to reproduce that allowed me to pinpoint\nthe cause quickly.",
            "language":"en",
            "entities":"[('fix', 'ACTION', ''), ('bug', 'FLAW', ''), ('decoding', 'SECWORD', ''), ('decode', 'SECWORD', ''), ('decode', 'SECWORD', ''), ('error', 'FLAW', ''), ('decoding', 'SECWORD', ''), ('error', 'FLAW', ''), ('decoding', 'SECWORD', ''), ('decoding', 'SECWORD', ''), ('decoding', 'SECWORD', ''), ('error', 'FLAW', ''), ('error', 'FLAW', ''), ('uninitialized memory', 'SECWORD', ''), ('bug', 'FLAW', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['src\/subframe.rs'])",
            "num_files":1.0,
            "patch_content":"From 8f28ec275e412dd3af4f3cda460605512faf332c Mon Sep 17 00:00:00 2001\nFrom: Ruud van Asseldonk <dev@veniogames.com>\nDate: Thu, 23 Aug 2018 20:01:40 +0200\nSubject: [PATCH] Fix bug in decoding residuals\n\nA partition order could occur, such that the block size was not a\nmultiple of 2^order. Computation of the number of samples per partition\ndid not account for this case, rounding down due to the bit shift. This\nmeant that we would not fill the entire decode buffer.\n\nClaxon does not zero the decode buffer because it is (should be)\noverwritten anyway, and in the case of a format error, where the buffer\nmight be only partially full, the buffer is not exposed again.\nFurthermore, the way decoding works in most places, is that we fill the\nentire buffer, just by looping to fill it. If the input bitstream does\nnot contain enough data to fill the buffer, then that's a format error.\nIn a few places though, we need to slice up the buffer before decoding\ninto it: for decoding individual channels, and also for decoding\nresiduals, which are split into partitions.\n\nThis particular format error was especially nasty because it did not\ncause a format error down the line. Instead, it caused the buffer to be\nsliced in a way where the slices together did not cover the entire\nbuffer, and so parts of uninitialized memory could remain in the buffer.\n\nThanks a lot to Sergey \"Shnatsel\" Davidoff for reporting this bug,\ntogether with elaborate steps to reproduce that allowed me to pinpoint\nthe cause quickly.\n---\n src\/subframe.rs | 25 +++++++++++++++++++------\n 1 file changed, 19 insertions(+), 6 deletions(-)\n\ndiff --git a\/src\/subframe.rs b\/src\/subframe.rs\nindex 01056bd..9d948b3 100644\n--- a\/src\/subframe.rs\n+++ b\/src\/subframe.rs\n@@ -254,12 +254,25 @@ fn decode_residual<R: ReadBytes>(input: &mut Bitstream<R>,\n     \/\/ most 2^16 - 1 samples in the block. No values have been marked as\n     \/\/ invalid by the specification though.\n     let n_partitions = 1u32 << order;\n-    let n_samples = block_size >> order;\n+    let n_samples_per_partition = block_size >> order;\n+\n+    \/\/ The partitions together must fill the block. If the block size is not a\n+    \/\/ multiple of 2^order; if we shifted off some bits, then we would not fill\n+    \/\/ the entire block. Such a partition order is invalid for this block size.\n+    if block_size & (n_partitions - 1) as u16 != 0 {\n+        return fmt_err(\"invalid partition order\")\n+    }\n+\n+    \/\/ NOTE: the check above checks that block_size is a multiple of n_partitions\n+    \/\/ (this works because n_partitions is a power of 2). The check below is\n+    \/\/ equivalent but more expensive.\n+    debug_assert_eq!(n_partitions * n_samples_per_partition as u32, block_size as u32);\n+\n     let n_warm_up = block_size - buffer.len() as u16;\n \n     \/\/ The partition size must be at least as big as the number of warm-up\n     \/\/ samples, otherwise the size of the first partition is negative.\n-    if n_warm_up > n_samples {\n+    if n_warm_up > n_samples_per_partition {\n         return fmt_err(\"invalid residual\");\n     }\n \n@@ -267,22 +280,22 @@ fn decode_residual<R: ReadBytes>(input: &mut Bitstream<R>,\n     match partition_type {\n         RicePartitionType::Rice => {\n             let mut start = 0;\n-            let mut len = n_samples - n_warm_up;\n+            let mut len = n_samples_per_partition - n_warm_up;\n             for _ in 0..n_partitions {\n                 let slice = &mut buffer[start..start + len as usize];\n                 try!(decode_rice_partition(input, slice));\n                 start = start + len as usize;\n-                len = n_samples;\n+                len = n_samples_per_partition;\n             }\n         }\n         RicePartitionType::Rice2 => {\n             let mut start = 0;\n-            let mut len = n_samples - n_warm_up;\n+            let mut len = n_samples_per_partition - n_warm_up;\n             for _ in 0..n_partitions {\n                 let slice = &mut buffer[start..start + len as usize];\n                 try!(decode_rice2_partition(input, slice));\n                 start = start + len as usize;\n-                len = n_samples;\n+                len = n_samples_per_partition;\n             }\n         }\n     }"
        },
        {
            "index":787,
            "vuln_id":"GHSA-4qwp-7c67-jmcc",
            "cwe_id":"{'CWE-94'}",
            "score":9.8,
            "chain":"{'https:\/\/github.com\/facade\/ignition\/commit\/11ffca14abd22db779d90b12e193f8000f6d184b'}",
            "dataset":"osv",
            "summary":"Unauthenticated remote code execution in Ignition Ignition before 2.5.2, as used in Laravel and other products, allows unauthenticated remote attackers to execute arbitrary code because of insecure usage of file_get_contents() and file_put_contents(). This is exploitable on sites using debug mode with Laravel before 8.4.2.",
            "published_date":"2021-03-29",
            "chain_len":1,
            "project":"https:\/\/github.com\/facade\/ignition",
            "commit_href":"https:\/\/github.com\/facade\/ignition\/commit\/11ffca14abd22db779d90b12e193f8000f6d184b",
            "commit_sha":"11ffca14abd22db779d90b12e193f8000f6d184b",
            "patch":"SINGLE",
            "chain_ord":"['11ffca14abd22db779d90b12e193f8000f6d184b']",
            "before_first_fix_commit":"{'9fc6c3d3de5271a1b94cff19dce2c9295abf0ffa'}",
            "last_fix_commit":"11ffca14abd22db779d90b12e193f8000f6d184b",
            "chain_ord_pos":1.0,
            "commit_datetime":"02\/18\/2021, 11:46:18",
            "message":"Fix MakeViewVariableOptionalSolution to disallow stream wrappers and files that do not end in .blade.php\n\nThis is already fixed in 2.5.2, See https:\/\/github.com\/facade\/ignition\/pull\/334\n\nI could not update to 2.5.2 due to some dependent package required php 7.3, currently clients site is running in php 7.2\n\nOn branch 2.4.1-branch\nChanges to be committed:\n\tmodified:   src\/Solutions\/MakeViewVariableOptionalSolution.php",
            "author":"Anas Mirza",
            "comments":null,
            "stats":"{'additions': 18, 'deletions': 0, 'total': 18}",
            "files":"{'src\/Solutions\/MakeViewVariableOptionalSolution.php': {'additions': 18, 'deletions': 0, 'changes': 18, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/facade\/ignition\/raw\/11ffca14abd22db779d90b12e193f8000f6d184b\/src%2FSolutions%2FMakeViewVariableOptionalSolution.php', 'patch': '@@ -4,6 +4,7 @@\\n \\n use Facade\\\\IgnitionContracts\\\\RunnableSolution;\\n use Illuminate\\\\Support\\\\Facades\\\\Blade;\\n+use Illuminate\\\\Support\\\\Str;\\n \\n class MakeViewVariableOptionalSolution implements RunnableSolution\\n {\\n@@ -70,8 +71,25 @@ public function run(array $parameters = [])\\n         }\\n     }\\n \\n+    protected function isSafePath(string $path): bool\\n+    {\\n+        if (!Str::startsWith($path, [\\'\/\\', \\'.\/\\'])) {\\n+            return false;\\n+        }\\n+\\n+        if (!Str::endsWith($path, \\'.blade.php\\')) {\\n+            return false;\\n+        }\\n+\\n+        return true;\\n+    }\\n+\\n     public function makeOptional(array $parameters = [])\\n     {\\n+        if (!$this->isSafePath($parameters[\\'viewFile\\'])) {\\n+            return false;\\n+        }\\n+\\n         $originalContents = file_get_contents($parameters[\\'viewFile\\']);\\n         $newContents = str_replace(\\'$\\'.$parameters[\\'variableName\\'], \\'$\\'.$parameters[\\'variableName\\'].\" ?? \\'\\'\", $originalContents);'}}",
            "message_norm":"fix makeviewvariableoptionalsolution to disallow stream wrappers and files that do not end in .blade.php\n\nthis is already fixed in 2.5.2, see https:\/\/github.com\/facade\/ignition\/pull\/334\n\ni could not update to 2.5.2 due to some dependent package required php 7.3, currently clients site is running in php 7.2\n\non branch 2.4.1-branch\nchanges to be committed:\n\tmodified:   src\/solutions\/makeviewvariableoptionalsolution.php",
            "language":"en",
            "entities":"[('fix', 'ACTION', ''), ('fixed', 'ACTION', ''), ('2.5.2', 'VERSION', ''), ('https:\/\/github.com\/facade\/ignition\/pull\/334', 'URL', ''), ('update', 'ACTION', ''), ('2.5.2', 'VERSION', ''), ('2.4.1', 'VERSION', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['src\/Solutions\/MakeViewVariableOptionalSolution.php'])",
            "num_files":1.0,
            "patch_content":"From 11ffca14abd22db779d90b12e193f8000f6d184b Mon Sep 17 00:00:00 2001\nFrom: Anas Mirza <anasmirza534@gmail.com>\nDate: Thu, 18 Feb 2021 17:16:18 +0530\nSubject: [PATCH] Fix MakeViewVariableOptionalSolution to disallow stream\n wrappers and files that do not end in .blade.php\n\nThis is already fixed in 2.5.2, See https:\/\/github.com\/facade\/ignition\/pull\/334\n\nI could not update to 2.5.2 due to some dependent package required php 7.3, currently clients site is running in php 7.2\n\nOn branch 2.4.1-branch\nChanges to be committed:\n\tmodified:   src\/Solutions\/MakeViewVariableOptionalSolution.php\n---\n ...\/MakeViewVariableOptionalSolution.php       | 18 ++++++++++++++++++\n 1 file changed, 18 insertions(+)\n\ndiff --git a\/src\/Solutions\/MakeViewVariableOptionalSolution.php b\/src\/Solutions\/MakeViewVariableOptionalSolution.php\nindex 5e4164de..cac2ed8d 100644\n--- a\/src\/Solutions\/MakeViewVariableOptionalSolution.php\n+++ b\/src\/Solutions\/MakeViewVariableOptionalSolution.php\n@@ -4,6 +4,7 @@\n \n use Facade\\IgnitionContracts\\RunnableSolution;\n use Illuminate\\Support\\Facades\\Blade;\n+use Illuminate\\Support\\Str;\n \n class MakeViewVariableOptionalSolution implements RunnableSolution\n {\n@@ -70,8 +71,25 @@ public function run(array $parameters = [])\n         }\n     }\n \n+    protected function isSafePath(string $path): bool\n+    {\n+        if (!Str::startsWith($path, ['\/', '.\/'])) {\n+            return false;\n+        }\n+\n+        if (!Str::endsWith($path, '.blade.php')) {\n+            return false;\n+        }\n+\n+        return true;\n+    }\n+\n     public function makeOptional(array $parameters = [])\n     {\n+        if (!$this->isSafePath($parameters['viewFile'])) {\n+            return false;\n+        }\n+\n         $originalContents = file_get_contents($parameters['viewFile']);\n         $newContents = str_replace('$'.$parameters['variableName'], '$'.$parameters['variableName'].\" ?? ''\", $originalContents);"
        },
        {
            "index":699,
            "vuln_id":"GHSA-gv3v-92v6-m48j",
            "cwe_id":"{'CWE-444'}",
            "score":9.8,
            "chain":"{'https:\/\/github.com\/jooby-project\/jooby\/commit\/b66e3342cf95205324023cfdf2cb5811e8a6dcf4'}",
            "dataset":"osv",
            "summary":"Improper Neutralization of CRLF Sequences in HTTP Headers in Jooby ('HTTP Response Splitting) ### Impact\n\n - Cross Site Scripting\n - Cache Poisoning\n - Page Hijacking\n\n### Patches\n\nThis was fixed in version `2.2.1`.\n\n### Workarounds\n\nIf you are unable to update, ensure that user supplied data isn't able to flow to HTTP headers. If it does, pre-sanitize for CRLF characters.\n\n### References\n\n##### [CWE-113: Improper Neutralization of CRLF Sequences in HTTP Headers ('HTTP Response Splitting')](https:\/\/cwe.mitre.org\/data\/definitions\/113.html)\n\nI've been poking at libraries to see if they are vulnerable to HTTP Response Splitting and Jooby is my third case of finding this vulnerability.\n\n### Root Cause\n\nThis roots cause back to this line in the Jooby codebase:\n\nhttps:\/\/github.com\/jooby-project\/jooby\/blob\/93cfc80aa20c188f71a442ea7a1827da380e1c27\/modules\/jooby-netty\/src\/main\/java\/io\/jooby\/internal\/netty\/NettyContext.java#L102\n\nThe `DefaultHttpHeaders` takes a parameter `validate` which, when `true` (as it is for the no-arg constructor) validates that the header isn't being abused to do HTTP Response Splitting.\n\n### Reported By\n\nThis vulnerability was reported by @JLLeitschuh ([Twitter](https:\/\/twitter.com\/JLLeitschuh))\n\n### For more information\nIf you have any questions or comments about this advisory:\n* Open an issue in [jooby-project\/jooby](https:\/\/github.com\/jooby-project\/jooby\/issues)",
            "published_date":"2020-04-03",
            "chain_len":1,
            "project":"https:\/\/github.com\/jooby-project\/jooby",
            "commit_href":"https:\/\/github.com\/jooby-project\/jooby\/commit\/b66e3342cf95205324023cfdf2cb5811e8a6dcf4",
            "commit_sha":"b66e3342cf95205324023cfdf2cb5811e8a6dcf4",
            "patch":"SINGLE",
            "chain_ord":"['b66e3342cf95205324023cfdf2cb5811e8a6dcf4']",
            "before_first_fix_commit":"{'d5708760bdd27f8f6e1dbbbabbda4379fd5ba926'}",
            "last_fix_commit":"b66e3342cf95205324023cfdf2cb5811e8a6dcf4",
            "chain_ord_pos":1.0,
            "commit_datetime":"10\/12\/2019, 13:30:52",
            "message":"CWE-113: Improper Neutralization of CRLF Sequences in HTTP Headers ('HTTP Response Splitting fix #GHSA-gv3v-92v6-m48j",
            "author":"Edgar Espina",
            "comments":null,
            "stats":"{'additions': 1, 'deletions': 1, 'total': 2}",
            "files":"{'modules\/jooby-netty\/src\/main\/java\/io\/jooby\/internal\/netty\/NettyContext.java': {'additions': 1, 'deletions': 1, 'changes': 2, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/jooby-project\/jooby\/raw\/b66e3342cf95205324023cfdf2cb5811e8a6dcf4\/modules%2Fjooby-netty%2Fsrc%2Fmain%2Fjava%2Fio%2Fjooby%2Finternal%2Fnetty%2FNettyContext.java', 'patch': '@@ -99,7 +99,7 @@\\n public class NettyContext implements DefaultContext, ChannelFutureListener {\\n \\n   private static final HttpHeaders NO_TRAILING = EmptyHttpHeaders.INSTANCE;\\n-  final DefaultHttpHeaders setHeaders = new DefaultHttpHeaders(false);\\n+  final DefaultHttpHeaders setHeaders = new DefaultHttpHeaders(true);\\n   private final int bufferSize;\\n   InterfaceHttpPostRequestDecoder decoder;\\n   private Router router;'}}",
            "message_norm":"cwe-113: improper neutralization of crlf sequences in http headers ('http response splitting fix #ghsa-gv3v-92v6-m48j",
            "language":"en",
            "entities":"[('cwe-113', 'CWEID', ''), ('improper neutralization', 'SECWORD', ''), ('http response splitting', 'SECWORD', ''), ('ghsa-gv3v-92v6-m48j', 'VULNID', 'GHSA')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['modules\/jooby-netty\/src\/main\/java\/io\/jooby\/internal\/netty\/NettyContext.java'])",
            "num_files":1.0,
            "patch_content":"From b66e3342cf95205324023cfdf2cb5811e8a6dcf4 Mon Sep 17 00:00:00 2001\nFrom: Edgar Espina <espina.edgar@gmail.com>\nDate: Sat, 12 Oct 2019 10:30:52 -0300\nSubject: [PATCH] CWE-113: Improper Neutralization of CRLF Sequences in HTTP\n Headers ('HTTP Response Splitting fix #GHSA-gv3v-92v6-m48j\n\n---\n ...\/src\/main\/java\/io\/jooby\/internal\/netty\/NettyContext.java     | 2 +-\n 1 file changed, 1 insertion(+), 1 deletion(-)\n\ndiff --git a\/modules\/jooby-netty\/src\/main\/java\/io\/jooby\/internal\/netty\/NettyContext.java b\/modules\/jooby-netty\/src\/main\/java\/io\/jooby\/internal\/netty\/NettyContext.java\nindex f2fcb0cd14..3809634db7 100644\n--- a\/modules\/jooby-netty\/src\/main\/java\/io\/jooby\/internal\/netty\/NettyContext.java\n+++ b\/modules\/jooby-netty\/src\/main\/java\/io\/jooby\/internal\/netty\/NettyContext.java\n@@ -99,7 +99,7 @@\n public class NettyContext implements DefaultContext, ChannelFutureListener {\n \n   private static final HttpHeaders NO_TRAILING = EmptyHttpHeaders.INSTANCE;\n-  final DefaultHttpHeaders setHeaders = new DefaultHttpHeaders(false);\n+  final DefaultHttpHeaders setHeaders = new DefaultHttpHeaders(true);\n   private final int bufferSize;\n   InterfaceHttpPostRequestDecoder decoder;\n   private Router router;"
        },
        {
            "index":507,
            "vuln_id":"GHSA-269q-hmxg-m83q",
            "cwe_id":"{'CWE-379', 'CWE-378', 'CWE-668'}",
            "score":5.5,
            "chain":"{'https:\/\/github.com\/netty\/netty\/commit\/185f8b2756a36aaa4f973f1a2a025e7d981823f1'}",
            "dataset":"osv",
            "summary":"Local Information Disclosure Vulnerability in io.netty:netty-codec-http ### Description ###\n[GHSA-5mcr-gq6c-3hq2](https:\/\/github.com\/netty\/netty\/security\/advisories\/GHSA-5mcr-gq6c-3hq2) (CVE-2021-21290) contains an insufficient fix for the vulnerability identified.\n\n### Impact ###\n\nWhen netty's multipart decoders are used local information disclosure can occur via the local system temporary directory if temporary storing uploads on the disk is enabled.\n\nThis only impacts applications running on Java version 6 and lower. Additionally, this vulnerability impacts code running on Unix-like systems, and very old versions of Mac OSX and Windows as they all share the system temporary directory between all users.\n\n### Vulnerability Details ###\n\nTo fix the vulnerability the code was changed to the following:\n\n```java\n    @SuppressJava6Requirement(reason = \"Guarded by version check\")\n    public static File createTempFile(String prefix, String suffix, File directory) throws IOException {\n        if (javaVersion() >= 7) {\n            if (directory == null) {\n                return Files.createTempFile(prefix, suffix).toFile();\n            }\n            return Files.createTempFile(directory.toPath(), prefix, suffix).toFile();\n        }\n        if (directory == null) {\n            return File.createTempFile(prefix, suffix);\n        }\n        File file = File.createTempFile(prefix, suffix, directory);\n        \/\/ Try to adjust the perms, if this fails there is not much else we can do...\n        file.setReadable(false, false);\n        file.setReadable(true, true);\n        return file;\n    }\n```\n\nUnfortunately, this logic path was left vulnerable:\n\n```java\n        if (directory == null) {\n            return File.createTempFile(prefix, suffix);\n        }\n```\n\nThis file is still readable by all local users.\n\n### Patches ###\n\nUpdate to 4.1.77.Final\n\n### Workarounds ###\n\nSpecify your own `java.io.tmpdir` when you start the JVM or use `DefaultHttpDataFactory.setBaseDir(...)` to set the directory to something that is only readable by the current user or update to Java 7 or above.\n\n### References ###\n\n - [CWE-378: Creation of Temporary File With Insecure Permissions](https:\/\/cwe.mitre.org\/data\/definitions\/378.html)\n - [CWE-379: Creation of Temporary File in Directory with Insecure Permissions](https:\/\/cwe.mitre.org\/data\/definitions\/379.html)\n\n\n### For more information ###\n\nIf you have any questions or comments about this advisory:\n\nOpen an issue in [netty](https:\/\/github.com\/netty\/netty)",
            "published_date":"2022-05-10",
            "chain_len":1,
            "project":"https:\/\/github.com\/netty\/netty",
            "commit_href":"https:\/\/github.com\/netty\/netty\/commit\/185f8b2756a36aaa4f973f1a2a025e7d981823f1",
            "commit_sha":"185f8b2756a36aaa4f973f1a2a025e7d981823f1",
            "patch":"SINGLE",
            "chain_ord":"['185f8b2756a36aaa4f973f1a2a025e7d981823f1']",
            "before_first_fix_commit":"{'7dbca6aedc8cf5971e2a26d8fc2b7f265f2b4bf1'}",
            "last_fix_commit":"185f8b2756a36aaa4f973f1a2a025e7d981823f1",
            "chain_ord_pos":1.0,
            "commit_datetime":"05\/06\/2022, 06:57:43",
            "message":"Merge pull request from GHSA-269q-hmxg-m83q\n\n* Correctly modify permission for temporary files when using Java 6 in all cases\n\nMotivation:\n\n[GHSA-5mcr-gq6c-3hq2](https:\/\/github.com\/netty\/netty\/security\/advisories\/GHSA-5mcr-gq6c-3hq2) did not correctly fix all cases for temprory files when running on java 6.\n\nModifications:\n\n- Add correctly adjust perms in all cases\n- Add logging if adjusting of permissions fails\n\nResult:\n\nFixes https:\/\/github.com\/netty\/netty\/security\/advisories\/GHSA-269q-hmxg-m83q\n\n* Throw on failure",
            "author":"Norman Maurer",
            "comments":null,
            "stats":"{'additions': 11, 'deletions': 4, 'total': 15}",
            "files":"{'common\/src\/main\/java\/io\/netty\/util\/internal\/PlatformDependent.java': {'additions': 11, 'deletions': 4, 'changes': 15, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/netty\/netty\/raw\/185f8b2756a36aaa4f973f1a2a025e7d981823f1\/common%2Fsrc%2Fmain%2Fjava%2Fio%2Fnetty%2Futil%2Finternal%2FPlatformDependent.java', 'patch': '@@ -1447,13 +1447,20 @@ public static File createTempFile(String prefix, String suffix, File directory)\\n             }\\n             return Files.createTempFile(directory.toPath(), prefix, suffix).toFile();\\n         }\\n+        final File file;\\n         if (directory == null) {\\n-            return File.createTempFile(prefix, suffix);\\n+            file = File.createTempFile(prefix, suffix);\\n+        } else {\\n+            file = File.createTempFile(prefix, suffix, directory);\\n         }\\n-        File file = File.createTempFile(prefix, suffix, directory);\\n+\\n         \/\/ Try to adjust the perms, if this fails there is not much else we can do...\\n-        file.setReadable(false, false);\\n-        file.setReadable(true, true);\\n+        if (!file.setReadable(false, false)) {\\n+            throw new IOException(\"Failed to set permissions on temporary file \" + file);\\n+        }\\n+        if (!file.setReadable(true, true)) {\\n+            throw new IOException(\"Failed to set permissions on temporary file \" + file);\\n+        }\\n         return file;\\n     }'}}",
            "message_norm":"merge pull request from ghsa-269q-hmxg-m83q\n\n* correctly modify permission for temporary files when using java 6 in all cases\n\nmotivation:\n\n[ghsa-5mcr-gq6c-3hq2](https:\/\/github.com\/netty\/netty\/security\/advisories\/ghsa-5mcr-gq6c-3hq2) did not correctly fix all cases for temprory files when running on java 6.\n\nmodifications:\n\n- add correctly adjust perms in all cases\n- add logging if adjusting of permissions fails\n\nresult:\n\nfixes https:\/\/github.com\/netty\/netty\/security\/advisories\/ghsa-269q-hmxg-m83q\n\n* throw on failure",
            "language":"en",
            "entities":"[('ghsa-269q-hmxg-m83q', 'VULNID', 'GHSA'), ('permission', 'SECWORD', ''), ('ghsa-5mcr-gq6c-3hq2](https:\/\/github.com\/netty\/netty', 'VULNID', 'GHSA'), ('security', 'SECWORD', ''), ('ghsa-5mcr-gq6c-3hq2) did not', 'VULNID', 'GHSA'), ('fix', 'ACTION', ''), ('add', 'ACTION', ''), ('add', 'ACTION', ''), ('permissions', 'SECWORD', ''), ('fixes', 'ACTION', ''), ('https:\/\/github.com\/netty\/netty\/security\/advisories\/ghsa-269q-hmxg-m83q', 'VULNID', 'GHSA')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['common\/src\/main\/java\/io\/netty\/util\/internal\/PlatformDependent.java'])",
            "num_files":1.0,
            "patch_content":"From 185f8b2756a36aaa4f973f1a2a025e7d981823f1 Mon Sep 17 00:00:00 2001\nFrom: Norman Maurer <norman_maurer@apple.com>\nDate: Fri, 6 May 2022 08:57:43 +0200\nSubject: [PATCH] Merge pull request from GHSA-269q-hmxg-m83q\n\n* Correctly modify permission for temporary files when using Java 6 in all cases\n\nMotivation:\n\n[GHSA-5mcr-gq6c-3hq2](https:\/\/github.com\/netty\/netty\/security\/advisories\/GHSA-5mcr-gq6c-3hq2) did not correctly fix all cases for temprory files when running on java 6.\n\nModifications:\n\n- Add correctly adjust perms in all cases\n- Add logging if adjusting of permissions fails\n\nResult:\n\nFixes https:\/\/github.com\/netty\/netty\/security\/advisories\/GHSA-269q-hmxg-m83q\n\n* Throw on failure\n---\n ...\/io\/netty\/util\/internal\/PlatformDependent.java | 15 +++++++++++----\n 1 file changed, 11 insertions(+), 4 deletions(-)\n\ndiff --git a\/common\/src\/main\/java\/io\/netty\/util\/internal\/PlatformDependent.java b\/common\/src\/main\/java\/io\/netty\/util\/internal\/PlatformDependent.java\nindex 853204636652..00310e585ea8 100644\n--- a\/common\/src\/main\/java\/io\/netty\/util\/internal\/PlatformDependent.java\n+++ b\/common\/src\/main\/java\/io\/netty\/util\/internal\/PlatformDependent.java\n@@ -1447,13 +1447,20 @@ public static File createTempFile(String prefix, String suffix, File directory)\n             }\n             return Files.createTempFile(directory.toPath(), prefix, suffix).toFile();\n         }\n+        final File file;\n         if (directory == null) {\n-            return File.createTempFile(prefix, suffix);\n+            file = File.createTempFile(prefix, suffix);\n+        } else {\n+            file = File.createTempFile(prefix, suffix, directory);\n         }\n-        File file = File.createTempFile(prefix, suffix, directory);\n+\n         \/\/ Try to adjust the perms, if this fails there is not much else we can do...\n-        file.setReadable(false, false);\n-        file.setReadable(true, true);\n+        if (!file.setReadable(false, false)) {\n+            throw new IOException(\"Failed to set permissions on temporary file \" + file);\n+        }\n+        if (!file.setReadable(true, true)) {\n+            throw new IOException(\"Failed to set permissions on temporary file \" + file);\n+        }\n         return file;\n     }"
        },
        {
            "index":703,
            "vuln_id":"GHSA-x4g7-fvjj-prg8",
            "cwe_id":"{'CWE-369'}",
            "score":2.5,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/cfa91be9863a91d5105a3b4941096044ab32036b'}",
            "dataset":"osv",
            "summary":"Division by 0 in `QuantizedConv2D` ### Impact\nAn attacker can trigger a division by 0 in `tf.raw_ops.QuantizedConv2D`:\n\n```python\nimport tensorflow as tf\n\ninput = tf.zeros([1, 1, 1, 1], dtype=tf.quint8)\nfilter = tf.constant([], shape=[1, 0, 1, 1], dtype=tf.quint8)\nmin_input = tf.constant(0.0)\nmax_input = tf.constant(0.0001)\nmin_filter = tf.constant(0.0)\nmax_filter = tf.constant(0.0001)\nstrides = [1, 1, 1, 1]\npadding = \"SAME\"               \n                               \n\ntf.raw_ops.QuantizedConv2D(input=input, filter=filter, min_input=min_input, max_input=max_input, min_filter=min_filter, max_filter=max_filter, strides=strides, padding=padding)\n```\nThis is because the [implementation](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/00e9a4d67d76703fa1aee33dac582acf317e0e81\/tensorflow\/core\/kernels\/quantized_conv_ops.cc#L257-L259) does a division by a quantity that is controlled by the caller: \n\n```cc\nconst int filter_value_count = filter_width * filter_height * input_depth;\nconst int64 patches_per_chunk = kMaxChunkSize \/ (filter_value_count * sizeof(T1));\n```\n  \n### Patches\nWe have patched the issue in GitHub commit [cfa91be9863a91d5105a3b4941096044ab32036b](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/cfa91be9863a91d5105a3b4941096044ab32036b).\n\nThe fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by Ying Wang and Yakun Zhang of Baidu X-Team.",
            "published_date":"2021-05-21",
            "chain_len":1,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/cfa91be9863a91d5105a3b4941096044ab32036b",
            "commit_sha":"cfa91be9863a91d5105a3b4941096044ab32036b",
            "patch":"SINGLE",
            "chain_ord":"['cfa91be9863a91d5105a3b4941096044ab32036b']",
            "before_first_fix_commit":"{'00e9a4d67d76703fa1aee33dac582acf317e0e81'}",
            "last_fix_commit":"cfa91be9863a91d5105a3b4941096044ab32036b",
            "chain_ord_pos":1.0,
            "commit_datetime":"04\/20\/2021, 01:58:47",
            "message":"Fix one FPE and remove two CHECK-fails.\n\nPiperOrigin-RevId: 369349640\nChange-Id: I1fedbfc2b5bab635c5cb51f103d7c9176f79831a",
            "author":"Mihai Maruseac",
            "comments":null,
            "stats":"{'additions': 11, 'deletions': 2, 'total': 13}",
            "files":"{'tensorflow\/core\/kernels\/quantized_conv_ops.cc': {'additions': 11, 'deletions': 2, 'changes': 13, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/cfa91be9863a91d5105a3b4941096044ab32036b\/tensorflow%2Fcore%2Fkernels%2Fquantized_conv_ops.cc', 'patch': '@@ -18,6 +18,8 @@ limitations under the License.\\n #include <algorithm>\\n #include <vector>\\n \\n+#include \"tensorflow\/core\/platform\/errors.h\"\\n+\\n #define EIGEN_USE_THREADS\\n \\n #define GEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK\\n@@ -227,8 +229,12 @@ class Im2ColConvFunctor {\\n       return;\\n     }\\n \\n-    CHECK_GT(output_width, 0);\\n-    CHECK_GT(output_height, 0);\\n+    OP_REQUIRES(\\n+        context, output_width > 0,\\n+        errors::InvalidArgument(\"output_width must be strictly positive\"));\\n+    OP_REQUIRES(\\n+        context, output_height > 0,\\n+        errors::InvalidArgument(\"output_height must be strictly positive\"));\\n     int filter_left_offset;\\n     int filter_top_offset;\\n     if (padding == VALID) {\\n@@ -255,6 +261,9 @@ class Im2ColConvFunctor {\\n     \/\/ by the width, then the height. This is the standard memory order in the\\n     \/\/ image world if it helps to visualize it.\\n     const int filter_value_count = filter_width * filter_height * input_depth;\\n+    OP_REQUIRES(context, filter_value_count > 0,\\n+                errors::InvalidArgument(\\n+                    \"filter patch must contain at least one element\"));\\n     const int64 patches_per_chunk =\\n         kMaxChunkSize \/ (filter_value_count * sizeof(T1));\\n     const int64 chunk_value_count ='}}",
            "message_norm":"fix one fpe and remove two check-fails.\n\npiperorigin-revid: 369349640\nchange-id: i1fedbfc2b5bab635c5cb51f103d7c9176f79831a",
            "language":"en",
            "entities":"[('fix', 'ACTION', ''), ('fpe', 'SECWORD', ''), ('remove', 'ACTION', ''), ('369349640', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/core\/kernels\/quantized_conv_ops.cc'])",
            "num_files":1.0,
            "patch_content":"From cfa91be9863a91d5105a3b4941096044ab32036b Mon Sep 17 00:00:00 2001\nFrom: Mihai Maruseac <mihaimaruseac@google.com>\nDate: Mon, 19 Apr 2021 18:58:47 -0700\nSubject: [PATCH] Fix one FPE and remove two CHECK-fails.\n\nPiperOrigin-RevId: 369349640\nChange-Id: I1fedbfc2b5bab635c5cb51f103d7c9176f79831a\n---\n tensorflow\/core\/kernels\/quantized_conv_ops.cc | 13 +++++++++++--\n 1 file changed, 11 insertions(+), 2 deletions(-)\n\ndiff --git a\/tensorflow\/core\/kernels\/quantized_conv_ops.cc b\/tensorflow\/core\/kernels\/quantized_conv_ops.cc\nindex a4d36cca3e4088..a339de8cfc8fa3 100644\n--- a\/tensorflow\/core\/kernels\/quantized_conv_ops.cc\n+++ b\/tensorflow\/core\/kernels\/quantized_conv_ops.cc\n@@ -18,6 +18,8 @@ limitations under the License.\n #include <algorithm>\n #include <vector>\n \n+#include \"tensorflow\/core\/platform\/errors.h\"\n+\n #define EIGEN_USE_THREADS\n \n #define GEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK\n@@ -227,8 +229,12 @@ class Im2ColConvFunctor {\n       return;\n     }\n \n-    CHECK_GT(output_width, 0);\n-    CHECK_GT(output_height, 0);\n+    OP_REQUIRES(\n+        context, output_width > 0,\n+        errors::InvalidArgument(\"output_width must be strictly positive\"));\n+    OP_REQUIRES(\n+        context, output_height > 0,\n+        errors::InvalidArgument(\"output_height must be strictly positive\"));\n     int filter_left_offset;\n     int filter_top_offset;\n     if (padding == VALID) {\n@@ -255,6 +261,9 @@ class Im2ColConvFunctor {\n     \/\/ by the width, then the height. This is the standard memory order in the\n     \/\/ image world if it helps to visualize it.\n     const int filter_value_count = filter_width * filter_height * input_depth;\n+    OP_REQUIRES(context, filter_value_count > 0,\n+                errors::InvalidArgument(\n+                    \"filter patch must contain at least one element\"));\n     const int64 patches_per_chunk =\n         kMaxChunkSize \/ (filter_value_count * sizeof(T1));\n     const int64 chunk_value_count ="
        },
        {
            "index":333,
            "vuln_id":"GHSA-4r8q-gv9j-3xx6",
            "cwe_id":"{'CWE-601'}",
            "score":4.3,
            "chain":"{'https:\/\/github.com\/tenancy\/multi-tenant\/commit\/9c837a21bccce9bcaeb90033ef200d84f0d9e164'}",
            "dataset":"osv",
            "summary":"Open Redirect ### Impact\nIn some situations, it is possible to have open redirects where users can be redirected from your site to any other site using a specially crafted URL.\nThis is only the case for installations where the default Hostname Identification is used and the environment uses tenants that have `force_https` set to `true` (default: `false`)\n\n### Patches\nVersion 5.7.2 contains the relevant patches to fix this bug. Stripping the URL from special characters to prevent specially crafted URL's from being redirected to.\n\n### Workarounds\nThere is a simple way to work around the security issue\n- Set the `force_https` to every tenant to `false`\n\n### References\nhttps:\/\/nvd.nist.gov\/vuln\/detail\/CVE-2018-11784\n\n### For more information\nIf you have any questions or comments about this advisory:\n* Contact us in Discord: https:\/\/tenancy.dev\/chat",
            "published_date":"2022-03-18",
            "chain_len":1,
            "project":"https:\/\/github.com\/tenancy\/multi-tenant",
            "commit_href":"https:\/\/github.com\/tenancy\/multi-tenant\/commit\/9c837a21bccce9bcaeb90033ef200d84f0d9e164",
            "commit_sha":"9c837a21bccce9bcaeb90033ef200d84f0d9e164",
            "patch":"SINGLE",
            "chain_ord":"['9c837a21bccce9bcaeb90033ef200d84f0d9e164']",
            "before_first_fix_commit":"{'41150dce84288f24ff68c8df433a45f966a60865'}",
            "last_fix_commit":"9c837a21bccce9bcaeb90033ef200d84f0d9e164",
            "chain_ord_pos":1.0,
            "commit_datetime":"05\/27\/2021, 04:40:08",
            "message":"Trim slashes from request uri before redirecting (#1001)",
            "author":"Jasper Zonneveld",
            "comments":null,
            "stats":"{'additions': 1, 'deletions': 1, 'total': 2}",
            "files":"{'src\/Middleware\/HostnameActions.php': {'additions': 1, 'deletions': 1, 'changes': 2, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tenancy\/multi-tenant\/raw\/9c837a21bccce9bcaeb90033ef200d84f0d9e164\/src%2FMiddleware%2FHostnameActions.php', 'patch': \"@@ -95,7 +95,7 @@ protected function secure(Hostname $hostname, Request $request)\\n     {\\n         $this->emitEvent(new Secured($hostname));\\n \\n-        return $this->redirect->secure($request->getRequestUri());\\n+        return $this->redirect->secure(ltrim($request->getRequestUri(), '\/'));\\n     }\\n \\n     \/**\"}}",
            "message_norm":"trim slashes from request uri before redirecting (#1001)",
            "language":"en",
            "entities":"[('#1001', 'ISSUE', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['src\/Middleware\/HostnameActions.php'])",
            "num_files":1.0,
            "patch_content":"From 9c837a21bccce9bcaeb90033ef200d84f0d9e164 Mon Sep 17 00:00:00 2001\nFrom: Jasper Zonneveld <JaZo@users.noreply.github.com>\nDate: Thu, 27 May 2021 06:40:08 +0200\nSubject: [PATCH] Trim slashes from request uri before redirecting (#1001)\n\n---\n src\/Middleware\/HostnameActions.php | 2 +-\n 1 file changed, 1 insertion(+), 1 deletion(-)\n\ndiff --git a\/src\/Middleware\/HostnameActions.php b\/src\/Middleware\/HostnameActions.php\nindex de5e0335..6578ec9c 100644\n--- a\/src\/Middleware\/HostnameActions.php\n+++ b\/src\/Middleware\/HostnameActions.php\n@@ -95,7 +95,7 @@ protected function secure(Hostname $hostname, Request $request)\n     {\n         $this->emitEvent(new Secured($hostname));\n \n-        return $this->redirect->secure($request->getRequestUri());\n+        return $this->redirect->secure(ltrim($request->getRequestUri(), '\/'));\n     }\n \n     \/**"
        },
        {
            "index":730,
            "vuln_id":"GHSA-3wcq-x3mq-6r9p",
            "cwe_id":"{'CWE-908', 'CWE-200'}",
            "score":7.7,
            "chain":"{'https:\/\/github.com\/mafintosh\/dns-packet\/commit\/0d0d593f8df4e2712c43957a6c62e95047f12b2d', 'https:\/\/github.com\/mafintosh\/dns-packet\/commit\/25f15dd0fedc53688b25fd053ebbdffe3d5c1c56'}",
            "dataset":"osv",
            "summary":"Potential memory exposure in dns-packet This affects the package dns-packet before versions 1.3.2 and 5.2.2. It creates buffers with allocUnsafe and does not always fill them before forming network packets. This can expose internal application memory over unencrypted network when querying crafted invalid domain names.",
            "published_date":"2021-05-24",
            "chain_len":2,
            "project":"https:\/\/github.com\/mafintosh\/dns-packet",
            "commit_href":"https:\/\/github.com\/mafintosh\/dns-packet\/commit\/0d0d593f8df4e2712c43957a6c62e95047f12b2d",
            "commit_sha":"0d0d593f8df4e2712c43957a6c62e95047f12b2d",
            "patch":"MULTI",
            "chain_ord":"['25f15dd0fedc53688b25fd053ebbdffe3d5c1c56', '0d0d593f8df4e2712c43957a6c62e95047f12b2d']",
            "before_first_fix_commit":"{'7f35bac5b4680d7bfbb34fbc475ecfdbf9d25092'}",
            "last_fix_commit":"0d0d593f8df4e2712c43957a6c62e95047f12b2d",
            "chain_ord_pos":2.0,
            "commit_datetime":"05\/25\/2021, 08:35:02",
            "message":"backport encodingLength fix to v1",
            "author":"Mathias Buus",
            "comments":null,
            "stats":"{'additions': 2, 'deletions': 1, 'total': 3}",
            "files":"{'index.js': {'additions': 2, 'deletions': 1, 'changes': 3, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/mafintosh\/dns-packet\/raw\/0d0d593f8df4e2712c43957a6c62e95047f12b2d\/index.js', 'patch': \"@@ -74,7 +74,8 @@ name.decode = function (buf, offset) {\\n name.decode.bytes = 0\\n \\n name.encodingLength = function (n) {\\n-  return Buffer.byteLength(n) + 2\\n+  if (n === '.') return 1\\n+  return Buffer.byteLength(n.replace(\/^\\\\.|\\\\.$\/gm, '')) + 2\\n }\\n \\n var string = {}\"}}",
            "message_norm":"backport encodinglength fix to v1",
            "language":"en",
            "entities":"[('encodinglength', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['index.js'])",
            "num_files":1.0,
            "patch_content":"From 0d0d593f8df4e2712c43957a6c62e95047f12b2d Mon Sep 17 00:00:00 2001\nFrom: Mathias Buus <mathiasbuus@gmail.com>\nDate: Tue, 25 May 2021 10:35:02 +0200\nSubject: [PATCH] backport encodingLength fix to v1\n\n---\n index.js | 3 ++-\n 1 file changed, 2 insertions(+), 1 deletion(-)\n\ndiff --git a\/index.js b\/index.js\nindex 997466e..2321857 100644\n--- a\/index.js\n+++ b\/index.js\n@@ -74,7 +74,8 @@ name.decode = function (buf, offset) {\n name.decode.bytes = 0\n \n name.encodingLength = function (n) {\n-  return Buffer.byteLength(n) + 2\n+  if (n === '.') return 1\n+  return Buffer.byteLength(n.replace(\/^\\.|\\.$\/gm, '')) + 2\n }\n \n var string = {}"
        },
        {
            "index":583,
            "vuln_id":"GHSA-8phj-f9w2-cjcc",
            "cwe_id":"{'CWE-22'}",
            "score":8.6,
            "chain":"{'https:\/\/github.com\/aimhubio\/aim\/pull\/1003\/commits\/f01266a1a479ef11d7d6c539e7dd89e9d5639738'}",
            "dataset":"osv",
            "summary":"Arbitrary file reading vulnerability in Aim ### Impact\nA path traversal attack aims to access files and directories that are stored outside the web root folder. By manipulating variables that reference files with \u201cdot-dot-slash (..\/)\u201d sequences and its variations or by using absolute file paths, it may be possible to access arbitrary files and directories stored on file system including application source code or configuration and critical system files.\n\nVulnerable code: https:\/\/github.com\/aimhubio\/aim\/blob\/0b99c6ca08e0ba7e7011453a2f68033e9b1d1bce\/aim\/web\/api\/views.py#L9-L16\n\n### Patches\nThe vulnerability issue is resolved in Aim v3.1.0.\n\n### References\nhttps:\/\/owasp.org\/www-community\/attacks\/Path_Traversal",
            "published_date":"2021-11-23",
            "chain_len":1,
            "project":"https:\/\/github.com\/aimhubio\/aim",
            "commit_href":"https:\/\/github.com\/aimhubio\/aim\/pull\/1003\/commits\/f01266a1a479ef11d7d6c539e7dd89e9d5639738",
            "commit_sha":"f01266a1a479ef11d7d6c539e7dd89e9d5639738",
            "patch":"SINGLE",
            "chain_ord":"['f01266a1a479ef11d7d6c539e7dd89e9d5639738']",
            "before_first_fix_commit":"{'0bcac8b709f9409518134b2eafee817278aca14f'}",
            "last_fix_commit":"f01266a1a479ef11d7d6c539e7dd89e9d5639738",
            "chain_ord_pos":1.0,
            "commit_datetime":"11\/12\/2021, 14:03:22",
            "message":"Fix security issue when incorrect path is given to the endpoint that serves static files which can lead to a leak of non wanted files (e.g. \/static-files\/..\/..\/..\/..\/etc\/passwd)",
            "author":"mihran113",
            "comments":null,
            "stats":"{'additions': 9, 'deletions': 1, 'total': 10}",
            "files":"{'aim\/web\/api\/views.py': {'additions': 9, 'deletions': 1, 'changes': 10, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/aimhubio\/aim\/raw\/f01266a1a479ef11d7d6c539e7dd89e9d5639738\/aim%2Fweb%2Fapi%2Fviews.py', 'patch': \"@@ -1,15 +1,23 @@\\n import os\\n+from pathlib import Path\\n \\n from aim.web.api.utils import APIRouter  # wrapper for fastapi.APIRouter\\n from fastapi.responses import FileResponse\\n+from fastapi import HTTPException\\n \\n statics_router = APIRouter()\\n \\n \\n @statics_router.get('\/static-files\/{path:path}\/')\\n async def serve_static_files(path):\\n     from aim import web\\n-    static_file_name = os.path.join(os.path.dirname(web.__file__), 'ui', 'build', path)\\n+    static_file_root = os.path.join(os.path.dirname(web.__file__), 'ui', 'build')\\n+    static_file_name = os.path.join(static_file_root, path)\\n+\\n+    # check if path is leading inside ui\/build directory\\n+    if not Path(static_file_root) in Path(static_file_name).resolve().parents:\\n+        raise HTTPException(404)\\n+\\n     compressed_file_name = '{}.gz'.format(static_file_name)\\n     if os.path.exists(compressed_file_name):\\n         return FileResponse(compressed_file_name, headers={'Content-Encoding': 'gzip'})\"}}",
            "message_norm":"fix security issue when incorrect path is given to the endpoint that serves static files which can lead to a leak of non wanted files (e.g. \/static-files\/..\/..\/..\/..\/etc\/passwd)",
            "language":"en",
            "entities":"[('fix', 'ACTION', ''), ('security', 'SECWORD', ''), ('issue', 'FLAW', ''), ('leak', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['aim\/web\/api\/views.py'])",
            "num_files":1.0,
            "patch_content":"From f01266a1a479ef11d7d6c539e7dd89e9d5639738 Mon Sep 17 00:00:00 2001\nFrom: mihran113 <vanyanmihran@gmail.com>\nDate: Fri, 12 Nov 2021 18:03:22 +0400\nSubject: [PATCH] Fix security issue when incorrect path is given to the\n endpoint that serves static files which can lead to a leak of non wanted\n files (e.g. \/static-files\/..\/..\/..\/..\/etc\/passwd)\n\n---\n aim\/web\/api\/views.py | 10 +++++++++-\n 1 file changed, 9 insertions(+), 1 deletion(-)\n\ndiff --git a\/aim\/web\/api\/views.py b\/aim\/web\/api\/views.py\nindex 30edbc3c03..04cad24fd7 100644\n--- a\/aim\/web\/api\/views.py\n+++ b\/aim\/web\/api\/views.py\n@@ -1,7 +1,9 @@\n import os\n+from pathlib import Path\n \n from aim.web.api.utils import APIRouter  # wrapper for fastapi.APIRouter\n from fastapi.responses import FileResponse\n+from fastapi import HTTPException\n \n statics_router = APIRouter()\n \n@@ -9,7 +11,13 @@\n @statics_router.get('\/static-files\/{path:path}\/')\n async def serve_static_files(path):\n     from aim import web\n-    static_file_name = os.path.join(os.path.dirname(web.__file__), 'ui', 'build', path)\n+    static_file_root = os.path.join(os.path.dirname(web.__file__), 'ui', 'build')\n+    static_file_name = os.path.join(static_file_root, path)\n+\n+    # check if path is leading inside ui\/build directory\n+    if not Path(static_file_root) in Path(static_file_name).resolve().parents:\n+        raise HTTPException(404)\n+\n     compressed_file_name = '{}.gz'.format(static_file_name)\n     if os.path.exists(compressed_file_name):\n         return FileResponse(compressed_file_name, headers={'Content-Encoding': 'gzip'})"
        },
        {
            "index":299,
            "vuln_id":"GHSA-pgcq-h79j-2f69",
            "cwe_id":"{'CWE-354'}",
            "score":7.0,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/4d74d8a00b07441cba090a02e0dd9ed385145bf4', 'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/da4aad5946be30e5f049920fa076e1f7ef021261', 'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/e7f497570abb6b4ae5af4970620cd880e4c0c904', 'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/4dddb2fd0b01cdd196101afbba6518658a2c9e07', 'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/579261dcd446385831fe4f7457d802a59685121d', 'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/68422b215e618df5ad375bcdc6d2052e9fd3080a'}",
            "dataset":"osv",
            "summary":"Incomplete validation of shapes in multiple TF ops ### Impact\nSeveral TensorFlow operations are missing validation for the shapes of the tensor arguments involved in the call. Depending on the API, this can result in undefined behavior and segfault or `CHECK`-fail related crashes but in some scenarios writes and reads from heap populated arrays are also possible.\n\nWe have discovered these issues internally via tooling while working on improving\/testing GPU op determinism. As such, we don't have reproducers and there will be multiple fixes for these issues.\n\n### Patches\nWe have patched the issue in GitHub commits [68422b215e618df5ad375bcdc6d2052e9fd3080a](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/68422b215e618df5ad375bcdc6d2052e9fd3080a), [4d74d8a00b07441cba090a02e0dd9ed385145bf4](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/4d74d8a00b07441cba090a02e0dd9ed385145bf4), [579261dcd446385831fe4f7457d802a59685121d](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/579261dcd446385831fe4f7457d802a59685121d), [da4aad5946be30e5f049920fa076e1f7ef021261](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/da4aad5946be30e5f049920fa076e1f7ef021261), [4dddb2fd0b01cdd196101afbba6518658a2c9e07](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/4dddb2fd0b01cdd196101afbba6518658a2c9e07), and [e7f497570abb6b4ae5af4970620cd880e4c0c904](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/e7f497570abb6b4ae5af4970620cd880e4c0c904).\n\nThese fixes will be included in TensorFlow 2.7.0. We will also cherrypick these commits on TensorFlow 2.6.1, TensorFlow 2.5.2, and TensorFlow 2.4.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.",
            "published_date":"2021-11-10",
            "chain_len":6,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/e7f497570abb6b4ae5af4970620cd880e4c0c904",
            "commit_sha":"e7f497570abb6b4ae5af4970620cd880e4c0c904",
            "patch":"MULTI",
            "chain_ord":"['579261dcd446385831fe4f7457d802a59685121d', '4d74d8a00b07441cba090a02e0dd9ed385145bf4', '68422b215e618df5ad375bcdc6d2052e9fd3080a', 'da4aad5946be30e5f049920fa076e1f7ef021261', '4dddb2fd0b01cdd196101afbba6518658a2c9e07', 'e7f497570abb6b4ae5af4970620cd880e4c0c904']",
            "before_first_fix_commit":"{'e0214528739cad3bd02fbf2696a793dc342ffb94'}",
            "last_fix_commit":"e7f497570abb6b4ae5af4970620cd880e4c0c904",
            "chain_ord_pos":6.0,
            "commit_datetime":"10\/20\/2021, 22:41:05",
            "message":"Fix segfault on OOM in Conv2D.\n\nPiperOrigin-RevId: 404655317\nChange-Id: I33588dbd3f5d0fef980e3c908bf5515a9ee09ce7",
            "author":"Reed Wanderman-Milne",
            "comments":null,
            "stats":"{'additions': 12, 'deletions': 3, 'total': 15}",
            "files":"{'tensorflow\/core\/kernels\/conv_ops.cc': {'additions': 12, 'deletions': 3, 'changes': 15, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/e7f497570abb6b4ae5af4970620cd880e4c0c904\/tensorflow%2Fcore%2Fkernels%2Fconv_ops.cc', 'patch': '@@ -183,20 +183,29 @@ struct LaunchGrouped {\\n     auto on_shuffled = [&]() { shuffles_completed.DecrementCount(); };\\n \\n     \/\/ Shuffle input into temporary tensor.\\n-    Tensor input_shuffled(input.dtype(), TensorShape(post_shuffle(input)));\\n+    Tensor input_shuffled;\\n+    OP_REQUIRES_OK(\\n+        ctx, ctx->allocate_temp(input.dtype(), TensorShape(post_shuffle(input)),\\n+                                &input_shuffled));\\n     input_shuffled.tensor<T, 5>().device(device, on_shuffled) =\\n         input.shaped<T, 5>(pre_shuffle(input)).shuffle(shuffle);\\n \\n     \/\/ Shuffle filter into temporary tensor.\\n-    Tensor filter_shuffled(filter.dtype(), TensorShape(post_shuffle(filter)));\\n+    Tensor filter_shuffled;\\n+    OP_REQUIRES_OK(ctx, ctx->allocate_temp(filter.dtype(),\\n+                                           TensorShape(post_shuffle(filter)),\\n+                                           &filter_shuffled));\\n     filter_shuffled.tensor<T, 5>().device(device, on_shuffled) =\\n         filter.shaped<T, 5>(pre_shuffle(filter)).shuffle(shuffle);\\n \\n     \/\/ Wait for the completion of input\/filter shuffles.\\n     shuffles_completed.Wait();\\n \\n     \/\/ Write group convolution results into temporary output tensor.\\n-    Tensor output_shuffled(output->dtype(), TensorShape(post_shuffle(*output)));\\n+    Tensor output_shuffled;\\n+    OP_REQUIRES_OK(ctx, ctx->allocate_temp(output->dtype(),\\n+                                           TensorShape(post_shuffle(*output)),\\n+                                           &output_shuffled));\\n \\n     for (int64_t i = 0; i < num_groups; ++i) {\\n       \/\/ TODO(ezhulenev): Run this loop using `parallelFor` (regular parallelFor'}}",
            "message_norm":"fix segfault on oom in conv2d.\n\npiperorigin-revid: 404655317\nchange-id: i33588dbd3f5d0fef980e3c908bf5515a9ee09ce7",
            "language":"en",
            "entities":"[('fix', 'ACTION', ''), ('segfault', 'SECWORD', ''), ('404655317', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/core\/kernels\/conv_ops.cc'])",
            "num_files":1.0,
            "patch_content":"From e7f497570abb6b4ae5af4970620cd880e4c0c904 Mon Sep 17 00:00:00 2001\nFrom: Reed Wanderman-Milne <reedwm@google.com>\nDate: Wed, 20 Oct 2021 15:41:05 -0700\nSubject: [PATCH] Fix segfault on OOM in Conv2D.\n\nPiperOrigin-RevId: 404655317\nChange-Id: I33588dbd3f5d0fef980e3c908bf5515a9ee09ce7\n---\n tensorflow\/core\/kernels\/conv_ops.cc | 15 ++++++++++++---\n 1 file changed, 12 insertions(+), 3 deletions(-)\n\ndiff --git a\/tensorflow\/core\/kernels\/conv_ops.cc b\/tensorflow\/core\/kernels\/conv_ops.cc\nindex 94926358675fb2..67418151a1cf2d 100644\n--- a\/tensorflow\/core\/kernels\/conv_ops.cc\n+++ b\/tensorflow\/core\/kernels\/conv_ops.cc\n@@ -183,12 +183,18 @@ struct LaunchGrouped {\n     auto on_shuffled = [&]() { shuffles_completed.DecrementCount(); };\n \n     \/\/ Shuffle input into temporary tensor.\n-    Tensor input_shuffled(input.dtype(), TensorShape(post_shuffle(input)));\n+    Tensor input_shuffled;\n+    OP_REQUIRES_OK(\n+        ctx, ctx->allocate_temp(input.dtype(), TensorShape(post_shuffle(input)),\n+                                &input_shuffled));\n     input_shuffled.tensor<T, 5>().device(device, on_shuffled) =\n         input.shaped<T, 5>(pre_shuffle(input)).shuffle(shuffle);\n \n     \/\/ Shuffle filter into temporary tensor.\n-    Tensor filter_shuffled(filter.dtype(), TensorShape(post_shuffle(filter)));\n+    Tensor filter_shuffled;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_temp(filter.dtype(),\n+                                           TensorShape(post_shuffle(filter)),\n+                                           &filter_shuffled));\n     filter_shuffled.tensor<T, 5>().device(device, on_shuffled) =\n         filter.shaped<T, 5>(pre_shuffle(filter)).shuffle(shuffle);\n \n@@ -196,7 +202,10 @@ struct LaunchGrouped {\n     shuffles_completed.Wait();\n \n     \/\/ Write group convolution results into temporary output tensor.\n-    Tensor output_shuffled(output->dtype(), TensorShape(post_shuffle(*output)));\n+    Tensor output_shuffled;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_temp(output->dtype(),\n+                                           TensorShape(post_shuffle(*output)),\n+                                           &output_shuffled));\n \n     for (int64_t i = 0; i < num_groups; ++i) {\n       \/\/ TODO(ezhulenev): Run this loop using `parallelFor` (regular parallelFor"
        },
        {
            "index":617,
            "vuln_id":"GHSA-4wv4-mgfq-598v",
            "cwe_id":"{'CWE-94'}",
            "score":0.0,
            "chain":"{'https:\/\/github.com\/AnneTheDev\/nobelprize\/commit\/00639d375b0efd097bc1eca18d9dc021691b9286'}",
            "dataset":"osv",
            "summary":"Code injection in nobelprizeparser Code injection through use of eval.",
            "published_date":"2021-03-12",
            "chain_len":1,
            "project":"https:\/\/github.com\/AnneTheDev\/nobelprize",
            "commit_href":"https:\/\/github.com\/AnneTheDev\/nobelprize\/commit\/00639d375b0efd097bc1eca18d9dc021691b9286",
            "commit_sha":"00639d375b0efd097bc1eca18d9dc021691b9286",
            "patch":"SINGLE",
            "chain_ord":"['00639d375b0efd097bc1eca18d9dc021691b9286']",
            "before_first_fix_commit":"{'23abc78c8bf9eddce8ec40f0ec7bbb586a3ebe9f', '29126617df6f313d81588d695d94982cba03d82e'}",
            "last_fix_commit":"00639d375b0efd097bc1eca18d9dc021691b9286",
            "chain_ord_pos":1.0,
            "commit_datetime":"02\/23\/2021, 09:03:46",
            "message":"Merge pull request from GHSA-4wv4-mgfq-598v\n\nReplace eval with JSON.parse",
            "author":"AnneTheDev",
            "comments":null,
            "stats":"{'additions': 1, 'deletions': 1, 'total': 2}",
            "files":"{'lib\/index.js': {'additions': 1, 'deletions': 1, 'changes': 2, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/AnneTheDev\/nobelprize\/raw\/00639d375b0efd097bc1eca18d9dc021691b9286\/lib%2Findex.js', 'patch': '@@ -10,7 +10,7 @@ function output(laureate) {\\n class Parser {\\n     \/\/ Parse JSON data\\n     constructor(data) {\\n-        this.laureates = eval(`(${data})`).laureates;\\n+        this.laureates = JSON.parse(data}).laureates;\\n     }\\n \\n     inYear(year) {'}}",
            "message_norm":"merge pull request from ghsa-4wv4-mgfq-598v\n\nreplace eval with json.parse",
            "language":"en",
            "entities":"[('ghsa-4wv4-mgfq-598v', 'VULNID', 'GHSA'), ('eval', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['lib\/index.js'])",
            "num_files":1.0,
            "patch_content":"From 29126617df6f313d81588d695d94982cba03d82e Mon Sep 17 00:00:00 2001\nFrom: AnneTheDev <77233736+AnneTheDev@users.noreply.github.com>\nDate: Tue, 23 Feb 2021 10:00:34 +0100\nSubject: [PATCH] Replace eval with JSON.parse\n\n---\n lib\/index.js | 2 +-\n 1 file changed, 1 insertion(+), 1 deletion(-)\n\ndiff --git a\/lib\/index.js b\/lib\/index.js\nindex 1793e7a..7d04db2 100644\n--- a\/lib\/index.js\n+++ b\/lib\/index.js\n@@ -10,7 +10,7 @@ function output(laureate) {\n class Parser {\n     \/\/ Parse JSON data\n     constructor(data) {\n-        this.laureates = eval(`(${data})`).laureates;\n+        this.laureates = JSON.parse(data}).laureates;\n     }\n \n     inYear(year) {"
        },
        {
            "index":835,
            "vuln_id":"GHSA-cchx-mfrc-fwqr",
            "cwe_id":"{'CWE-200', 'CWE-287'}",
            "score":7.5,
            "chain":"{'https:\/\/github.com\/symfony\/symfony\/commit\/a29ce2817cf43bb1850cf6af114004ac26c7a081'}",
            "dataset":"osv",
            "summary":"Improper authentication in Symfony In Symfony before 2.7.51, 2.8.x before 2.8.50, 3.x before 3.4.26, 4.x before 4.1.12, and 4.2.x before 4.2.7, a vulnerability would allow an attacker to authenticate as a privileged user on sites with user registration and remember me login functionality enabled. This is related to symfony\/security.",
            "published_date":"2020-02-12",
            "chain_len":1,
            "project":"https:\/\/github.com\/symfony\/symfony",
            "commit_href":"https:\/\/github.com\/symfony\/symfony\/commit\/a29ce2817cf43bb1850cf6af114004ac26c7a081",
            "commit_sha":"a29ce2817cf43bb1850cf6af114004ac26c7a081",
            "patch":"SINGLE",
            "chain_ord":"['a29ce2817cf43bb1850cf6af114004ac26c7a081']",
            "before_first_fix_commit":"{'3e0b2354dbc8813a1f5ff91757e1dce40dfe31b4'}",
            "last_fix_commit":"a29ce2817cf43bb1850cf6af114004ac26c7a081",
            "chain_ord_pos":1.0,
            "commit_datetime":"04\/06\/2019, 10:40:18",
            "message":"[Security] Add a separator in the remember me cookie hash",
            "author":"Pascal Borreli",
            "comments":"{'com_1': {'author': 'simoheinonen', 'datetime': '06\/05\/2019, 12:10:12', 'body': 'This logs out all users with the old hash. \ud83d\ude10'}, 'com_2': {'author': 'stof', 'datetime': '06\/05\/2019, 12:18:28', 'body': '@simoheinonen which is better than allowing to spoof remember me cookies'}, 'com_3': {'author': 'simoheinonen', 'datetime': '06\/05\/2019, 12:24:05', 'body': 'Yeah but worth mentioning imo. Logging out thousands of users might cost a lot'}, 'com_4': {'author': 'stefanospetrakis', 'datetime': '06\/25\/2019, 16:29:42', 'body': \"One remark regarding this (a bit too late perhaps);\\r\\nI would like this a little bit shorter for readability\/redundancy\/etc., sth like that:\\r\\n`return hash_hmac('sha256', implode(self::COOKIE_DELIMITER, func_get_args()), $this->getSecret());`\\r\\n\\r\\nAny point to opening a follow-up issue for this?\"}, 'com_5': {'author': 'stof', 'datetime': '06\/25\/2019, 16:37:07', 'body': '@stefanospetrakis this code is less explicit about what gets included in the hash exactly, due to using `func_get_args` instead of the actual arguments. So to me, this actually makes it less readable.'}, 'com_6': {'author': 'stefanospetrakis', 'datetime': '06\/25\/2019, 19:53:54', 'body': '@stof Fair enough, how about the following:\\r\\n\\r\\n`implode(self::COOKIE_DELIMITER, [$class, $username, $expires, $password])`'}}",
            "stats":"{'additions': 1, 'deletions': 1, 'total': 2}",
            "files":"{'src\/Symfony\/Component\/Security\/Http\/RememberMe\/TokenBasedRememberMeServices.php': {'additions': 1, 'deletions': 1, 'changes': 2, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/symfony\/symfony\/raw\/a29ce2817cf43bb1850cf6af114004ac26c7a081\/src%2FSymfony%2FComponent%2FSecurity%2FHttp%2FRememberMe%2FTokenBasedRememberMeServices.php', 'patch': \"@@ -120,6 +120,6 @@ protected function generateCookieValue($class, $username, $expires, $password)\\n      *\/\\n     protected function generateCookieHash($class, $username, $expires, $password)\\n     {\\n-        return hash_hmac('sha256', $class.$username.$expires.$password, $this->getSecret());\\n+        return hash_hmac('sha256', $class.self::COOKIE_DELIMITER.$username.self::COOKIE_DELIMITER.$expires.self::COOKIE_DELIMITER.$password, $this->getSecret());\\n     }\\n }\"}}",
            "message_norm":"[security] add a separator in the remember me cookie hash",
            "language":"en",
            "entities":"[('security', 'SECWORD', ''), ('add', 'ACTION', ''), ('cookie', 'SECWORD', ''), ('hash', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['src\/Symfony\/Component\/Security\/Http\/RememberMe\/TokenBasedRememberMeServices.php'])",
            "num_files":1.0,
            "patch_content":"From a29ce2817cf43bb1850cf6af114004ac26c7a081 Mon Sep 17 00:00:00 2001\nFrom: Pascal Borreli <pascal@borreli.com>\nDate: Sat, 6 Apr 2019 11:40:18 +0100\nSubject: [PATCH] [Security] Add a separator in the remember me cookie hash\n\n---\n ...\/Security\/Http\/RememberMe\/TokenBasedRememberMeServices.php   | 2 +-\n 1 file changed, 1 insertion(+), 1 deletion(-)\n\ndiff --git a\/src\/Symfony\/Component\/Security\/Http\/RememberMe\/TokenBasedRememberMeServices.php b\/src\/Symfony\/Component\/Security\/Http\/RememberMe\/TokenBasedRememberMeServices.php\nindex 48d88e5730b27..952211333930e 100644\n--- a\/src\/Symfony\/Component\/Security\/Http\/RememberMe\/TokenBasedRememberMeServices.php\n+++ b\/src\/Symfony\/Component\/Security\/Http\/RememberMe\/TokenBasedRememberMeServices.php\n@@ -120,6 +120,6 @@ protected function generateCookieValue($class, $username, $expires, $password)\n      *\/\n     protected function generateCookieHash($class, $username, $expires, $password)\n     {\n-        return hash_hmac('sha256', $class.$username.$expires.$password, $this->getSecret());\n+        return hash_hmac('sha256', $class.self::COOKIE_DELIMITER.$username.self::COOKIE_DELIMITER.$expires.self::COOKIE_DELIMITER.$password, $this->getSecret());\n     }\n }"
        },
        {
            "index":515,
            "vuln_id":"GHSA-7mpx-vg3c-cmr4",
            "cwe_id":"{'CWE-287'}",
            "score":8.2,
            "chain":"{'https:\/\/github.com\/salvoravida\/react-adal\/commit\/74158dba1647b12fe96fa401e306a6287fe9e2a9'}",
            "dataset":"osv",
            "summary":"Improper Authentication in react-adal This affects versions of react-adal < 0.5.1. It is possible for a specially crafted JWT token and request URL can cause the nonce, session and refresh values to be incorrectly validated, causing the application to treat an attacker-generated JWT token as authentic. The logical defect is caused by how the nonce, session and refresh values are stored in the browser local storage or session storage. Each key is automatically appended by ||. When the received nonce and session keys are generated, the list of values is stored in the browser storage, separated by ||, with || always appended to the end of the list. Since || will always be the last 2 characters of the stored values, an empty string (\"\") will always be in the list of the valid values. Therefore, if an empty session parameter is provided in the callback URL, and a specially-crafted JWT token contains an nonce value of \"\" (empty string), then adal.js will consider the JWT token as authentic.",
            "published_date":"2021-04-13",
            "chain_len":1,
            "project":"https:\/\/github.com\/salvoravida\/react-adal",
            "commit_href":"https:\/\/github.com\/salvoravida\/react-adal\/commit\/74158dba1647b12fe96fa401e306a6287fe9e2a9",
            "commit_sha":"74158dba1647b12fe96fa401e306a6287fe9e2a9",
            "patch":"SINGLE",
            "chain_ord":"['74158dba1647b12fe96fa401e306a6287fe9e2a9']",
            "before_first_fix_commit":"{'e82bc421d70805ff308e11a1f0f1fcd7fb2b3186'}",
            "last_fix_commit":"74158dba1647b12fe96fa401e306a6287fe9e2a9",
            "chain_ord_pos":1.0,
            "commit_datetime":"10\/05\/2020, 20:19:06",
            "message":"ADAL.js update",
            "author":"Kris Hardy",
            "comments":null,
            "stats":"{'additions': 32, 'deletions': 51, 'total': 83}",
            "files":"{'src\/adal.js': {'additions': 32, 'deletions': 51, 'changes': 83, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/salvoravida\/react-adal\/raw\/74158dba1647b12fe96fa401e306a6287fe9e2a9\/src%2Fadal.js', 'patch': \"@@ -135,10 +135,6 @@ var AuthenticationContext = (function () {\\n         this._openedWindows = [];\\n         this._requestType = this.REQUEST_TYPE.LOGIN;\\n         window._adalInstance = this;\\n-        this._storageSupport = {\\n-            localStorage: null,\\n-            sessionStorage: null\\n-        };\\n \\n         \/\/ validate before constructor assignments\\n         if (config.displayCall && typeof config.displayCall !== 'function') {\\n@@ -813,7 +809,6 @@ var AuthenticationContext = (function () {\\n      * Clears cache items.\\n      *\/\\n     AuthenticationContext.prototype.clearCache = function () {\\n-        this._user = null;\\n         this._saveItem(this.CONSTANTS.STORAGE.LOGIN_REQUEST, '');\\n         this._saveItem(this.CONSTANTS.STORAGE.ANGULAR_LOGIN_REQUEST, '');\\n         this._saveItem(this.CONSTANTS.STORAGE.SESSION_STATE, '');\\n@@ -860,6 +855,7 @@ var AuthenticationContext = (function () {\\n      *\/\\n     AuthenticationContext.prototype.logOut = function () {\\n         this.clearCache();\\n+        this._user = null;\\n         var urlNavigate;\\n \\n         if (this.config.logOutUri) {\\n@@ -928,8 +924,7 @@ var AuthenticationContext = (function () {\\n      * @ignore\\n      *\/\\n     AuthenticationContext.prototype._addHintParameters = function (urlNavigate) {\\n-\\n-        \/\/If you don't use prompt=none, then if the session does not exist, there will be a failure.\\n+        \/\/If you don\ufffdt use prompt=none, then if the session does not exist, there will be a failure.\\n         \/\/If sid is sent alongside domain or login hints, there will be a failure since request is ambiguous.\\n         \/\/If sid is sent with a prompt value other than none or attempt_none, there will be a failure since the request is ambiguous.\\n \\n@@ -1103,7 +1098,7 @@ var AuthenticationContext = (function () {\\n         if (requestNonce) {\\n             requestNonce = requestNonce.split(this.CONSTANTS.CACHE_DELIMETER);\\n             for (var i = 0; i < requestNonce.length; i++) {\\n-                if (requestNonce[i] === user.profile.nonce) {\\n+                if (requestNonce[i] && requestNonce[i] === user.profile.nonce) {\\n                     return true;\\n                 }\\n             }\\n@@ -1122,7 +1117,7 @@ var AuthenticationContext = (function () {\\n         if (loginStates) {\\n             loginStates = loginStates.split(this.CONSTANTS.CACHE_DELIMETER);\\n             for (var i = 0; i < loginStates.length; i++) {\\n-                if (loginStates[i] === requestInfo.stateResponse) {\\n+                if (loginStates[i] && loginStates[i] === requestInfo.stateResponse) {\\n                     requestInfo.requestType = this.REQUEST_TYPE.LOGIN;\\n                     requestInfo.stateMatch = true;\\n                     return true;\\n@@ -1135,7 +1130,7 @@ var AuthenticationContext = (function () {\\n         if (acquireTokenStates) {\\n             acquireTokenStates = acquireTokenStates.split(this.CONSTANTS.CACHE_DELIMETER);\\n             for (var i = 0; i < acquireTokenStates.length; i++) {\\n-                if (acquireTokenStates[i] === requestInfo.stateResponse) {\\n+                if (acquireTokenStates[i] && acquireTokenStates[i] === requestInfo.stateResponse) {\\n                     requestInfo.requestType = this.REQUEST_TYPE.RENEW_TOKEN;\\n                     requestInfo.stateMatch = true;\\n                     return true;\\n@@ -1218,16 +1213,17 @@ var AuthenticationContext = (function () {\\n                             this._user = null;\\n                         } else {\\n                             this._saveItem(this.CONSTANTS.STORAGE.IDTOKEN, requestInfo.parameters[this.CONSTANTS.ID_TOKEN]);\\n+\\n                             \/\/ Save idtoken as access token for app itself\\n-                            var idTokenResource = this.config.loginResource ? this.config.loginResource : this.config.clientId;\\n+                            resource = this.config.loginResource ? this.config.loginResource : this.config.clientId;\\n \\n-                            if (!this._hasResource(idTokenResource)) {\\n+                            if (!this._hasResource(resource)) {\\n                                 keys = this._getItem(this.CONSTANTS.STORAGE.TOKEN_KEYS) || '';\\n-                                this._saveItem(this.CONSTANTS.STORAGE.TOKEN_KEYS, keys + idTokenResource + this.CONSTANTS.RESOURCE_DELIMETER);\\n+                                this._saveItem(this.CONSTANTS.STORAGE.TOKEN_KEYS, keys + resource + this.CONSTANTS.RESOURCE_DELIMETER);\\n                             }\\n \\n-                            this._saveItem(this.CONSTANTS.STORAGE.ACCESS_TOKEN_KEY + idTokenResource, requestInfo.parameters[this.CONSTANTS.ID_TOKEN]);\\n-                            this._saveItem(this.CONSTANTS.STORAGE.EXPIRATION_KEY + idTokenResource, this._user.profile.exp);\\n+                            this._saveItem(this.CONSTANTS.STORAGE.ACCESS_TOKEN_KEY + resource, requestInfo.parameters[this.CONSTANTS.ID_TOKEN]);\\n+                            this._saveItem(this.CONSTANTS.STORAGE.EXPIRATION_KEY + resource, this._user.profile.exp);\\n                         }\\n                     }\\n                     else {\\n@@ -1689,7 +1685,7 @@ var AuthenticationContext = (function () {\\n                 ifr.setAttribute('aria-hidden', 'true');\\n                 ifr.style.visibility = 'hidden';\\n                 ifr.style.position = 'absolute';\\n-                ifr.style.width = ifr.style.height = ifr.style.borderWidth = '0px';\\n+                ifr.style.width = ifr.style.height = ifr.borderWidth = '0px';\\n \\n                 adalFrame = document.getElementsByTagName('body')[0].appendChild(ifr);\\n             }\\n@@ -1764,52 +1760,37 @@ var AuthenticationContext = (function () {\\n     };\\n \\n     \/**\\n-     * Returns true if the browser supports given storage type\\n+     * Returns true if browser supports localStorage, false otherwise.\\n      * @ignore\\n      *\/\\n-    AuthenticationContext.prototype._supportsStorage = function(storageType) {\\n-        if (!(storageType in this._storageSupport)) {\\n-            return false;\\n-        }\\n-\\n-        if (this._storageSupport[storageType] !== null) {\\n-            return this._storageSupport[storageType];\\n-        }\\n-\\n+    AuthenticationContext.prototype._supportsLocalStorage = function () {\\n         try {\\n-            if (!(storageType in window) || window[storageType] === null) {\\n-                throw new Error();\\n-            }\\n-            var testKey = '__storageTest__';\\n-            window[storageType].setItem(testKey, 'A');\\n-            if (window[storageType].getItem(testKey) !== 'A') {\\n-                throw new Error();\\n-            }\\n-            window[storageType].removeItem(testKey);\\n-            if (window[storageType].getItem(testKey)) {\\n-                throw new Error();\\n-            }\\n-            this._storageSupport[storageType] = true;\\n+            if (!window.localStorage) return false; \/\/ Test availability\\n+            window.localStorage.setItem('storageTest', 'A'); \/\/ Try write\\n+            if (window.localStorage.getItem('storageTest') != 'A') return false; \/\/ Test read\/write\\n+            window.localStorage.removeItem('storageTest'); \/\/ Try delete\\n+            if (window.localStorage.getItem('storageTest')) return false; \/\/ Test delete\\n+            return true; \/\/ Success\\n         } catch (e) {\\n-            this._storageSupport[storageType] = false;\\n+            return false;\\n         }\\n-        return this._storageSupport[storageType];\\n-    }\\n-\\n-    \/**\\n-     * Returns true if browser supports localStorage, false otherwise.\\n-     * @ignore\\n-     *\/\\n-    AuthenticationContext.prototype._supportsLocalStorage = function () {        \\n-        return this._supportsStorage('localStorage');\\n     };\\n \\n     \/**\\n      * Returns true if browser supports sessionStorage, false otherwise.\\n      * @ignore\\n      *\/\\n     AuthenticationContext.prototype._supportsSessionStorage = function () {\\n-        return this._supportsStorage('sessionStorage');\\n+        try {\\n+            if (!window.sessionStorage) return false; \/\/ Test availability\\n+            window.sessionStorage.setItem('storageTest', 'A'); \/\/ Try write\\n+            if (window.sessionStorage.getItem('storageTest') != 'A') return false; \/\/ Test read\/write\\n+            window.sessionStorage.removeItem('storageTest'); \/\/ Try delete\\n+            if (window.sessionStorage.getItem('storageTest')) return false; \/\/ Test delete\\n+            return true; \/\/ Success\\n+        } catch (e) {\\n+            return false;\\n+        }\\n     };\\n \\n     \/**\\n@@ -1955,4 +1936,4 @@ var AuthenticationContext = (function () {\\n \\n     return AuthenticationContext;\\n \\n-}());\\n\\\\ No newline at end of file\\n+}());\"}}",
            "message_norm":"adal.js update",
            "language":"id",
            "entities":null,
            "classification_level_1":"POORLY_DOCUMENTED",
            "classification_level_2":"SUBMIT_CENTERED",
            "list_files":"dict_keys(['src\/adal.js'])",
            "num_files":1.0,
            "patch_content":"From 74158dba1647b12fe96fa401e306a6287fe9e2a9 Mon Sep 17 00:00:00 2001\nFrom: Kris Hardy <kris@rkrishardy.com>\nDate: Tue, 6 Oct 2020 09:19:06 +1300\nSubject: [PATCH] ADAL.js update\n\n---\n src\/adal.js | 83 +++++++++++++++++++++--------------------------------\n 1 file changed, 32 insertions(+), 51 deletions(-)\n\ndiff --git a\/src\/adal.js b\/src\/adal.js\nindex 5e9a074..235788a 100644\n--- a\/src\/adal.js\n+++ b\/src\/adal.js\n@@ -135,10 +135,6 @@ var AuthenticationContext = (function () {\n         this._openedWindows = [];\n         this._requestType = this.REQUEST_TYPE.LOGIN;\n         window._adalInstance = this;\n-        this._storageSupport = {\n-            localStorage: null,\n-            sessionStorage: null\n-        };\n \n         \/\/ validate before constructor assignments\n         if (config.displayCall && typeof config.displayCall !== 'function') {\n@@ -813,7 +809,6 @@ var AuthenticationContext = (function () {\n      * Clears cache items.\n      *\/\n     AuthenticationContext.prototype.clearCache = function () {\n-        this._user = null;\n         this._saveItem(this.CONSTANTS.STORAGE.LOGIN_REQUEST, '');\n         this._saveItem(this.CONSTANTS.STORAGE.ANGULAR_LOGIN_REQUEST, '');\n         this._saveItem(this.CONSTANTS.STORAGE.SESSION_STATE, '');\n@@ -860,6 +855,7 @@ var AuthenticationContext = (function () {\n      *\/\n     AuthenticationContext.prototype.logOut = function () {\n         this.clearCache();\n+        this._user = null;\n         var urlNavigate;\n \n         if (this.config.logOutUri) {\n@@ -928,8 +924,7 @@ var AuthenticationContext = (function () {\n      * @ignore\n      *\/\n     AuthenticationContext.prototype._addHintParameters = function (urlNavigate) {\n-\n-        \/\/If you don't use prompt=none, then if the session does not exist, there will be a failure.\n+        \/\/If you don\ufffdt use prompt=none, then if the session does not exist, there will be a failure.\n         \/\/If sid is sent alongside domain or login hints, there will be a failure since request is ambiguous.\n         \/\/If sid is sent with a prompt value other than none or attempt_none, there will be a failure since the request is ambiguous.\n \n@@ -1103,7 +1098,7 @@ var AuthenticationContext = (function () {\n         if (requestNonce) {\n             requestNonce = requestNonce.split(this.CONSTANTS.CACHE_DELIMETER);\n             for (var i = 0; i < requestNonce.length; i++) {\n-                if (requestNonce[i] === user.profile.nonce) {\n+                if (requestNonce[i] && requestNonce[i] === user.profile.nonce) {\n                     return true;\n                 }\n             }\n@@ -1122,7 +1117,7 @@ var AuthenticationContext = (function () {\n         if (loginStates) {\n             loginStates = loginStates.split(this.CONSTANTS.CACHE_DELIMETER);\n             for (var i = 0; i < loginStates.length; i++) {\n-                if (loginStates[i] === requestInfo.stateResponse) {\n+                if (loginStates[i] && loginStates[i] === requestInfo.stateResponse) {\n                     requestInfo.requestType = this.REQUEST_TYPE.LOGIN;\n                     requestInfo.stateMatch = true;\n                     return true;\n@@ -1135,7 +1130,7 @@ var AuthenticationContext = (function () {\n         if (acquireTokenStates) {\n             acquireTokenStates = acquireTokenStates.split(this.CONSTANTS.CACHE_DELIMETER);\n             for (var i = 0; i < acquireTokenStates.length; i++) {\n-                if (acquireTokenStates[i] === requestInfo.stateResponse) {\n+                if (acquireTokenStates[i] && acquireTokenStates[i] === requestInfo.stateResponse) {\n                     requestInfo.requestType = this.REQUEST_TYPE.RENEW_TOKEN;\n                     requestInfo.stateMatch = true;\n                     return true;\n@@ -1218,16 +1213,17 @@ var AuthenticationContext = (function () {\n                             this._user = null;\n                         } else {\n                             this._saveItem(this.CONSTANTS.STORAGE.IDTOKEN, requestInfo.parameters[this.CONSTANTS.ID_TOKEN]);\n+\n                             \/\/ Save idtoken as access token for app itself\n-                            var idTokenResource = this.config.loginResource ? this.config.loginResource : this.config.clientId;\n+                            resource = this.config.loginResource ? this.config.loginResource : this.config.clientId;\n \n-                            if (!this._hasResource(idTokenResource)) {\n+                            if (!this._hasResource(resource)) {\n                                 keys = this._getItem(this.CONSTANTS.STORAGE.TOKEN_KEYS) || '';\n-                                this._saveItem(this.CONSTANTS.STORAGE.TOKEN_KEYS, keys + idTokenResource + this.CONSTANTS.RESOURCE_DELIMETER);\n+                                this._saveItem(this.CONSTANTS.STORAGE.TOKEN_KEYS, keys + resource + this.CONSTANTS.RESOURCE_DELIMETER);\n                             }\n \n-                            this._saveItem(this.CONSTANTS.STORAGE.ACCESS_TOKEN_KEY + idTokenResource, requestInfo.parameters[this.CONSTANTS.ID_TOKEN]);\n-                            this._saveItem(this.CONSTANTS.STORAGE.EXPIRATION_KEY + idTokenResource, this._user.profile.exp);\n+                            this._saveItem(this.CONSTANTS.STORAGE.ACCESS_TOKEN_KEY + resource, requestInfo.parameters[this.CONSTANTS.ID_TOKEN]);\n+                            this._saveItem(this.CONSTANTS.STORAGE.EXPIRATION_KEY + resource, this._user.profile.exp);\n                         }\n                     }\n                     else {\n@@ -1689,7 +1685,7 @@ var AuthenticationContext = (function () {\n                 ifr.setAttribute('aria-hidden', 'true');\n                 ifr.style.visibility = 'hidden';\n                 ifr.style.position = 'absolute';\n-                ifr.style.width = ifr.style.height = ifr.style.borderWidth = '0px';\n+                ifr.style.width = ifr.style.height = ifr.borderWidth = '0px';\n \n                 adalFrame = document.getElementsByTagName('body')[0].appendChild(ifr);\n             }\n@@ -1764,44 +1760,20 @@ var AuthenticationContext = (function () {\n     };\n \n     \/**\n-     * Returns true if the browser supports given storage type\n+     * Returns true if browser supports localStorage, false otherwise.\n      * @ignore\n      *\/\n-    AuthenticationContext.prototype._supportsStorage = function(storageType) {\n-        if (!(storageType in this._storageSupport)) {\n-            return false;\n-        }\n-\n-        if (this._storageSupport[storageType] !== null) {\n-            return this._storageSupport[storageType];\n-        }\n-\n+    AuthenticationContext.prototype._supportsLocalStorage = function () {\n         try {\n-            if (!(storageType in window) || window[storageType] === null) {\n-                throw new Error();\n-            }\n-            var testKey = '__storageTest__';\n-            window[storageType].setItem(testKey, 'A');\n-            if (window[storageType].getItem(testKey) !== 'A') {\n-                throw new Error();\n-            }\n-            window[storageType].removeItem(testKey);\n-            if (window[storageType].getItem(testKey)) {\n-                throw new Error();\n-            }\n-            this._storageSupport[storageType] = true;\n+            if (!window.localStorage) return false; \/\/ Test availability\n+            window.localStorage.setItem('storageTest', 'A'); \/\/ Try write\n+            if (window.localStorage.getItem('storageTest') != 'A') return false; \/\/ Test read\/write\n+            window.localStorage.removeItem('storageTest'); \/\/ Try delete\n+            if (window.localStorage.getItem('storageTest')) return false; \/\/ Test delete\n+            return true; \/\/ Success\n         } catch (e) {\n-            this._storageSupport[storageType] = false;\n+            return false;\n         }\n-        return this._storageSupport[storageType];\n-    }\n-\n-    \/**\n-     * Returns true if browser supports localStorage, false otherwise.\n-     * @ignore\n-     *\/\n-    AuthenticationContext.prototype._supportsLocalStorage = function () {        \n-        return this._supportsStorage('localStorage');\n     };\n \n     \/**\n@@ -1809,7 +1781,16 @@ var AuthenticationContext = (function () {\n      * @ignore\n      *\/\n     AuthenticationContext.prototype._supportsSessionStorage = function () {\n-        return this._supportsStorage('sessionStorage');\n+        try {\n+            if (!window.sessionStorage) return false; \/\/ Test availability\n+            window.sessionStorage.setItem('storageTest', 'A'); \/\/ Try write\n+            if (window.sessionStorage.getItem('storageTest') != 'A') return false; \/\/ Test read\/write\n+            window.sessionStorage.removeItem('storageTest'); \/\/ Try delete\n+            if (window.sessionStorage.getItem('storageTest')) return false; \/\/ Test delete\n+            return true; \/\/ Success\n+        } catch (e) {\n+            return false;\n+        }\n     };\n \n     \/**\n@@ -1955,4 +1936,4 @@ var AuthenticationContext = (function () {\n \n     return AuthenticationContext;\n \n-}());\n\\ No newline at end of file\n+}());"
        },
        {
            "index":346,
            "vuln_id":"GHSA-xc7v-wxcw-j472",
            "cwe_id":"{'CWE-200'}",
            "score":0.0,
            "chain":"{'https:\/\/github.com\/request\/tunnel-agent\/commit\/9ca95ec7219daface8a6fc2674000653de0922c0'}",
            "dataset":"osv",
            "summary":"Memory Exposure in tunnel-agent Versions of `tunnel-agent` before 0.6.0 are vulnerable to memory exposure.\n\nThis is exploitable if user supplied input is provided to the auth value and is a number.\n\nProof-of-concept:\n```js\nrequire('request')({\n  method: 'GET',\n  uri: 'http:\/\/www.example.com',\n  tunnel: true,\n  proxy:{\n    protocol: 'http:',\n    host:'127.0.0.1',\n    port:8080,\n    auth:USERSUPPLIEDINPUT \/\/ number\n  }\n});\n```\n\n\n## Recommendation\n\nUpdate to version 0.6.0 or later.",
            "published_date":"2019-06-03",
            "chain_len":1,
            "project":"https:\/\/github.com\/request\/tunnel-agent",
            "commit_href":"https:\/\/github.com\/request\/tunnel-agent\/commit\/9ca95ec7219daface8a6fc2674000653de0922c0",
            "commit_sha":"9ca95ec7219daface8a6fc2674000653de0922c0",
            "patch":"SINGLE",
            "chain_ord":"['9ca95ec7219daface8a6fc2674000653de0922c0']",
            "before_first_fix_commit":"{'8a7c86e6e2a1c3fa8577e5b0e14923d54c659552'}",
            "last_fix_commit":"9ca95ec7219daface8a6fc2674000653de0922c0",
            "chain_ord_pos":1.0,
            "commit_datetime":"03\/05\/2017, 00:29:52",
            "message":"Use .from",
            "author":"Mikeal Rogers",
            "comments":null,
            "stats":"{'additions': 1, 'deletions': 1, 'total': 2}",
            "files":"{'index.js': {'additions': 1, 'deletions': 1, 'changes': 2, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/request\/tunnel-agent\/raw\/9ca95ec7219daface8a6fc2674000653de0922c0\/index.js', 'patch': \"@@ -128,7 +128,7 @@ TunnelingAgent.prototype.createSocket = function createSocket(options, cb) {\\n   if (connectOptions.proxyAuth) {\\n     connectOptions.headers = connectOptions.headers || {}\\n     connectOptions.headers['Proxy-Authorization'] = 'Basic ' +\\n-        new Buffer(connectOptions.proxyAuth).toString('base64')\\n+        Buffer.from(connectOptions.proxyAuth).toString('base64')\\n   }\\n \\n   debug('making CONNECT request')\"}}",
            "message_norm":"use .from",
            "language":"en",
            "entities":null,
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['index.js'])",
            "num_files":1.0,
            "patch_content":"From 9ca95ec7219daface8a6fc2674000653de0922c0 Mon Sep 17 00:00:00 2001\nFrom: Mikeal Rogers <mikeal.rogers@gmail.com>\nDate: Sat, 4 Mar 2017 16:29:52 -0800\nSubject: [PATCH] Use .from\n\n---\n index.js | 2 +-\n 1 file changed, 1 insertion(+), 1 deletion(-)\n\ndiff --git a\/index.js b\/index.js\nindex cb63452..3ee9abc 100644\n--- a\/index.js\n+++ b\/index.js\n@@ -128,7 +128,7 @@ TunnelingAgent.prototype.createSocket = function createSocket(options, cb) {\n   if (connectOptions.proxyAuth) {\n     connectOptions.headers = connectOptions.headers || {}\n     connectOptions.headers['Proxy-Authorization'] = 'Basic ' +\n-        new Buffer(connectOptions.proxyAuth).toString('base64')\n+        Buffer.from(connectOptions.proxyAuth).toString('base64')\n   }\n \n   debug('making CONNECT request')"
        },
        {
            "index":656,
            "vuln_id":"GHSA-hxf9-7h4c-f5jv",
            "cwe_id":"{'CWE-200'}",
            "score":9.1,
            "chain":"{'https:\/\/github.com\/anymail\/django-anymail\/commit\/db586ede1fbb41dce21310ea28ae15a1cf1286c5', 'https:\/\/github.com\/anymail\/django-anymail\/commit\/c07998304b4a31df4c61deddcb03d3607a04691b'}",
            "dataset":"osv",
            "summary":"Django-Anymail prone to a timing attack webhooks\/base.py in Anymail (aka django-anymail) before 1.2.1 is prone to a timing attack vulnerability on the WEBHOOK_AUTHORIZATION secret, which allows remote attackers to post arbitrary e-mail tracking events.",
            "published_date":"2018-07-12",
            "chain_len":2,
            "project":"https:\/\/github.com\/anymail\/django-anymail",
            "commit_href":"https:\/\/github.com\/anymail\/django-anymail\/commit\/db586ede1fbb41dce21310ea28ae15a1cf1286c5",
            "commit_sha":"db586ede1fbb41dce21310ea28ae15a1cf1286c5",
            "patch":"MULTI",
            "chain_ord":"['db586ede1fbb41dce21310ea28ae15a1cf1286c5', 'c07998304b4a31df4c61deddcb03d3607a04691b']",
            "before_first_fix_commit":"{'7029298b930620b1655dab2548f72d6640a5905e'}",
            "last_fix_commit":"c07998304b4a31df4c61deddcb03d3607a04691b",
            "chain_ord_pos":1.0,
            "commit_datetime":"02\/02\/2018, 19:41:14",
            "message":"Security: prevent timing attack on WEBHOOK_AUTHORIZATION secret\n\nAnymail's webhook validation was vulnerable to a timing attack.\nAn attacker could have used this to recover your WEBHOOK_AUTHORIZATION\nshared secret, potentially allowing them to post fabricated or malicious\nemail tracking events to your app.\n\nThere have not been any reports of attempted exploit in the wild. (The\nvulnerability was discovered through code review.) Attempts would be\nvisible in http logs as a very large number of 400 responses on\nAnymail's webhook urls, or in Python error monitoring as a very large\nnumber of AnymailWebhookValidationFailure exceptions.\n\nIf you are using Anymail's webhooks, you should upgrade to this release.\nIn addition, you may want to rotate to a new WEBHOOK_AUTHORIZATION\nsecret ([docs](http:\/\/anymail.readthedocs.io\/en\/stable\/tips\/securing_webhooks\/#use-a-shared-authorization-secret)),\nparticularly if your logs indicate attempted exploit.",
            "author":"medmunds",
            "comments":null,
            "stats":"{'additions': 12, 'deletions': 3, 'total': 15}",
            "files":"{'anymail\/webhooks\/base.py': {'additions': 12, 'deletions': 3, 'changes': 15, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/anymail\/django-anymail\/raw\/db586ede1fbb41dce21310ea28ae15a1cf1286c5\/anymail%2Fwebhooks%2Fbase.py', 'patch': '@@ -2,6 +2,7 @@\\n \\n import six\\n from django.http import HttpResponse\\n+from django.utils.crypto import constant_time_compare\\n from django.utils.decorators import method_decorator\\n from django.views.decorators.csrf import csrf_exempt\\n from django.views.generic import View\\n@@ -40,8 +41,13 @@ def __init__(self, **kwargs):\\n     def validate_request(self, request):\\n         \"\"\"If configured for webhook basic auth, validate request has correct auth.\"\"\"\\n         if self.basic_auth:\\n-            basic_auth = get_request_basic_auth(request)\\n-            if basic_auth is None or basic_auth not in self.basic_auth:\\n+            request_auth = get_request_basic_auth(request)\\n+            # Use constant_time_compare to avoid timing attack on basic auth. (It\\'s OK that any()\\n+            # can terminate early: we\\'re not trying to protect how many auth strings are allowed,\\n+            # just the contents of each individual auth string.)\\n+            auth_ok = any(constant_time_compare(request_auth, allowed_auth)\\n+                          for allowed_auth in self.basic_auth)\\n+            if not auth_ok:\\n                 # noinspection PyUnresolvedReferences\\n                 raise AnymailWebhookValidationFailure(\\n                     \"Missing or invalid basic auth in Anymail %s webhook\" % self.esp_name)\\n@@ -77,8 +83,11 @@ def validate_request(self, request):\\n         *All* definitions of this method in the class chain (including mixins)\\n         will be called. There is no need to chain to the superclass.\\n         (See self.run_validators and collect_all_methods.)\\n+\\n+        Security note: use django.utils.crypto.constant_time_compare for string\\n+        comparisons, to avoid exposing your validation to a timing attack.\\n         \"\"\"\\n-        # if request.POST[\\'signature\\'] != expected_signature:\\n+        # if not constant_time_compare(request.POST[\\'signature\\'], expected_signature):\\n         #     raise AnymailWebhookValidationFailure(\"...message...\")\\n         # (else just do nothing)\\n         pass'}}",
            "message_norm":"security: prevent timing attack on webhook_authorization secret\n\nanymail's webhook validation was vulnerable to a timing attack.\nan attacker could have used this to recover your webhook_authorization\nshared secret, potentially allowing them to post fabricated or malicious\nemail tracking events to your app.\n\nthere have not been any reports of attempted exploit in the wild. (the\nvulnerability was discovered through code review.) attempts would be\nvisible in http logs as a very large number of 400 responses on\nanymail's webhook urls, or in python error monitoring as a very large\nnumber of anymailwebhookvalidationfailure exceptions.\n\nif you are using anymail's webhooks, you should upgrade to this release.\nin addition, you may want to rotate to a new webhook_authorization\nsecret ([docs](http:\/\/anymail.readthedocs.io\/en\/stable\/tips\/securing_webhooks\/#use-a-shared-authorization-secret)),\nparticularly if your logs indicate attempted exploit.",
            "language":"en",
            "entities":"[('security', 'SECWORD', ''), ('prevent', 'ACTION', ''), ('attack', 'SECWORD', ''), ('vulnerable', 'SECWORD', ''), ('attack', 'FLAW', ''), ('attacker', 'SECWORD', ''), ('malicious', 'SECWORD', ''), ('exploit', 'SECWORD', ''), ('vulnerability', 'SECWORD', ''), ('error', 'FLAW', ''), ('upgrade', 'ACTION', ''), ('docs](http:\/\/anymail.readthedocs.io', 'URL', ''), ('exploit', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['anymail\/webhooks\/base.py'])",
            "num_files":1.0,
            "patch_content":"From db586ede1fbb41dce21310ea28ae15a1cf1286c5 Mon Sep 17 00:00:00 2001\nFrom: medmunds <medmunds@gmail.com>\nDate: Fri, 2 Feb 2018 11:41:14 -0800\nSubject: [PATCH] Security: prevent timing attack on WEBHOOK_AUTHORIZATION\n secret\n\nAnymail's webhook validation was vulnerable to a timing attack.\nAn attacker could have used this to recover your WEBHOOK_AUTHORIZATION\nshared secret, potentially allowing them to post fabricated or malicious\nemail tracking events to your app.\n\nThere have not been any reports of attempted exploit in the wild. (The\nvulnerability was discovered through code review.) Attempts would be\nvisible in http logs as a very large number of 400 responses on\nAnymail's webhook urls, or in Python error monitoring as a very large\nnumber of AnymailWebhookValidationFailure exceptions.\n\nIf you are using Anymail's webhooks, you should upgrade to this release.\nIn addition, you may want to rotate to a new WEBHOOK_AUTHORIZATION\nsecret ([docs](http:\/\/anymail.readthedocs.io\/en\/stable\/tips\/securing_webhooks\/#use-a-shared-authorization-secret)),\nparticularly if your logs indicate attempted exploit.\n---\n anymail\/webhooks\/base.py | 15 ++++++++++++---\n 1 file changed, 12 insertions(+), 3 deletions(-)\n\ndiff --git a\/anymail\/webhooks\/base.py b\/anymail\/webhooks\/base.py\nindex 28380311..2bfd36e0 100644\n--- a\/anymail\/webhooks\/base.py\n+++ b\/anymail\/webhooks\/base.py\n@@ -2,6 +2,7 @@\n \n import six\n from django.http import HttpResponse\n+from django.utils.crypto import constant_time_compare\n from django.utils.decorators import method_decorator\n from django.views.decorators.csrf import csrf_exempt\n from django.views.generic import View\n@@ -40,8 +41,13 @@ def __init__(self, **kwargs):\n     def validate_request(self, request):\n         \"\"\"If configured for webhook basic auth, validate request has correct auth.\"\"\"\n         if self.basic_auth:\n-            basic_auth = get_request_basic_auth(request)\n-            if basic_auth is None or basic_auth not in self.basic_auth:\n+            request_auth = get_request_basic_auth(request)\n+            # Use constant_time_compare to avoid timing attack on basic auth. (It's OK that any()\n+            # can terminate early: we're not trying to protect how many auth strings are allowed,\n+            # just the contents of each individual auth string.)\n+            auth_ok = any(constant_time_compare(request_auth, allowed_auth)\n+                          for allowed_auth in self.basic_auth)\n+            if not auth_ok:\n                 # noinspection PyUnresolvedReferences\n                 raise AnymailWebhookValidationFailure(\n                     \"Missing or invalid basic auth in Anymail %s webhook\" % self.esp_name)\n@@ -77,8 +83,11 @@ def validate_request(self, request):\n         *All* definitions of this method in the class chain (including mixins)\n         will be called. There is no need to chain to the superclass.\n         (See self.run_validators and collect_all_methods.)\n+\n+        Security note: use django.utils.crypto.constant_time_compare for string\n+        comparisons, to avoid exposing your validation to a timing attack.\n         \"\"\"\n-        # if request.POST['signature'] != expected_signature:\n+        # if not constant_time_compare(request.POST['signature'], expected_signature):\n         #     raise AnymailWebhookValidationFailure(\"...message...\")\n         # (else just do nothing)\n         pass"
        },
        {
            "index":741,
            "vuln_id":"GHSA-vfr4-x8j2-3rf9",
            "cwe_id":"{'CWE-369'}",
            "score":2.5,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/801c1c6be5324219689c98e1bd3e0ca365ee834d'}",
            "dataset":"osv",
            "summary":"Division by zero in TFLite's implementation of `TransposeConv` ### Impact\nThe optimized implementation of the `TransposeConv` TFLite operator is [vulnerable to a division by zero error](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/0d45ea1ca641b21b73bcf9c00e0179cda284e7e7\/tensorflow\/lite\/kernels\/internal\/optimized\/optimized_ops.h#L5221-L5222):\n\n```cc \nint height_col = (height + pad_t + pad_b - filter_h) \/ stride_h + 1;\nint width_col = (width + pad_l + pad_r - filter_w) \/ stride_w + 1;\n```\n\nAn attacker can craft a model such that `stride_{h,w}` values are 0. Code calling this function must validate these arguments.\n\n### Patches\nWe have patched the issue in GitHub commit [801c1c6be5324219689c98e1bd3e0ca365ee834d](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/801c1c6be5324219689c98e1bd3e0ca365ee834d).\n\nThe fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by members of the Aivul Team from Qihoo 360.",
            "published_date":"2021-05-21",
            "chain_len":1,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/801c1c6be5324219689c98e1bd3e0ca365ee834d",
            "commit_sha":"801c1c6be5324219689c98e1bd3e0ca365ee834d",
            "patch":"SINGLE",
            "chain_ord":"['801c1c6be5324219689c98e1bd3e0ca365ee834d']",
            "before_first_fix_commit":"{'8e45822aa0b9f5df4b4c64f221e64dc930a70a9d'}",
            "last_fix_commit":"801c1c6be5324219689c98e1bd3e0ca365ee834d",
            "chain_ord_pos":1.0,
            "commit_datetime":"04\/28\/2021, 00:46:25",
            "message":"Fix another division by 0 in TFLite\n\nPiperOrigin-RevId: 370800181\nChange-Id: I924809166a6131f5075e6d45c455106538d755f9",
            "author":"Mihai Maruseac",
            "comments":null,
            "stats":"{'additions': 4, 'deletions': 0, 'total': 4}",
            "files":"{'tensorflow\/lite\/kernels\/transpose_conv.cc': {'additions': 4, 'deletions': 0, 'changes': 4, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/801c1c6be5324219689c98e1bd3e0ca365ee834d\/tensorflow%2Flite%2Fkernels%2Ftranspose_conv.cc', 'patch': '@@ -591,6 +591,10 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\\n   const auto* params =\\n       reinterpret_cast<TfLiteTransposeConvParams*>(node->builtin_data);\\n \\n+  \/\/ Prevent divisions by 0\\n+  TF_LITE_ENSURE(context, params->stride_height > 0);\\n+  TF_LITE_ENSURE(context, params->stride_width > 0);\\n+\\n   \/\/ Resize any deferred dynamic tensors\\n   if (IsDynamicTensor(output)) {\\n     TF_LITE_ENSURE_OK(context, ResizeTensor(context, output_shape, output));'}}",
            "message_norm":"fix another division by 0 in tflite\n\npiperorigin-revid: 370800181\nchange-id: i924809166a6131f5075e6d45c455106538d755f9",
            "language":"en",
            "entities":"[('fix', 'ACTION', ''), ('division by 0', 'SECWORD', ''), ('370800181', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/lite\/kernels\/transpose_conv.cc'])",
            "num_files":1.0,
            "patch_content":"From 801c1c6be5324219689c98e1bd3e0ca365ee834d Mon Sep 17 00:00:00 2001\nFrom: Mihai Maruseac <mihaimaruseac@google.com>\nDate: Tue, 27 Apr 2021 17:46:25 -0700\nSubject: [PATCH] Fix another division by 0 in TFLite\n\nPiperOrigin-RevId: 370800181\nChange-Id: I924809166a6131f5075e6d45c455106538d755f9\n---\n tensorflow\/lite\/kernels\/transpose_conv.cc | 4 ++++\n 1 file changed, 4 insertions(+)\n\ndiff --git a\/tensorflow\/lite\/kernels\/transpose_conv.cc b\/tensorflow\/lite\/kernels\/transpose_conv.cc\nindex 497edac5762dce..cf9d53fb3b6ee7 100644\n--- a\/tensorflow\/lite\/kernels\/transpose_conv.cc\n+++ b\/tensorflow\/lite\/kernels\/transpose_conv.cc\n@@ -591,6 +591,10 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n   const auto* params =\n       reinterpret_cast<TfLiteTransposeConvParams*>(node->builtin_data);\n \n+  \/\/ Prevent divisions by 0\n+  TF_LITE_ENSURE(context, params->stride_height > 0);\n+  TF_LITE_ENSURE(context, params->stride_width > 0);\n+\n   \/\/ Resize any deferred dynamic tensors\n   if (IsDynamicTensor(output)) {\n     TF_LITE_ENSURE_OK(context, ResizeTensor(context, output_shape, output));"
        },
        {
            "index":732,
            "vuln_id":"GHSA-j47f-4232-hvv8",
            "cwe_id":"{'CWE-125'}",
            "score":2.5,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/44b7f486c0143f68b56c34e2d01e146ee445134a'}",
            "dataset":"osv",
            "summary":"Heap out of bounds read in `RaggedCross` ### Impact\nAn attacker can force accesses outside the bounds of heap allocated arrays by passing in invalid tensor values to `tf.raw_ops.RaggedCross`:\n\n```python\nimport tensorflow as tf\n\nragged_values = []\nragged_row_splits = [] \nsparse_indices = []\nsparse_values = []\nsparse_shape = []\n\ndense_inputs_elem = tf.constant([], shape=[92, 0], dtype=tf.int64)\ndense_inputs = [dense_inputs_elem]\n\ninput_order = \"R\"\nhashed_output = False\nnum_buckets = 0\nhash_key = 0 \n\ntf.raw_ops.RaggedCross(ragged_values=ragged_values,\n    ragged_row_splits=ragged_row_splits,\n    sparse_indices=sparse_indices,\n    sparse_values=sparse_values,\n    sparse_shape=sparse_shape,\n    dense_inputs=dense_inputs,\n    input_order=input_order,\n    hashed_output=hashed_output,\n    num_buckets=num_buckets,\n    hash_key=hash_key,\n    out_values_type=tf.int64,\n    out_row_splits_type=tf.int64)\n```\n\nThis is because the [implementation](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/efea03b38fb8d3b81762237dc85e579cc5fc6e87\/tensorflow\/core\/kernels\/ragged_cross_op.cc#L456-L487) lacks validation for the user supplied arguments:\n\n```cc\nint next_ragged = 0;\nint next_sparse = 0;\nint next_dense = 0;\nfor (char c : input_order_) {\n  if (c == 'R') {\n    TF_RETURN_IF_ERROR(BuildRaggedFeatureReader(\n        ragged_values_list[next_ragged], ragged_splits_list[next_ragged],\n        features));\n    next_ragged++;\n  } else if (c == 'S') {\n    TF_RETURN_IF_ERROR(BuildSparseFeatureReader(\n        sparse_indices_list[next_sparse], sparse_values_list[next_sparse],\n        batch_size, features));\n    next_sparse++;\n  } else if (c == 'D') {\n    TF_RETURN_IF_ERROR(\n        BuildDenseFeatureReader(dense_list[next_dense++], features));\n  }\n  ...\n}\n```\n\nEach of the above branches call a helper function after accessing array elements via a `*_list[next_*]` pattern, followed by incrementing the `next_*` index. However, as there is no validation that the `next_*` values are in the valid range for the corresponding `*_list` arrays, this results in heap OOB reads.\n\n### Patches\nWe have patched the issue in GitHub commit [44b7f486c0143f68b56c34e2d01e146ee445134a](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/44b7f486c0143f68b56c34e2d01e146ee445134a).\n\nThe fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by Ying Wang and Yakun Zhang of Baidu X-Team.",
            "published_date":"2021-05-21",
            "chain_len":1,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/44b7f486c0143f68b56c34e2d01e146ee445134a",
            "commit_sha":"44b7f486c0143f68b56c34e2d01e146ee445134a",
            "patch":"SINGLE",
            "chain_ord":"['44b7f486c0143f68b56c34e2d01e146ee445134a']",
            "before_first_fix_commit":"{'efea03b38fb8d3b81762237dc85e579cc5fc6e87'}",
            "last_fix_commit":"44b7f486c0143f68b56c34e2d01e146ee445134a",
            "chain_ord_pos":1.0,
            "commit_datetime":"04\/21\/2021, 23:19:54",
            "message":"Fix out of bounds read in `ragged_cross_op.cc`.\n\nPiperOrigin-RevId: 369757702\nChange-Id: Ie6e5d2c21513a8d56bf41fcf35960caf76e890f9",
            "author":"Mihai Maruseac",
            "comments":null,
            "stats":"{'additions': 30, 'deletions': 0, 'total': 30}",
            "files":"{'tensorflow\/core\/kernels\/ragged_cross_op.cc': {'additions': 30, 'deletions': 0, 'changes': 30, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/44b7f486c0143f68b56c34e2d01e146ee445134a\/tensorflow%2Fcore%2Fkernels%2Fragged_cross_op.cc', 'patch': '@@ -21,6 +21,7 @@ limitations under the License.\\n #include \"tensorflow\/core\/framework\/register_types.h\"\\n #include \"tensorflow\/core\/framework\/tensor.h\"\\n #include \"tensorflow\/core\/framework\/tensor_shape.h\"\\n+#include \"tensorflow\/core\/platform\/errors.h\"\\n #include \"tensorflow\/core\/platform\/fingerprint.h\"\\n #include \"tensorflow\/core\/util\/util.h\"\\n #include \"tensorflow\/core\/util\/work_sharder.h\"\\n@@ -466,16 +467,45 @@ class RaggedCrossOp : public OpKernel {\\n     int next_dense = 0;\\n     for (char c : input_order_) {\\n       if (c == \\'R\\') {\\n+        if (next_ragged >= ragged_values_list.size())\\n+          return errors::InvalidArgument(\\n+              \"input_order \\\\\"\", input_order_,\\n+              \"\\\\\" specifies reading a ragged tensor value at index \",\\n+              next_ragged, \" from a list of \", ragged_values_list.size(),\\n+              \" values.\");\\n+        if (next_ragged >= ragged_splits_list.size())\\n+          return errors::InvalidArgument(\\n+              \"input_order \\\\\"\", input_order_,\\n+              \"\\\\\" specifies reading a ragged tensor split at index \",\\n+              next_ragged, \" from a list of \", ragged_splits_list.size(),\\n+              \" splits.\");\\n         TF_RETURN_IF_ERROR(BuildRaggedFeatureReader(\\n             ragged_values_list[next_ragged], ragged_splits_list[next_ragged],\\n             features));\\n         next_ragged++;\\n       } else if (c == \\'S\\') {\\n+        if (next_sparse >= sparse_values_list.size())\\n+          return errors::InvalidArgument(\\n+              \"input_order \\\\\"\", input_order_,\\n+              \"\\\\\" specifies reading a sparse tensor value at index \",\\n+              next_sparse, \" from a list of \", sparse_values_list.size(),\\n+              \" values.\");\\n+        if (next_sparse >= sparse_indices_list.size())\\n+          return errors::InvalidArgument(\\n+              \"input_order \\\\\"\", input_order_,\\n+              \"\\\\\" specifies reading a sparse tensor index at index \",\\n+              next_sparse, \" from a list of \", sparse_indices_list.size(),\\n+              \" indices.\");\\n         TF_RETURN_IF_ERROR(BuildSparseFeatureReader(\\n             sparse_indices_list[next_sparse], sparse_values_list[next_sparse],\\n             batch_size, features));\\n         next_sparse++;\\n       } else if (c == \\'D\\') {\\n+        if (next_dense >= dense_list.size())\\n+          return errors::InvalidArgument(\\n+              \"input_order \\\\\"\", input_order_,\\n+              \"\\\\\" specifies reading a dense tensor at index \", next_dense,\\n+              \" from a list of \", dense_list.size(), \" tensors.\");\\n         TF_RETURN_IF_ERROR(\\n             BuildDenseFeatureReader(dense_list[next_dense++], features));\\n       } else {'}}",
            "message_norm":"fix out of bounds read in `ragged_cross_op.cc`.\n\npiperorigin-revid: 369757702\nchange-id: ie6e5d2c21513a8d56bf41fcf35960caf76e890f9",
            "language":"en",
            "entities":"[('fix', 'ACTION', ''), ('out of bounds read', 'SECWORD', ''), ('369757702', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/core\/kernels\/ragged_cross_op.cc'])",
            "num_files":1.0,
            "patch_content":"From 44b7f486c0143f68b56c34e2d01e146ee445134a Mon Sep 17 00:00:00 2001\nFrom: Mihai Maruseac <mihaimaruseac@google.com>\nDate: Wed, 21 Apr 2021 16:19:54 -0700\nSubject: [PATCH] Fix out of bounds read in `ragged_cross_op.cc`.\n\nPiperOrigin-RevId: 369757702\nChange-Id: Ie6e5d2c21513a8d56bf41fcf35960caf76e890f9\n---\n tensorflow\/core\/kernels\/ragged_cross_op.cc | 30 ++++++++++++++++++++++\n 1 file changed, 30 insertions(+)\n\ndiff --git a\/tensorflow\/core\/kernels\/ragged_cross_op.cc b\/tensorflow\/core\/kernels\/ragged_cross_op.cc\nindex ea65c0ee2b5b21..5dfe93f4166592 100644\n--- a\/tensorflow\/core\/kernels\/ragged_cross_op.cc\n+++ b\/tensorflow\/core\/kernels\/ragged_cross_op.cc\n@@ -21,6 +21,7 @@ limitations under the License.\n #include \"tensorflow\/core\/framework\/register_types.h\"\n #include \"tensorflow\/core\/framework\/tensor.h\"\n #include \"tensorflow\/core\/framework\/tensor_shape.h\"\n+#include \"tensorflow\/core\/platform\/errors.h\"\n #include \"tensorflow\/core\/platform\/fingerprint.h\"\n #include \"tensorflow\/core\/util\/util.h\"\n #include \"tensorflow\/core\/util\/work_sharder.h\"\n@@ -466,16 +467,45 @@ class RaggedCrossOp : public OpKernel {\n     int next_dense = 0;\n     for (char c : input_order_) {\n       if (c == 'R') {\n+        if (next_ragged >= ragged_values_list.size())\n+          return errors::InvalidArgument(\n+              \"input_order \\\"\", input_order_,\n+              \"\\\" specifies reading a ragged tensor value at index \",\n+              next_ragged, \" from a list of \", ragged_values_list.size(),\n+              \" values.\");\n+        if (next_ragged >= ragged_splits_list.size())\n+          return errors::InvalidArgument(\n+              \"input_order \\\"\", input_order_,\n+              \"\\\" specifies reading a ragged tensor split at index \",\n+              next_ragged, \" from a list of \", ragged_splits_list.size(),\n+              \" splits.\");\n         TF_RETURN_IF_ERROR(BuildRaggedFeatureReader(\n             ragged_values_list[next_ragged], ragged_splits_list[next_ragged],\n             features));\n         next_ragged++;\n       } else if (c == 'S') {\n+        if (next_sparse >= sparse_values_list.size())\n+          return errors::InvalidArgument(\n+              \"input_order \\\"\", input_order_,\n+              \"\\\" specifies reading a sparse tensor value at index \",\n+              next_sparse, \" from a list of \", sparse_values_list.size(),\n+              \" values.\");\n+        if (next_sparse >= sparse_indices_list.size())\n+          return errors::InvalidArgument(\n+              \"input_order \\\"\", input_order_,\n+              \"\\\" specifies reading a sparse tensor index at index \",\n+              next_sparse, \" from a list of \", sparse_indices_list.size(),\n+              \" indices.\");\n         TF_RETURN_IF_ERROR(BuildSparseFeatureReader(\n             sparse_indices_list[next_sparse], sparse_values_list[next_sparse],\n             batch_size, features));\n         next_sparse++;\n       } else if (c == 'D') {\n+        if (next_dense >= dense_list.size())\n+          return errors::InvalidArgument(\n+              \"input_order \\\"\", input_order_,\n+              \"\\\" specifies reading a dense tensor at index \", next_dense,\n+              \" from a list of \", dense_list.size(), \" tensors.\");\n         TF_RETURN_IF_ERROR(\n             BuildDenseFeatureReader(dense_list[next_dense++], features));\n       } else {"
        },
        {
            "index":178,
            "vuln_id":"GHSA-3hxh-8cp2-g4hg",
            "cwe_id":"{'CWE-416'}",
            "score":6.6,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/ee119d4a498979525046fba1c3dd3f13a039fbb1'}",
            "dataset":"osv",
            "summary":"Use after free and segfault in shape inference functions ### Impact\nWhen running shape functions, some functions (such as `MutableHashTableShape`) produce extra output information in the form of a `ShapeAndType` struct. The shapes embedded in this struct are owned by an inference context that is cleaned up almost immediately; if the upstream code attempts to access this shape information, it can trigger a segfault.\n\n`ShapeRefiner` is mitigating this for normal output shapes by cloning them (and thus putting the newly created shape under ownership of an inference context that will not die), but we were not doing the same for shapes and types. This commit fixes that by doing similar logic on output shapes and types.\n\n### Patches\nWe have patched the issue in GitHub commit [ee119d4a498979525046fba1c3dd3f13a039fbb1](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/ee119d4a498979525046fba1c3dd3f13a039fbb1).                                                                                                          \n\nThe fix will be included in TensorFlow 2.6.0. We will also cherrypick this commit on TensorFlow 2.5.1, TensorFlow 2.4.3, and TensorFlow 2.3.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.",
            "published_date":"2021-08-25",
            "chain_len":1,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/ee119d4a498979525046fba1c3dd3f13a039fbb1",
            "commit_sha":"ee119d4a498979525046fba1c3dd3f13a039fbb1",
            "patch":"SINGLE",
            "chain_ord":"['ee119d4a498979525046fba1c3dd3f13a039fbb1']",
            "before_first_fix_commit":"{'f118ff1538ac7aa8a628bba03fe66dc6811cc7fc'}",
            "last_fix_commit":"ee119d4a498979525046fba1c3dd3f13a039fbb1",
            "chain_ord_pos":1.0,
            "commit_datetime":"07\/14\/2021, 19:43:17",
            "message":"Fix segmentation fault in shape inference logic.\n\nWhen running shape functions, some functions (such as `MutableHashTableShape`)\nproduce extra output information in the form of a `ShapeAndType` struct.  The\nshapes embedded in this struct are owned by an inference context that is\ncleaned up almost immediately; if the upstream code attempts to access this\nshape information, it can trigger a segfault.\n\n`ShapeRefiner` is mitigating this for normal output shapes by cloning them\n(and thus putting the newly created shape under ownership of an inference\ncontext that will not die), but we were not doing the same for shapes and\ntypes.  This commit fixes that by doing similar logic on output shapes and\ntypes.\n\nPiperOrigin-RevId: 384761124\nChange-Id: I07c0c42d29dfbb55bfa13ec1f09ef825fb0a1a1d",
            "author":"Daniel Ellis",
            "comments":null,
            "stats":"{'additions': 19, 'deletions': 2, 'total': 21}",
            "files":"{'tensorflow\/core\/common_runtime\/shape_refiner.cc': {'additions': 19, 'deletions': 2, 'changes': 21, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/ee119d4a498979525046fba1c3dd3f13a039fbb1\/tensorflow%2Fcore%2Fcommon_runtime%2Fshape_refiner.cc', 'patch': '@@ -120,9 +120,26 @@ Status ShapeRefiner::InferShapesForFunctionSubNode(\\n     TF_RETURN_IF_ERROR(outer_context->MakeShapeFromShapeProto(proto, &handle));\\n     outer_context->set_output(index, handle);\\n \\n-    auto* resource = node_context->input_handle_shapes_and_types(0);\\n+    const std::vector<ShapeAndType>* resource =\\n+        node_context->input_handle_shapes_and_types(0);\\n     if (resource) {\\n-      outer_context->set_output_handle_shapes_and_types(index, *resource);\\n+      \/\/ `ShapesAndType`s contain `ShapeHandle`s.  These `ShapeHandle`s point\\n+      \/\/ to `Shape`s that are owned by a different inference context too.  We\\n+      \/\/ need to copy them to the outer context to prevent them from being\\n+      \/\/ destroyed before they are used.\\n+      std::vector<ShapeAndType> copied_shapes_and_types;\\n+      for (auto& shape_and_type : *resource) {\\n+        ShapeHandle handle;\\n+        TensorShapeProto proto;\\n+        node_context->ShapeHandleToProto(shape_and_type.shape, &proto);\\n+        TF_RETURN_IF_ERROR(\\n+            outer_context->MakeShapeFromShapeProto(proto, &handle));\\n+        copied_shapes_and_types.push_back(\\n+            ShapeAndType(handle, shape_and_type.dtype, shape_and_type.type));\\n+      }\\n+\\n+      outer_context->set_output_handle_shapes_and_types(\\n+          index, copied_shapes_and_types);\\n     }\\n   }'}}",
            "message_norm":"fix segmentation fault in shape inference logic.\n\nwhen running shape functions, some functions (such as `mutablehashtableshape`)\nproduce extra output information in the form of a `shapeandtype` struct.  the\nshapes embedded in this struct are owned by an inference context that is\ncleaned up almost immediately; if the upstream code attempts to access this\nshape information, it can trigger a segfault.\n\n`shaperefiner` is mitigating this for normal output shapes by cloning them\n(and thus putting the newly created shape under ownership of an inference\ncontext that will not die), but we were not doing the same for shapes and\ntypes.  this commit fixes that by doing similar logic on output shapes and\ntypes.\n\npiperorigin-revid: 384761124\nchange-id: i07c0c42d29dfbb55bfa13ec1f09ef825fb0a1a1d",
            "language":"en",
            "entities":"[('fix', 'ACTION', ''), ('segmentation fault', 'SECWORD', ''), ('segfault', 'SECWORD', ''), ('mitigating', 'ACTION', ''), ('fixes', 'ACTION', ''), ('384761124', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/core\/common_runtime\/shape_refiner.cc'])",
            "num_files":1.0,
            "patch_content":"From ee119d4a498979525046fba1c3dd3f13a039fbb1 Mon Sep 17 00:00:00 2001\nFrom: Daniel Ellis <danielellis@google.com>\nDate: Wed, 14 Jul 2021 12:43:17 -0700\nSubject: [PATCH] Fix segmentation fault in shape inference logic.\n\nWhen running shape functions, some functions (such as `MutableHashTableShape`)\nproduce extra output information in the form of a `ShapeAndType` struct.  The\nshapes embedded in this struct are owned by an inference context that is\ncleaned up almost immediately; if the upstream code attempts to access this\nshape information, it can trigger a segfault.\n\n`ShapeRefiner` is mitigating this for normal output shapes by cloning them\n(and thus putting the newly created shape under ownership of an inference\ncontext that will not die), but we were not doing the same for shapes and\ntypes.  This commit fixes that by doing similar logic on output shapes and\ntypes.\n\nPiperOrigin-RevId: 384761124\nChange-Id: I07c0c42d29dfbb55bfa13ec1f09ef825fb0a1a1d\n---\n ...\/core\/common_runtime\/shape_refiner.cc      | 21 +++++++++++++++++--\n 1 file changed, 19 insertions(+), 2 deletions(-)\n\ndiff --git a\/tensorflow\/core\/common_runtime\/shape_refiner.cc b\/tensorflow\/core\/common_runtime\/shape_refiner.cc\nindex 375f809b31b369..2e29ef48189a59 100644\n--- a\/tensorflow\/core\/common_runtime\/shape_refiner.cc\n+++ b\/tensorflow\/core\/common_runtime\/shape_refiner.cc\n@@ -120,9 +120,26 @@ Status ShapeRefiner::InferShapesForFunctionSubNode(\n     TF_RETURN_IF_ERROR(outer_context->MakeShapeFromShapeProto(proto, &handle));\n     outer_context->set_output(index, handle);\n \n-    auto* resource = node_context->input_handle_shapes_and_types(0);\n+    const std::vector<ShapeAndType>* resource =\n+        node_context->input_handle_shapes_and_types(0);\n     if (resource) {\n-      outer_context->set_output_handle_shapes_and_types(index, *resource);\n+      \/\/ `ShapesAndType`s contain `ShapeHandle`s.  These `ShapeHandle`s point\n+      \/\/ to `Shape`s that are owned by a different inference context too.  We\n+      \/\/ need to copy them to the outer context to prevent them from being\n+      \/\/ destroyed before they are used.\n+      std::vector<ShapeAndType> copied_shapes_and_types;\n+      for (auto& shape_and_type : *resource) {\n+        ShapeHandle handle;\n+        TensorShapeProto proto;\n+        node_context->ShapeHandleToProto(shape_and_type.shape, &proto);\n+        TF_RETURN_IF_ERROR(\n+            outer_context->MakeShapeFromShapeProto(proto, &handle));\n+        copied_shapes_and_types.push_back(\n+            ShapeAndType(handle, shape_and_type.dtype, shape_and_type.type));\n+      }\n+\n+      outer_context->set_output_handle_shapes_and_types(\n+          index, copied_shapes_and_types);\n     }\n   }"
        },
        {
            "index":920,
            "vuln_id":"GHSA-434h-p4gx-jm89",
            "cwe_id":"{'CWE-203'}",
            "score":5.3,
            "chain":"{'https:\/\/github.com\/dpgaspar\/Flask-AppBuilder\/commit\/780bd0e8fbf2d36ada52edb769477e0a4edae580'}",
            "dataset":"osv",
            "summary":"Observable Response Discrepancy in Flask-AppBuilder ### Impact\nUser enumeration in database authentication in Flask-AppBuilder <= 3.2.3. Allows for a non authenticated user to enumerate existing accounts by timing the response time from the server when you are logging in.\n\n### Patches\nUpgrade to 3.3.0\n\n### For more information\nIf you have any questions or comments about this advisory:\n* Open an issue in [Flask-AppBuilder](https:\/\/github.com\/dpgaspar\/Flask-AppBuilder)",
            "published_date":"2021-05-27",
            "chain_len":1,
            "project":"https:\/\/github.com\/dpgaspar\/Flask-AppBuilder",
            "commit_href":"https:\/\/github.com\/dpgaspar\/Flask-AppBuilder\/commit\/780bd0e8fbf2d36ada52edb769477e0a4edae580",
            "commit_sha":"780bd0e8fbf2d36ada52edb769477e0a4edae580",
            "patch":"SINGLE",
            "chain_ord":"['780bd0e8fbf2d36ada52edb769477e0a4edae580']",
            "before_first_fix_commit":"{'b60dea9cedf98b56c926ba41020c73f287d5826e'}",
            "last_fix_commit":"780bd0e8fbf2d36ada52edb769477e0a4edae580",
            "chain_ord_pos":1.0,
            "commit_datetime":"05\/10\/2021, 08:37:55",
            "message":"fix: auth balance (#1634)",
            "author":"Daniel Vaz Gaspar",
            "comments":null,
            "stats":"{'additions': 6, 'deletions': 0, 'total': 6}",
            "files":"{'flask_appbuilder\/security\/manager.py': {'additions': 6, 'deletions': 0, 'changes': 6, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/dpgaspar\/Flask-AppBuilder\/raw\/780bd0e8fbf2d36ada52edb769477e0a4edae580\/flask_appbuilder%2Fsecurity%2Fmanager.py', 'patch': '@@ -833,6 +833,12 @@ def auth_user_db(self, username, password):\\n         if user is None:\\n             user = self.find_user(email=username)\\n         if user is None or (not user.is_active):\\n+            # Balance failure and success\\n+            check_password_hash(\\n+                \"pbkdf2:sha256:150000$Z3t6fmj2$22da622d94a1f8118\"\\n+                \"c0976a03d2f18f680bfff877c9a965db9eedc51bc0be87c\",\\n+                \"password\",\\n+            )\\n             log.info(LOGMSG_WAR_SEC_LOGIN_FAILED.format(username))\\n             return None\\n         elif check_password_hash(user.password, password):'}}",
            "message_norm":"fix: auth balance (#1634)",
            "language":"en",
            "entities":"[('auth', 'SECWORD', ''), ('#1634', 'ISSUE', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['flask_appbuilder\/security\/manager.py'])",
            "num_files":1.0,
            "patch_content":"From 780bd0e8fbf2d36ada52edb769477e0a4edae580 Mon Sep 17 00:00:00 2001\nFrom: Daniel Vaz Gaspar <danielvazgaspar@gmail.com>\nDate: Mon, 10 May 2021 09:37:55 +0100\nSubject: [PATCH] fix: auth balance (#1634)\n\n---\n flask_appbuilder\/security\/manager.py | 6 ++++++\n 1 file changed, 6 insertions(+)\n\ndiff --git a\/flask_appbuilder\/security\/manager.py b\/flask_appbuilder\/security\/manager.py\nindex f837e5c43..7154dc50c 100644\n--- a\/flask_appbuilder\/security\/manager.py\n+++ b\/flask_appbuilder\/security\/manager.py\n@@ -833,6 +833,12 @@ def auth_user_db(self, username, password):\n         if user is None:\n             user = self.find_user(email=username)\n         if user is None or (not user.is_active):\n+            # Balance failure and success\n+            check_password_hash(\n+                \"pbkdf2:sha256:150000$Z3t6fmj2$22da622d94a1f8118\"\n+                \"c0976a03d2f18f680bfff877c9a965db9eedc51bc0be87c\",\n+                \"password\",\n+            )\n             log.info(LOGMSG_WAR_SEC_LOGIN_FAILED.format(username))\n             return None\n         elif check_password_hash(user.password, password):"
        },
        {
            "index":742,
            "vuln_id":"GHSA-fq86-3f29-px2c",
            "cwe_id":"{'CWE-617'}",
            "score":6.5,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/240655511cd3e701155f944a972db71b6c0b1bb6', 'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/ebc1a2ffe5a7573d905e99bd0ee3568ee07c12c1', 'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/1fb27733f943295d874417630edd3b38b34ce082'}",
            "dataset":"osv",
            "summary":"`CHECK`-failures during Grappler's `IsSimplifiableReshape` in Tensorflow ### Impact\nThe Grappler optimizer in TensorFlow can be used to cause a denial of service by altering a `SavedModel` such that [`IsSimplifiableReshape`](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/a1320ec1eac186da1d03f033109191f715b2b130\/tensorflow\/core\/grappler\/optimizers\/constant_folding.cc#L1687-L1742) would trigger `CHECK` failures.\n\n### Patches\nWe have patched the issue in GitHub commits [ebc1a2ffe5a7573d905e99bd0ee3568ee07c12c1](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/ebc1a2ffe5a7573d905e99bd0ee3568ee07c12c1), [1fb27733f943295d874417630edd3b38b34ce082](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/1fb27733f943295d874417630edd3b38b34ce082), and [240655511cd3e701155f944a972db71b6c0b1bb6](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/240655511cd3e701155f944a972db71b6c0b1bb6).\n\nThe fix will be included in TensorFlow 2.8.0. We will also cherrypick this commit on TensorFlow 2.7.1, TensorFlow 2.6.3, and TensorFlow 2.5.3, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.",
            "published_date":"2022-02-07",
            "chain_len":3,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/240655511cd3e701155f944a972db71b6c0b1bb6",
            "commit_sha":"240655511cd3e701155f944a972db71b6c0b1bb6",
            "patch":"MULTI",
            "chain_ord":"['ebc1a2ffe5a7573d905e99bd0ee3568ee07c12c1', '1fb27733f943295d874417630edd3b38b34ce082', '240655511cd3e701155f944a972db71b6c0b1bb6']",
            "before_first_fix_commit":"{'1fb27733f943295d874417630edd3b38b34ce082'}",
            "last_fix_commit":"240655511cd3e701155f944a972db71b6c0b1bb6",
            "chain_ord_pos":3.0,
            "commit_datetime":"11\/11\/2021, 17:24:31",
            "message":"Eliminate `CHECK`-fails from `IsSimplifiableReshape` via `MakeShape(<invalid shape>)`\n\nPiperOrigin-RevId: 409166738\nChange-Id: I7f0a3590b8acae3f3e3e2fe636e1f5ef285693cf",
            "author":"Mihai Maruseac",
            "comments":null,
            "stats":"{'additions': 4, 'deletions': 2, 'total': 6}",
            "files":"{'tensorflow\/core\/grappler\/optimizers\/constant_folding.cc': {'additions': 4, 'deletions': 2, 'changes': 6, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/240655511cd3e701155f944a972db71b6c0b1bb6\/tensorflow%2Fcore%2Fgrappler%2Foptimizers%2Fconstant_folding.cc', 'patch': '@@ -1741,14 +1741,16 @@ Status ConstantFolding::IsSimplifiableReshape(\\n       int32_t dim = outputs[0]->flat<int32>()(i);\\n       shp.push_back(dim);\\n     }\\n-    TF_CHECK_OK(TensorShapeUtils::MakeShape(shp, &new_dims));\\n+    s = TensorShapeUtils::MakeShape(shp, &new_dims);\\n+    if (!s.ok()) return s;\\n   } else {\\n     std::vector<int64_t> shp;\\n     for (int i = 0; i < outputs[0]->NumElements(); ++i) {\\n       int64_t dim = outputs[0]->flat<int64_t>()(i);\\n       shp.push_back(dim);\\n     }\\n-    TF_CHECK_OK(TensorShapeUtils::MakeShape(shp, &new_dims));\\n+    s = TensorShapeUtils::MakeShape(shp, &new_dims);\\n+    if (!s.ok()) return s;\\n   }\\n \\n   if (!shape.IsCompatibleWith(new_dims)) {'}}",
            "message_norm":"eliminate `check`-fails from `issimplifiablereshape` via `makeshape(<invalid shape>)`\n\npiperorigin-revid: 409166738\nchange-id: i7f0a3590b8acae3f3e3e2fe636e1f5ef285693cf",
            "language":"en",
            "entities":"[('409166738', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/core\/grappler\/optimizers\/constant_folding.cc'])",
            "num_files":1.0,
            "patch_content":"From 240655511cd3e701155f944a972db71b6c0b1bb6 Mon Sep 17 00:00:00 2001\nFrom: Mihai Maruseac <mihaimaruseac@google.com>\nDate: Thu, 11 Nov 2021 09:24:31 -0800\nSubject: [PATCH] Eliminate `CHECK`-fails from `IsSimplifiableReshape` via\n `MakeShape(<invalid shape>)`\n\nPiperOrigin-RevId: 409166738\nChange-Id: I7f0a3590b8acae3f3e3e2fe636e1f5ef285693cf\n---\n tensorflow\/core\/grappler\/optimizers\/constant_folding.cc | 6 ++++--\n 1 file changed, 4 insertions(+), 2 deletions(-)\n\ndiff --git a\/tensorflow\/core\/grappler\/optimizers\/constant_folding.cc b\/tensorflow\/core\/grappler\/optimizers\/constant_folding.cc\nindex a2050899f60726..d5fadb311a75cc 100644\n--- a\/tensorflow\/core\/grappler\/optimizers\/constant_folding.cc\n+++ b\/tensorflow\/core\/grappler\/optimizers\/constant_folding.cc\n@@ -1741,14 +1741,16 @@ Status ConstantFolding::IsSimplifiableReshape(\n       int32_t dim = outputs[0]->flat<int32>()(i);\n       shp.push_back(dim);\n     }\n-    TF_CHECK_OK(TensorShapeUtils::MakeShape(shp, &new_dims));\n+    s = TensorShapeUtils::MakeShape(shp, &new_dims);\n+    if (!s.ok()) return s;\n   } else {\n     std::vector<int64_t> shp;\n     for (int i = 0; i < outputs[0]->NumElements(); ++i) {\n       int64_t dim = outputs[0]->flat<int64_t>()(i);\n       shp.push_back(dim);\n     }\n-    TF_CHECK_OK(TensorShapeUtils::MakeShape(shp, &new_dims));\n+    s = TensorShapeUtils::MakeShape(shp, &new_dims);\n+    if (!s.ok()) return s;\n   }\n \n   if (!shape.IsCompatibleWith(new_dims)) {"
        },
        {
            "index":332,
            "vuln_id":"GHSA-rhcw-wjcm-9h6g",
            "cwe_id":"{'CWE-400'}",
            "score":7.5,
            "chain":"{'https:\/\/github.com\/undertow-io\/undertow\/pull\/997\/commits\/98a9ab7f2d7fe7a7254eaf17d47816c452169c90'}",
            "dataset":"osv",
            "summary":"Denial of service in Undertow A flaw was found in the Undertow AJP connector. Malicious requests and abrupt connection closes could be triggered by an attacker using query strings with non-RFC compliant characters resulting in a denial of service. The highest threat from this vulnerability is to system availability. This affects Undertow 2.1.5.SP1, 2.0.33.SP2, and 2.2.3.SP1.",
            "published_date":"2022-02-09",
            "chain_len":1,
            "project":"https:\/\/github.com\/undertow-io\/undertow",
            "commit_href":"https:\/\/github.com\/undertow-io\/undertow\/pull\/997\/commits\/98a9ab7f2d7fe7a7254eaf17d47816c452169c90",
            "commit_sha":"98a9ab7f2d7fe7a7254eaf17d47816c452169c90",
            "patch":"SINGLE",
            "chain_ord":"['98a9ab7f2d7fe7a7254eaf17d47816c452169c90']",
            "before_first_fix_commit":"{'47dc5e37cb20d8eeb4d4f632fe959d436f86128a'}",
            "last_fix_commit":"98a9ab7f2d7fe7a7254eaf17d47816c452169c90",
            "chain_ord_pos":1.0,
            "commit_datetime":"11\/29\/2020, 13:24:40",
            "message":"UNDERTOW-1813 Make PathResourceManager.getResource rethrow a SecurityException",
            "author":"Boris Unckel",
            "comments":null,
            "stats":"{'additions': 7, 'deletions': 1, 'total': 8}",
            "files":"{'core\/src\/main\/java\/io\/undertow\/server\/handlers\/resource\/PathResourceManager.java': {'additions': 7, 'deletions': 1, 'changes': 8, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/undertow-io\/undertow\/raw\/98a9ab7f2d7fe7a7254eaf17d47816c452169c90\/core%2Fsrc%2Fmain%2Fjava%2Fio%2Fundertow%2Fserver%2Fhandlers%2Fresource%2FPathResourceManager.java', 'patch': '@@ -230,9 +230,15 @@ public Resource getResource(final String p) {\\n                 log.tracef(\"Failed to get path resource %s from path resource manager with base %s, as the path did not exist\", p, base);\\n                 return null;\\n             }\\n-        } catch (Exception e) {\\n+        } catch (IOException e) {\\n             UndertowLogger.REQUEST_LOGGER.debugf(e, \"Invalid path %s\", p);\\n             return null;\\n+        } catch (SecurityException e) {\\n+            UndertowLogger.REQUEST_LOGGER.errorf(e, \"Missing JSM permissions for path %s\", p);\\n+            throw e;\\n+        } catch (Exception e) {\\n+            UndertowLogger.REQUEST_LOGGER.debugf(e, \"Other issue for path %s\", p);\\n+            return null;\\n         }\\n     }'}}",
            "message_norm":"undertow-1813 make pathresourcemanager.getresource rethrow a securityexception",
            "language":"en",
            "entities":"[('securityexception', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['core\/src\/main\/java\/io\/undertow\/server\/handlers\/resource\/PathResourceManager.java'])",
            "num_files":1.0,
            "patch_content":"From 98a9ab7f2d7fe7a7254eaf17d47816c452169c90 Mon Sep 17 00:00:00 2001\nFrom: Boris Unckel <boris@unckel.net>\nDate: Sun, 29 Nov 2020 14:24:40 +0100\nSubject: [PATCH] UNDERTOW-1813 Make PathResourceManager.getResource rethrow a\n SecurityException\n\n---\n ...\/server\/handlers\/resource\/PathResourceManager.java     | 8 +++++++-\n 1 file changed, 7 insertions(+), 1 deletion(-)\n\ndiff --git a\/core\/src\/main\/java\/io\/undertow\/server\/handlers\/resource\/PathResourceManager.java b\/core\/src\/main\/java\/io\/undertow\/server\/handlers\/resource\/PathResourceManager.java\nindex a13b082f46..d48ea9d375 100644\n--- a\/core\/src\/main\/java\/io\/undertow\/server\/handlers\/resource\/PathResourceManager.java\n+++ b\/core\/src\/main\/java\/io\/undertow\/server\/handlers\/resource\/PathResourceManager.java\n@@ -230,9 +230,15 @@ public Resource getResource(final String p) {\n                 log.tracef(\"Failed to get path resource %s from path resource manager with base %s, as the path did not exist\", p, base);\n                 return null;\n             }\n-        } catch (Exception e) {\n+        } catch (IOException e) {\n             UndertowLogger.REQUEST_LOGGER.debugf(e, \"Invalid path %s\", p);\n             return null;\n+        } catch (SecurityException e) {\n+            UndertowLogger.REQUEST_LOGGER.errorf(e, \"Missing JSM permissions for path %s\", p);\n+            throw e;\n+        } catch (Exception e) {\n+            UndertowLogger.REQUEST_LOGGER.debugf(e, \"Other issue for path %s\", p);\n+            return null;\n         }\n     }"
        },
        {
            "index":51,
            "vuln_id":"GHSA-49j7-qghp-5wj8",
            "cwe_id":"{'CWE-94'}",
            "score":0.0,
            "chain":"{'https:\/\/github.com\/fusesource\/hawtjni\/commit\/92c266170ce98edc200c656bd034a237098b8aa5'}",
            "dataset":"osv",
            "summary":"Improper Control of Generation of Code in HawtJNI Race condition in hawtjni-runtime\/src\/main\/java\/org\/fusesource\/hawtjni\/runtime\/Library.java in HawtJNI before 1.8, when a custom library path is not specified, allows local users to execute arbitrary Java code by overwriting a temporary JAR file with a predictable name in \/tmp.",
            "published_date":"2022-05-17",
            "chain_len":1,
            "project":"https:\/\/github.com\/fusesource\/hawtjni",
            "commit_href":"https:\/\/github.com\/fusesource\/hawtjni\/commit\/92c266170ce98edc200c656bd034a237098b8aa5",
            "commit_sha":"92c266170ce98edc200c656bd034a237098b8aa5",
            "patch":"SINGLE",
            "chain_ord":"['92c266170ce98edc200c656bd034a237098b8aa5']",
            "before_first_fix_commit":"{'357bb279b0c8c67b7d357c1363efe86870ad9a81'}",
            "last_fix_commit":"92c266170ce98edc200c656bd034a237098b8aa5",
            "chain_ord_pos":1.0,
            "commit_datetime":"05\/06\/2013, 13:49:55",
            "message":"Simplify shared lib extraction.",
            "author":"Hiram Chirino",
            "comments":null,
            "stats":"{'additions': 29, 'deletions': 50, 'total': 79}",
            "files":"{'hawtjni-runtime\/src\/main\/java\/org\/fusesource\/hawtjni\/runtime\/Library.java': {'additions': 29, 'deletions': 50, 'changes': 79, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/fusesource\/hawtjni\/raw\/92c266170ce98edc200c656bd034a237098b8aa5\/hawtjni-runtime%2Fsrc%2Fmain%2Fjava%2Forg%2Ffusesource%2Fhawtjni%2Fruntime%2FLibrary.java', 'patch': '@@ -9,13 +9,11 @@\\n  *******************************************************************************\/\\n package org.fusesource.hawtjni.runtime;\\n \\n-import java.io.File;\\n-import java.io.FileOutputStream;\\n-import java.io.IOException;\\n-import java.io.InputStream;\\n+import java.io.*;\\n import java.net.MalformedURLException;\\n import java.net.URL;\\n import java.util.ArrayList;\\n+import java.util.Random;\\n import java.util.regex.Pattern;\\n \\n \/**\\n@@ -206,16 +204,19 @@ final public String getLibraryFileName() {\\n     private boolean exractAndLoad(ArrayList<String> errors, String version, String customPath, String resourcePath) {\\n         URL resource = classLoader.getResource(resourcePath);\\n         if( resource !=null ) {\\n-            \\n+\\n             String libName = name + \"-\" + getBitModel();\\n             if( version !=null) {\\n                 libName += \"-\" + version;\\n             }\\n-            \\n+            String []libNameParts = map(libName).split(\"\\\\\\\\.\");\\n+            String prefix = libNameParts[0]+\"-\";\\n+            String suffix = \".\"+libNameParts[1];\\n+\\n             if( customPath!=null ) {\\n                 \/\/ Try to extract it to the custom path...\\n-                File target = file(customPath, map(libName));\\n-                if( extract(errors, resource, target) ) {\\n+                File target = extract(errors, resource, prefix, suffix, file(customPath));\\n+                if( target!=null ) {\\n                     if( load(errors, target) ) {\\n                         return true;\\n                     }\\n@@ -224,8 +225,8 @@ private boolean exractAndLoad(ArrayList<String> errors, String version, String c\\n             \\n             \/\/ Fall back to extracting to the tmp dir\\n             customPath = System.getProperty(\"java.io.tmpdir\");\\n-            File target = file(customPath, map(libName));\\n-            if( extract(errors, resource, target) ) {\\n+            File target = extract(errors, resource, prefix, suffix, file(customPath));\\n+            if( target!=null ) {\\n                 if( load(errors, target) ) {\\n                     return true;\\n                 }\\n@@ -259,67 +260,45 @@ private String map(String libName) {\\n         return libName;\\n     }\\n \\n-    private boolean extract(ArrayList<String> errors, URL source, File target) {\\n-        FileOutputStream os = null;\\n-        InputStream is = null;\\n-        boolean extracting = false;\\n+    private File extract(ArrayList<String> errors, URL source, String prefix, String suffix, File directory) {\\n+        File target = null;\\n         try {\\n-            if (!target.exists() || isStale(source, target) ) {\\n+            FileOutputStream os = null;\\n+            InputStream is = null;\\n+            try {\\n+                target = File.createTempFile(prefix, suffix, directory);\\n                 is = source.openStream();\\n                 if (is != null) {\\n                     byte[] buffer = new byte[4096];\\n                     os = new FileOutputStream(target);\\n-                    extracting = true;\\n                     int read;\\n                     while ((read = is.read(buffer)) != -1) {\\n                         os.write(buffer, 0, read);\\n                     }\\n-                    os.close();\\n-                    is.close();\\n                     chmod(\"755\", target);\\n                 }\\n+                target.deleteOnExit();\\n+                return target;\\n+            } finally {\\n+                close(os);\\n+                close(is);\\n             }\\n         } catch (Throwable e) {\\n-            try {\\n-                if (os != null)\\n-                    os.close();\\n-            } catch (IOException e1) {\\n-            }\\n-            try {\\n-                if (is != null)\\n-                    is.close();\\n-            } catch (IOException e1) {\\n-            }\\n-            if (extracting && target.exists())\\n+            if( target!=null ) {\\n                 target.delete();\\n+            }\\n             errors.add(e.getMessage());\\n-            return false;\\n         }\\n-        return true;\\n+        return null;\\n     }\\n \\n-    private boolean isStale(URL source, File target) {\\n-        \\n-        if( source.getProtocol().equals(\"jar\") ) {\\n-            \/\/ unwrap the jar protocol...\\n+    static private void close(Closeable file) {\\n+        if(file!=null) {\\n             try {\\n-                String parts[] = source.getFile().split(Pattern.quote(\"!\"));\\n-                source = new URL(parts[0]);\\n-            } catch (MalformedURLException e) {\\n-                return false;\\n-            }\\n-        }\\n-        \\n-        File sourceFile=null;\\n-        if( source.getProtocol().equals(\"file\") ) {\\n-            sourceFile = new File(source.getFile());\\n-        }\\n-        if( sourceFile!=null && sourceFile.exists() ) {\\n-            if( sourceFile.lastModified() > target.lastModified() ) {\\n-                return true;\\n+                file.close();\\n+            } catch (Exception ignore) {\\n             }\\n         }\\n-        return false;\\n     }\\n \\n     private void chmod(String permision, File path) {'}}",
            "message_norm":"simplify shared lib extraction.",
            "language":"en",
            "entities":null,
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['hawtjni-runtime\/src\/main\/java\/org\/fusesource\/hawtjni\/runtime\/Library.java'])",
            "num_files":1.0,
            "patch_content":"From 92c266170ce98edc200c656bd034a237098b8aa5 Mon Sep 17 00:00:00 2001\nFrom: Hiram Chirino <hiram@hiramchirino.com>\nDate: Mon, 6 May 2013 09:49:55 -0400\nSubject: [PATCH] Simplify shared lib extraction.\n\n---\n ...\/fusesource\/hawtjni\/runtime\/Library.java   | 79 +++++++------------\n 1 file changed, 29 insertions(+), 50 deletions(-)\n\ndiff --git a\/hawtjni-runtime\/src\/main\/java\/org\/fusesource\/hawtjni\/runtime\/Library.java b\/hawtjni-runtime\/src\/main\/java\/org\/fusesource\/hawtjni\/runtime\/Library.java\nindex c23081d..422bd2f 100755\n--- a\/hawtjni-runtime\/src\/main\/java\/org\/fusesource\/hawtjni\/runtime\/Library.java\n+++ b\/hawtjni-runtime\/src\/main\/java\/org\/fusesource\/hawtjni\/runtime\/Library.java\n@@ -9,13 +9,11 @@\n  *******************************************************************************\/\n package org.fusesource.hawtjni.runtime;\n \n-import java.io.File;\n-import java.io.FileOutputStream;\n-import java.io.IOException;\n-import java.io.InputStream;\n+import java.io.*;\n import java.net.MalformedURLException;\n import java.net.URL;\n import java.util.ArrayList;\n+import java.util.Random;\n import java.util.regex.Pattern;\n \n \/**\n@@ -206,16 +204,19 @@ final public String getLibraryFileName() {\n     private boolean exractAndLoad(ArrayList<String> errors, String version, String customPath, String resourcePath) {\n         URL resource = classLoader.getResource(resourcePath);\n         if( resource !=null ) {\n-            \n+\n             String libName = name + \"-\" + getBitModel();\n             if( version !=null) {\n                 libName += \"-\" + version;\n             }\n-            \n+            String []libNameParts = map(libName).split(\"\\\\.\");\n+            String prefix = libNameParts[0]+\"-\";\n+            String suffix = \".\"+libNameParts[1];\n+\n             if( customPath!=null ) {\n                 \/\/ Try to extract it to the custom path...\n-                File target = file(customPath, map(libName));\n-                if( extract(errors, resource, target) ) {\n+                File target = extract(errors, resource, prefix, suffix, file(customPath));\n+                if( target!=null ) {\n                     if( load(errors, target) ) {\n                         return true;\n                     }\n@@ -224,8 +225,8 @@ private boolean exractAndLoad(ArrayList<String> errors, String version, String c\n             \n             \/\/ Fall back to extracting to the tmp dir\n             customPath = System.getProperty(\"java.io.tmpdir\");\n-            File target = file(customPath, map(libName));\n-            if( extract(errors, resource, target) ) {\n+            File target = extract(errors, resource, prefix, suffix, file(customPath));\n+            if( target!=null ) {\n                 if( load(errors, target) ) {\n                     return true;\n                 }\n@@ -259,67 +260,45 @@ private String map(String libName) {\n         return libName;\n     }\n \n-    private boolean extract(ArrayList<String> errors, URL source, File target) {\n-        FileOutputStream os = null;\n-        InputStream is = null;\n-        boolean extracting = false;\n+    private File extract(ArrayList<String> errors, URL source, String prefix, String suffix, File directory) {\n+        File target = null;\n         try {\n-            if (!target.exists() || isStale(source, target) ) {\n+            FileOutputStream os = null;\n+            InputStream is = null;\n+            try {\n+                target = File.createTempFile(prefix, suffix, directory);\n                 is = source.openStream();\n                 if (is != null) {\n                     byte[] buffer = new byte[4096];\n                     os = new FileOutputStream(target);\n-                    extracting = true;\n                     int read;\n                     while ((read = is.read(buffer)) != -1) {\n                         os.write(buffer, 0, read);\n                     }\n-                    os.close();\n-                    is.close();\n                     chmod(\"755\", target);\n                 }\n+                target.deleteOnExit();\n+                return target;\n+            } finally {\n+                close(os);\n+                close(is);\n             }\n         } catch (Throwable e) {\n-            try {\n-                if (os != null)\n-                    os.close();\n-            } catch (IOException e1) {\n-            }\n-            try {\n-                if (is != null)\n-                    is.close();\n-            } catch (IOException e1) {\n-            }\n-            if (extracting && target.exists())\n+            if( target!=null ) {\n                 target.delete();\n+            }\n             errors.add(e.getMessage());\n-            return false;\n         }\n-        return true;\n+        return null;\n     }\n \n-    private boolean isStale(URL source, File target) {\n-        \n-        if( source.getProtocol().equals(\"jar\") ) {\n-            \/\/ unwrap the jar protocol...\n+    static private void close(Closeable file) {\n+        if(file!=null) {\n             try {\n-                String parts[] = source.getFile().split(Pattern.quote(\"!\"));\n-                source = new URL(parts[0]);\n-            } catch (MalformedURLException e) {\n-                return false;\n-            }\n-        }\n-        \n-        File sourceFile=null;\n-        if( source.getProtocol().equals(\"file\") ) {\n-            sourceFile = new File(source.getFile());\n-        }\n-        if( sourceFile!=null && sourceFile.exists() ) {\n-            if( sourceFile.lastModified() > target.lastModified() ) {\n-                return true;\n+                file.close();\n+            } catch (Exception ignore) {\n             }\n         }\n-        return false;\n     }\n \n     private void chmod(String permision, File path) {"
        },
        {
            "index":259,
            "vuln_id":"GHSA-3mw4-6rj6-74g5",
            "cwe_id":"{'CWE-476'}",
            "score":6.5,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/53b0dd6dc5957652f35964af16b892ec9af4a559'}",
            "dataset":"osv",
            "summary":"Null pointer dereference in TensorFlow ### Impact \nThe [implementation of `QuantizedMaxPool`](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/5100e359aef5c8021f2e71c7b986420b85ce7b3d\/tensorflow\/core\/kernels\/quantized_pooling_ops.cc#L114-L130) has an undefined behavior where user controlled inputs can trigger a reference binding to null pointer.\n\n```python\nimport tensorflow as tf\n\ntf.raw_ops.QuantizedMaxPool(\n    input = tf.constant([[[[4]]]], dtype=tf.quint8),\n    min_input = [],\n    max_input = [1],\n    ksize = [1, 1, 1, 1],\n    strides = [1, 1, 1, 1],\n    padding = \"SAME\", name=None\n)\n```\n\n### Patches\nWe have patched the issue in GitHub commit [53b0dd6dc5957652f35964af16b892ec9af4a559](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/53b0dd6dc5957652f35964af16b892ec9af4a559).\n\nThe fix will be included in TensorFlow 2.8.0. We will also cherrypick this commit on TensorFlow 2.7.1, TensorFlow 2.6.3, and TensorFlow 2.5.3, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by Faysal Hossain Shezan from University of Virginia.",
            "published_date":"2022-02-09",
            "chain_len":1,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/53b0dd6dc5957652f35964af16b892ec9af4a559",
            "commit_sha":"53b0dd6dc5957652f35964af16b892ec9af4a559",
            "patch":"SINGLE",
            "chain_ord":"['53b0dd6dc5957652f35964af16b892ec9af4a559']",
            "before_first_fix_commit":"{'19cff800e5805097da69bb1ad0b0a2dd9f83741a'}",
            "last_fix_commit":"53b0dd6dc5957652f35964af16b892ec9af4a559",
            "chain_ord_pos":1.0,
            "commit_datetime":"12\/03\/2021, 18:02:20",
            "message":"Fix nullptr exception in QuantizedMaxPool op when empty list is sent to min_input or max_input parameters.\n\nPiperOrigin-RevId: 413960973\nChange-Id: I9e3ded593f3c4eabf0d6d5dc356e6a19a3ad2682",
            "author":"Isha Arkatkar",
            "comments":null,
            "stats":"{'additions': 14, 'deletions': 0, 'total': 14}",
            "files":"{'tensorflow\/core\/kernels\/quantized_pooling_ops.cc': {'additions': 14, 'deletions': 0, 'changes': 14, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/53b0dd6dc5957652f35964af16b892ec9af4a559\/tensorflow%2Fcore%2Fkernels%2Fquantized_pooling_ops.cc', 'patch': '@@ -15,6 +15,8 @@ limitations under the License.\\n \\n \/\/ See docs in ..\/ops\/nn_ops.cc.\\n \\n+#include \"tensorflow\/core\/framework\/op_requires.h\"\\n+#include \"tensorflow\/core\/platform\/errors.h\"\\n #define EIGEN_USE_THREADS\\n \\n #include \"third_party\/eigen3\/unsupported\/Eigen\/CXX11\/Tensor\"\\n@@ -117,6 +119,18 @@ class QuantizedMaxPoolingOp : public MaxPoolingOp<Device, T> {\\n       : MaxPoolingOp<Device, T>(context) {}\\n \\n   void Compute(OpKernelContext* context) override {\\n+    auto min_input_tensor = context->input(1);\\n+    auto max_input_tensor = context->input(2);\\n+    OP_REQUIRES(\\n+        context, min_input_tensor.NumElements() == 1,\\n+        errors::InvalidArgument(\\n+            \"min_input must be a scalar float value, got tensor with shape \",\\n+            min_input_tensor.shape()));\\n+    OP_REQUIRES(\\n+        context, max_input_tensor.NumElements() == 1,\\n+        errors::InvalidArgument(\\n+            \"max_input must be a scalar float value, got tensor with shape \",\\n+            max_input_tensor.shape()));\\n     const float min_input = context->input(1).flat<float>()(0);\\n     const float max_input = context->input(2).flat<float>()(0);\\n     MaxPoolingOp<Device, T>::Compute(context);'}}",
            "message_norm":"fix nullptr exception in quantizedmaxpool op when empty list is sent to min_input or max_input parameters.\n\npiperorigin-revid: 413960973\nchange-id: i9e3ded593f3c4eabf0d6d5dc356e6a19a3ad2682",
            "language":"en",
            "entities":"[('fix', 'ACTION', ''), ('nullptr exception', 'SECWORD', ''), ('413960973', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/core\/kernels\/quantized_pooling_ops.cc'])",
            "num_files":1.0,
            "patch_content":"From 53b0dd6dc5957652f35964af16b892ec9af4a559 Mon Sep 17 00:00:00 2001\nFrom: Isha Arkatkar <ishark@google.com>\nDate: Fri, 3 Dec 2021 10:02:20 -0800\nSubject: [PATCH] Fix nullptr exception in QuantizedMaxPool op when empty list\n is sent to min_input or max_input parameters.\n\nPiperOrigin-RevId: 413960973\nChange-Id: I9e3ded593f3c4eabf0d6d5dc356e6a19a3ad2682\n---\n tensorflow\/core\/kernels\/quantized_pooling_ops.cc | 14 ++++++++++++++\n 1 file changed, 14 insertions(+)\n\ndiff --git a\/tensorflow\/core\/kernels\/quantized_pooling_ops.cc b\/tensorflow\/core\/kernels\/quantized_pooling_ops.cc\nindex 17df3c223676aa..b512369b3c4dd9 100644\n--- a\/tensorflow\/core\/kernels\/quantized_pooling_ops.cc\n+++ b\/tensorflow\/core\/kernels\/quantized_pooling_ops.cc\n@@ -15,6 +15,8 @@ limitations under the License.\n \n \/\/ See docs in ..\/ops\/nn_ops.cc.\n \n+#include \"tensorflow\/core\/framework\/op_requires.h\"\n+#include \"tensorflow\/core\/platform\/errors.h\"\n #define EIGEN_USE_THREADS\n \n #include \"third_party\/eigen3\/unsupported\/Eigen\/CXX11\/Tensor\"\n@@ -117,6 +119,18 @@ class QuantizedMaxPoolingOp : public MaxPoolingOp<Device, T> {\n       : MaxPoolingOp<Device, T>(context) {}\n \n   void Compute(OpKernelContext* context) override {\n+    auto min_input_tensor = context->input(1);\n+    auto max_input_tensor = context->input(2);\n+    OP_REQUIRES(\n+        context, min_input_tensor.NumElements() == 1,\n+        errors::InvalidArgument(\n+            \"min_input must be a scalar float value, got tensor with shape \",\n+            min_input_tensor.shape()));\n+    OP_REQUIRES(\n+        context, max_input_tensor.NumElements() == 1,\n+        errors::InvalidArgument(\n+            \"max_input must be a scalar float value, got tensor with shape \",\n+            max_input_tensor.shape()));\n     const float min_input = context->input(1).flat<float>()(0);\n     const float max_input = context->input(2).flat<float>()(0);\n     MaxPoolingOp<Device, T>::Compute(context);"
        },
        {
            "index":73,
            "vuln_id":"GHSA-q85f-69q7-55h2",
            "cwe_id":"{'CWE-908'}",
            "score":7.6,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/ef1d027be116f25e25bb94a60da491c2cf55bd0b'}",
            "dataset":"osv",
            "summary":"Uninitialized variable access in Tensorflow ### Impact\nThe [implementation of `AssignOp`](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/a1320ec1eac186da1d03f033109191f715b2b130\/tensorflow\/core\/kernels\/assign_op.h#L30-L143) can result in copying unitialized data to a new tensor. This later results in undefined behavior.\n\nThe implementation has a check that the left hand side of the assignment is initialized (to minimize number of allocations), but does not check that the right hand side is also initialized.\n  \n### Patches\nWe have patched the issue in GitHub commit [ef1d027be116f25e25bb94a60da491c2cf55bd0b](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/ef1d027be116f25e25bb94a60da491c2cf55bd0b).\n    \nThe fix will be included in TensorFlow 2.8.0. We will also cherrypick this commit on TensorFlow 2.7.1, TensorFlow 2.6.3, and TensorFlow 2.5.3, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.",
            "published_date":"2022-02-09",
            "chain_len":1,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/ef1d027be116f25e25bb94a60da491c2cf55bd0b",
            "commit_sha":"ef1d027be116f25e25bb94a60da491c2cf55bd0b",
            "patch":"SINGLE",
            "chain_ord":"['ef1d027be116f25e25bb94a60da491c2cf55bd0b']",
            "before_first_fix_commit":"{'eeb5e2168a5b3a701656b7366e3bc60d5234471e'}",
            "last_fix_commit":"ef1d027be116f25e25bb94a60da491c2cf55bd0b",
            "chain_ord_pos":1.0,
            "commit_datetime":"11\/09\/2021, 19:04:04",
            "message":"Prevent copying uninitialized data in `AssignOp`.\n\nThis prevents harder to debug undefined behaviors that cannot be traced back to the original tensor after assignments occur earlier in the graph execution. Several of these undefined behaviors are just reference bindings to null pointers, which are caught when running under ubsan\/asan.\n\nPiperOrigin-RevId: 408654780\nChange-Id: Iad2ec40d43f5fd7ea016c20283356c12d5ddeab1",
            "author":"Mihai Maruseac",
            "comments":null,
            "stats":"{'additions': 6, 'deletions': 0, 'total': 6}",
            "files":"{'tensorflow\/core\/kernels\/assign_op.h': {'additions': 6, 'deletions': 0, 'changes': 6, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/ef1d027be116f25e25bb94a60da491c2cf55bd0b\/tensorflow%2Fcore%2Fkernels%2Fassign_op.h', 'patch': '@@ -50,6 +50,12 @@ class AssignOp : public OpKernel {\\n     \/\/ We always return the input ref.\\n     context->forward_ref_input_to_ref_output(0, 0);\\n \\n+    \/\/ Prevent copying uninitialized data, to solve harder to debug undefined\\n+    \/\/ behaviors that cannot be traced back to the original tensor.\\n+    OP_REQUIRES(\\n+        context, rhs.IsInitialized(),\\n+        errors::Internal(\"Right hand side of AssignOp is not initialized\"));\\n+\\n     \/\/ We can\\'t always know how this value will be used downstream, so make\\n     \/\/ conservative assumptions in specifying constraints on the memory\\n     \/\/ allocation attributes, unless the Grappler graph analysis determined that'}}",
            "message_norm":"prevent copying uninitialized data in `assignop`.\n\nthis prevents harder to debug undefined behaviors that cannot be traced back to the original tensor after assignments occur earlier in the graph execution. several of these undefined behaviors are just reference bindings to null pointers, which are caught when running under ubsan\/asan.\n\npiperorigin-revid: 408654780\nchange-id: iad2ec40d43f5fd7ea016c20283356c12d5ddeab1",
            "language":"en",
            "entities":"[('prevent', 'ACTION', ''), ('uninitialized', 'SECWORD', ''), ('prevents', 'ACTION', ''), ('408654780', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/core\/kernels\/assign_op.h'])",
            "num_files":1.0,
            "patch_content":"From ef1d027be116f25e25bb94a60da491c2cf55bd0b Mon Sep 17 00:00:00 2001\nFrom: Mihai Maruseac <mihaimaruseac@google.com>\nDate: Tue, 9 Nov 2021 11:04:04 -0800\nSubject: [PATCH] Prevent copying uninitialized data in `AssignOp`.\n\nThis prevents harder to debug undefined behaviors that cannot be traced back to the original tensor after assignments occur earlier in the graph execution. Several of these undefined behaviors are just reference bindings to null pointers, which are caught when running under ubsan\/asan.\n\nPiperOrigin-RevId: 408654780\nChange-Id: Iad2ec40d43f5fd7ea016c20283356c12d5ddeab1\n---\n tensorflow\/core\/kernels\/assign_op.h | 6 ++++++\n 1 file changed, 6 insertions(+)\n\ndiff --git a\/tensorflow\/core\/kernels\/assign_op.h b\/tensorflow\/core\/kernels\/assign_op.h\nindex ca822a58cdacc4..1e84436b27e0d8 100644\n--- a\/tensorflow\/core\/kernels\/assign_op.h\n+++ b\/tensorflow\/core\/kernels\/assign_op.h\n@@ -50,6 +50,12 @@ class AssignOp : public OpKernel {\n     \/\/ We always return the input ref.\n     context->forward_ref_input_to_ref_output(0, 0);\n \n+    \/\/ Prevent copying uninitialized data, to solve harder to debug undefined\n+    \/\/ behaviors that cannot be traced back to the original tensor.\n+    OP_REQUIRES(\n+        context, rhs.IsInitialized(),\n+        errors::Internal(\"Right hand side of AssignOp is not initialized\"));\n+\n     \/\/ We can't always know how this value will be used downstream, so make\n     \/\/ conservative assumptions in specifying constraints on the memory\n     \/\/ allocation attributes, unless the Grappler graph analysis determined that"
        },
        {
            "index":453,
            "vuln_id":"GHSA-4gp3-p7ph-x2jr",
            "cwe_id":"{'CWE-78'}",
            "score":9.8,
            "chain":"{'https:\/\/github.com\/guybedford\/devcert\/commit\/571f4e6d077f7f21c6aed655ae380d85a7a5d3b8'}",
            "dataset":"osv",
            "summary":"OS Command Injection in devcert-sanscache devcert-sanscache before 0.4.7 allows remote attackers to execute arbitrary code or cause a Command Injection via the exec function. The variable `commonName` controlled by user input is used as part of the `exec` function without any sanitization.",
            "published_date":"2020-04-14",
            "chain_len":1,
            "project":"https:\/\/github.com\/guybedford\/devcert",
            "commit_href":"https:\/\/github.com\/guybedford\/devcert\/commit\/571f4e6d077f7f21c6aed655ae380d85a7a5d3b8",
            "commit_sha":"571f4e6d077f7f21c6aed655ae380d85a7a5d3b8",
            "patch":"SINGLE",
            "chain_ord":"['571f4e6d077f7f21c6aed655ae380d85a7a5d3b8']",
            "before_first_fix_commit":"{'651bb8ebb97a029be13656c0476d48db85887451'}",
            "last_fix_commit":"571f4e6d077f7f21c6aed655ae380d85a7a5d3b8",
            "chain_ord_pos":1.0,
            "commit_datetime":"01\/08\/2020, 01:35:58",
            "message":"common name sanitization",
            "author":"Guy Bedford",
            "comments":null,
            "stats":"{'additions': 1, 'deletions': 1, 'total': 2}",
            "files":"{'src\/index.ts': {'additions': 1, 'deletions': 1, 'changes': 2, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/guybedford\/devcert\/raw\/571f4e6d077f7f21c6aed655ae380d85a7a5d3b8\/src%2Findex.ts', 'patch': \"@@ -6,7 +6,7 @@ import fs = require('fs');\\n export default async function generateDevCert (commonName: string) {\\n   if (!commandExists.sync('openssl'))\\n     throw new Error('Unable to find openssl - make sure it is installed and available in your PATH');\\n-  if (!commonName.match(\/^(.|\\\\.){1,64}$\/))\\n+  if (!commonName.match(\/^(a-zA-Z0-9|\\\\.){1,64}$\/))\\n     throw new Error(`Invalid Common Name ${commonName}.`);\\n   try {\\n     const opensslConfPath = generateOpensslConf(commonName);\"}}",
            "message_norm":"common name sanitization",
            "language":"it",
            "entities":"[('sanitization', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['src\/index.ts'])",
            "num_files":1.0,
            "patch_content":"From 571f4e6d077f7f21c6aed655ae380d85a7a5d3b8 Mon Sep 17 00:00:00 2001\nFrom: Guy Bedford <guybedford@gmail.com>\nDate: Wed, 8 Jan 2020 03:35:58 +0200\nSubject: [PATCH] common name sanitization\n\n---\n src\/index.ts | 2 +-\n 1 file changed, 1 insertion(+), 1 deletion(-)\n\ndiff --git a\/src\/index.ts b\/src\/index.ts\nindex fa3ef3b..ce366d1 100644\n--- a\/src\/index.ts\n+++ b\/src\/index.ts\n@@ -6,7 +6,7 @@ import fs = require('fs');\n export default async function generateDevCert (commonName: string) {\n   if (!commandExists.sync('openssl'))\n     throw new Error('Unable to find openssl - make sure it is installed and available in your PATH');\n-  if (!commonName.match(\/^(.|\\.){1,64}$\/))\n+  if (!commonName.match(\/^(a-zA-Z0-9|\\.){1,64}$\/))\n     throw new Error(`Invalid Common Name ${commonName}.`);\n   try {\n     const opensslConfPath = generateOpensslConf(commonName);"
        },
        {
            "index":316,
            "vuln_id":"GHSA-gv26-jpj9-c8gq",
            "cwe_id":"{'CWE-754'}",
            "score":5.3,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/ba6822bd7b7324ba201a28b2f278c29a98edbef2', 'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/f6fde895ef9c77d848061c0517f19d0ec2682f3a'}",
            "dataset":"osv",
            "summary":"Incomplete validation in `SparseSparseMinimum` ### Impact\nIncomplete validation in `SparseAdd` results in allowing attackers to exploit undefined behavior (dereferencing null pointers) as well as write outside of bounds of heap allocated data:\n\n```python \nimport tensorflow as tf\n\na_indices = tf.ones([45, 92], dtype=tf.int64)\na_values = tf.ones([45], dtype=tf.int64)\na_shape = tf.ones([1], dtype=tf.int64)\nb_indices = tf.ones([1, 1], dtype=tf.int64)\nb_values = tf.ones([1], dtype=tf.int64)\nb_shape = tf.ones([1], dtype=tf.int64)\n                    \ntf.raw_ops.SparseSparseMinimum(a_indices=a_indices,\n    a_values=a_values,\n    a_shape=a_shape,\n    b_indices=b_indices,\n    b_values=b_values,\n    b_shape=b_shape)\n```\n\nThe [implementation](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/656e7673b14acd7835dc778867f84916c6d1cac2\/tensorflow\/core\/kernels\/sparse_sparse_binary_op_shared.cc) has a large set of validation for the two sparse tensor inputs (6 tensors in total), but does not validate that the tensors are not empty or that the second dimension of `*_indices` matches the size of corresponding `*_shape`. This allows attackers to send tensor triples that represent invalid sparse tensors to abuse code assumptions that are not protected by validation.\n\n### Patches \nWe have patched the issue in GitHub commit [ba6822bd7b7324ba201a28b2f278c29a98edbef2](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/ba6822bd7b7324ba201a28b2f278c29a98edbef2) followed by GitHub commit [f6fde895ef9c77d848061c0517f19d0ec2682f3a](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/f6fde895ef9c77d848061c0517f19d0ec2682f3a).\n\nThe fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by Ying Wang and Yakun Zhang of Baidu X-Team.",
            "published_date":"2022-03-18",
            "chain_len":2,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/ba6822bd7b7324ba201a28b2f278c29a98edbef2",
            "commit_sha":"ba6822bd7b7324ba201a28b2f278c29a98edbef2",
            "patch":"MULTI",
            "chain_ord":"['ba6822bd7b7324ba201a28b2f278c29a98edbef2', 'f6fde895ef9c77d848061c0517f19d0ec2682f3a']",
            "before_first_fix_commit":"{'cae81a7ae3ca6207396d5c893e8163f4acb34037'}",
            "last_fix_commit":"f6fde895ef9c77d848061c0517f19d0ec2682f3a",
            "chain_ord_pos":1.0,
            "commit_datetime":"04\/28\/2021, 23:06:54",
            "message":"Fix OOB issue with `tf.raw_ops.SparseSparseMinimum`.\n\nPiperOrigin-RevId: 371005787\nChange-Id: Ib686ccc077836e8b980b8b5a03936d36a8ecaf71",
            "author":"Amit Patankar",
            "comments":null,
            "stats":"{'additions': 5, 'deletions': 0, 'total': 5}",
            "files":"{'tensorflow\/core\/kernels\/sparse_sparse_binary_op_shared.cc': {'additions': 5, 'deletions': 0, 'changes': 5, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/ba6822bd7b7324ba201a28b2f278c29a98edbef2\/tensorflow%2Fcore%2Fkernels%2Fsparse_sparse_binary_op_shared.cc', 'patch': '@@ -180,6 +180,11 @@ class SparseSparseBinaryOpShared : public OpKernel {\\n                                           \" for dimension \", i));\\n     }\\n \\n+    OP_REQUIRES(\\n+        ctx, a_indices_t->dim_size(1) == b_indices_t->dim_size(1),\\n+        errors::InvalidArgument(\\n+            \"Indices\\' dimensions do not match: got \", a_indices_t->dim_size(1),\\n+            \" and \", b_indices_t->dim_size(1), \" for the second dimension.\"));\\n     const int num_dims = a_indices_t->dim_size(1);\\n     const auto a_indices_mat = a_indices_t->matrix<int64>();\\n     const auto b_indices_mat = b_indices_t->matrix<int64>();'}}",
            "message_norm":"fix oob issue with `tf.raw_ops.sparsesparseminimum`.\n\npiperorigin-revid: 371005787\nchange-id: ib686ccc077836e8b980b8b5a03936d36a8ecaf71",
            "language":"en",
            "entities":"[('fix', 'ACTION', ''), ('oob', 'SECWORD', ''), ('issue', 'FLAW', ''), ('371005787', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/core\/kernels\/sparse_sparse_binary_op_shared.cc'])",
            "num_files":1.0,
            "patch_content":"From ba6822bd7b7324ba201a28b2f278c29a98edbef2 Mon Sep 17 00:00:00 2001\nFrom: Amit Patankar <amitpatankar@google.com>\nDate: Wed, 28 Apr 2021 16:06:54 -0700\nSubject: [PATCH] Fix OOB issue with `tf.raw_ops.SparseSparseMinimum`.\n\nPiperOrigin-RevId: 371005787\nChange-Id: Ib686ccc077836e8b980b8b5a03936d36a8ecaf71\n---\n tensorflow\/core\/kernels\/sparse_sparse_binary_op_shared.cc | 5 +++++\n 1 file changed, 5 insertions(+)\n\ndiff --git a\/tensorflow\/core\/kernels\/sparse_sparse_binary_op_shared.cc b\/tensorflow\/core\/kernels\/sparse_sparse_binary_op_shared.cc\nindex 43dc9ae70cd627..9fe42e05d879ee 100644\n--- a\/tensorflow\/core\/kernels\/sparse_sparse_binary_op_shared.cc\n+++ b\/tensorflow\/core\/kernels\/sparse_sparse_binary_op_shared.cc\n@@ -180,6 +180,11 @@ class SparseSparseBinaryOpShared : public OpKernel {\n                                           \" for dimension \", i));\n     }\n \n+    OP_REQUIRES(\n+        ctx, a_indices_t->dim_size(1) == b_indices_t->dim_size(1),\n+        errors::InvalidArgument(\n+            \"Indices' dimensions do not match: got \", a_indices_t->dim_size(1),\n+            \" and \", b_indices_t->dim_size(1), \" for the second dimension.\"));\n     const int num_dims = a_indices_t->dim_size(1);\n     const auto a_indices_mat = a_indices_t->matrix<int64>();\n     const auto b_indices_mat = b_indices_t->matrix<int64>();"
        },
        {
            "index":716,
            "vuln_id":"GHSA-jhq9-wm9m-cf89",
            "cwe_id":"{'CWE-617'}",
            "score":2.5,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/704866eabe03a9aeda044ec91a8d0c83fc1ebdbe'}",
            "dataset":"osv",
            "summary":"CHECK-failure in `UnsortedSegmentJoin` ### Impact\nAn attacker can cause a denial of service by controlling the values of `num_segments` tensor argument for `UnsortedSegmentJoin`:\n\n```python\nimport tensorflow as tf\n\ninputs = tf.constant([], dtype=tf.string)\nsegment_ids = tf.constant([], dtype=tf.int32)\nnum_segments = tf.constant([], dtype=tf.int32)\nseparator = ''\n\ntf.raw_ops.UnsortedSegmentJoin(\n  inputs=inputs, segment_ids=segment_ids,\n  num_segments=num_segments, separator=separator)\n```\n\nThis is because the [implementation](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/a2a607db15c7cd01d754d37e5448d72a13491bdb\/tensorflow\/core\/kernels\/unsorted_segment_join_op.cc#L92-L93) assumes that the `num_segments` tensor is a valid scalar:\n\n```cc\nconst Tensor& num_segments_tensor = context->input(2);\nauto num_segments = num_segments_tensor.scalar<NUM_SEGMENTS_TYPE>()();\n```\n\nSince the tensor is empty the `CHECK` involved in `.scalar<T>()()` that checks that the number of elements is exactly 1 will be invalidated and this would result in process termination.\n\n### Patches\nWe have patched the issue in GitHub commit [704866eabe03a9aeda044ec91a8d0c83fc1ebdbe](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/704866eabe03a9aeda044ec91a8d0c83fc1ebdbe).\n\nThe fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by Ying Wang and Yakun Zhang of Baidu X-Team.",
            "published_date":"2021-05-21",
            "chain_len":1,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/704866eabe03a9aeda044ec91a8d0c83fc1ebdbe",
            "commit_sha":"704866eabe03a9aeda044ec91a8d0c83fc1ebdbe",
            "patch":"SINGLE",
            "chain_ord":"['704866eabe03a9aeda044ec91a8d0c83fc1ebdbe']",
            "before_first_fix_commit":"{'a2a607db15c7cd01d754d37e5448d72a13491bdb'}",
            "last_fix_commit":"704866eabe03a9aeda044ec91a8d0c83fc1ebdbe",
            "chain_ord_pos":1.0,
            "commit_datetime":"04\/27\/2021, 21:41:40",
            "message":"Fix overflow CHECK issue with `tf.raw_ops.UnsortedSegmentJoin`.\n\nPiperOrigin-RevId: 370766155\nChange-Id: I33e7c6626224e1060a8a4ab51ad5d861c6d4c63e",
            "author":"Amit Patankar",
            "comments":null,
            "stats":"{'additions': 2, 'deletions': 0, 'total': 2}",
            "files":"{'tensorflow\/core\/kernels\/unsorted_segment_join_op.cc': {'additions': 2, 'deletions': 0, 'changes': 2, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/704866eabe03a9aeda044ec91a8d0c83fc1ebdbe\/tensorflow%2Fcore%2Fkernels%2Funsorted_segment_join_op.cc', 'patch': '@@ -90,6 +90,8 @@ class UnsortedSegmentJoinOp : public OpKernel {\\n     const int32 segment_dims = segment_id_shape.dims();\\n \\n     const Tensor& num_segments_tensor = context->input(2);\\n+    OP_REQUIRES(context, num_segments_tensor.NumElements() != 0,\\n+                errors::InvalidArgument(\"Number of segments cannot be empty.\"));\\n     auto num_segments = num_segments_tensor.scalar<NUM_SEGMENTS_TYPE>()();\\n \\n     OP_REQUIRES(context, segment_dims != 0,'}}",
            "message_norm":"fix overflow check issue with `tf.raw_ops.unsortedsegmentjoin`.\n\npiperorigin-revid: 370766155\nchange-id: i33e7c6626224e1060a8a4ab51ad5d861c6d4c63e",
            "language":"en",
            "entities":"[('fix', 'ACTION', ''), ('overflow', 'SECWORD', ''), ('issue', 'FLAW', ''), ('370766155', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/core\/kernels\/unsorted_segment_join_op.cc'])",
            "num_files":1.0,
            "patch_content":"From 704866eabe03a9aeda044ec91a8d0c83fc1ebdbe Mon Sep 17 00:00:00 2001\nFrom: Amit Patankar <amitpatankar@google.com>\nDate: Tue, 27 Apr 2021 14:41:40 -0700\nSubject: [PATCH] Fix overflow CHECK issue with\n `tf.raw_ops.UnsortedSegmentJoin`.\n\nPiperOrigin-RevId: 370766155\nChange-Id: I33e7c6626224e1060a8a4ab51ad5d861c6d4c63e\n---\n tensorflow\/core\/kernels\/unsorted_segment_join_op.cc | 2 ++\n 1 file changed, 2 insertions(+)\n\ndiff --git a\/tensorflow\/core\/kernels\/unsorted_segment_join_op.cc b\/tensorflow\/core\/kernels\/unsorted_segment_join_op.cc\nindex 7464e165e46c8b..9acfe7fb1e4952 100644\n--- a\/tensorflow\/core\/kernels\/unsorted_segment_join_op.cc\n+++ b\/tensorflow\/core\/kernels\/unsorted_segment_join_op.cc\n@@ -90,6 +90,8 @@ class UnsortedSegmentJoinOp : public OpKernel {\n     const int32 segment_dims = segment_id_shape.dims();\n \n     const Tensor& num_segments_tensor = context->input(2);\n+    OP_REQUIRES(context, num_segments_tensor.NumElements() != 0,\n+                errors::InvalidArgument(\"Number of segments cannot be empty.\"));\n     auto num_segments = num_segments_tensor.scalar<NUM_SEGMENTS_TYPE>()();\n \n     OP_REQUIRES(context, segment_dims != 0,"
        },
        {
            "index":249,
            "vuln_id":"GHSA-23cm-x6j7-6hq3",
            "cwe_id":"{'CWE-200'}",
            "score":5.9,
            "chain":"{'https:\/\/github.com\/matrix-org\/matrix-js-sdk\/commit\/894c24880da0e1cc81818f51c0db80e3c9fb2be9'}",
            "dataset":"osv",
            "summary":"matrix-js-sdk can be tricked into disclosing E2EE room keys to a participating homeserver ### Impact\n\nA logic error in the room key sharing functionality of matrix-js-sdk before 12.4.1 allows a malicious Matrix homeserver\u2020 participating in an encrypted room to steal room encryption keys from affected Matrix clients participating in that room. This allows the homeserver to decrypt end-to-end encrypted messages sent by affected clients.\n\n\u2020 Or anyone with access to the account of the original recipient of an encrypted message.\n\nKnown clients affected (via their use of vulnerable versions of matrix-js-sdk):\n\n- Element Web (1.8.2 and earlier)\n- Element Desktop (1.8.2 and earlier)\n- SchildiChat Web (1.7.32-sc1 and earlier)\n- SchildiChat Desktop (1.7.32-sc1 and earlier)\n- Cinny (1.2.0 and earlier)\n\n### Patch\n\nThis was fixed in https:\/\/github.com\/matrix-org\/matrix-js-sdk\/commit\/894c24880da0e1cc81818f51c0db80e3c9fb2be9.\n\n### Workarounds\nTo prevent a homeserver from being able to steal the room keys, vulnerable clients can be taken offline or signed out. If signing out, care should be taken to either set up Secure Backup or export E2E room keys in order to preserve access to past messages.",
            "published_date":"2021-09-14",
            "chain_len":1,
            "project":"https:\/\/github.com\/matrix-org\/matrix-js-sdk",
            "commit_href":"https:\/\/github.com\/matrix-org\/matrix-js-sdk\/commit\/894c24880da0e1cc81818f51c0db80e3c9fb2be9",
            "commit_sha":"894c24880da0e1cc81818f51c0db80e3c9fb2be9",
            "patch":"SINGLE",
            "chain_ord":"['894c24880da0e1cc81818f51c0db80e3c9fb2be9']",
            "before_first_fix_commit":"{'f8186add92dd5f0ca2f6a1cda10bc0ece3730f86'}",
            "last_fix_commit":"894c24880da0e1cc81818f51c0db80e3c9fb2be9",
            "chain_ord_pos":1.0,
            "commit_datetime":"09\/13\/2021, 11:34:48",
            "message":"Verify target device key on reshare",
            "author":"RiotRobot",
            "comments":null,
            "stats":"{'additions': 29, 'deletions': 9, 'total': 38}",
            "files":"{'src\/crypto\/algorithms\/megolm.ts': {'additions': 29, 'deletions': 9, 'changes': 38, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/matrix-org\/matrix-js-sdk\/raw\/894c24880da0e1cc81818f51c0db80e3c9fb2be9\/src%2Fcrypto%2Falgorithms%2Fmegolm.ts', 'patch': '@@ -101,6 +101,13 @@ interface IPayload extends Partial<IMessage> {\\n }\\n \/* eslint-enable camelcase *\/\\n \\n+interface SharedWithData {\\n+    \/\/ The identity key of the device we shared with\\n+    deviceKey: string;\\n+    \/\/ The message index of the ratchet we shared with that device\\n+    messageIndex: number;\\n+}\\n+\\n \/**\\n  * @private\\n  * @constructor\\n@@ -115,12 +122,12 @@ interface IPayload extends Partial<IMessage> {\\n  *\\n  * @property {object} sharedWithDevices\\n  *    devices with which we have shared the session key\\n- *        userId -> {deviceId -> msgindex}\\n+ *        userId -> {deviceId -> SharedWithData}\\n  *\/\\n class OutboundSessionInfo {\\n     public useCount = 0;\\n     public creationTime: number;\\n-    public sharedWithDevices: Record<string, Record<string, number>> = {};\\n+    public sharedWithDevices: Record<string, Record<string, SharedWithData>> = {};\\n     public blockedDevicesNotified: Record<string, Record<string, boolean>> = {};\\n \\n     constructor(public readonly sessionId: string, public readonly sharedHistory = false) {\\n@@ -150,11 +157,11 @@ class OutboundSessionInfo {\\n         return false;\\n     }\\n \\n-    public markSharedWithDevice(userId: string, deviceId: string, chainIndex: number): void {\\n+    public markSharedWithDevice(userId: string, deviceId: string, deviceKey: string, chainIndex: number): void {\\n         if (!this.sharedWithDevices[userId]) {\\n             this.sharedWithDevices[userId] = {};\\n         }\\n-        this.sharedWithDevices[userId][deviceId] = chainIndex;\\n+        this.sharedWithDevices[userId][deviceId] = { deviceKey, messageIndex: chainIndex };\\n     }\\n \\n     public markNotifiedBlockedDevice(userId: string, deviceId: string): void {\\n@@ -572,6 +579,7 @@ class MegolmEncryption extends EncryptionAlgorithm {\\n         payload: IPayload,\\n     ): Promise<void> {\\n         const contentMap = {};\\n+        const deviceInfoByDeviceId = new Map<string, DeviceInfo>();\\n \\n         const promises = [];\\n         for (let i = 0; i < userDeviceMap.length; i++) {\\n@@ -584,6 +592,7 @@ class MegolmEncryption extends EncryptionAlgorithm {\\n             const userId = val.userId;\\n             const deviceInfo = val.deviceInfo;\\n             const deviceId = deviceInfo.deviceId;\\n+            deviceInfoByDeviceId.set(deviceId, deviceInfo);\\n \\n             if (!contentMap[userId]) {\\n                 contentMap[userId] = {};\\n@@ -636,7 +645,10 @@ class MegolmEncryption extends EncryptionAlgorithm {\\n                 for (const userId of Object.keys(contentMap)) {\\n                     for (const deviceId of Object.keys(contentMap[userId])) {\\n                         session.markSharedWithDevice(\\n-                            userId, deviceId, chainIndex,\\n+                            userId,\\n+                            deviceId,\\n+                            deviceInfoByDeviceId.get(deviceId).getIdentityKey(),\\n+                            chainIndex,\\n                         );\\n                     }\\n                 }\\n@@ -719,19 +731,27 @@ class MegolmEncryption extends EncryptionAlgorithm {\\n             logger.debug(`megolm session ${sessionId} never shared with user ${userId}`);\\n             return;\\n         }\\n-        const sentChainIndex = obSessionInfo.sharedWithDevices[userId][device.deviceId];\\n-        if (sentChainIndex === undefined) {\\n+        const sessionSharedData = obSessionInfo.sharedWithDevices[userId][device.deviceId];\\n+        if (sessionSharedData === undefined) {\\n             logger.debug(\\n                 \"megolm session ID \" + sessionId + \" never shared with device \" +\\n                 userId + \":\" + device.deviceId,\\n             );\\n             return;\\n         }\\n \\n+        if (sessionSharedData.deviceKey !== device.getIdentityKey()) {\\n+            logger.warn(\\n+                `Session has been shared with device ${device.deviceId} but with identity ` +\\n+                `key ${sessionSharedData.deviceKey}. Key is now ${device.getIdentityKey()}!`,\\n+            );\\n+            return;\\n+        }\\n+\\n         \/\/ get the key from the inbound session: the outbound one will already\\n         \/\/ have been ratcheted to the next chain index.\\n         const key = await this.olmDevice.getInboundGroupSessionKey(\\n-            this.roomId, senderKey, sessionId, sentChainIndex,\\n+            this.roomId, senderKey, sessionId, sessionSharedData.messageIndex,\\n         );\\n \\n         if (!key) {\\n@@ -882,7 +902,7 @@ class MegolmEncryption extends EncryptionAlgorithm {\\n             const deviceId = deviceInfo.deviceId;\\n \\n             session.markSharedWithDevice(\\n-                userId, deviceId, key.chain_index,\\n+                userId, deviceId, deviceInfo.getIdentityKey(), key.chain_index,\\n             );\\n         }'}}",
            "message_norm":"verify target device key on reshare",
            "language":"en",
            "entities":"[('verify', 'ACTION', ''), ('key', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['src\/crypto\/algorithms\/megolm.ts'])",
            "num_files":1.0,
            "patch_content":"From 894c24880da0e1cc81818f51c0db80e3c9fb2be9 Mon Sep 17 00:00:00 2001\nFrom: RiotRobot <releases@riot.im>\nDate: Mon, 13 Sep 2021 12:34:48 +0100\nSubject: [PATCH] Verify target device key on reshare\n\n---\n src\/crypto\/algorithms\/megolm.ts | 38 +++++++++++++++++++++++++--------\n 1 file changed, 29 insertions(+), 9 deletions(-)\n\ndiff --git a\/src\/crypto\/algorithms\/megolm.ts b\/src\/crypto\/algorithms\/megolm.ts\nindex 381f8518880..55a766620c2 100644\n--- a\/src\/crypto\/algorithms\/megolm.ts\n+++ b\/src\/crypto\/algorithms\/megolm.ts\n@@ -101,6 +101,13 @@ interface IPayload extends Partial<IMessage> {\n }\n \/* eslint-enable camelcase *\/\n \n+interface SharedWithData {\n+    \/\/ The identity key of the device we shared with\n+    deviceKey: string;\n+    \/\/ The message index of the ratchet we shared with that device\n+    messageIndex: number;\n+}\n+\n \/**\n  * @private\n  * @constructor\n@@ -115,12 +122,12 @@ interface IPayload extends Partial<IMessage> {\n  *\n  * @property {object} sharedWithDevices\n  *    devices with which we have shared the session key\n- *        userId -> {deviceId -> msgindex}\n+ *        userId -> {deviceId -> SharedWithData}\n  *\/\n class OutboundSessionInfo {\n     public useCount = 0;\n     public creationTime: number;\n-    public sharedWithDevices: Record<string, Record<string, number>> = {};\n+    public sharedWithDevices: Record<string, Record<string, SharedWithData>> = {};\n     public blockedDevicesNotified: Record<string, Record<string, boolean>> = {};\n \n     constructor(public readonly sessionId: string, public readonly sharedHistory = false) {\n@@ -150,11 +157,11 @@ class OutboundSessionInfo {\n         return false;\n     }\n \n-    public markSharedWithDevice(userId: string, deviceId: string, chainIndex: number): void {\n+    public markSharedWithDevice(userId: string, deviceId: string, deviceKey: string, chainIndex: number): void {\n         if (!this.sharedWithDevices[userId]) {\n             this.sharedWithDevices[userId] = {};\n         }\n-        this.sharedWithDevices[userId][deviceId] = chainIndex;\n+        this.sharedWithDevices[userId][deviceId] = { deviceKey, messageIndex: chainIndex };\n     }\n \n     public markNotifiedBlockedDevice(userId: string, deviceId: string): void {\n@@ -572,6 +579,7 @@ class MegolmEncryption extends EncryptionAlgorithm {\n         payload: IPayload,\n     ): Promise<void> {\n         const contentMap = {};\n+        const deviceInfoByDeviceId = new Map<string, DeviceInfo>();\n \n         const promises = [];\n         for (let i = 0; i < userDeviceMap.length; i++) {\n@@ -584,6 +592,7 @@ class MegolmEncryption extends EncryptionAlgorithm {\n             const userId = val.userId;\n             const deviceInfo = val.deviceInfo;\n             const deviceId = deviceInfo.deviceId;\n+            deviceInfoByDeviceId.set(deviceId, deviceInfo);\n \n             if (!contentMap[userId]) {\n                 contentMap[userId] = {};\n@@ -636,7 +645,10 @@ class MegolmEncryption extends EncryptionAlgorithm {\n                 for (const userId of Object.keys(contentMap)) {\n                     for (const deviceId of Object.keys(contentMap[userId])) {\n                         session.markSharedWithDevice(\n-                            userId, deviceId, chainIndex,\n+                            userId,\n+                            deviceId,\n+                            deviceInfoByDeviceId.get(deviceId).getIdentityKey(),\n+                            chainIndex,\n                         );\n                     }\n                 }\n@@ -719,8 +731,8 @@ class MegolmEncryption extends EncryptionAlgorithm {\n             logger.debug(`megolm session ${sessionId} never shared with user ${userId}`);\n             return;\n         }\n-        const sentChainIndex = obSessionInfo.sharedWithDevices[userId][device.deviceId];\n-        if (sentChainIndex === undefined) {\n+        const sessionSharedData = obSessionInfo.sharedWithDevices[userId][device.deviceId];\n+        if (sessionSharedData === undefined) {\n             logger.debug(\n                 \"megolm session ID \" + sessionId + \" never shared with device \" +\n                 userId + \":\" + device.deviceId,\n@@ -728,10 +740,18 @@ class MegolmEncryption extends EncryptionAlgorithm {\n             return;\n         }\n \n+        if (sessionSharedData.deviceKey !== device.getIdentityKey()) {\n+            logger.warn(\n+                `Session has been shared with device ${device.deviceId} but with identity ` +\n+                `key ${sessionSharedData.deviceKey}. Key is now ${device.getIdentityKey()}!`,\n+            );\n+            return;\n+        }\n+\n         \/\/ get the key from the inbound session: the outbound one will already\n         \/\/ have been ratcheted to the next chain index.\n         const key = await this.olmDevice.getInboundGroupSessionKey(\n-            this.roomId, senderKey, sessionId, sentChainIndex,\n+            this.roomId, senderKey, sessionId, sessionSharedData.messageIndex,\n         );\n \n         if (!key) {\n@@ -882,7 +902,7 @@ class MegolmEncryption extends EncryptionAlgorithm {\n             const deviceId = deviceInfo.deviceId;\n \n             session.markSharedWithDevice(\n-                userId, deviceId, key.chain_index,\n+                userId, deviceId, deviceInfo.getIdentityKey(), key.chain_index,\n             );\n         }"
        },
        {
            "index":153,
            "vuln_id":"GHSA-q26w-wjj2-22vv",
            "cwe_id":"{'CWE-79'}",
            "score":6.1,
            "chain":"{'https:\/\/github.com\/laurent22\/joplin\/commit\/fd90a490c0e5cacd17bfe0ffc422be1d2a9b1c13'}",
            "dataset":"osv",
            "summary":"Cross-site scripting in Joplin Joplin allows XSS via a LINK element in a note.",
            "published_date":"2021-05-10",
            "chain_len":1,
            "project":"https:\/\/github.com\/laurent22\/joplin",
            "commit_href":"https:\/\/github.com\/laurent22\/joplin\/commit\/fd90a490c0e5cacd17bfe0ffc422be1d2a9b1c13",
            "commit_sha":"fd90a490c0e5cacd17bfe0ffc422be1d2a9b1c13",
            "patch":"SINGLE",
            "chain_ord":"['fd90a490c0e5cacd17bfe0ffc422be1d2a9b1c13']",
            "before_first_fix_commit":"{'4a184721e4e4aa00a39d508cdc1a3ae660d3610e'}",
            "last_fix_commit":"fd90a490c0e5cacd17bfe0ffc422be1d2a9b1c13",
            "chain_ord_pos":1.0,
            "commit_datetime":"10\/29\/2020, 16:19:56",
            "message":"All: Security: Remove \"link\" and \"meta\" tags from notes to prevent XSS",
            "author":"Laurent Cozic",
            "comments":null,
            "stats":"{'additions': 10, 'deletions': 5, 'total': 15}",
            "files":"{'ReactNativeClient\/lib\/joplin-renderer\/htmlUtils.js': {'additions': 10, 'deletions': 5, 'changes': 15, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/laurent22\/joplin\/raw\/fd90a490c0e5cacd17bfe0ffc422be1d2a9b1c13\/ReactNativeClient%2Flib%2Fjoplin-renderer%2FhtmlUtils.js', 'patch': '@@ -87,11 +87,16 @@ class HtmlUtils {\\n \\t\\t\\treturn tagStack[tagStack.length - 1];\\n \\t\\t};\\n \\n-\\t\\t\/\/ The BASE tag allows changing the base URL from which files are loaded, and\\n-\\t\\t\/\/ that can break several plugins, such as Katex (which needs to load CSS\\n-\\t\\t\/\/ files using a relative URL). For that reason it is disabled.\\n-\\t\\t\/\/ More info: https:\/\/github.com\/laurent22\/joplin\/issues\/3021\\n-\\t\\tconst disallowedTags = [\\'script\\', \\'iframe\\', \\'frameset\\', \\'frame\\', \\'object\\', \\'base\\', \\'embed\\'];\\n+\\t\\t\/\/ The BASE tag allows changing the base URL from which files are\\n+\\t\\t\/\/ loaded, and that can break several plugins, such as Katex (which\\n+\\t\\t\/\/ needs to load CSS files using a relative URL). For that reason\\n+\\t\\t\/\/ it is disabled. More info:\\n+\\t\\t\/\/ https:\/\/github.com\/laurent22\/joplin\/issues\/3021\\n+\\t\\t\/\/\\n+\\t\\t\/\/ \"link\" can be used to escape the parser and inject JavaScript.\\n+\\t\\t\/\/ Adding \"meta\" too for the same reason as it shouldn\\'t be used in\\n+\\t\\t\/\/ notes anyway.\\n+\\t\\tconst disallowedTags = [\\'script\\', \\'iframe\\', \\'frameset\\', \\'frame\\', \\'object\\', \\'base\\', \\'embed\\', \\'link\\', \\'meta\\'];\\n \\n \\t\\tconst parser = new htmlparser2.Parser({'}}",
            "message_norm":"all: security: remove \"link\" and \"meta\" tags from notes to prevent xss",
            "language":"en",
            "entities":"[('security', 'SECWORD', ''), ('remove', 'ACTION', ''), ('prevent', 'ACTION', ''), ('xss', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['ReactNativeClient\/lib\/joplin-renderer\/htmlUtils.js'])",
            "num_files":1.0,
            "patch_content":"From fd90a490c0e5cacd17bfe0ffc422be1d2a9b1c13 Mon Sep 17 00:00:00 2001\nFrom: Laurent Cozic <laurent@cozic.net>\nDate: Thu, 29 Oct 2020 16:19:56 +0000\nSubject: [PATCH] All: Security: Remove \"link\" and \"meta\" tags from notes to\n prevent XSS\n\n---\n ...\/lib\/joplin-renderer\/htmlUtils.js              | 15 ++++++++++-----\n 1 file changed, 10 insertions(+), 5 deletions(-)\n\ndiff --git a\/ReactNativeClient\/lib\/joplin-renderer\/htmlUtils.js b\/ReactNativeClient\/lib\/joplin-renderer\/htmlUtils.js\nindex f9af4d97886..a9581f70aae 100644\n--- a\/ReactNativeClient\/lib\/joplin-renderer\/htmlUtils.js\n+++ b\/ReactNativeClient\/lib\/joplin-renderer\/htmlUtils.js\n@@ -87,11 +87,16 @@ class HtmlUtils {\n \t\t\treturn tagStack[tagStack.length - 1];\n \t\t};\n \n-\t\t\/\/ The BASE tag allows changing the base URL from which files are loaded, and\n-\t\t\/\/ that can break several plugins, such as Katex (which needs to load CSS\n-\t\t\/\/ files using a relative URL). For that reason it is disabled.\n-\t\t\/\/ More info: https:\/\/github.com\/laurent22\/joplin\/issues\/3021\n-\t\tconst disallowedTags = ['script', 'iframe', 'frameset', 'frame', 'object', 'base', 'embed'];\n+\t\t\/\/ The BASE tag allows changing the base URL from which files are\n+\t\t\/\/ loaded, and that can break several plugins, such as Katex (which\n+\t\t\/\/ needs to load CSS files using a relative URL). For that reason\n+\t\t\/\/ it is disabled. More info:\n+\t\t\/\/ https:\/\/github.com\/laurent22\/joplin\/issues\/3021\n+\t\t\/\/\n+\t\t\/\/ \"link\" can be used to escape the parser and inject JavaScript.\n+\t\t\/\/ Adding \"meta\" too for the same reason as it shouldn't be used in\n+\t\t\/\/ notes anyway.\n+\t\tconst disallowedTags = ['script', 'iframe', 'frameset', 'frame', 'object', 'base', 'embed', 'link', 'meta'];\n \n \t\tconst parser = new htmlparser2.Parser({"
        },
        {
            "index":270,
            "vuln_id":"GHSA-pwwm-pwx2-2hw7",
            "cwe_id":"{'CWE-209'}",
            "score":5.3,
            "chain":"{'https:\/\/github.com\/snipe\/snipe-it\/commit\/178e44095141ab805c282f563fb088df1a10b2e2'}",
            "dataset":"osv",
            "summary":"Generation of Error Message Containing Sensitive Information in Snipe-IT Snipe-IT prior to version 5.3.11 is vulnerable to Generation of Error Message Containing Sensitive Information.",
            "published_date":"2022-02-18",
            "chain_len":1,
            "project":"https:\/\/github.com\/snipe\/snipe-it",
            "commit_href":"https:\/\/github.com\/snipe\/snipe-it\/commit\/178e44095141ab805c282f563fb088df1a10b2e2",
            "commit_sha":"178e44095141ab805c282f563fb088df1a10b2e2",
            "patch":"SINGLE",
            "chain_ord":"['178e44095141ab805c282f563fb088df1a10b2e2']",
            "before_first_fix_commit":"{'321be4733d3997fc738f0118e1b9af5905f95439'}",
            "last_fix_commit":"178e44095141ab805c282f563fb088df1a10b2e2",
            "chain_ord_pos":1.0,
            "commit_datetime":"02\/16\/2022, 02:09:58",
            "message":"Added usleep :(\n\nSigned-off-by: snipe <snipe@snipe.net>",
            "author":"snipe",
            "comments":null,
            "stats":"{'additions': 2, 'deletions': 0, 'total': 2}",
            "files":"{'app\/Http\/Controllers\/Auth\/ForgotPasswordController.php': {'additions': 2, 'deletions': 0, 'changes': 2, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/snipe\/snipe-it\/raw\/178e44095141ab805c282f563fb088df1a10b2e2\/app%2FHttp%2FControllers%2FAuth%2FForgotPasswordController.php', 'patch': \"@@ -87,6 +87,8 @@ public function sendResetLinkEmail(Request $request)\\n             \\\\Log::info('Password reset attempt: User '.$request->input('username').'failed with exception: '.$e );\\n         }\\n \\n+        \/\/ Prevent timing attack to enumerate users.\\n+        usleep(500000 + random_int(0, 1500000));\\n \\n         if ($response === \\\\Password::RESET_LINK_SENT) {\\n             \\\\Log::info('Password reset attempt: User '.$request->input('username').' WAS found, password reset sent');\"}}",
            "message_norm":"added usleep :(\n\nsigned-off-by: snipe <snipe@snipe.net>",
            "language":"en",
            "entities":"[('added', 'ACTION', ''), ('snipe@snipe.net', 'EMAIL', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['app\/Http\/Controllers\/Auth\/ForgotPasswordController.php'])",
            "num_files":1.0,
            "patch_content":"From 178e44095141ab805c282f563fb088df1a10b2e2 Mon Sep 17 00:00:00 2001\nFrom: snipe <snipe@snipe.net>\nDate: Tue, 15 Feb 2022 18:09:58 -0800\nSubject: [PATCH] Added usleep :(\n\nSigned-off-by: snipe <snipe@snipe.net>\n---\n app\/Http\/Controllers\/Auth\/ForgotPasswordController.php | 2 ++\n 1 file changed, 2 insertions(+)\n\ndiff --git a\/app\/Http\/Controllers\/Auth\/ForgotPasswordController.php b\/app\/Http\/Controllers\/Auth\/ForgotPasswordController.php\nindex 3619b4e5bfdd..62798745a791 100644\n--- a\/app\/Http\/Controllers\/Auth\/ForgotPasswordController.php\n+++ b\/app\/Http\/Controllers\/Auth\/ForgotPasswordController.php\n@@ -87,6 +87,8 @@ public function sendResetLinkEmail(Request $request)\n             \\Log::info('Password reset attempt: User '.$request->input('username').'failed with exception: '.$e );\n         }\n \n+        \/\/ Prevent timing attack to enumerate users.\n+        usleep(500000 + random_int(0, 1500000));\n \n         if ($response === \\Password::RESET_LINK_SENT) {\n             \\Log::info('Password reset attempt: User '.$request->input('username').' WAS found, password reset sent');"
        },
        {
            "index":505,
            "vuln_id":"GHSA-5c8j-xr24-2665",
            "cwe_id":"{'CWE-77'}",
            "score":9.8,
            "chain":"{'https:\/\/github.com\/tojocky\/node-printer\/commit\/e001e38738c17219a1d9dd8c31f7d82b9c0013c7'}",
            "dataset":"osv",
            "summary":"Potential Command Injection in printer Versions 0.0.1 and earlier of `printer` are affected by a command injection vulnerability resulting from a failure to sanitize command arguments properly in the `printDirect()` function. \n\n\n\n## Recommendation\n\nUpdate to version 0.0.2 or later.",
            "published_date":"2017-11-28",
            "chain_len":1,
            "project":"https:\/\/github.com\/tojocky\/node-printer",
            "commit_href":"https:\/\/github.com\/tojocky\/node-printer\/commit\/e001e38738c17219a1d9dd8c31f7d82b9c0013c7",
            "commit_sha":"e001e38738c17219a1d9dd8c31f7d82b9c0013c7",
            "patch":"SINGLE",
            "chain_ord":"['e001e38738c17219a1d9dd8c31f7d82b9c0013c7']",
            "before_first_fix_commit":"{'7987544670c37fdef659f8ee9e5db20fae118705'}",
            "last_fix_commit":"e001e38738c17219a1d9dd8c31f7d82b9c0013c7",
            "chain_ord_pos":1.0,
            "commit_datetime":"06\/28\/2013, 18:30:28",
            "message":"Removed possible command injection",
            "author":"chieffancypants",
            "comments":null,
            "stats":"{'additions': 1, 'deletions': 1, 'total': 2}",
            "files":"{'lib\/printer.js': {'additions': 1, 'deletions': 1, 'changes': 2, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tojocky\/node-printer\/raw\/e001e38738c17219a1d9dd8c31f7d82b9c0013c7\/lib%2Fprinter.js', 'patch': '@@ -93,7 +93,7 @@ function printDirect(parameters){\\n     }else if (!printer_helper.printDirect){\/\/ should be POSIX\\n         var temp_file_name = path.join(os.tmpDir(),\"printing\");\\n         fs.writeFileSync(temp_file_name, data);\\n-        child_process.exec(\\'lpr -P\\'+printer+\\' -oraw -r\\'+\\' \\'+temp_file_name, function(err, stdout, stderr){\\n+        child_process.execFile(\\'lpr\\', [\\'-P\\' + printer, \\'-oraw\\', \\'-r\\', temp_file_name], function(err, stdout, stderr){\\n             if (err !== null) {\\n                 error(\\'ERROR: \\' + err);\\n                 return;'}}",
            "message_norm":"removed possible command injection",
            "language":"en",
            "entities":"[('removed', 'ACTION', ''), ('possible command injection', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['lib\/printer.js'])",
            "num_files":1.0,
            "patch_content":"From e001e38738c17219a1d9dd8c31f7d82b9c0013c7 Mon Sep 17 00:00:00 2001\nFrom: chieffancypants <chieffancypants@gmail.com>\nDate: Fri, 28 Jun 2013 14:30:28 -0400\nSubject: [PATCH] Removed possible command injection\n\n---\n lib\/printer.js | 2 +-\n 1 file changed, 1 insertion(+), 1 deletion(-)\n\ndiff --git a\/lib\/printer.js b\/lib\/printer.js\nindex a1840347..6cb06da1 100644\n--- a\/lib\/printer.js\n+++ b\/lib\/printer.js\n@@ -93,7 +93,7 @@ function printDirect(parameters){\n     }else if (!printer_helper.printDirect){\/\/ should be POSIX\n         var temp_file_name = path.join(os.tmpDir(),\"printing\");\n         fs.writeFileSync(temp_file_name, data);\n-        child_process.exec('lpr -P'+printer+' -oraw -r'+' '+temp_file_name, function(err, stdout, stderr){\n+        child_process.execFile('lpr', ['-P' + printer, '-oraw', '-r', temp_file_name], function(err, stdout, stderr){\n             if (err !== null) {\n                 error('ERROR: ' + err);\n                 return;"
        },
        {
            "index":98,
            "vuln_id":"GHSA-g4h2-gqm3-c9wq",
            "cwe_id":"{'CWE-681'}",
            "score":2.5,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/4f663d4b8f0bec1b48da6fa091a7d29609980fa4'}",
            "dataset":"osv",
            "summary":"Segfault in tf.raw_ops.ImmutableConst ### Impact\nCalling [`tf.raw_ops.ImmutableConst`](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/raw_ops\/ImmutableConst) with a `dtype` of `tf.resource` or `tf.variant` results in a segfault in the implementation as code assumes that the tensor contents are pure scalars.\n\n```python\n>>> import tensorflow as tf\n>>> tf.raw_ops.ImmutableConst(dtype=tf.resource, shape=[], memory_region_name=\"\/tmp\/test.txt\")\n...\nSegmentation fault\n```\n\n### Patches\nWe have patched the issue in 4f663d4b8f0bec1b48da6fa091a7d29609980fa4 and will release TensorFlow 2.5.0 containing the patch. TensorFlow nightly packages after this commit will also have the issue resolved.\n\n### Workarounds\nIf using `tf.raw_ops.ImmutableConst` in code, you can prevent the segfault by inserting a filter for the `dtype` argument.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.",
            "published_date":"2021-05-21",
            "chain_len":1,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/4f663d4b8f0bec1b48da6fa091a7d29609980fa4",
            "commit_sha":"4f663d4b8f0bec1b48da6fa091a7d29609980fa4",
            "patch":"SINGLE",
            "chain_ord":"['4f663d4b8f0bec1b48da6fa091a7d29609980fa4']",
            "before_first_fix_commit":"{'f0e867da82025a97524a233eaedfccfcf6295d5c'}",
            "last_fix_commit":"4f663d4b8f0bec1b48da6fa091a7d29609980fa4",
            "chain_ord_pos":1.0,
            "commit_datetime":"02\/08\/2021, 20:29:30",
            "message":"Allowlist certain data types to avoid a seg fault.\n\nPiperOrigin-RevId: 356326671\nChange-Id: I23b65b52e93798cb5a6744632d31b0f88c6b6b31",
            "author":"Amit Patankar",
            "comments":null,
            "stats":"{'additions': 5, 'deletions': 0, 'total': 5}",
            "files":"{'tensorflow\/core\/kernels\/immutable_constant_op.cc': {'additions': 5, 'deletions': 0, 'changes': 5, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/4f663d4b8f0bec1b48da6fa091a7d29609980fa4\/tensorflow%2Fcore%2Fkernels%2Fimmutable_constant_op.cc', 'patch': '@@ -17,6 +17,8 @@ limitations under the License.\\n \\n #include <unordered_set>\\n \\n+#include \"tensorflow\/core\/framework\/types.pb.h\"\\n+\\n namespace tensorflow {\\n \\n namespace {\\n@@ -86,6 +88,9 @@ ImmutableConstantOp::ImmutableConstantOp(OpKernelConstruction* context)\\n   OP_REQUIRES_OK(context,\\n                  context->GetAttr(kMemoryRegionNameAttr, &region_name_));\\n   OP_REQUIRES_OK(context, context->GetAttr(kDTypeAttr, &dtype_));\\n+  OP_REQUIRES(context, dtype_ != DT_RESOURCE && dtype_ != DT_VARIANT,\\n+              errors::InvalidArgument(\\n+                  \"Resource and variant dtypes are invalid for this op.\"));\\n   OP_REQUIRES_OK(context, context->GetAttr(kShapeAttr, &shape_));\\n }'}}",
            "message_norm":"allowlist certain data types to avoid a seg fault.\n\npiperorigin-revid: 356326671\nchange-id: i23b65b52e93798cb5a6744632d31b0f88c6b6b31",
            "language":"en",
            "entities":"[('fault', 'FLAW', ''), ('356326671', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/core\/kernels\/immutable_constant_op.cc'])",
            "num_files":1.0,
            "patch_content":"From 4f663d4b8f0bec1b48da6fa091a7d29609980fa4 Mon Sep 17 00:00:00 2001\nFrom: Amit Patankar <amitpatankar@google.com>\nDate: Mon, 8 Feb 2021 12:29:30 -0800\nSubject: [PATCH] Allowlist certain data types to avoid a seg fault.\n\nPiperOrigin-RevId: 356326671\nChange-Id: I23b65b52e93798cb5a6744632d31b0f88c6b6b31\n---\n tensorflow\/core\/kernels\/immutable_constant_op.cc | 5 +++++\n 1 file changed, 5 insertions(+)\n\ndiff --git a\/tensorflow\/core\/kernels\/immutable_constant_op.cc b\/tensorflow\/core\/kernels\/immutable_constant_op.cc\nindex 1cfbdb82778913..19aa865c1fbe4d 100644\n--- a\/tensorflow\/core\/kernels\/immutable_constant_op.cc\n+++ b\/tensorflow\/core\/kernels\/immutable_constant_op.cc\n@@ -17,6 +17,8 @@ limitations under the License.\n \n #include <unordered_set>\n \n+#include \"tensorflow\/core\/framework\/types.pb.h\"\n+\n namespace tensorflow {\n \n namespace {\n@@ -86,6 +88,9 @@ ImmutableConstantOp::ImmutableConstantOp(OpKernelConstruction* context)\n   OP_REQUIRES_OK(context,\n                  context->GetAttr(kMemoryRegionNameAttr, &region_name_));\n   OP_REQUIRES_OK(context, context->GetAttr(kDTypeAttr, &dtype_));\n+  OP_REQUIRES(context, dtype_ != DT_RESOURCE && dtype_ != DT_VARIANT,\n+              errors::InvalidArgument(\n+                  \"Resource and variant dtypes are invalid for this op.\"));\n   OP_REQUIRES_OK(context, context->GetAttr(kShapeAttr, &shape_));\n }"
        },
        {
            "index":853,
            "vuln_id":"GHSA-pqrv-8r2f-7278",
            "cwe_id":"{'CWE-754'}",
            "score":5.9,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/955059813cc325dc1db5e2daa6221271406d4439'}",
            "dataset":"osv",
            "summary":"Crash due to erroneous `StatusOr` in TensorFlow ### Impact\nA `GraphDef` from a TensorFlow `SavedModel` can be maliciously altered to cause a TensorFlow process to crash due to encountering [a `StatusOr` value that is an error and forcibly extracting the value from it](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/274df9b02330b790aa8de1cee164b70f72b9b244\/tensorflow\/core\/graph\/graph.cc#L560-L567):\n\n```cc\n  if (op_reg_data->type_ctor != nullptr) {\n    VLOG(3) << \"AddNode: found type constructor for \" << node_def.name();\n    const auto ctor_type =\n        full_type::SpecializeType(AttrSlice(node_def), op_reg_data->op_def);\n    const FullTypeDef ctor_typedef = ctor_type.ValueOrDie();\n    if (ctor_typedef.type_id() != TFT_UNSET) {\n      *(node_def.mutable_experimental_type()) = ctor_typedef;\n    }\n  }\n```   \n      \nIf `ctor_type` is an error status, `ValueOrDie` results in a crash.\n        \n### Patches\nWe have patched the issue in GitHub commit [955059813cc325dc1db5e2daa6221271406d4439](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/955059813cc325dc1db5e2daa6221271406d4439).\n  \nWe have patched the issue in multiple GitHub commits and these will be included in TensorFlow 2.8.0 and TensorFlow 2.7.1, as both are affected.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.",
            "published_date":"2022-02-09",
            "chain_len":1,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/955059813cc325dc1db5e2daa6221271406d4439",
            "commit_sha":"955059813cc325dc1db5e2daa6221271406d4439",
            "patch":"SINGLE",
            "chain_ord":"['955059813cc325dc1db5e2daa6221271406d4439']",
            "before_first_fix_commit":"{'3d89911481ba6ebe8c88c1c0b595412121e6c645'}",
            "last_fix_commit":"955059813cc325dc1db5e2daa6221271406d4439",
            "chain_ord_pos":1.0,
            "commit_datetime":"11\/12\/2021, 16:17:57",
            "message":"Check for type inference error on node construction.\n\nPiperOrigin-RevId: 409415804\nChange-Id: Ieb6e020906b96f522bf8e2fa103715ddbbdc434a",
            "author":"Dan Moldovan",
            "comments":null,
            "stats":"{'additions': 5, 'deletions': 0, 'total': 5}",
            "files":"{'tensorflow\/core\/graph\/graph.cc': {'additions': 5, 'deletions': 0, 'changes': 5, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/955059813cc325dc1db5e2daa6221271406d4439\/tensorflow%2Fcore%2Fgraph%2Fgraph.cc', 'patch': '@@ -561,6 +561,11 @@ Node* Graph::AddNode(NodeDef node_def, Status* status) {\\n     VLOG(3) << \"AddNode: found type constructor for \" << node_def.name();\\n     const auto ctor_type =\\n         full_type::SpecializeType(AttrSlice(node_def), op_reg_data->op_def);\\n+    if (!ctor_type.ok()) {\\n+      *status = errors::InvalidArgument(\"type error: \",\\n+                                        ctor_type.status().ToString());\\n+      return nullptr;\\n+    }\\n     const FullTypeDef ctor_typedef = ctor_type.ValueOrDie();\\n     if (ctor_typedef.type_id() != TFT_UNSET) {\\n       *(node_def.mutable_experimental_type()) = ctor_typedef;'}}",
            "message_norm":"check for type inference error on node construction.\n\npiperorigin-revid: 409415804\nchange-id: ieb6e020906b96f522bf8e2fa103715ddbbdc434a",
            "language":"en",
            "entities":"[('error', 'FLAW', ''), ('409415804', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/core\/graph\/graph.cc'])",
            "num_files":1.0,
            "patch_content":"From 955059813cc325dc1db5e2daa6221271406d4439 Mon Sep 17 00:00:00 2001\nFrom: Dan Moldovan <mdan@google.com>\nDate: Fri, 12 Nov 2021 08:17:57 -0800\nSubject: [PATCH] Check for type inference error on node construction.\n\nPiperOrigin-RevId: 409415804\nChange-Id: Ieb6e020906b96f522bf8e2fa103715ddbbdc434a\n---\n tensorflow\/core\/graph\/graph.cc | 5 +++++\n 1 file changed, 5 insertions(+)\n\ndiff --git a\/tensorflow\/core\/graph\/graph.cc b\/tensorflow\/core\/graph\/graph.cc\nindex 397a45c97737ab..2e3703b66030f4 100644\n--- a\/tensorflow\/core\/graph\/graph.cc\n+++ b\/tensorflow\/core\/graph\/graph.cc\n@@ -561,6 +561,11 @@ Node* Graph::AddNode(NodeDef node_def, Status* status) {\n     VLOG(3) << \"AddNode: found type constructor for \" << node_def.name();\n     const auto ctor_type =\n         full_type::SpecializeType(AttrSlice(node_def), op_reg_data->op_def);\n+    if (!ctor_type.ok()) {\n+      *status = errors::InvalidArgument(\"type error: \",\n+                                        ctor_type.status().ToString());\n+      return nullptr;\n+    }\n     const FullTypeDef ctor_typedef = ctor_type.ValueOrDie();\n     if (ctor_typedef.type_id() != TFT_UNSET) {\n       *(node_def.mutable_experimental_type()) = ctor_typedef;"
        },
        {
            "index":734,
            "vuln_id":"GHSA-4p8f-mmfj-r45g",
            "cwe_id":"{'CWE-79'}",
            "score":6.1,
            "chain":"{'https:\/\/github.com\/fatfreecrm\/fat_free_crm\/commit\/6d60bc8ed010c4eda05d6645c64849f415f68d65'}",
            "dataset":"osv",
            "summary":"Cross-site scripting in fat_free_crm Fat Free CRM before 0.18.1 has XSS in the tags_helper in app\/helpers\/tags_helper.rb.",
            "published_date":"2019-08-21",
            "chain_len":1,
            "project":"https:\/\/github.com\/fatfreecrm\/fat_free_crm",
            "commit_href":"https:\/\/github.com\/fatfreecrm\/fat_free_crm\/commit\/6d60bc8ed010c4eda05d6645c64849f415f68d65",
            "commit_sha":"6d60bc8ed010c4eda05d6645c64849f415f68d65",
            "patch":"SINGLE",
            "chain_ord":"['6d60bc8ed010c4eda05d6645c64849f415f68d65']",
            "before_first_fix_commit":"{'557fe238a65ca94ce21e3940724ab96e55e1b27d'}",
            "last_fix_commit":"6d60bc8ed010c4eda05d6645c64849f415f68d65",
            "chain_ord_pos":1.0,
            "commit_datetime":"10\/26\/2018, 23:55:55",
            "message":"Context-sensitive XSS bugfix.",
            "author":"Antonin Steinhauser",
            "comments":null,
            "stats":"{'additions': 1, 'deletions': 1, 'total': 2}",
            "files":"{'app\/helpers\/tags_helper.rb': {'additions': 1, 'deletions': 1, 'changes': 2, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/fatfreecrm\/fat_free_crm\/raw\/6d60bc8ed010c4eda05d6645c64849f415f68d65\/app%2Fhelpers%2Ftags_helper.rb', 'patch': '@@ -17,7 +17,7 @@ def tags_for_index(model)\\n       elsif !query.include?(hashtag)\\n         query += \" #{hashtag}\"\\n       end\\n-      out << link_to_function(tag, \"crm.search_tagged(\\'#{query}\\', \\'#{model.class.to_s.tableize}\\')\", title: tag)\\n+      out << link_to_function(tag, \"crm.search_tagged(\\'#{escape_javascript(query)}\\', \\'#{model.class.to_s.tableize}\\')\", title: tag)\\n     end\\n   end'}}",
            "message_norm":"context-sensitive xss bugfix.",
            "language":"fr",
            "entities":"[('sensitive', 'SECWORD', ''), ('xss', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['app\/helpers\/tags_helper.rb'])",
            "num_files":1.0,
            "patch_content":"From 6d60bc8ed010c4eda05d6645c64849f415f68d65 Mon Sep 17 00:00:00 2001\nFrom: Antonin Steinhauser <steinhauser@d3s.mff.cuni.cz>\nDate: Fri, 26 Oct 2018 16:55:55 -0700\nSubject: [PATCH] Context-sensitive XSS bugfix.\n\n---\n app\/helpers\/tags_helper.rb | 2 +-\n 1 file changed, 1 insertion(+), 1 deletion(-)\n\ndiff --git a\/app\/helpers\/tags_helper.rb b\/app\/helpers\/tags_helper.rb\nindex 266060c6ea..397977682f 100755\n--- a\/app\/helpers\/tags_helper.rb\n+++ b\/app\/helpers\/tags_helper.rb\n@@ -17,7 +17,7 @@ def tags_for_index(model)\n       elsif !query.include?(hashtag)\n         query += \" #{hashtag}\"\n       end\n-      out << link_to_function(tag, \"crm.search_tagged('#{query}', '#{model.class.to_s.tableize}')\", title: tag)\n+      out << link_to_function(tag, \"crm.search_tagged('#{escape_javascript(query)}', '#{model.class.to_s.tableize}')\", title: tag)\n     end\n   end"
        },
        {
            "index":845,
            "vuln_id":"GHSA-35m5-8cvj-8783",
            "cwe_id":"{'CWE-916', 'CWE-327', 'CWE-328'}",
            "score":7.5,
            "chain":"{'https:\/\/github.com\/Morgan-Phoenix\/EnroCrypt\/commit\/e652d56ac60eadfc26489ab83927af13a9b9d8ce'}",
            "dataset":"osv",
            "summary":"Improper hashing in enrocrypt ### Impact\nThe vulnerability is we used MD5 hashing Algorithm In our hashing file. If anyone who is a beginner(and doesn't know about hashes)  can face problems as MD5 is considered a Insecure Hashing Algorithm. \n\n### Patches\nThe vulnerability is patched in v1.1.4 of the product, the users can upgrade to version 1.1.4.\n\n### Workarounds\nIf u specifically want a version and don't want to upgrade, you can remove the `MD5` hashing function from the file `hashing.py` and this vulnerability will be gone\n\n### References\nhttps:\/\/www.cybersecurity-help.cz\/vdb\/cwe\/916\/\nhttps:\/\/www.cybersecurity-help.cz\/vdb\/cwe\/327\/\nhttps:\/\/www.cybersecurity-help.cz\/vdb\/cwe\/328\/\nhttps:\/\/www.section.io\/engineering-education\/what-is-md5\/\nhttps:\/\/www.johndcook.com\/blog\/2019\/01\/24\/reversing-an-md5-hash\/\n\n### For more information\nIf you have any questions or comments about this advisory:\n* Open an issue in [**Enrocrypt's Official Repo**](http:\/\/www.github.com\/Morgan-Phoenix\/EnroCrypt)\n* Create a Discussion in  [**Enrocrypt's Official Repo**](http:\/\/www.github.com\/Morgan-Phoenix\/EnroCrypt)",
            "published_date":"2021-11-10",
            "chain_len":1,
            "project":"https:\/\/github.com\/Morgan-Phoenix\/EnroCrypt",
            "commit_href":"https:\/\/github.com\/Morgan-Phoenix\/EnroCrypt\/commit\/e652d56ac60eadfc26489ab83927af13a9b9d8ce",
            "commit_sha":"e652d56ac60eadfc26489ab83927af13a9b9d8ce",
            "patch":"SINGLE",
            "chain_ord":"['e652d56ac60eadfc26489ab83927af13a9b9d8ce']",
            "before_first_fix_commit":"{'d02050267cecbe4f2877a07ca8a930129528ac05'}",
            "last_fix_commit":"e652d56ac60eadfc26489ab83927af13a9b9d8ce",
            "chain_ord_pos":1.0,
            "commit_datetime":"11\/06\/2021, 14:04:45",
            "message":"Fixed GHSA-35m5-8cvj-8783",
            "author":"Morgan-Phoenix",
            "comments":null,
            "stats":"{'additions': 0, 'deletions': 5, 'total': 5}",
            "files":"{'enrocrypt\/hashing.py': {'additions': 0, 'deletions': 5, 'changes': 5, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/Morgan-Phoenix\/EnroCrypt\/raw\/e652d56ac60eadfc26489ab83927af13a9b9d8ce\/enrocrypt%2Fhashing.py', 'patch': '@@ -66,11 +66,6 @@ def SHA244(self,data:str):\\n         hash = str(sha.digest())\\n         return self.__Salt(hash,salt=self.salt)\\n \\n-    def MD5(self,data:str):\\n-        sha = hashlib.md5(bytes(data.encode()))\\n-        hash = str(sha.digest())\\n-        return self.__Salt(hash,salt=self.salt)\\n-\\n     def SHA384(self,data:str):\\n         sha = hashlib.sha384(bytes(data.encode()))\\n         hash = str(sha.digest())'}}",
            "message_norm":"fixed ghsa-35m5-8cvj-8783",
            "language":"ca",
            "entities":"[('fixed', 'ACTION', ''), ('ghsa-35m5-8cvj-8783', 'VULNID', 'GHSA')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['enrocrypt\/hashing.py'])",
            "num_files":1.0,
            "patch_content":"From e652d56ac60eadfc26489ab83927af13a9b9d8ce Mon Sep 17 00:00:00 2001\nFrom: Morgan-Phoenix <73711602+Morgan-Phoenix@users.noreply.github.com>\nDate: Sat, 6 Nov 2021 19:34:45 +0530\nSubject: [PATCH] Fixed GHSA-35m5-8cvj-8783\n\n---\n enrocrypt\/hashing.py | 5 -----\n 1 file changed, 5 deletions(-)\n\ndiff --git a\/enrocrypt\/hashing.py b\/enrocrypt\/hashing.py\nindex 275d2e3..14328f0 100644\n--- a\/enrocrypt\/hashing.py\n+++ b\/enrocrypt\/hashing.py\n@@ -66,11 +66,6 @@ def SHA244(self,data:str):\n         hash = str(sha.digest())\n         return self.__Salt(hash,salt=self.salt)\n \n-    def MD5(self,data:str):\n-        sha = hashlib.md5(bytes(data.encode()))\n-        hash = str(sha.digest())\n-        return self.__Salt(hash,salt=self.salt)\n-\n     def SHA384(self,data:str):\n         sha = hashlib.sha384(bytes(data.encode()))\n         hash = str(sha.digest())"
        },
        {
            "index":571,
            "vuln_id":"GHSA-5xjx-4xcm-hpcm",
            "cwe_id":"{'CWE-1321', 'CWE-915'}",
            "score":7.3,
            "chain":"{'https:\/\/github.com\/BadOPCode\/NoDash\/commit\/b9cc2b3b49f6cd5228e406bc57e17a28b998fea5'}",
            "dataset":"osv",
            "summary":"Prototype Pollution in ts-nodash `ts-nodash` before version 1.2.7 is vulnerable to Prototype Pollution via the Merge() function due to lack of validation input.",
            "published_date":"2021-12-10",
            "chain_len":1,
            "project":"https:\/\/github.com\/BadOPCode\/NoDash",
            "commit_href":"https:\/\/github.com\/BadOPCode\/NoDash\/commit\/b9cc2b3b49f6cd5228e406bc57e17a28b998fea5",
            "commit_sha":"b9cc2b3b49f6cd5228e406bc57e17a28b998fea5",
            "patch":"SINGLE",
            "chain_ord":"['b9cc2b3b49f6cd5228e406bc57e17a28b998fea5']",
            "before_first_fix_commit":"{'78f4ffab4ed76c43f6f7fb91d8b329acb0d6e684'}",
            "last_fix_commit":"b9cc2b3b49f6cd5228e406bc57e17a28b998fea5",
            "chain_ord_pos":1.0,
            "commit_datetime":"11\/11\/2021, 02:50:07",
            "message":"Security fix for Prototype Pollution (#20)\n\nCo-authored-by: Arjun Shibu <arjunshibu1999@gmail.com>\r\nCo-authored-by: Jamie Slome <jamie@418sec.com>\r\nCo-authored-by: Shawn <BadOPCode@users.noreply.github.com>",
            "author":"huntr.dev | the place to protect open source",
            "comments":null,
            "stats":"{'additions': 6, 'deletions': 1, 'total': 7}",
            "files":"{'src\/Merge.ts': {'additions': 6, 'deletions': 1, 'changes': 7, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/BadOPCode\/NoDash\/raw\/b9cc2b3b49f6cd5228e406bc57e17a28b998fea5\/src%2FMerge.ts', 'patch': '@@ -47,13 +47,18 @@ const  handleDefaultBehavior = (originalObject: any, newObject: any, behavior?:\\n     if (originalTypeName === \"Object\" && newTypeName === \"Object\") { \/\/ built-in behavior\\n         \/\/ tslint:disable:forin\\n         for (const p in newObject) {\\n+            if (isPrototypePolluted(p)) continue\\n             originalObject[p] = processBehavior(originalObject[p], newObject[p], behavior);\\n         }\\n         \/\/ tslint:enable:forin\\n         return originalObject;\\n     }\\n };\\n \\n+const isPrototypePolluted = (key: any) => {\\n+    return [\\'__proto__\\', \\'constructor\\', \\'prototype\\'].includes(key)\\n+}\\n+\\n \/**\\n  * Recursively merge two objects together.\\n  * @param originalObject The base object. Properties here will be overwritten\\n@@ -72,7 +77,7 @@ export const Merge = (originalObject: any, newObject: any, behavior?: IMergeBeha\\n             return definedBehaviorResults;\\n         }\\n     }\\n-\\n+    \\n     return handleDefaultBehavior(originalObject, newObject, behavior);\\n };'}}",
            "message_norm":"security fix for prototype pollution (#20)\n\nco-authored-by: arjun shibu <arjunshibu1999@gmail.com>\r\nco-authored-by: jamie slome <jamie@418sec.com>\r\nco-authored-by: shawn <badopcode@users.noreply.github.com>",
            "language":"en",
            "entities":"[('security', 'SECWORD', ''), ('prototype pollution', 'SECWORD', ''), ('#20', 'ISSUE', ''), ('arjunshibu1999@gmail.com', 'EMAIL', ''), ('jamie@418sec.com', 'EMAIL', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['src\/Merge.ts'])",
            "num_files":1.0,
            "patch_content":"From b9cc2b3b49f6cd5228e406bc57e17a28b998fea5 Mon Sep 17 00:00:00 2001\nFrom: \"huntr.dev | the place to protect open source\" <security@huntr.dev>\nDate: Thu, 11 Nov 2021 02:50:07 +0000\nSubject: [PATCH] Security fix for Prototype Pollution (#20)\n\nCo-authored-by: Arjun Shibu <arjunshibu1999@gmail.com>\nCo-authored-by: Jamie Slome <jamie@418sec.com>\nCo-authored-by: Shawn <BadOPCode@users.noreply.github.com>\n---\n src\/Merge.ts | 7 ++++++-\n 1 file changed, 6 insertions(+), 1 deletion(-)\n\ndiff --git a\/src\/Merge.ts b\/src\/Merge.ts\nindex 8df692c..5700dea 100644\n--- a\/src\/Merge.ts\n+++ b\/src\/Merge.ts\n@@ -47,6 +47,7 @@ const  handleDefaultBehavior = (originalObject: any, newObject: any, behavior?:\n     if (originalTypeName === \"Object\" && newTypeName === \"Object\") { \/\/ built-in behavior\n         \/\/ tslint:disable:forin\n         for (const p in newObject) {\n+            if (isPrototypePolluted(p)) continue\n             originalObject[p] = processBehavior(originalObject[p], newObject[p], behavior);\n         }\n         \/\/ tslint:enable:forin\n@@ -54,6 +55,10 @@ const  handleDefaultBehavior = (originalObject: any, newObject: any, behavior?:\n     }\n };\n \n+const isPrototypePolluted = (key: any) => {\n+    return ['__proto__', 'constructor', 'prototype'].includes(key)\n+}\n+\n \/**\n  * Recursively merge two objects together.\n  * @param originalObject The base object. Properties here will be overwritten\n@@ -72,7 +77,7 @@ export const Merge = (originalObject: any, newObject: any, behavior?: IMergeBeha\n             return definedBehaviorResults;\n         }\n     }\n-\n+    \n     return handleDefaultBehavior(originalObject, newObject, behavior);\n };"
        },
        {
            "index":704,
            "vuln_id":"GHSA-4vf2-4xcg-65cx",
            "cwe_id":"{'CWE-369'}",
            "score":2.5,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/b12aa1d44352de21d1a6faaf04172d8c2508b42b'}",
            "dataset":"osv",
            "summary":"Division by 0 in `Conv2D` ### Impact\nAn attacker can trigger a division by 0 in `tf.raw_ops.Conv2D`:\n\n```python\nimport tensorflow as tf\n\ninput = tf.constant([], shape=[0, 0, 0, 0], dtype=tf.float32)\nfilter = tf.constant([], shape=[0, 0, 0, 0], dtype=tf.float32)\n\nstrides = [1, 1, 1, 1]\npadding = \"SAME\"\n                               \ntf.raw_ops.Conv2D(input=input, filter=filter, strides=strides, padding=padding)\n```                            \n                               \nThis is because the [implementation](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/988087bd83f144af14087fe4fecee2d250d93737\/tensorflow\/core\/kernels\/conv_ops.cc#L261-L263) does a division by a quantity that is controlled by the caller:\n```cc\n  const int64 patch_depth = filter.dim_size(2);\n  if (in_depth % patch_depth != 0) { ... }\n```\n  \n### Patches\nWe have patched the issue in GitHub commit [b12aa1d44352de21d1a6faaf04172d8c2508b42b](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/b12aa1d44352de21d1a6faaf04172d8c2508b42b).\n  \nThe fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution \nThis vulnerability has been reported by Ying Wang and Yakun Zhang of Baidu X-Team.",
            "published_date":"2021-05-21",
            "chain_len":1,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/b12aa1d44352de21d1a6faaf04172d8c2508b42b",
            "commit_sha":"b12aa1d44352de21d1a6faaf04172d8c2508b42b",
            "patch":"SINGLE",
            "chain_ord":"['b12aa1d44352de21d1a6faaf04172d8c2508b42b']",
            "before_first_fix_commit":"{'988087bd83f144af14087fe4fecee2d250d93737'}",
            "last_fix_commit":"b12aa1d44352de21d1a6faaf04172d8c2508b42b",
            "chain_ord_pos":1.0,
            "commit_datetime":"04\/20\/2021, 01:32:56",
            "message":"Fix one more FPE.\n\nPiperOrigin-RevId: 369346568\nChange-Id: I840fd575962adc879713a4c9cc59e6da3331caa7",
            "author":"Mihai Maruseac",
            "comments":null,
            "stats":"{'additions': 13, 'deletions': 0, 'total': 13}",
            "files":"{'tensorflow\/core\/kernels\/conv_ops.cc': {'additions': 13, 'deletions': 0, 'changes': 13, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/b12aa1d44352de21d1a6faaf04172d8c2508b42b\/tensorflow%2Fcore%2Fkernels%2Fconv_ops.cc', 'patch': '@@ -260,6 +260,11 @@ struct LaunchConv2DOp<CPUDevice, T> {\\n     const int64 out_depth = output->dim_size(3);\\n     const int64 patch_depth = filter.dim_size(2);\\n \\n+    if (patch_depth <= 0) {\\n+      ctx->SetStatus(errors::InvalidArgument(\\n+          \"filter depth must be stricly positive, got \", patch_depth));\\n+      return;\\n+    }\\n     if (in_depth % patch_depth != 0) {\\n       ctx->SetStatus(errors::InvalidArgument(\\n           \"input depth must be evenly divisible by filter depth: \", in_depth,\\n@@ -268,6 +273,11 @@ struct LaunchConv2DOp<CPUDevice, T> {\\n     }\\n \\n     const int64 num_groups = in_depth \/ patch_depth;\\n+    if (num_groups <= 0) {\\n+      ctx->SetStatus(errors::InvalidArgument(\\n+          \"number of groups must be stricly positive, got \", num_groups));\\n+      return;\\n+    }\\n     if (out_depth % num_groups != 0 || out_depth < num_groups) {\\n       ctx->SetStatus(errors::InvalidArgument(\\n           \"output depth must be evenly divisible by number of groups: \",\\n@@ -536,6 +546,9 @@ Status ComputeConv2DDimension(const Conv2DParameters& params,\\n               errors::InvalidArgument(\"Patch depth too large\"));\\n   const int in_depth = static_cast<int>(in_depth_raw);\\n   const int patch_depth = static_cast<int>(patch_depth_raw);\\n+  TF_REQUIRES(patch_depth > 0,\\n+              errors::InvalidArgument(\\n+                  \"filter depth must be stricly positive, got \", patch_depth));\\n   TF_REQUIRES(in_depth % patch_depth == 0,\\n               errors::InvalidArgument(\\n                   \"input depth must be evenly divisible by filter depth: \",'}}",
            "message_norm":"fix one more fpe.\n\npiperorigin-revid: 369346568\nchange-id: i840fd575962adc879713a4c9cc59e6da3331caa7",
            "language":"it",
            "entities":"[('fix', 'ACTION', ''), ('fpe', 'SECWORD', ''), ('369346568', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/core\/kernels\/conv_ops.cc'])",
            "num_files":1.0,
            "patch_content":"From b12aa1d44352de21d1a6faaf04172d8c2508b42b Mon Sep 17 00:00:00 2001\nFrom: Mihai Maruseac <mihaimaruseac@google.com>\nDate: Mon, 19 Apr 2021 18:32:56 -0700\nSubject: [PATCH] Fix one more FPE.\n\nPiperOrigin-RevId: 369346568\nChange-Id: I840fd575962adc879713a4c9cc59e6da3331caa7\n---\n tensorflow\/core\/kernels\/conv_ops.cc | 13 +++++++++++++\n 1 file changed, 13 insertions(+)\n\ndiff --git a\/tensorflow\/core\/kernels\/conv_ops.cc b\/tensorflow\/core\/kernels\/conv_ops.cc\nindex 363e3737a809b0..9bacebe7d265dc 100644\n--- a\/tensorflow\/core\/kernels\/conv_ops.cc\n+++ b\/tensorflow\/core\/kernels\/conv_ops.cc\n@@ -260,6 +260,11 @@ struct LaunchConv2DOp<CPUDevice, T> {\n     const int64 out_depth = output->dim_size(3);\n     const int64 patch_depth = filter.dim_size(2);\n \n+    if (patch_depth <= 0) {\n+      ctx->SetStatus(errors::InvalidArgument(\n+          \"filter depth must be stricly positive, got \", patch_depth));\n+      return;\n+    }\n     if (in_depth % patch_depth != 0) {\n       ctx->SetStatus(errors::InvalidArgument(\n           \"input depth must be evenly divisible by filter depth: \", in_depth,\n@@ -268,6 +273,11 @@ struct LaunchConv2DOp<CPUDevice, T> {\n     }\n \n     const int64 num_groups = in_depth \/ patch_depth;\n+    if (num_groups <= 0) {\n+      ctx->SetStatus(errors::InvalidArgument(\n+          \"number of groups must be stricly positive, got \", num_groups));\n+      return;\n+    }\n     if (out_depth % num_groups != 0 || out_depth < num_groups) {\n       ctx->SetStatus(errors::InvalidArgument(\n           \"output depth must be evenly divisible by number of groups: \",\n@@ -536,6 +546,9 @@ Status ComputeConv2DDimension(const Conv2DParameters& params,\n               errors::InvalidArgument(\"Patch depth too large\"));\n   const int in_depth = static_cast<int>(in_depth_raw);\n   const int patch_depth = static_cast<int>(patch_depth_raw);\n+  TF_REQUIRES(patch_depth > 0,\n+              errors::InvalidArgument(\n+                  \"filter depth must be stricly positive, got \", patch_depth));\n   TF_REQUIRES(in_depth % patch_depth == 0,\n               errors::InvalidArgument(\n                   \"input depth must be evenly divisible by filter depth: \","
        },
        {
            "index":772,
            "vuln_id":"GHSA-crch-j389-5f84",
            "cwe_id":"{'CWE-787'}",
            "score":2.5,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/c59c37e7b2d563967da813fa50fe20b21f4da683'}",
            "dataset":"osv",
            "summary":"Heap OOB write in TFLite ### Impact\nA specially crafted TFLite model could trigger an OOB write on heap in the TFLite implementation of [`ArgMin`\/`ArgMax`](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/102b211d892f3abc14f845a72047809b39cc65ab\/tensorflow\/lite\/kernels\/arg_min_max.cc#L52-L59):\n\n```cc\nTfLiteIntArray* output_dims = TfLiteIntArrayCreate(NumDimensions(input) - 1);\nint j = 0;\nfor (int i = 0; i < NumDimensions(input); ++i) { \n  if (i != axis_value) {\n    output_dims->data[j] = SizeOfDimension(input, i);\n    ++j;\n  }\n}\n```\n\nIf `axis_value` is not a value between 0 and `NumDimensions(input)`, then the condition in the `if` is never true, so code writes past the last valid element of `output_dims->data`.\n  \n### Patches \nWe have patched the issue in GitHub commit [c59c37e7b2d563967da813fa50fe20b21f4da683](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/c59c37e7b2d563967da813fa50fe20b21f4da683).\n\nThe fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by members of the Aivul Team from Qihoo 360.",
            "published_date":"2021-05-21",
            "chain_len":1,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/c59c37e7b2d563967da813fa50fe20b21f4da683",
            "commit_sha":"c59c37e7b2d563967da813fa50fe20b21f4da683",
            "patch":"SINGLE",
            "chain_ord":"['c59c37e7b2d563967da813fa50fe20b21f4da683']",
            "before_first_fix_commit":"{'102b211d892f3abc14f845a72047809b39cc65ab'}",
            "last_fix_commit":"c59c37e7b2d563967da813fa50fe20b21f4da683",
            "chain_ord_pos":1.0,
            "commit_datetime":"04\/29\/2021, 00:50:10",
            "message":"Prevent array write out-of-bounds.\n\nIf user passes an invalid axis, then we copy one too many dimensions to the output in the loop below these checks. Even if we didn't do that, there will be further issues with an invalid axis, so we check for that right now.\n\nPiperOrigin-RevId: 371023299\nChange-Id: I9eca37ffc2b29e8e48710f500701270ef0790224",
            "author":"Mihai Maruseac",
            "comments":null,
            "stats":"{'additions': 3, 'deletions': 0, 'total': 3}",
            "files":"{'tensorflow\/lite\/kernels\/arg_min_max.cc': {'additions': 3, 'deletions': 0, 'changes': 3, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/c59c37e7b2d563967da813fa50fe20b21f4da683\/tensorflow%2Flite%2Fkernels%2Farg_min_max.cc', 'patch': '@@ -48,6 +48,9 @@ TfLiteStatus ResizeOutput(TfLiteContext* context, const TfLiteTensor* input,\\n     axis_value += NumDimensions(input);\\n   }\\n \\n+  TF_LITE_ENSURE(context, axis_value >= 0);\\n+  TF_LITE_ENSURE(context, axis_value < NumDimensions(input));\\n+\\n   \/\/ Copy the input dimensions to output except the axis dimension.\\n   TfLiteIntArray* output_dims = TfLiteIntArrayCreate(NumDimensions(input) - 1);\\n   int j = 0;'}}",
            "message_norm":"prevent array write out-of-bounds.\n\nif user passes an invalid axis, then we copy one too many dimensions to the output in the loop below these checks. even if we didn't do that, there will be further issues with an invalid axis, so we check for that right now.\n\npiperorigin-revid: 371023299\nchange-id: i9eca37ffc2b29e8e48710f500701270ef0790224",
            "language":"en",
            "entities":"[('prevent', 'ACTION', ''), ('out-of-bounds', 'SECWORD', ''), ('issues', 'FLAW', ''), ('371023299', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/lite\/kernels\/arg_min_max.cc'])",
            "num_files":1.0,
            "patch_content":"From c59c37e7b2d563967da813fa50fe20b21f4da683 Mon Sep 17 00:00:00 2001\nFrom: Mihai Maruseac <mihaimaruseac@google.com>\nDate: Wed, 28 Apr 2021 17:50:10 -0700\nSubject: [PATCH] Prevent array write out-of-bounds.\n\nIf user passes an invalid axis, then we copy one too many dimensions to the output in the loop below these checks. Even if we didn't do that, there will be further issues with an invalid axis, so we check for that right now.\n\nPiperOrigin-RevId: 371023299\nChange-Id: I9eca37ffc2b29e8e48710f500701270ef0790224\n---\n tensorflow\/lite\/kernels\/arg_min_max.cc | 3 +++\n 1 file changed, 3 insertions(+)\n\ndiff --git a\/tensorflow\/lite\/kernels\/arg_min_max.cc b\/tensorflow\/lite\/kernels\/arg_min_max.cc\nindex a0ba8cb9f8bbe7..291fd61681f2a8 100644\n--- a\/tensorflow\/lite\/kernels\/arg_min_max.cc\n+++ b\/tensorflow\/lite\/kernels\/arg_min_max.cc\n@@ -48,6 +48,9 @@ TfLiteStatus ResizeOutput(TfLiteContext* context, const TfLiteTensor* input,\n     axis_value += NumDimensions(input);\n   }\n \n+  TF_LITE_ENSURE(context, axis_value >= 0);\n+  TF_LITE_ENSURE(context, axis_value < NumDimensions(input));\n+\n   \/\/ Copy the input dimensions to output except the axis dimension.\n   TfLiteIntArray* output_dims = TfLiteIntArrayCreate(NumDimensions(input) - 1);\n   int j = 0;"
        },
        {
            "index":526,
            "vuln_id":"GHSA-v6r6-84gr-92rm",
            "cwe_id":"{'CWE-787', 'CWE-119'}",
            "score":2.5,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/6fc9141f42f6a72180ecd24021c3e6b36165fe0d'}",
            "dataset":"osv",
            "summary":"Heap buffer overflow in `AvgPool3DGrad` ### Impact\nThe implementation of `tf.raw_ops.AvgPool3DGrad` is vulnerable to a heap buffer overflow:\n\n```python\nimport tensorflow as tf\n\norig_input_shape = tf.constant([10, 6, 3, 7, 7], shape=[5], dtype=tf.int32)\ngrad = tf.constant([0.01, 0, 0], shape=[3, 1, 1, 1, 1], dtype=tf.float32)\nksize = [1, 1, 1, 1, 1]\nstrides = [1, 1, 1, 1, 1]\npadding = \"SAME\"\n\ntf.raw_ops.AvgPool3DGrad(\n  orig_input_shape=orig_input_shape, grad=grad, ksize=ksize, strides=strides,\n  padding=padding)\n```\n\nThe [implementation](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/d80ffba9702dc19d1fac74fc4b766b3fa1ee976b\/tensorflow\/core\/kernels\/pooling_ops_3d.cc#L376-L450) assumes that the `orig_input_shape` and `grad` tensors have similar first and last dimensions but does not check that this assumption is validated.\n\n### Patches\nWe have patched the issue in GitHub commit [6fc9141f42f6a72180ecd24021c3e6b36165fe0d](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/6fc9141f42f6a72180ecd24021c3e6b36165fe0d).\n\nThe fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by Ying Wang and Yakun Zhang of Baidu X-Team.",
            "published_date":"2021-05-21",
            "chain_len":1,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/6fc9141f42f6a72180ecd24021c3e6b36165fe0d",
            "commit_sha":"6fc9141f42f6a72180ecd24021c3e6b36165fe0d",
            "patch":"SINGLE",
            "chain_ord":"['6fc9141f42f6a72180ecd24021c3e6b36165fe0d']",
            "before_first_fix_commit":"{'d80ffba9702dc19d1fac74fc4b766b3fa1ee976b'}",
            "last_fix_commit":"6fc9141f42f6a72180ecd24021c3e6b36165fe0d",
            "chain_ord_pos":1.0,
            "commit_datetime":"05\/06\/2021, 16:51:26",
            "message":"Fix assertion failure in pooling_ops_3d\n\nPiperOrigin-RevId: 372364504\nChange-Id: Iecde4fe26b47a8fa935d6e2611b5585ed5777781",
            "author":"Mihai Maruseac",
            "comments":null,
            "stats":"{'additions': 13, 'deletions': 0, 'total': 13}",
            "files":"{'tensorflow\/core\/kernels\/pooling_ops_3d.cc': {'additions': 13, 'deletions': 0, 'changes': 13, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/6fc9141f42f6a72180ecd24021c3e6b36165fe0d\/tensorflow%2Fcore%2Fkernels%2Fpooling_ops_3d.cc', 'patch': '@@ -383,6 +383,19 @@ struct LaunchAvgPooling3dGradOp<CPUDevice, T> {\\n                      const std::array<int64, 3>& output_shape,\\n                      const std::array<int64, 3>& padding,\\n                      TensorFormat data_format, Tensor* output) {\\n+    OP_REQUIRES(\\n+        context, tensor_in_shape.dim_size(0) == out_backprop.dim_size(0),\\n+        errors::InvalidArgument(\\n+            \"Expected first dimension of tensor_in_shape and \"\\n+            \"out_backprop to match, got \",\\n+            tensor_in_shape.dim_size(0), \" and \", out_backprop.dim_size(0)));\\n+    OP_REQUIRES(\\n+        context, tensor_in_shape.dim_size(4) == out_backprop.dim_size(4),\\n+        errors::InvalidArgument(\\n+            \"Expected last dimension of tensor_in_shape and \"\\n+            \"out_backprop to match, got \",\\n+            tensor_in_shape.dim_size(4), \" and \", out_backprop.dim_size(4)));\\n+\\n     output->flat<T>().setZero();\\n     std::array<int64, 3> input_size = {{tensor_in_shape.dim_size(3),\\n                                         tensor_in_shape.dim_size(2),'}}",
            "message_norm":"fix assertion failure in pooling_ops_3d\n\npiperorigin-revid: 372364504\nchange-id: iecde4fe26b47a8fa935d6e2611b5585ed5777781",
            "language":"en",
            "entities":"[('fix', 'ACTION', ''), ('372364504', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/core\/kernels\/pooling_ops_3d.cc'])",
            "num_files":1.0,
            "patch_content":"From 6fc9141f42f6a72180ecd24021c3e6b36165fe0d Mon Sep 17 00:00:00 2001\nFrom: Mihai Maruseac <mihaimaruseac@google.com>\nDate: Thu, 6 May 2021 09:51:26 -0700\nSubject: [PATCH] Fix assertion failure in pooling_ops_3d\n\nPiperOrigin-RevId: 372364504\nChange-Id: Iecde4fe26b47a8fa935d6e2611b5585ed5777781\n---\n tensorflow\/core\/kernels\/pooling_ops_3d.cc | 13 +++++++++++++\n 1 file changed, 13 insertions(+)\n\ndiff --git a\/tensorflow\/core\/kernels\/pooling_ops_3d.cc b\/tensorflow\/core\/kernels\/pooling_ops_3d.cc\nindex 9da2d62b0a21d3..56a55bc2ec87bf 100644\n--- a\/tensorflow\/core\/kernels\/pooling_ops_3d.cc\n+++ b\/tensorflow\/core\/kernels\/pooling_ops_3d.cc\n@@ -383,6 +383,19 @@ struct LaunchAvgPooling3dGradOp<CPUDevice, T> {\n                      const std::array<int64, 3>& output_shape,\n                      const std::array<int64, 3>& padding,\n                      TensorFormat data_format, Tensor* output) {\n+    OP_REQUIRES(\n+        context, tensor_in_shape.dim_size(0) == out_backprop.dim_size(0),\n+        errors::InvalidArgument(\n+            \"Expected first dimension of tensor_in_shape and \"\n+            \"out_backprop to match, got \",\n+            tensor_in_shape.dim_size(0), \" and \", out_backprop.dim_size(0)));\n+    OP_REQUIRES(\n+        context, tensor_in_shape.dim_size(4) == out_backprop.dim_size(4),\n+        errors::InvalidArgument(\n+            \"Expected last dimension of tensor_in_shape and \"\n+            \"out_backprop to match, got \",\n+            tensor_in_shape.dim_size(4), \" and \", out_backprop.dim_size(4)));\n+\n     output->flat<T>().setZero();\n     std::array<int64, 3> input_size = {{tensor_in_shape.dim_size(3),\n                                         tensor_in_shape.dim_size(2),"
        },
        {
            "index":52,
            "vuln_id":"GHSA-rr8m-29g8-8cgc",
            "cwe_id":"{'CWE-89'}",
            "score":8.8,
            "chain":"{'https:\/\/github.com\/forkcms\/forkcms\/commit\/6aca30e10b4181534f73f96d6e2ebeb45ec15069'}",
            "dataset":"osv",
            "summary":"SQL Injection in Fork CMS Fork CMS is vulnerable to SQL injection through marking blog comments on bulk as spam in versions prior to 5.11.1.",
            "published_date":"2022-03-26",
            "chain_len":1,
            "project":"https:\/\/github.com\/forkcms\/forkcms",
            "commit_href":"https:\/\/github.com\/forkcms\/forkcms\/commit\/6aca30e10b4181534f73f96d6e2ebeb45ec15069",
            "commit_sha":"6aca30e10b4181534f73f96d6e2ebeb45ec15069",
            "patch":"SINGLE",
            "chain_ord":"['6aca30e10b4181534f73f96d6e2ebeb45ec15069']",
            "before_first_fix_commit":"{'1b38e33a98992793e998a937b717355212346993'}",
            "last_fix_commit":"6aca30e10b4181534f73f96d6e2ebeb45ec15069",
            "chain_ord_pos":1.0,
            "commit_datetime":"03\/23\/2022, 12:21:47",
            "message":"Prevent sql injection through the ids of the blog comments",
            "author":"Jelmer Prins",
            "comments":null,
            "stats":"{'additions': 1, 'deletions': 1, 'total': 2}",
            "files":"{'src\/Backend\/Modules\/Blog\/Engine\/Model.php': {'additions': 1, 'deletions': 1, 'changes': 2, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/forkcms\/forkcms\/raw\/6aca30e10b4181534f73f96d6e2ebeb45ec15069\/src%2FBackend%2FModules%2FBlog%2FEngine%2FModel.php', 'patch': \"@@ -501,7 +501,7 @@ public static function getComments(array $ids): array\\n             'SELECT *\\n              FROM blog_comments AS i\\n              WHERE i.id IN (' . implode(', ', array_fill(0, count($ids), '?')) . ')',\\n-            $ids\\n+            array_map('intval', $ids)\\n         );\\n     }\"}}",
            "message_norm":"prevent sql injection through the ids of the blog comments",
            "language":"en",
            "entities":"[('prevent', 'ACTION', ''), ('sql injection', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['src\/Backend\/Modules\/Blog\/Engine\/Model.php'])",
            "num_files":1.0,
            "patch_content":"From 6aca30e10b4181534f73f96d6e2ebeb45ec15069 Mon Sep 17 00:00:00 2001\nFrom: Jelmer Prins <jelmer@pageon.be>\nDate: Wed, 23 Mar 2022 13:21:47 +0100\nSubject: [PATCH] Prevent sql injection through the ids of the blog comments\n\n---\n src\/Backend\/Modules\/Blog\/Engine\/Model.php | 2 +-\n 1 file changed, 1 insertion(+), 1 deletion(-)\n\ndiff --git a\/src\/Backend\/Modules\/Blog\/Engine\/Model.php b\/src\/Backend\/Modules\/Blog\/Engine\/Model.php\nindex 6ee1dce527..129cfdf55a 100644\n--- a\/src\/Backend\/Modules\/Blog\/Engine\/Model.php\n+++ b\/src\/Backend\/Modules\/Blog\/Engine\/Model.php\n@@ -501,7 +501,7 @@ public static function getComments(array $ids): array\n             'SELECT *\n              FROM blog_comments AS i\n              WHERE i.id IN (' . implode(', ', array_fill(0, count($ids), '?')) . ')',\n-            $ids\n+            array_map('intval', $ids)\n         );\n     }"
        },
        {
            "index":364,
            "vuln_id":"GHSA-gvmf-wcx6-p974",
            "cwe_id":"{'CWE-89'}",
            "score":8.1,
            "chain":"{'https:\/\/github.com\/pimcore\/pimcore\/commit\/21559c6bf0e4e828d33ff7af6e88caecb5ac6549'}",
            "dataset":"osv",
            "summary":"Improper quoting of columns when using setOrderBy() or setGroupBy() on listing classes in Pimcore ### Impact\nPimcore offers developers listing classes to make querying data easier. This listing classes also allow to order or group the results based on one or more columns which should be quoted by default. \nThe actual issue is that quoting is not done properly in both cases, so there's the theoretical possibility to inject custom SQL if the developer is using this methods with input data and not doing proper input validation in advance and  so relies on the auto-quoting being done by the listing classes. \n\n##### Example: \n```php\n\/\/ request url: https:\/\/example.com\/foo?groupBy=o_id`; SELECT SLEEP(20);--\n\n$list = new DataObject\\Car\\Listing();\n$list->setOrderKey($request->get('orderBy'));\n$list->setGroupBy($request->get('groupBy'));\n$list->load();\n```\n\n### Patches\nUpgrade to >= 10.4.4 or apply the following patch manually: \nhttps:\/\/github.com\/pimcore\/pimcore\/commit\/21559c6bf0e4e828d33ff7af6e88caecb5ac6549.patch\n\n### Workarounds\nApply this patch manually: \nhttps:\/\/github.com\/pimcore\/pimcore\/commit\/21559c6bf0e4e828d33ff7af6e88caecb5ac6549.patch\n\n### References\nhttps:\/\/github.com\/pimcore\/pimcore\/pull\/12444",
            "published_date":"2022-06-22",
            "chain_len":1,
            "project":"https:\/\/github.com\/pimcore\/pimcore",
            "commit_href":"https:\/\/github.com\/pimcore\/pimcore\/commit\/21559c6bf0e4e828d33ff7af6e88caecb5ac6549",
            "commit_sha":"21559c6bf0e4e828d33ff7af6e88caecb5ac6549",
            "patch":"SINGLE",
            "chain_ord":"['21559c6bf0e4e828d33ff7af6e88caecb5ac6549']",
            "before_first_fix_commit":"{'4c66ac7305ee0a5027ade88020d811761555148b'}",
            "last_fix_commit":"21559c6bf0e4e828d33ff7af6e88caecb5ac6549",
            "chain_ord_pos":1.0,
            "commit_datetime":"06\/20\/2022, 13:37:31",
            "message":"[Security] SQL Injection in Data Hub GraphQL (#12444)\n\n* [Security] SQL Injection in Data Hub GraphQL (AbstractListing)\r\n\r\n* Update lib\/Model\/Listing\/AbstractListing.php\r\n\r\nCo-authored-by: Jacob Dreesen <j.dreesen@neusta.de>\r\n\r\n* Update lib\/Model\/Listing\/AbstractListing.php\r\n\r\nCo-authored-by: mcop1 <89011527+mcop1@users.noreply.github.com>\r\n\r\nCo-authored-by: Jacob Dreesen <j.dreesen@neusta.de>\r\nCo-authored-by: Bernhard Rusch <brusch@users.noreply.github.com>",
            "author":"mcop1",
            "comments":null,
            "stats":"{'additions': 16, 'deletions': 3, 'total': 19}",
            "files":"{'lib\/Model\/Listing\/AbstractListing.php': {'additions': 16, 'deletions': 3, 'changes': 19, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/pimcore\/pimcore\/raw\/21559c6bf0e4e828d33ff7af6e88caecb5ac6549\/lib%2FModel%2FListing%2FAbstractListing.php', 'patch': '@@ -235,7 +235,7 @@ public function setOrderKey($orderKey, $quote = true)\\n                 if ($quote === false) {\\n                     $this->orderKey[] = $o;\\n                 } elseif ($this->isValidOrderKey($o)) {\\n-                    $this->orderKey[] = \\'`\\' . $o . \\'`\\';\\n+                    $this->orderKey[] = $this->quoteIdentifier($o);\\n                 }\\n             }\\n         }\\n@@ -411,8 +411,14 @@ public function setGroupBy($groupBy, $qoute = true)\\n         if ($groupBy) {\\n             $this->groupBy = $groupBy;\\n \\n-            if ($qoute && strpos($groupBy, \\'`\\') !== 0) {\\n-                $this->groupBy = \\'`\\' . $this->groupBy . \\'`\\';\\n+          if ($qoute) {\\n+                $quotedParts = [];\\n+                $parts = explode(\",\", trim($groupBy, \\'`\\'));\\n+                foreach($parts as $part) {\\n+                    $quotedParts[] = $this->quoteIdentifier(trim($part));\\n+                }\\n+\\n+                $this->groupBy = implode(\", \", $quotedParts);\\n             }\\n         }\\n \\n@@ -431,6 +437,13 @@ public function setValidOrders($validOrders)\\n         return $this;\\n     }\\n \\n+    public function quoteIdentifier(string $value): string\\n+    {\\n+        $db = Db::get();\\n+\\n+        return $db->quoteIdentifier($value);\\n+    }\\n+\\n     \/**\\n      * @param mixed $value\\n      * @param int|null $type'}}",
            "message_norm":"[security] sql injection in data hub graphql (#12444)\n\n* [security] sql injection in data hub graphql (abstractlisting)\r\n\r\n* update lib\/model\/listing\/abstractlisting.php\r\n\r\nco-authored-by: jacob dreesen <j.dreesen@neusta.de>\r\n\r\n* update lib\/model\/listing\/abstractlisting.php\r\n\r\nco-authored-by: mcop1 <89011527+mcop1@users.noreply.github.com>\r\n\r\nco-authored-by: jacob dreesen <j.dreesen@neusta.de>\r\nco-authored-by: bernhard rusch <brusch@users.noreply.github.com>",
            "language":"en",
            "entities":"[('security', 'SECWORD', ''), ('sql injection', 'SECWORD', ''), ('#12444', 'ISSUE', ''), ('security', 'SECWORD', ''), ('sql injection', 'SECWORD', ''), ('j.dreesen@neusta.de', 'EMAIL', ''), ('j.dreesen@neusta.de', 'EMAIL', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['lib\/Model\/Listing\/AbstractListing.php'])",
            "num_files":1.0,
            "patch_content":"From 21559c6bf0e4e828d33ff7af6e88caecb5ac6549 Mon Sep 17 00:00:00 2001\nFrom: mcop1 <89011527+mcop1@users.noreply.github.com>\nDate: Mon, 20 Jun 2022 15:37:31 +0200\nSubject: [PATCH] [Security] SQL Injection in Data Hub GraphQL (#12444)\n\n* [Security] SQL Injection in Data Hub GraphQL (AbstractListing)\n\n* Update lib\/Model\/Listing\/AbstractListing.php\n\nCo-authored-by: Jacob Dreesen <j.dreesen@neusta.de>\n\n* Update lib\/Model\/Listing\/AbstractListing.php\n\nCo-authored-by: mcop1 <89011527+mcop1@users.noreply.github.com>\n\nCo-authored-by: Jacob Dreesen <j.dreesen@neusta.de>\nCo-authored-by: Bernhard Rusch <brusch@users.noreply.github.com>\n---\n lib\/Model\/Listing\/AbstractListing.php | 19 ++++++++++++++++---\n 1 file changed, 16 insertions(+), 3 deletions(-)\n\ndiff --git a\/lib\/Model\/Listing\/AbstractListing.php b\/lib\/Model\/Listing\/AbstractListing.php\nindex 1683a2e2734..66a4728635f 100644\n--- a\/lib\/Model\/Listing\/AbstractListing.php\n+++ b\/lib\/Model\/Listing\/AbstractListing.php\n@@ -235,7 +235,7 @@ public function setOrderKey($orderKey, $quote = true)\n                 if ($quote === false) {\n                     $this->orderKey[] = $o;\n                 } elseif ($this->isValidOrderKey($o)) {\n-                    $this->orderKey[] = '`' . $o . '`';\n+                    $this->orderKey[] = $this->quoteIdentifier($o);\n                 }\n             }\n         }\n@@ -411,8 +411,14 @@ public function setGroupBy($groupBy, $qoute = true)\n         if ($groupBy) {\n             $this->groupBy = $groupBy;\n \n-            if ($qoute && strpos($groupBy, '`') !== 0) {\n-                $this->groupBy = '`' . $this->groupBy . '`';\n+          if ($qoute) {\n+                $quotedParts = [];\n+                $parts = explode(\",\", trim($groupBy, '`'));\n+                foreach($parts as $part) {\n+                    $quotedParts[] = $this->quoteIdentifier(trim($part));\n+                }\n+\n+                $this->groupBy = implode(\", \", $quotedParts);\n             }\n         }\n \n@@ -431,6 +437,13 @@ public function setValidOrders($validOrders)\n         return $this;\n     }\n \n+    public function quoteIdentifier(string $value): string\n+    {\n+        $db = Db::get();\n+\n+        return $db->quoteIdentifier($value);\n+    }\n+\n     \/**\n      * @param mixed $value\n      * @param int|null $type"
        },
        {
            "index":404,
            "vuln_id":"GHSA-cx2r-mf6x-55rx",
            "cwe_id":"{'CWE-79'}",
            "score":4.1,
            "chain":"{'https:\/\/github.com\/PrestaShop\/ps_linklist\/commit\/83e6e0bdda2287f4d6e64127cb90c41d26b5ad82'}",
            "dataset":"osv",
            "summary":"Stored XSS with custom URLs in PrestaShop module ps_linklist ### Impact\nStored XSS when using custom URLs.\n\n### Patches\nThe problem is fixed in 3.1.0\n\n### References\n[Cross-site Scripting (XSS) - Stored (CWE-79)](https:\/\/cwe.mitre.org\/data\/definitions\/79.html)",
            "published_date":"2021-10-12",
            "chain_len":1,
            "project":"https:\/\/github.com\/PrestaShop\/ps_linklist",
            "commit_href":"https:\/\/github.com\/PrestaShop\/ps_linklist\/commit\/83e6e0bdda2287f4d6e64127cb90c41d26b5ad82",
            "commit_sha":"83e6e0bdda2287f4d6e64127cb90c41d26b5ad82",
            "patch":"SINGLE",
            "chain_ord":"['83e6e0bdda2287f4d6e64127cb90c41d26b5ad82']",
            "before_first_fix_commit":"{'b90005c2cfed949ab564228b277a728e0a62a876', '632e61961553a5cdd4c12ad7218e914455dbaa6b'}",
            "last_fix_commit":"83e6e0bdda2287f4d6e64127cb90c41d26b5ad82",
            "chain_ord_pos":1.0,
            "commit_datetime":"04\/15\/2020, 14:16:34",
            "message":"Merge pull request from GHSA-cx2r-mf6x-55rx\n\nThe custom url field must be a valid url",
            "author":"GoT",
            "comments":null,
            "stats":"{'additions': 2, 'deletions': 0, 'total': 2}",
            "files":"{'src\/Form\/Type\/CustomUrlType.php': {'additions': 2, 'deletions': 0, 'changes': 2, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/PrestaShop\/ps_linklist\/raw\/83e6e0bdda2287f4d6e64127cb90c41d26b5ad82\/src%2FForm%2FType%2FCustomUrlType.php', 'patch': \"@@ -29,6 +29,7 @@\\n use PrestaShopBundle\\\\Form\\\\Admin\\\\Type\\\\TranslatorAwareType;\\n use Symfony\\\\Component\\\\Form\\\\Extension\\\\Core\\\\Type\\\\TextType;\\n use Symfony\\\\Component\\\\Form\\\\FormBuilderInterface;\\n+use Symfony\\\\Component\\\\Validator\\\\Constraints as Assert;\\n \\n class CustomUrlType extends TranslatorAwareType\\n {\\n@@ -45,6 +46,7 @@ public function buildForm(FormBuilderInterface $builder, array $options)\\n             ->add('url', TextType::class, [\\n                 'label' => $this->trans('URL', 'Modules.Linklist.Admin'),\\n                 'required' => true,\\n+                'constraints' => [new Assert\\\\Url()],\\n             ])\\n         ;\\n     }\"}}",
            "message_norm":"merge pull request from ghsa-cx2r-mf6x-55rx\n\nthe custom url field must be a valid url",
            "language":"en",
            "entities":"[('ghsa-cx2r-mf6x-55rx', 'VULNID', 'GHSA')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['src\/Form\/Type\/CustomUrlType.php'])",
            "num_files":1.0,
            "patch_content":"From 632e61961553a5cdd4c12ad7218e914455dbaa6b Mon Sep 17 00:00:00 2001\nFrom: Pierre RAMBAUD <pierre.rambaud86@gmail.com>\nDate: Wed, 18 Mar 2020 12:13:09 +0100\nSubject: [PATCH] Check that the url is a real url accepting only http & https\n\n---\n src\/Form\/Type\/CustomUrlType.php | 2 ++\n 1 file changed, 2 insertions(+)\n\ndiff --git a\/src\/Form\/Type\/CustomUrlType.php b\/src\/Form\/Type\/CustomUrlType.php\nindex da78efe4..943f38fb 100644\n--- a\/src\/Form\/Type\/CustomUrlType.php\n+++ b\/src\/Form\/Type\/CustomUrlType.php\n@@ -29,6 +29,7 @@\n use PrestaShopBundle\\Form\\Admin\\Type\\TranslatorAwareType;\n use Symfony\\Component\\Form\\Extension\\Core\\Type\\TextType;\n use Symfony\\Component\\Form\\FormBuilderInterface;\n+use Symfony\\Component\\Validator\\Constraints as Assert;\n \n class CustomUrlType extends TranslatorAwareType\n {\n@@ -45,6 +46,7 @@ public function buildForm(FormBuilderInterface $builder, array $options)\n             ->add('url', TextType::class, [\n                 'label' => $this->trans('URL', 'Modules.Linklist.Admin'),\n                 'required' => true,\n+                'constraints' => [new Assert\\Url()],\n             ])\n         ;\n     }"
        },
        {
            "index":891,
            "vuln_id":"GHSA-874w-m2v2-mj64",
            "cwe_id":"{'CWE-415'}",
            "score":9.8,
            "chain":"{'https:\/\/github.com\/adplug\/adplug\/commit\/1a282a486a8e33fef3e15998bf6408d3515dc07e', 'https:\/\/github.com\/miller-alex\/adplug\/commit\/8abb9328bf27dcbdafc67ade3e75af0ffd8f7633'}",
            "dataset":"osv",
            "summary":"Double Free in Adplug AdPlug 2.3.1 has a double free in the Cu6mPlayer class in u6m.h.",
            "published_date":"2021-03-29",
            "chain_len":2,
            "project":"https:\/\/github.com\/miller-alex\/adplug",
            "commit_href":"https:\/\/github.com\/miller-alex\/adplug\/commit\/8abb9328bf27dcbdafc67ade3e75af0ffd8f7633",
            "commit_sha":"8abb9328bf27dcbdafc67ade3e75af0ffd8f7633",
            "patch":"MULTI",
            "chain_ord":"['8abb9328bf27dcbdafc67ade3e75af0ffd8f7633', '1a282a486a8e33fef3e15998bf6408d3515dc07e']",
            "before_first_fix_commit":"{'a8903d884e2c900e77af5c70ef440e72626646ad'}",
            "last_fix_commit":"1a282a486a8e33fef3e15998bf6408d3515dc07e",
            "chain_ord_pos":1.0,
            "commit_datetime":"03\/24\/2020, 14:43:22",
            "message":"Fix double free in Cu6mPlayer::~Cu6mPlayer() (issue #91)\n\nLeave deallocation of song_data to destructor when\ndecompression fails, just like on success.\n\nThis fixes CVE-2019-15151.\n\nEven though load() is apparently not supposed to be called\ntwice (and bad things happen in many players if you do),\nlet's also avoid leaking song_data's memory in that case.\n\nFixes: https:\/\/github.com\/adplug\/adplug\/issues\/91",
            "author":"Alexander Miller",
            "comments":null,
            "stats":"{'additions': 1, 'deletions': 2, 'total': 3}",
            "files":"{'src\/u6m.cpp': {'additions': 1, 'deletions': 2, 'changes': 3, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/miller-alex\/adplug\/raw\/8abb9328bf27dcbdafc67ade3e75af0ffd8f7633\/src%2Fu6m.cpp', 'patch': '@@ -66,6 +66,7 @@ bool Cu6mPlayer::load(const std::string &filename, const CFileProvider &fp)\\n     }\\n \\n   \/\/ load section\\n+  delete[] song_data;\\n   song_data = new unsigned char[decompressed_filesize];\\n   unsigned char* compressed_song_data = new unsigned char[filesize-3];\\n \\n@@ -74,7 +75,6 @@ bool Cu6mPlayer::load(const std::string &filename, const CFileProvider &fp)\\n   fp.close(f);\\n \\n   \/\/ attempt to decompress the song data\\n-  \/\/ if unsuccessful, deallocate song_data[] on the spot, and return(false)\\n   data_block source, destination;\\n   source.size = filesize-4;\\n   source.data = compressed_song_data;\\n@@ -84,7 +84,6 @@ bool Cu6mPlayer::load(const std::string &filename, const CFileProvider &fp)\\n   if (!lzw_decompress(source,destination))\\n     {\\n       delete[] compressed_song_data;\\n-      delete[] song_data;\\n       return(false);\\n     }'}}",
            "message_norm":"fix double free in cu6mplayer::~cu6mplayer() (issue #91)\n\nleave deallocation of song_data to destructor when\ndecompression fails, just like on success.\n\nthis fixes cve-2019-15151.\n\neven though load() is apparently not supposed to be called\ntwice (and bad things happen in many players if you do),\nlet's also avoid leaking song_data's memory in that case.\n\nfixes: https:\/\/github.com\/adplug\/adplug\/issues\/91",
            "language":"en",
            "entities":"[('fix', 'ACTION', ''), ('double free', 'SECWORD', ''), ('#91', 'ISSUE', ''), ('decompression', 'SECWORD', ''), ('fixes', 'ACTION', ''), ('cve-2019-15151', 'VULNID', 'CVE'), ('fixes', 'ACTION', ''), ('https:\/\/github.com\/adplug\/adplug\/issues\/91', 'URL', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['src\/u6m.cpp'])",
            "num_files":1.0,
            "patch_content":"From 8abb9328bf27dcbdafc67ade3e75af0ffd8f7633 Mon Sep 17 00:00:00 2001\nFrom: Alexander Miller <alex.miller@gmx.de>\nDate: Tue, 24 Mar 2020 15:43:22 +0100\nSubject: [PATCH] Fix double free in Cu6mPlayer::~Cu6mPlayer() (issue #91)\n\nLeave deallocation of song_data to destructor when\ndecompression fails, just like on success.\n\nThis fixes CVE-2019-15151.\n\nEven though load() is apparently not supposed to be called\ntwice (and bad things happen in many players if you do),\nlet's also avoid leaking song_data's memory in that case.\n\nFixes: https:\/\/github.com\/adplug\/adplug\/issues\/91\n---\n src\/u6m.cpp | 3 +--\n 1 file changed, 1 insertion(+), 2 deletions(-)\n\ndiff --git a\/src\/u6m.cpp b\/src\/u6m.cpp\nindex 144e0de2..b01c968f 100644\n--- a\/src\/u6m.cpp\n+++ b\/src\/u6m.cpp\n@@ -66,6 +66,7 @@ bool Cu6mPlayer::load(const std::string &filename, const CFileProvider &fp)\n     }\n \n   \/\/ load section\n+  delete[] song_data;\n   song_data = new unsigned char[decompressed_filesize];\n   unsigned char* compressed_song_data = new unsigned char[filesize-3];\n \n@@ -74,7 +75,6 @@ bool Cu6mPlayer::load(const std::string &filename, const CFileProvider &fp)\n   fp.close(f);\n \n   \/\/ attempt to decompress the song data\n-  \/\/ if unsuccessful, deallocate song_data[] on the spot, and return(false)\n   data_block source, destination;\n   source.size = filesize-4;\n   source.data = compressed_song_data;\n@@ -84,7 +84,6 @@ bool Cu6mPlayer::load(const std::string &filename, const CFileProvider &fp)\n   if (!lzw_decompress(source,destination))\n     {\n       delete[] compressed_song_data;\n-      delete[] song_data;\n       return(false);\n     }"
        },
        {
            "index":768,
            "vuln_id":"GHSA-h9px-9vqg-222h",
            "cwe_id":"{'CWE-125'}",
            "score":2.5,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/99085e8ff02c3763a0ec2263e44daec416f6a387'}",
            "dataset":"osv",
            "summary":"Heap OOB in `QuantizeAndDequantizeV3` ### Impact\nAn attacker can read data outside of bounds of heap allocated buffer in `tf.raw_ops.QuantizeAndDequantizeV3`:\n\n```python\nimport tensorflow as tf\n\ntf.raw_ops.QuantizeAndDequantizeV3(\n  input=[2.5,2.5], input_min=[0,0], input_max=[1,1], num_bits=[30],\n  signed_input=False, range_given=False, narrow_range=False, axis=3)\n```   \n\nThis is because the [implementation](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/11ff7f80667e6490d7b5174aa6bf5e01886e770f\/tensorflow\/core\/kernels\/quantize_and_dequantize_op.cc#L237) does not validate the value of user supplied `axis` attribute before using it to index in the array backing the `input` argument:\n\n```cc\nconst int depth = (axis_ == -1) ? 1 : input.dim_size(axis_);\n```\n\n### Patches\nWe have patched the issue in GitHub commit [99085e8ff02c3763a0ec2263e44daec416f6a387](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/99085e8ff02c3763a0ec2263e44daec416f6a387).\n  \nThe fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by Aivul Team from Qihoo 360.",
            "published_date":"2021-05-21",
            "chain_len":1,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/99085e8ff02c3763a0ec2263e44daec416f6a387",
            "commit_sha":"99085e8ff02c3763a0ec2263e44daec416f6a387",
            "patch":"SINGLE",
            "chain_ord":"['99085e8ff02c3763a0ec2263e44daec416f6a387']",
            "before_first_fix_commit":"{'11ff7f80667e6490d7b5174aa6bf5e01886e770f'}",
            "last_fix_commit":"99085e8ff02c3763a0ec2263e44daec416f6a387",
            "chain_ord_pos":1.0,
            "commit_datetime":"04\/27\/2021, 00:32:41",
            "message":"Fix `tf.raw_ops.QuantizeAndDequantizeV3` array index failure.\n\nPiperOrigin-RevId: 370577691\nChange-Id: Ifeae64212f6bcd139435824fa2748d1329213c4c",
            "author":"Amit Patankar",
            "comments":null,
            "stats":"{'additions': 5, 'deletions': 0, 'total': 5}",
            "files":"{'tensorflow\/core\/kernels\/quantize_and_dequantize_op.cc': {'additions': 5, 'deletions': 0, 'changes': 5, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/99085e8ff02c3763a0ec2263e44daec416f6a387\/tensorflow%2Fcore%2Fkernels%2Fquantize_and_dequantize_op.cc', 'patch': '@@ -13,6 +13,7 @@ See the License for the specific language governing permissions and\\n limitations under the License.\\n ==============================================================================*\/\\n \\n+#include \"tensorflow\/core\/framework\/op_requires.h\"\\n #define EIGEN_USE_THREADS\\n \\n #if (defined(GOOGLE_CUDA) && GOOGLE_CUDA) || \\\\\\n@@ -234,6 +235,10 @@ class QuantizeAndDequantizeV3Op : public OpKernel {\\n \\n   void Compute(OpKernelContext* ctx) override {\\n     const Tensor& input = ctx->input(0);\\n+    OP_REQUIRES(ctx, axis_ < input.dims(),\\n+                errors::InvalidArgument(\\n+                    \"Axis requested is larger than input dimensions. Axis: \",\\n+                    axis_, \" Input Dimensions: \", input.dims()));\\n     const int depth = (axis_ == -1) ? 1 : input.dim_size(axis_);\\n     Tensor* output = nullptr;\\n     OP_REQUIRES_OK(ctx, ctx->allocate_output(0, input.shape(), &output));'}}",
            "message_norm":"fix `tf.raw_ops.quantizeanddequantizev3` array index failure.\n\npiperorigin-revid: 370577691\nchange-id: ifeae64212f6bcd139435824fa2748d1329213c4c",
            "language":"en",
            "entities":"[('fix', 'ACTION', ''), ('370577691', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/core\/kernels\/quantize_and_dequantize_op.cc'])",
            "num_files":1.0,
            "patch_content":"From 99085e8ff02c3763a0ec2263e44daec416f6a387 Mon Sep 17 00:00:00 2001\nFrom: Amit Patankar <amitpatankar@google.com>\nDate: Mon, 26 Apr 2021 17:32:41 -0700\nSubject: [PATCH] Fix `tf.raw_ops.QuantizeAndDequantizeV3` array index failure.\n\nPiperOrigin-RevId: 370577691\nChange-Id: Ifeae64212f6bcd139435824fa2748d1329213c4c\n---\n tensorflow\/core\/kernels\/quantize_and_dequantize_op.cc | 5 +++++\n 1 file changed, 5 insertions(+)\n\ndiff --git a\/tensorflow\/core\/kernels\/quantize_and_dequantize_op.cc b\/tensorflow\/core\/kernels\/quantize_and_dequantize_op.cc\nindex c2a7a90d8713d8..f01a70114591bf 100644\n--- a\/tensorflow\/core\/kernels\/quantize_and_dequantize_op.cc\n+++ b\/tensorflow\/core\/kernels\/quantize_and_dequantize_op.cc\n@@ -13,6 +13,7 @@ See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================*\/\n \n+#include \"tensorflow\/core\/framework\/op_requires.h\"\n #define EIGEN_USE_THREADS\n \n #if (defined(GOOGLE_CUDA) && GOOGLE_CUDA) || \\\n@@ -234,6 +235,10 @@ class QuantizeAndDequantizeV3Op : public OpKernel {\n \n   void Compute(OpKernelContext* ctx) override {\n     const Tensor& input = ctx->input(0);\n+    OP_REQUIRES(ctx, axis_ < input.dims(),\n+                errors::InvalidArgument(\n+                    \"Axis requested is larger than input dimensions. Axis: \",\n+                    axis_, \" Input Dimensions: \", input.dims()));\n     const int depth = (axis_ == -1) ? 1 : input.dim_size(axis_);\n     Tensor* output = nullptr;\n     OP_REQUIRES_OK(ctx, ctx->allocate_output(0, input.shape(), &output));"
        },
        {
            "index":520,
            "vuln_id":"GHSA-5qjq-69w6-fg57",
            "cwe_id":"{'CWE-79'}",
            "score":10.0,
            "chain":"{'https:\/\/github.com\/flarum\/core\/commit\/440bed81b8019dff00642c8f493b4909d505a28a'}",
            "dataset":"osv",
            "summary":"XSS vulnerability with translator Flarum's translation system allowed for string inputs to be converted into HTML DOM nodes when rendered. This change was made after v0.1.0-beta.16 (our last beta before v1.0.0) and was not noticed or documented.\n\nThis allowed for any user to type malicious HTML markup within certain user input fields and have this execute on client browsers. The example which led to the discovery of this vulnerability was in the forum search box. Entering faux-malicious HTML markup, such as <script>alert('test')<\/script> resulted in an alert box appearing on the forum. This attack could also be modified to perform AJAX requests on behalf of a user, possibly deleting discussions, modifying their settings or profile, or even modifying settings on the Admin panel if the attack was targetted towards a privileged user.\n\n### Impact\n\nAll Flarum communities that run flarum v1.0.0 or v1.0.1 are impacted.\n\n### Patches\n\nThe vulnerability has been fixed and published as flarum\/core v1.0.2. All communities running Flarum v1.0 have to upgrade as soon as possible to v1.0.2 using:\n\n```\ncomposer update --prefer-dist --no-dev -a -W\n```\n\nYou can then confirm you run the latest version using:\n\n```\ncomposer show flarum\/core\n```\n\n### Workarounds\n\n__None.__\n\n### For more information\n\nFor any questions or comments on this vulnerability please visit https:\/\/discuss.flarum.org\/d\/27558.\n\nFor support questions create a discussion at https:\/\/discuss.flarum.org\/t\/support.\n\nA reminder that if you ever become aware of a security issue in Flarum, please report it to us privately by emailing security@flarum.org, and we will address it promptly.",
            "published_date":"2021-06-07",
            "chain_len":1,
            "project":"https:\/\/github.com\/flarum\/core",
            "commit_href":"https:\/\/github.com\/flarum\/core\/commit\/440bed81b8019dff00642c8f493b4909d505a28a",
            "commit_sha":"440bed81b8019dff00642c8f493b4909d505a28a",
            "patch":"SINGLE",
            "chain_ord":"['440bed81b8019dff00642c8f493b4909d505a28a']",
            "before_first_fix_commit":"{'eeb8fe1443b98f5f622ca52b4a02732f62d1aa77'}",
            "last_fix_commit":"440bed81b8019dff00642c8f493b4909d505a28a",
            "chain_ord_pos":1.0,
            "commit_datetime":"06\/06\/2021, 01:41:48",
            "message":"Fix XSS vulnerability",
            "author":"David Wheatley",
            "comments":"{'com_1': {'author': 'davwheat', 'datetime': '06\/07\/2021, 20:53:34', 'body': 'The details about this vulnerability have now been made public.\\r\\n\\r\\nFor more information, please see: https:\/\/discuss.flarum.org\/d\/27558-critical-security-update-to-flarum-core-v102'}}",
            "stats":"{'additions': 12, 'deletions': 1, 'total': 13}",
            "files":"{'js\/src\/common\/Translator.tsx': {'additions': 12, 'deletions': 1, 'changes': 13, 'status': 'renamed', 'raw_url': 'https:\/\/github.com\/flarum\/framework\/raw\/440bed81b8019dff00642c8f493b4909d505a28a\/js%2Fsrc%2Fcommon%2FTranslator.tsx', 'patch': \"@@ -48,12 +48,23 @@ export default class Translator {\\n     \/\/ future there should be a hook here to inspect the user and change the\\n     \/\/ translation key. This will allow a gender property to determine which\\n     \/\/ translation key is used.\\n+\\n     if ('user' in parameters) {\\n       const user = extract(parameters, 'user');\\n \\n       if (!parameters.username) parameters.username = username(user);\\n     }\\n-    return parameters;\\n+\\n+    const escapedParameters: TranslatorParameters = {};\\n+\\n+    for (const param in parameters) {\\n+      const paramValue = parameters[param];\\n+\\n+      if (typeof paramValue === 'string') escapedParameters[param] = <>{parameters[param]}<\/>;\\n+      else escapedParameters[param] = parameters[param];\\n+    }\\n+\\n+    return escapedParameters;\\n   }\\n \\n   trans(id: string, parameters: TranslatorParameters = {}) {\"}}",
            "message_norm":"fix xss vulnerability",
            "language":"ca",
            "entities":"[('fix', 'ACTION', ''), ('xss', 'SECWORD', ''), ('vulnerability', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['js\/src\/common\/Translator.tsx'])",
            "num_files":1.0,
            "patch_content":"From 440bed81b8019dff00642c8f493b4909d505a28a Mon Sep 17 00:00:00 2001\nFrom: David Wheatley <hi@davwheat.dev>\nDate: Sun, 6 Jun 2021 02:41:48 +0100\nSubject: [PATCH] Fix XSS vulnerability\n\n---\n js\/src\/common\/{Translator.ts => Translator.tsx} | 13 ++++++++++++-\n 1 file changed, 12 insertions(+), 1 deletion(-)\n rename js\/src\/common\/{Translator.ts => Translator.tsx} (86%)\n\ndiff --git a\/js\/src\/common\/Translator.ts b\/js\/src\/common\/Translator.tsx\nsimilarity index 86%\nrename from js\/src\/common\/Translator.ts\nrename to js\/src\/common\/Translator.tsx\nindex 51b9c95a6e..2d2036ff3f 100644\n--- a\/js\/src\/common\/Translator.ts\n+++ b\/js\/src\/common\/Translator.tsx\n@@ -48,12 +48,23 @@ export default class Translator {\n     \/\/ future there should be a hook here to inspect the user and change the\n     \/\/ translation key. This will allow a gender property to determine which\n     \/\/ translation key is used.\n+\n     if ('user' in parameters) {\n       const user = extract(parameters, 'user');\n \n       if (!parameters.username) parameters.username = username(user);\n     }\n-    return parameters;\n+\n+    const escapedParameters: TranslatorParameters = {};\n+\n+    for (const param in parameters) {\n+      const paramValue = parameters[param];\n+\n+      if (typeof paramValue === 'string') escapedParameters[param] = <>{parameters[param]}<\/>;\n+      else escapedParameters[param] = parameters[param];\n+    }\n+\n+    return escapedParameters;\n   }\n \n   trans(id: string, parameters: TranslatorParameters = {}) {"
        },
        {
            "index":611,
            "vuln_id":"GHSA-3374-7h99-xr85",
            "cwe_id":"{'CWE-79'}",
            "score":5.4,
            "chain":"{'https:\/\/github.com\/forkcms\/forkcms\/commit\/6ec6171206a7507a39695edc8bbd1b97ef1041c6'}",
            "dataset":"osv",
            "summary":"Cross-site scripting in forkcms Fork CMS Content Management System v5.8.0 was discovered to contain a cross-site scripting (XSS) vulnerability in the `Displayname` field when using the `Add`, `Edit` or `Register' functions. This vulnerability allows attackers to execute arbitrary web scripts or HTML.",
            "published_date":"2021-10-25",
            "chain_len":1,
            "project":"https:\/\/github.com\/forkcms\/forkcms",
            "commit_href":"https:\/\/github.com\/forkcms\/forkcms\/commit\/6ec6171206a7507a39695edc8bbd1b97ef1041c6",
            "commit_sha":"6ec6171206a7507a39695edc8bbd1b97ef1041c6",
            "patch":"SINGLE",
            "chain_ord":"['6ec6171206a7507a39695edc8bbd1b97ef1041c6']",
            "before_first_fix_commit":"{'f439d630c2f46a85b251488cd7073068a66fae5c'}",
            "last_fix_commit":"6ec6171206a7507a39695edc8bbd1b97ef1041c6",
            "chain_ord_pos":1.0,
            "commit_datetime":"04\/17\/2020, 10:44:12",
            "message":"Fix xss in profiles display name",
            "author":"Jelmer Prins",
            "comments":null,
            "stats":"{'additions': 1, 'deletions': 0, 'total': 1}",
            "files":"{'src\/Backend\/Modules\/Profiles\/Actions\/Index.php': {'additions': 1, 'deletions': 0, 'changes': 1, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/forkcms\/forkcms\/raw\/6ec6171206a7507a39695edc8bbd1b97ef1041c6\/src%2FBackend%2FModules%2FProfiles%2FActions%2FIndex.php', 'patch': \"@@ -128,6 +128,7 @@ private function loadDataGrid(): void\\n             'registered_on',\\n             true\\n         );\\n+        $this->dgProfiles->setColumnFunction('htmlspecialchars', ['[display_name]'], 'display_name');\\n \\n         \/\/ add the mass action controls\\n         $this->dgProfiles->setMassActionCheckboxes('check', '[id]');\"}}",
            "message_norm":"fix xss in profiles display name",
            "language":"en",
            "entities":"[('fix', 'ACTION', ''), ('xss', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['src\/Backend\/Modules\/Profiles\/Actions\/Index.php'])",
            "num_files":1.0,
            "patch_content":"From 6ec6171206a7507a39695edc8bbd1b97ef1041c6 Mon Sep 17 00:00:00 2001\nFrom: Jelmer Prins <jelmer@sumocoders.be>\nDate: Fri, 17 Apr 2020 12:44:12 +0200\nSubject: [PATCH] Fix xss in profiles display name\n\n---\n src\/Backend\/Modules\/Profiles\/Actions\/Index.php | 1 +\n 1 file changed, 1 insertion(+)\n\ndiff --git a\/src\/Backend\/Modules\/Profiles\/Actions\/Index.php b\/src\/Backend\/Modules\/Profiles\/Actions\/Index.php\nindex b6047acfb0..87f87f2344 100644\n--- a\/src\/Backend\/Modules\/Profiles\/Actions\/Index.php\n+++ b\/src\/Backend\/Modules\/Profiles\/Actions\/Index.php\n@@ -128,6 +128,7 @@ private function loadDataGrid(): void\n             'registered_on',\n             true\n         );\n+        $this->dgProfiles->setColumnFunction('htmlspecialchars', ['[display_name]'], 'display_name');\n \n         \/\/ add the mass action controls\n         $this->dgProfiles->setMassActionCheckboxes('check', '[id]');"
        },
        {
            "index":944,
            "vuln_id":"GHSA-h8v5-p258-pqf4",
            "cwe_id":"{'CWE-327'}",
            "score":5.4,
            "chain":"{'https:\/\/github.com\/xwiki\/xwiki-platform\/commit\/26728f3f23658288683667a5182a916c7ecefc52'}",
            "dataset":"osv",
            "summary":"Use of a Broken or Risky Cryptographic Algorithm in XWiki Crypto API ### Impact\nXWiki Crypto API will generate X509 certificates signed by default using SHA1 with RSA, which is not considered safe anymore for use in certificate signatures, due to the risk of collisions with SHA1.\nNote that this API is never used in XWiki Standard but it might be used in some extensions of XWiki.\n\n### Patches\nThe problem has been patched in XWiki version 13.10.6, 14.3.1 and 14.4-rc-1. Since then, the Crypto API will generate X509 certificates signed by default using SHA256 with RSA.\n\n### Workarounds\nAdministrators are advised to upgrade their XWiki installation to one of the patched versions.\nIf the upgrade is not possible, it is possible to patch the module xwiki-platform-crypto in a local installation by applying the change exposed in https:\/\/github.com\/xwiki\/xwiki-platform\/commit\/26728f3f23658288683667a5182a916c7ecefc52 and re-compiling the module.\n\n### References\nhttps:\/\/jira.xwiki.org\/browse\/XWIKI-19676\nhttps:\/\/github.com\/openssl\/openssl\/blob\/master\/CHANGES.md?plain=1#L938\nhttps:\/\/github.com\/openssl\/openssl\/issues\/16650\n\n### For more information\nIf you have any questions or comments about this advisory:\n* Open an issue in [Jira XWiki](https:\/\/jira.xwiki.org)\n* Email us at [security ML](mailto:security@xwiki.org)",
            "published_date":"2022-05-24",
            "chain_len":1,
            "project":"https:\/\/github.com\/xwiki\/xwiki-platform",
            "commit_href":"https:\/\/github.com\/xwiki\/xwiki-platform\/commit\/26728f3f23658288683667a5182a916c7ecefc52",
            "commit_sha":"26728f3f23658288683667a5182a916c7ecefc52",
            "patch":"SINGLE",
            "chain_ord":"['26728f3f23658288683667a5182a916c7ecefc52']",
            "before_first_fix_commit":"{'3b871d906e664fa1875fbeb088404cf31e9f0094'}",
            "last_fix_commit":"26728f3f23658288683667a5182a916c7ecefc52",
            "chain_ord_pos":1.0,
            "commit_datetime":"04\/30\/2022, 10:13:25",
            "message":"XWIKI-19676: Update the RSA Crypto script service to use SHA256 instead of SHA1 for certificate signature",
            "author":"Cl\u00e9ment Aubin",
            "comments":"{'com_1': {'author': 'surli', 'datetime': '05\/02\/2022, 06:58:35', 'body': \"I don't know much this class, but is that ok in term of backward compatibility? If you have some signed stuff in the wiki with that script service, will it be still be able to verify the signature?\"}, 'com_2': {'author': 'aubincleme', 'datetime': '05\/02\/2022, 07:27:55', 'body': 'Yes to me it is fine : the SignerFactory is only used to sign certificates, not verify them. For this the CMSSignedDataVerifier is used instead, which is able to verify signatures based on the different algorithms supported by the crypto API.'}, 'com_3': {'author': 'tmortagne', 'datetime': '05\/02\/2022, 09:10:05', 'body': \"In that case, I'm wondering if this should be cherry-picked in 13.10.x. WDYT @aubincleme ?\"}, 'com_4': {'author': 'aubincleme', 'datetime': '05\/02\/2022, 09:46:37', 'body': 'Yes why not ; doing it now'}, 'com_5': {'author': 'aubincleme', 'datetime': '05\/02\/2022, 09:48:21', 'body': 'done as part of a7c3628609f63b04de80935efa2e1f82e1356846 ;\\xa0updating issue + release notes'}}",
            "stats":"{'additions': 1, 'deletions': 1, 'total': 2}",
            "files":"{'xwiki-platform-core\/xwiki-platform-crypto\/xwiki-platform-crypto-script\/src\/main\/java\/org\/xwiki\/crypto\/script\/RSACryptoScriptService.java': {'additions': 1, 'deletions': 1, 'changes': 2, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/xwiki\/xwiki-platform\/raw\/26728f3f23658288683667a5182a916c7ecefc52\/xwiki-platform-core%2Fxwiki-platform-crypto%2Fxwiki-platform-crypto-script%2Fsrc%2Fmain%2Fjava%2Forg%2Fxwiki%2Fcrypto%2Fscript%2FRSACryptoScriptService.java', 'patch': '@@ -86,7 +86,7 @@ public class RSACryptoScriptService implements ScriptService\\n     private KeyPairGenerator keyPairGenerator;\\n \\n     @Inject\\n-    @Named(\"SHA1withRSAEncryption\")\\n+    @Named(\"SHA256withRSAEncryption\")\\n     private SignerFactory signerFactory;\\n \\n     @Inject'}}",
            "message_norm":"xwiki-19676: update the rsa crypto script service to use sha256 instead of sha1 for certificate signature",
            "language":"en",
            "entities":"[('update', 'ACTION', ''), ('rsa', 'SECWORD', ''), ('crypto', 'SECWORD', ''), ('certificate', 'SECWORD', ''), ('signature', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['xwiki-platform-core\/xwiki-platform-crypto\/xwiki-platform-crypto-script\/src\/main\/java\/org\/xwiki\/crypto\/script\/RSACryptoScriptService.java'])",
            "num_files":1.0,
            "patch_content":"From 26728f3f23658288683667a5182a916c7ecefc52 Mon Sep 17 00:00:00 2001\nFrom: =?UTF-8?q?Cl=C3=A9ment=20Aubin?= <aubincleme@gmail.com>\nDate: Sat, 30 Apr 2022 12:13:25 +0200\nSubject: [PATCH] XWIKI-19676: Update the RSA Crypto script service to use\n SHA256 instead of SHA1 for certificate signature\n\n---\n ...\/java\/org\/xwiki\/crypto\/script\/RSACryptoScriptService.java    | 2 +-\n 1 file changed, 1 insertion(+), 1 deletion(-)\n\ndiff --git a\/xwiki-platform-core\/xwiki-platform-crypto\/xwiki-platform-crypto-script\/src\/main\/java\/org\/xwiki\/crypto\/script\/RSACryptoScriptService.java b\/xwiki-platform-core\/xwiki-platform-crypto\/xwiki-platform-crypto-script\/src\/main\/java\/org\/xwiki\/crypto\/script\/RSACryptoScriptService.java\nindex c9b2600834d3..879616f3005d 100644\n--- a\/xwiki-platform-core\/xwiki-platform-crypto\/xwiki-platform-crypto-script\/src\/main\/java\/org\/xwiki\/crypto\/script\/RSACryptoScriptService.java\n+++ b\/xwiki-platform-core\/xwiki-platform-crypto\/xwiki-platform-crypto-script\/src\/main\/java\/org\/xwiki\/crypto\/script\/RSACryptoScriptService.java\n@@ -86,7 +86,7 @@ public class RSACryptoScriptService implements ScriptService\n     private KeyPairGenerator keyPairGenerator;\n \n     @Inject\n-    @Named(\"SHA1withRSAEncryption\")\n+    @Named(\"SHA256withRSAEncryption\")\n     private SignerFactory signerFactory;\n \n     @Inject"
        },
        {
            "index":225,
            "vuln_id":"GHSA-qqxp-xp9v-vvx6",
            "cwe_id":"{'CWE-79'}",
            "score":0.0,
            "chain":"{'https:\/\/github.com\/jquery\/jquery-ui\/commit\/f2854408cce7e4b7fc6bf8676761904af9c96bde', 'https:\/\/github.com\/jquery\/jquery-ui\/commit\/5fee6fd5000072ff32f2d65b6451f39af9e0e39e'}",
            "dataset":"osv",
            "summary":"Moderate severity vulnerability that affects jquery-ui Cross-site scripting (XSS) vulnerability in the default content option in jquery.ui.tooltip.js in the Tooltip widget in jQuery UI before 1.10.0 allows remote attackers to inject arbitrary web script or HTML via the title attribute, which is not properly handled in the autocomplete combo box demo.",
            "published_date":"2017-10-24",
            "chain_len":2,
            "project":"https:\/\/github.com\/jquery\/jquery-ui",
            "commit_href":"https:\/\/github.com\/jquery\/jquery-ui\/commit\/5fee6fd5000072ff32f2d65b6451f39af9e0e39e",
            "commit_sha":"5fee6fd5000072ff32f2d65b6451f39af9e0e39e",
            "patch":"MULTI",
            "chain_ord":"['5fee6fd5000072ff32f2d65b6451f39af9e0e39e', 'f2854408cce7e4b7fc6bf8676761904af9c96bde']",
            "before_first_fix_commit":"{'5fee6fd5000072ff32f2d65b6451f39af9e0e39e'}",
            "last_fix_commit":"f2854408cce7e4b7fc6bf8676761904af9c96bde",
            "chain_ord_pos":1.0,
            "commit_datetime":"11\/27\/2012, 15:52:19",
            "message":"Autocomplete demo: Combobox: Encode search term inside tooltips. Fixes #8859 - Autocomplete: XSS in combobox demo.",
            "author":"Scott Gonz\u00e1lez",
            "comments":"{'com_1': {'author': 'jzaefferer', 'datetime': '11\/27\/2012, 16:05:23', 'body': \"Doesn't this just hide the underlying tooltip vulnerability? If so, tooltip would have to use `.text()` instead of `.html()`, and make it sane to override that.\"}, 'com_2': {'author': 'scottgonzalez', 'datetime': '11\/27\/2012, 16:13:04', 'body': \"hmm...yeah, tooltip should handle this in the default `content` option. Good catch, I'll fix that.\"}, 'com_3': {'author': 'scottgonzalez', 'datetime': '11\/27\/2012, 16:22:17', 'body': 'Fixed in f2854408cce7e4b7fc6bf8676761904af9c96bde.'}}",
            "stats":"{'additions': 1, 'deletions': 1, 'total': 2}",
            "files":"{'demos\/autocomplete\/combobox.html': {'additions': 1, 'deletions': 1, 'changes': 2, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/jquery\/jquery-ui\/raw\/5fee6fd5000072ff32f2d65b6451f39af9e0e39e\/demos%2Fautocomplete%2Fcombobox.html', 'patch': '@@ -61,7 +61,7 @@\\n \\t\\t\\t\\t\\t\\t\/\/ remove invalid value, as it didn\\'t match anything\\n \\t\\t\\t\\t\\t\\t$( element )\\n \\t\\t\\t\\t\\t\\t\\t.val( \"\" )\\n-\\t\\t\\t\\t\\t\\t\\t.attr( \"title\", value + \" didn\\'t match any item\" )\\n+\\t\\t\\t\\t\\t\\t\\t.attr( \"title\", $( \"<a>\" ).text( value ).html() + \" didn\\'t match any item\" )\\n \\t\\t\\t\\t\\t\\t\\t.tooltip( \"open\" );\\n \\t\\t\\t\\t\\t\\tselect.val( \"\" );\\n \\t\\t\\t\\t\\t\\tsetTimeout(function() {'}}",
            "message_norm":"autocomplete demo: combobox: encode search term inside tooltips. fixes #8859 - autocomplete: xss in combobox demo.",
            "language":"pt",
            "entities":"[('encode', 'SECWORD', ''), ('fixes', 'ACTION', ''), ('#8859', 'ISSUE', ''), ('xss', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['demos\/autocomplete\/combobox.html'])",
            "num_files":1.0,
            "patch_content":"From 5fee6fd5000072ff32f2d65b6451f39af9e0e39e Mon Sep 17 00:00:00 2001\nFrom: =?UTF-8?q?Scott=20Gonz=C3=A1lez?= <scott.gonzalez@gmail.com>\nDate: Tue, 27 Nov 2012 10:52:19 -0500\nSubject: [PATCH] Autocomplete demo: Combobox: Encode search term inside\n tooltips. Fixes #8859 - Autocomplete: XSS in combobox demo.\n\n---\n demos\/autocomplete\/combobox.html | 2 +-\n 1 file changed, 1 insertion(+), 1 deletion(-)\n\ndiff --git a\/demos\/autocomplete\/combobox.html b\/demos\/autocomplete\/combobox.html\nindex 6229d47b216..8c6f59fc198 100644\n--- a\/demos\/autocomplete\/combobox.html\n+++ b\/demos\/autocomplete\/combobox.html\n@@ -61,7 +61,7 @@\n \t\t\t\t\t\t\/\/ remove invalid value, as it didn't match anything\n \t\t\t\t\t\t$( element )\n \t\t\t\t\t\t\t.val( \"\" )\n-\t\t\t\t\t\t\t.attr( \"title\", value + \" didn't match any item\" )\n+\t\t\t\t\t\t\t.attr( \"title\", $( \"<a>\" ).text( value ).html() + \" didn't match any item\" )\n \t\t\t\t\t\t\t.tooltip( \"open\" );\n \t\t\t\t\t\tselect.val( \"\" );\n \t\t\t\t\t\tsetTimeout(function() {"
        },
        {
            "index":483,
            "vuln_id":"GHSA-hjxc-462x-x77j",
            "cwe_id":"{'CWE-367'}",
            "score":5.9,
            "chain":"{'https:\/\/github.com\/yarnpkg\/yarn\/commit\/0474b8c66a8ea298f5e4dedc67b2de464297ad1c'}",
            "dataset":"osv",
            "summary":"TOCTOU Race Condition in Yarn The package integrity validation in yarn &lt; 1.19.0 contains a TOCTOU vulnerability where the hash is computed before writing a package to cache. It&#39;s not computed again when reading from the cache. This may lead to a cache pollution attack. This issue is fixed in 1.19.0.",
            "published_date":"2022-02-09",
            "chain_len":1,
            "project":"https:\/\/github.com\/yarnpkg\/yarn",
            "commit_href":"https:\/\/github.com\/yarnpkg\/yarn\/commit\/0474b8c66a8ea298f5e4dedc67b2de464297ad1c",
            "commit_sha":"0474b8c66a8ea298f5e4dedc67b2de464297ad1c",
            "patch":"SINGLE",
            "chain_ord":"['0474b8c66a8ea298f5e4dedc67b2de464297ad1c']",
            "before_first_fix_commit":"{'7f606ec3a31b53873056d48840e8acc647dca879'}",
            "last_fix_commit":"0474b8c66a8ea298f5e4dedc67b2de464297ad1c",
            "chain_ord_pos":1.0,
            "commit_datetime":"09\/28\/2019, 12:16:15",
            "message":"Prevents loading the cache if the stored integrity doesnt match",
            "author":"Ma\u00ebl Nison",
            "comments":null,
            "stats":"{'additions': 12, 'deletions': 3, 'total': 15}",
            "files":"{'src\/package-fetcher.js': {'additions': 12, 'deletions': 3, 'changes': 15, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/yarnpkg\/yarn\/raw\/0474b8c66a8ea298f5e4dedc67b2de464297ad1c\/src%2Fpackage-fetcher.js', 'patch': \"@@ -9,8 +9,17 @@ import * as fetchers from '.\/fetchers\/index.js';\\n import * as fs from '.\/util\/fs.js';\\n import * as promise from '.\/util\/promise.js';\\n \\n-async function fetchCache(dest: string, fetcher: Fetchers, config: Config): Promise<FetchedMetadata> {\\n-  const {hash, package: pkg} = await config.readPackageMetadata(dest);\\n+const ssri = require('ssri');\\n+\\n+async function fetchCache(dest: string, fetcher: Fetchers, config: Config, integrity: ?string): Promise<FetchedMetadata> {\\n+  const {hash, package: pkg, remote} = await config.readPackageMetadata(dest);\\n+\\n+  if (integrity) {\\n+    if (!remote.integrity || !ssri.parse(integrity).match(remote.integrity)) {\\n+      throw new MessageError('Incorrect integrity when fetching from the cache');\\n+    }\\n+  }\\n+\\n   await fetcher.setupMirrorFromCache();\\n   return {\\n     package: pkg,\\n@@ -40,7 +49,7 @@ export async function fetchOneRemote(\\n \\n   const fetcher = new Fetcher(dest, remote, config);\\n   if (await config.isValidModuleDest(dest)) {\\n-    return fetchCache(dest, fetcher, config);\\n+      return fetchCache(dest, fetcher, config, remote.integrity);\\n   }\\n \\n   \/\/ remove as the module may be invalid\"}}",
            "message_norm":"prevents loading the cache if the stored integrity doesnt match",
            "language":"en",
            "entities":"[('prevents', 'ACTION', ''), ('integrity', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['src\/package-fetcher.js'])",
            "num_files":1.0,
            "patch_content":"From 0474b8c66a8ea298f5e4dedc67b2de464297ad1c Mon Sep 17 00:00:00 2001\nFrom: =?UTF-8?q?Ma=C3=ABl=20Nison?= <nison.mael@gmail.com>\nDate: Sat, 28 Sep 2019 14:16:15 +0200\nSubject: [PATCH] Prevents loading the cache if the stored integrity doesnt\n match\n\n---\n src\/package-fetcher.js | 15 ++++++++++++---\n 1 file changed, 12 insertions(+), 3 deletions(-)\n\ndiff --git a\/src\/package-fetcher.js b\/src\/package-fetcher.js\nindex 3f4c24e42e..0913698b78 100644\n--- a\/src\/package-fetcher.js\n+++ b\/src\/package-fetcher.js\n@@ -9,8 +9,17 @@ import * as fetchers from '.\/fetchers\/index.js';\n import * as fs from '.\/util\/fs.js';\n import * as promise from '.\/util\/promise.js';\n \n-async function fetchCache(dest: string, fetcher: Fetchers, config: Config): Promise<FetchedMetadata> {\n-  const {hash, package: pkg} = await config.readPackageMetadata(dest);\n+const ssri = require('ssri');\n+\n+async function fetchCache(dest: string, fetcher: Fetchers, config: Config, integrity: ?string): Promise<FetchedMetadata> {\n+  const {hash, package: pkg, remote} = await config.readPackageMetadata(dest);\n+\n+  if (integrity) {\n+    if (!remote.integrity || !ssri.parse(integrity).match(remote.integrity)) {\n+      throw new MessageError('Incorrect integrity when fetching from the cache');\n+    }\n+  }\n+\n   await fetcher.setupMirrorFromCache();\n   return {\n     package: pkg,\n@@ -40,7 +49,7 @@ export async function fetchOneRemote(\n \n   const fetcher = new Fetcher(dest, remote, config);\n   if (await config.isValidModuleDest(dest)) {\n-    return fetchCache(dest, fetcher, config);\n+      return fetchCache(dest, fetcher, config, remote.integrity);\n   }\n \n   \/\/ remove as the module may be invalid"
        },
        {
            "index":342,
            "vuln_id":"GHSA-374m-jm66-3vj8",
            "cwe_id":"{'CWE-125'}",
            "score":7.1,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/f410212e373eb2aec4c9e60bf3702eba99a38aba'}",
            "dataset":"osv",
            "summary":"Heap OOB in `SparseBinCount` ### Impact\nThe [implementation](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/e71b86d47f8bc1816bf54d7bddc4170e47670b97\/tensorflow\/core\/kernels\/bincount_op.cc#L353-L417) of `SparseBinCount` is vulnerable to a heap OOB:\n\n```python\nimport tensorflow as tf\n  \n  \ntf.raw_ops.SparseBincount(\n  indices=[[0],[1],[2]]\n  values=[0,-10000000]\n  dense_shape=[1,1]\n  size=[1]\n  weights=[3,2,1]\n  binary_output=False)\n```\n\nThis is because of missing validation between the elements of the `values` argument and the shape of the sparse output:\n\n\n```cc\nfor (int64_t i = 0; i < indices_mat.dimension(0); ++i) {\n  const int64_t batch = indices_mat(i, 0);\n  const Tidx bin = values(i);\n  ...\n  out(batch, bin) = ...;\n}\n```\n\n### Patches\nWe have patched the issue in GitHub commit [f410212e373eb2aec4c9e60bf3702eba99a38aba](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/f410212e373eb2aec4c9e60bf3702eba99a38aba).\n\nThe fix will be included in TensorFlow 2.7.0. We will also cherrypick this commit on TensorFlow 2.6.1, TensorFlow 2.5.2, and TensorFlow 2.4.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by members of the Aivul Team from Qihoo 360.",
            "published_date":"2021-11-10",
            "chain_len":1,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/f410212e373eb2aec4c9e60bf3702eba99a38aba",
            "commit_sha":"f410212e373eb2aec4c9e60bf3702eba99a38aba",
            "patch":"SINGLE",
            "chain_ord":"['f410212e373eb2aec4c9e60bf3702eba99a38aba']",
            "before_first_fix_commit":"{'4656caa7d74420454da967288af143ec73fb4c9b'}",
            "last_fix_commit":"f410212e373eb2aec4c9e60bf3702eba99a38aba",
            "chain_ord_pos":1.0,
            "commit_datetime":"09\/30\/2021, 13:36:55",
            "message":"Prevent out-of-bound accesses in SparseBincount.\n\nPiperOrigin-RevId: 399918616\nChange-Id: I11d154f4444d3fde1f09c5c40628b8671791a30d",
            "author":"Penporn Koanantakool",
            "comments":null,
            "stats":"{'additions': 10, 'deletions': 0, 'total': 10}",
            "files":"{'tensorflow\/core\/kernels\/bincount_op.cc': {'additions': 10, 'deletions': 0, 'changes': 10, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/f410212e373eb2aec4c9e60bf3702eba99a38aba\/tensorflow%2Fcore%2Fkernels%2Fbincount_op.cc', 'patch': '@@ -405,6 +405,16 @@ class SparseBincountOp : public OpKernel {\\n       for (int64_t i = 0; i < indices_mat.dimension(0); ++i) {\\n         const int64_t batch = indices_mat(i, 0);\\n         const Tidx bin = values(i);\\n+        OP_REQUIRES(\\n+            ctx, batch < out.dimension(0),\\n+            errors::InvalidArgument(\"Index out of bound. `batch` (\", batch,\\n+                                    \") must be less than the dimension size (\",\\n+                                    out.dimension(0), \").\"));\\n+        OP_REQUIRES(\\n+            ctx, bin < out.dimension(1),\\n+            errors::InvalidArgument(\"Index out ouf bound. `bin` (\", bin,\\n+                                    \") must be less then the dimension size (\",\\n+                                    out.dimension(1), \").\"));\\n         if (bin < size) {\\n           if (binary_output_) {\\n             out(batch, bin) = T(1);'}}",
            "message_norm":"prevent out-of-bound accesses in sparsebincount.\n\npiperorigin-revid: 399918616\nchange-id: i11d154f4444d3fde1f09c5c40628b8671791a30d",
            "language":"en",
            "entities":"[('prevent', 'ACTION', ''), ('out-of-bound', 'SECWORD', ''), ('399918616', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/core\/kernels\/bincount_op.cc'])",
            "num_files":1.0,
            "patch_content":"From f410212e373eb2aec4c9e60bf3702eba99a38aba Mon Sep 17 00:00:00 2001\nFrom: Penporn Koanantakool <penporn@google.com>\nDate: Thu, 30 Sep 2021 06:36:55 -0700\nSubject: [PATCH] Prevent out-of-bound accesses in SparseBincount.\n\nPiperOrigin-RevId: 399918616\nChange-Id: I11d154f4444d3fde1f09c5c40628b8671791a30d\n---\n tensorflow\/core\/kernels\/bincount_op.cc | 10 ++++++++++\n 1 file changed, 10 insertions(+)\n\ndiff --git a\/tensorflow\/core\/kernels\/bincount_op.cc b\/tensorflow\/core\/kernels\/bincount_op.cc\nindex 6d668211da0968..0f661dd9f201a6 100644\n--- a\/tensorflow\/core\/kernels\/bincount_op.cc\n+++ b\/tensorflow\/core\/kernels\/bincount_op.cc\n@@ -405,6 +405,16 @@ class SparseBincountOp : public OpKernel {\n       for (int64_t i = 0; i < indices_mat.dimension(0); ++i) {\n         const int64_t batch = indices_mat(i, 0);\n         const Tidx bin = values(i);\n+        OP_REQUIRES(\n+            ctx, batch < out.dimension(0),\n+            errors::InvalidArgument(\"Index out of bound. `batch` (\", batch,\n+                                    \") must be less than the dimension size (\",\n+                                    out.dimension(0), \").\"));\n+        OP_REQUIRES(\n+            ctx, bin < out.dimension(1),\n+            errors::InvalidArgument(\"Index out ouf bound. `bin` (\", bin,\n+                                    \") must be less then the dimension size (\",\n+                                    out.dimension(1), \").\"));\n         if (bin < size) {\n           if (binary_output_) {\n             out(batch, bin) = T(1);"
        },
        {
            "index":247,
            "vuln_id":"GHSA-63m4-fhf2-cmf7",
            "cwe_id":"{'CWE-78'}",
            "score":9.8,
            "chain":"{'https:\/\/github.com\/KyleRoss\/windows-cpu\/commit\/b75e19aa2f7459a9506bceb577ba2341fe273117'}",
            "dataset":"osv",
            "summary":"Command Execution in windows-cpu Version of `windows-cpu` before 0.1.5 will execute arbitrary code passed into the first argument of the `findLoad` method, resulting in remote code execution.\n\n## Proof of Concept\n\n```\nvar win = require('windows-cpu');\nwind.findLoad('foo & calc.exe');\n```\n\n\n## Recommendation\n\nUpdate to version 0.1.5 or later.",
            "published_date":"2020-09-01",
            "chain_len":1,
            "project":"https:\/\/github.com\/KyleRoss\/windows-cpu",
            "commit_href":"https:\/\/github.com\/KyleRoss\/windows-cpu\/commit\/b75e19aa2f7459a9506bceb577ba2341fe273117",
            "commit_sha":"b75e19aa2f7459a9506bceb577ba2341fe273117",
            "patch":"SINGLE",
            "chain_ord":"['b75e19aa2f7459a9506bceb577ba2341fe273117']",
            "before_first_fix_commit":"{'da656c1a9d5edbf4e8bf0640f349aeb714a4f1a0'}",
            "last_fix_commit":"b75e19aa2f7459a9506bceb577ba2341fe273117",
            "chain_ord_pos":1.0,
            "commit_datetime":"04\/13\/2017, 04:32:09",
            "message":"ES6 Refactor + fix vulnerability",
            "author":"Kyle Ross",
            "comments":null,
            "stats":"{'additions': 120, 'deletions': 143, 'total': 263}",
            "files":"{'index.js': {'additions': 120, 'deletions': 143, 'changes': 263, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/KyleRoss\/windows-cpu\/raw\/b75e19aa2f7459a9506bceb577ba2341fe273117\/index.js', 'patch': '@@ -1,120 +1,92 @@\\n \/**\\n  * windows-cpu module for Node.js to get various load statistics.\\n  * @module windows-cpu\\n- * @version 0.1.4\\n- * @author Kyle Ross <kylerross1324@gmail.com>\\n+ * @version 1.0.0\\n+ * @author Kyle Ross\\n  * @license MIT License\\n- * \\n- * @requires os\\n- * @requires child_process\\n- *\\n- * @example\\n- *\\n- * var cpu = require(\\'windows-cpu\\');\\n  *\/\\n+\"use strict\";\\n \\n-(function() {\\n-    var platform = require(\\'os\\').platform(),\\n-        path     = require(\\'path\\'),\\n-        exec     = require(\\'child_process\\').exec,\\n-        execFile = require(\\'child_process\\').execFile,\\n-        wmic     = platform === \\'win32\\'? path.join(process.env.SystemRoot, \\'System32\\', \\'wbem\\', \\'wmic.exe\\') : null,\\n-        emptyFn  = function(){},\\n-        findLoad;\\n-    \\n-    \/*\\n-     * Checks current platform to ensure we are running on `win32`.\\n-     * @private\\n-     * @param {function} cb A callback function to call if there is an error.\\n-     * @returns {boolean} True if `win32` platform, else false.\\n-     *\/\\n-    function checkPlatform(cb) {\\n-        if(platform !== \\'win32\\') {\\n-            if(isFunction(cb)) cb(new Error(\\'windows-cpu> [ERROR] This module only works on Windows platforms.\\'));\\n-            return false;\\n-        }\\n-        return true;\\n-    }\\n+const fs = require(\\'fs\\');\\n+const path = require(\\'path\\');\\n+const cp = require(\\'child_process\\');\\n+const platform = require(\\'os\\').platform();\\n+\\n+const exec = cp.exec;\\n+const execFile = cp.execFile;\\n+const wmic = path.join(process.env.SystemRoot, \\'System32\\', \\'wbem\\', \\'wmic.exe\\');\\n+\\n+\/**\\n+ * Finds the current processor load of a specific process name or id.\\n+ * @private\\n+ * @param  {String}   arg Process name or id to lookup\\n+ * @param  {Function} cb  Callback to call with results\\n+ *\/\\n+function findLoad(arg, cb) {\\n+    let cmd = `wmic path Win32_PerfFormattedData_PerfProc_Process get Name,PercentProcessorTime,IDProcess | findstr \/i \/c:${arg}`;\\n     \\n-    \/*\\n-     * Proper checking to see if variable is a function.\\n-     * @private\\n-     * @param {*} fn The variable to check if is a function.\\n-     * @returns {boolean} True if is a function, else false.\\n-     *\/\\n-    function isFunction(fn) {\\n-        var getType = {};\\n-        return fn && getType.toString.call(fn) === \\'[object Function]\\';\\n+    exec(cmd, function(error, res, stderr) {\\n+        if(error !== null || stderr) return cb(error || stderr);\\n+        if(!res) return cb(`Cannot find results for provided arg: ${arg}`, { load: 0, results: [] });\\n+        \\n+        let found = res.replace(\/[^\\\\S\\\\n]+\/g, \\':\\').replace(\/:\\\\s\/g, \\'|\\').split(\\'|\\').filter(function(v) {\\n+            return !!v;\\n+        }).map(function(v) {\\n+            let [pid, proc, load] = v.split(\\':\\');\\n+            return {\\n+                pid: +pid,\\n+                process: proc,\\n+                load: +load\\n+            };\\n+        });\\n+        \\n+        let load = found.reduce((acc, val) => {\\n+            return acc + val.load;\\n+        }, 0);\\n+        \\n+        cb(null, { load, found });\\n+    });\\n+}\\n+\\n+\/**\\n+ * @class Public class for WindowsCPU\\n+ *\/\\n+class WindowsCPU {\\n+    constructor() {\\n+        \/**\\n+         * Access to uninstantiated WindowsCPU class\\n+         * @type {Class}\\n+         *\/\\n+        this.WindowsCPU = WindowsCPU;\\n+        this.checkPlatform();\\n     }\\n     \\n     \/**\\n-     * Gets the total load in percent for process(es) by a specific search parameter.\\n-     * @param {string|number} arg Specific search parameter. Can be a Process ID or Process Name.\\n-     * @param {function} cb A callback function to handle the results (error, results).\\n-     * @example\\n-     *\\n-     * var cpu = require(\\'windows-cpu\\');\\n-     *\\n-     * \/\/ Find the total load for \"chrome\" processes\\n-     * cpu.findLoad(\\'chrome\\', function(error, results) {\\n-     *      if(error) {\\n-     *          return console.log(error);\\n-     *      }\\n-     *\\n-     *      \/\/ results =>\\n-     *      \/\/ {\\n-     *      \/\/    load: 8,\\n-     *      \/\/    found: [\\n-     *      \/\/        { pid: \\'900\\', process: \\'chrome\\', load: 4 },\\n-     *      \/\/        { pid: \\'905\\', process: \\'chrome#1\\', load: 0 },\\n-     *      \/\/        { pid: \\'910\\', process: \\'chrome#2\\', load: 4 }\\n-     *      \/\/    ]\\n-     *      \/\/ }\\n-     *\\n-     *      console.log(\\'Google Chrome is currently using \\' + results.load + \\'% of the cpu.\\');\\n-     * });\\n+     * Checks if the current platform is supported by windows-cpu\\n+     * @return {Boolean} Returns `true` if platform is supported\\n+     * @throws {Error} If platform is not Windows\\n+     * @throws {Error} If wmic.exe process does not exist or cannot be accessed\\n      *\/\\n-    findLoad = exports.findLoad = function findLoad(arg, cb) {\\n-        if(!isFunction(cb)) cb = emptyFn;\\n-        if(!checkPlatform(cb)) return;\\n+    checkPlatform() {\\n+        if(platform !== \\'win32\\') \\n+            throw new Error(\\'windows-cpu only works on Windows platforms.\\');\\n         \\n-        var cmd = \"wmic path Win32_PerfFormattedData_PerfProc_Process get Name,PercentProcessorTime,IDProcess | findstr \/i \/c:\" + arg;\\n-        exec(cmd, function (error, res, stderr) {\\n-            if(error !== null || stderr) return cb(error || stderr);\\n-            if(!res) return cb(\\'Cannot find results for provided arg: \\' + arg, { load: 0, results: [] });\\n-            \\n-            var found = res.replace(\/[^\\\\S\\\\n]+\/g, \\':\\').replace(\/\\\\:\\\\s\/g, \\'|\\').split(\\'|\\').filter(function(v) {\\n-                return !!v;\\n-            }).map(function(v) {\\n-                var data = v.split(\\':\\');\\n-                return {\\n-                    pid: +data[0],\\n-                    process: data[1],\\n-                    load: +data[2]\\n-                };\\n-            });\\n-            \\n-            var totalLoad = 0;\\n-            \\n-            found.forEach(function(obj) {\\n-                totalLoad += obj.load;\\n-            });\\n-            \\n-            var output = {\\n-                load: totalLoad,\\n-                found: found\\n-            };\\n-            \\n-            cb(null, output);\\n-        });\\n-    };\\n+        try {\\n+            fs.accessSync(wmic);\\n+        } catch(e) {\\n+            throw new Error(\\'windows-cpu is not supported on your version of Windows or you are not running as administrator.\\');\\n+        }\\n+        \\n+        return true;\\n+    }\\n     \\n     \/**\\n      * Gets the total load in percent for all processes running on the current machine per CPU.\\n-     * @param {function} cb A callback function to handle the results (error, results).\\n+     * @param  {Function} cb Callback to call with results (error, results)\\n+     * @return {WindowsCPU}  Instance of the WindowsCPU class\\n      * @example\\n      *\\n-     * var cpu = require(\\'windows-cpu\\');\\n+     * const cpu = require(\\'windows-cpu\\');\\n      *\\n      * \/\/ Get total load on server for each CPU\\n      * cpu.totalLoad(function(error, results) {\\n@@ -129,27 +101,27 @@\\n      *      \/\/ [3, 10]\\n      * });\\n      *\/\\n-    exports.totalLoad = function totalLoad(cb) {\\n-        if (!isFunction(cb)) cb = emptyFn;\\n-        if (!checkPlatform(cb)) return;\\n-        \\n-        execFile(wmic, [\\'cpu\\', \\'get\\', \\'loadpercentage\\'], function (error, res, stderr) {\\n+    totalLoad(cb) {\\n+        execFile(wmic, [\\'cpu\\', \\'get\\', \\'loadpercentage\\'], function(error, res, stderr) {\\n             if(error !== null || stderr) return cb(error || stderr);\\n             \\n-            var cpus = (res.match(\/\\\\d+\/g) || []).map(function(x) { \\n+            let cpus = (res.match(\/\\\\d+\/g) || []).map(function(x) { \\n                 return +(x.trim()); \\n             });\\n             \\n             cb(null, cpus);\\n         });\\n-    };\\n+        \\n+        return this;\\n+    }\\n     \\n     \/**\\n-     * Gets the total load in percent for all Node.js processes running on the current machine.\\n-     * @param {function} cb A callback function to handle the results (error, results).\\n+     * Retrieves the current cpu load for all node processes running on the current machine\\n+     * @param  {Function} cb Callback to call with results (error, results)\\n+     * @return {WindowsCPU}  Instance of the WindowsCPU class\\n      * @example\\n      *\\n-     * var cpu = require(\\'windows-cpu\\');\\n+     * const cpu = require(\\'windows-cpu\\');\\n      *\\n      * \/\/ Get total load for all node processes\\n      * cpu.nodeLoad(function(error, results) {\\n@@ -167,19 +139,21 @@\\n      *      \/\/    ]\\n      *      \/\/ }\\n      *\\n-     *      console.log(\\'Total Node.js Load: \\' + results.load);\\n+     *      console.log(`Total Node.js Load: ${results.load}%`);\\n      * });\\n      *\/\\n-    exports.nodeLoad = function nodeLoad(cb) {\\n+    nodeLoad(cb) {\\n         findLoad(\\'node\\', cb);\\n-    };\\n+        return this;\\n+    }\\n     \\n     \/**\\n-     * Gets the total load in percent for all processes running on the current machine per CPU.\\n-     * @param {function} cb A callback function to handle the results (error, results).\\n+     * Retrieves the current cpu load for this process.\\n+     * @param  {Function} cb Callback to call with results (error, results)\\n+     * @return {WindowsCPU}  Instance of the WindowsCPU class\\n      * @example\\n      *\\n-     * var cpu = require(\\'windows-cpu\\');\\n+     * const cpu = require(\\'windows-cpu\\');\\n      *\\n      * \/\/ Get load for current running node process\\n      * cpu.processLoad(function(error, results) {\\n@@ -195,19 +169,21 @@\\n      *      \/\/    ]\\n      *      \/\/ }\\n      *\\n-     *      console.log(\\'Total Process Load: \\' + results.load);\\n+     *      console.log(`Total Process Load: ${results.load}%`);\\n      * });\\n      *\/\\n-    exports.processLoad = function processLoad(cb) {\\n+    processLoad(cb) {\\n         findLoad(process.pid, cb);\\n-    };\\n+        return this;\\n+    }\\n     \\n     \/**\\n-     * Gets the name of each processor in the machine.\\n-     * @param {function} cb A callback function to handle the results (error, results).\\n+     * Gets list of all processors in the current machine.\\n+     * @param  {Function} cb Callback to call with results (error, results)\\n+     * @return {WindowsCPU}  Instance of the WindowsCPU class\\n      * @example\\n      *\\n-     * var cpu = require(\\'windows-cpu\\');\\n+     * const cpu = require(\\'windows-cpu\\');\\n      *\\n      * \/\/ Get listing of processors\\n      * cpu.cpuInfo(function(error, results) {\\n@@ -224,28 +200,28 @@\\n      *      console.log(\\'Installed Processors: \\', results);\\n      * });\\n      *\/\\n-    exports.cpuInfo = function cpuInfo(cb) {\\n-        if(!isFunction(cb)) cb = emptyFn;\\n-        if(!checkPlatform(cb)) return;\\n-        \\n-        execFile(wmic, [\\'cpu\\', \\'get\\', \\'Name\\'], function (error, res, stderr) {\\n+    cpuInfo(cb) {\\n+        execFile(wmic, [\\'cpu\\', \\'get\\', \\'Name\\'], function(error, res, stderr) {\\n             if(error !== null || stderr) return cb(error || stderr);\\n             \\n-            var cpus = res.match(\/[^\\\\r\\\\n]+\/g).map(function(v) {\\n+            let cpus = res.match(\/[^\\\\r\\\\n]+\/g).map(function(v) {\\n                 return v.trim();\\n             });\\n             \\n             cpus.shift();\\n             cb(null, cpus);\\n         });\\n-    };\\n-\\n+        \\n+        return this;\\n+    }\\n+    \\n     \/**\\n-     * Gets the total memory usage value in KB , MB and GB .\\n-     * @param {function} cb A callback function to handle the result (error, results).\\n+     * Gets the total memory usage on the machine in KB, MB and GB.\\n+     * @param  {Function} cb Callback to call with results (error, results)\\n+     * @return {WindowsCPU}  Instance of the WindowsCPU class\\n      * @example\\n      *\\n-     * var cpu = require(\\'windows-cpu\\');\\n+     * const cpu = require(\\'windows-cpu\\');\\n      *\\n      * \/\/ Get the memory usage\\n      * cpu.totalMemoryUsage(function(error, results) {\\n@@ -263,17 +239,14 @@\\n      *      console.log(\\'Total Memory Usage: \\', result);\\n      * });\\n      *\/\\n-    exports.totalMemoryUsage = function totalMemoryUsage(cb) {\\n-        if (!isFunction(cb)) cb = emptyFn;\\n-        if (!checkPlatform(cb)) return;\\n-        \\n-        var cmd = \"tasklist \/FO csv \/nh\";\\n-        exec(cmd, function (error, res, stderr) {\\n+    totalMemoryUsage(cb) {\\n+        let cmd = \\'tasklist \/FO csv \/nh\\';\\n+        exec(cmd, function(error, res, stderr) {\\n             if(error !== null || stderr) return cb(error || stderr);\\n-            var results = { usageInKb: 0 , usageInMb: 0 , usageInGb: 0 };\\n+            let results = { usageInKb: 0 , usageInMb: 0 , usageInGb: 0 };\\n             \\n             results.usageInKb = res.match(\/[^\\\\r\\\\n]+\/g).map(function(v) {\\n-                var amt = +v.split(\\'\",\"\\')[4].replace(\/[^\\\\d]\/g, \\'\\');\\n+                let amt = +v.split(\\'\",\"\\')[4].replace(\/[^\\\\d]\/g, \\'\\');\\n                 return (!isNaN(amt) && typeof amt === \\'number\\')? amt : 0;\\n             }).reduce(function(prev, current) {\\n                 return prev + current;\\n@@ -284,5 +257,9 @@\\n             \\n             cb(null, results);\\n         });\\n-    };\\n-}());\\n+        \\n+        return this;\\n+    }\\n+}\\n+\\n+module.exports = new WindowsCPU();'}}",
            "message_norm":"es6 refactor + fix vulnerability",
            "language":"ca",
            "entities":"[('fix', 'ACTION', ''), ('vulnerability', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['index.js'])",
            "num_files":1.0,
            "patch_content":"From b75e19aa2f7459a9506bceb577ba2341fe273117 Mon Sep 17 00:00:00 2001\nFrom: Kyle Ross <kylerross1324@gmail.com>\nDate: Thu, 13 Apr 2017 00:32:09 -0400\nSubject: [PATCH] ES6 Refactor + fix vulnerability\n\n---\n index.js | 263 +++++++++++++++++++++++++------------------------------\n 1 file changed, 120 insertions(+), 143 deletions(-)\n\ndiff --git a\/index.js b\/index.js\nindex aab3bf6..a39876c 100644\n--- a\/index.js\n+++ b\/index.js\n@@ -1,120 +1,92 @@\n \/**\n  * windows-cpu module for Node.js to get various load statistics.\n  * @module windows-cpu\n- * @version 0.1.4\n- * @author Kyle Ross <kylerross1324@gmail.com>\n+ * @version 1.0.0\n+ * @author Kyle Ross\n  * @license MIT License\n- * \n- * @requires os\n- * @requires child_process\n- *\n- * @example\n- *\n- * var cpu = require('windows-cpu');\n  *\/\n+\"use strict\";\n \n-(function() {\n-    var platform = require('os').platform(),\n-        path     = require('path'),\n-        exec     = require('child_process').exec,\n-        execFile = require('child_process').execFile,\n-        wmic     = platform === 'win32'? path.join(process.env.SystemRoot, 'System32', 'wbem', 'wmic.exe') : null,\n-        emptyFn  = function(){},\n-        findLoad;\n-    \n-    \/*\n-     * Checks current platform to ensure we are running on `win32`.\n-     * @private\n-     * @param {function} cb A callback function to call if there is an error.\n-     * @returns {boolean} True if `win32` platform, else false.\n-     *\/\n-    function checkPlatform(cb) {\n-        if(platform !== 'win32') {\n-            if(isFunction(cb)) cb(new Error('windows-cpu> [ERROR] This module only works on Windows platforms.'));\n-            return false;\n-        }\n-        return true;\n-    }\n+const fs = require('fs');\n+const path = require('path');\n+const cp = require('child_process');\n+const platform = require('os').platform();\n+\n+const exec = cp.exec;\n+const execFile = cp.execFile;\n+const wmic = path.join(process.env.SystemRoot, 'System32', 'wbem', 'wmic.exe');\n+\n+\/**\n+ * Finds the current processor load of a specific process name or id.\n+ * @private\n+ * @param  {String}   arg Process name or id to lookup\n+ * @param  {Function} cb  Callback to call with results\n+ *\/\n+function findLoad(arg, cb) {\n+    let cmd = `wmic path Win32_PerfFormattedData_PerfProc_Process get Name,PercentProcessorTime,IDProcess | findstr \/i \/c:${arg}`;\n     \n-    \/*\n-     * Proper checking to see if variable is a function.\n-     * @private\n-     * @param {*} fn The variable to check if is a function.\n-     * @returns {boolean} True if is a function, else false.\n-     *\/\n-    function isFunction(fn) {\n-        var getType = {};\n-        return fn && getType.toString.call(fn) === '[object Function]';\n+    exec(cmd, function(error, res, stderr) {\n+        if(error !== null || stderr) return cb(error || stderr);\n+        if(!res) return cb(`Cannot find results for provided arg: ${arg}`, { load: 0, results: [] });\n+        \n+        let found = res.replace(\/[^\\S\\n]+\/g, ':').replace(\/:\\s\/g, '|').split('|').filter(function(v) {\n+            return !!v;\n+        }).map(function(v) {\n+            let [pid, proc, load] = v.split(':');\n+            return {\n+                pid: +pid,\n+                process: proc,\n+                load: +load\n+            };\n+        });\n+        \n+        let load = found.reduce((acc, val) => {\n+            return acc + val.load;\n+        }, 0);\n+        \n+        cb(null, { load, found });\n+    });\n+}\n+\n+\/**\n+ * @class Public class for WindowsCPU\n+ *\/\n+class WindowsCPU {\n+    constructor() {\n+        \/**\n+         * Access to uninstantiated WindowsCPU class\n+         * @type {Class}\n+         *\/\n+        this.WindowsCPU = WindowsCPU;\n+        this.checkPlatform();\n     }\n     \n     \/**\n-     * Gets the total load in percent for process(es) by a specific search parameter.\n-     * @param {string|number} arg Specific search parameter. Can be a Process ID or Process Name.\n-     * @param {function} cb A callback function to handle the results (error, results).\n-     * @example\n-     *\n-     * var cpu = require('windows-cpu');\n-     *\n-     * \/\/ Find the total load for \"chrome\" processes\n-     * cpu.findLoad('chrome', function(error, results) {\n-     *      if(error) {\n-     *          return console.log(error);\n-     *      }\n-     *\n-     *      \/\/ results =>\n-     *      \/\/ {\n-     *      \/\/    load: 8,\n-     *      \/\/    found: [\n-     *      \/\/        { pid: '900', process: 'chrome', load: 4 },\n-     *      \/\/        { pid: '905', process: 'chrome#1', load: 0 },\n-     *      \/\/        { pid: '910', process: 'chrome#2', load: 4 }\n-     *      \/\/    ]\n-     *      \/\/ }\n-     *\n-     *      console.log('Google Chrome is currently using ' + results.load + '% of the cpu.');\n-     * });\n+     * Checks if the current platform is supported by windows-cpu\n+     * @return {Boolean} Returns `true` if platform is supported\n+     * @throws {Error} If platform is not Windows\n+     * @throws {Error} If wmic.exe process does not exist or cannot be accessed\n      *\/\n-    findLoad = exports.findLoad = function findLoad(arg, cb) {\n-        if(!isFunction(cb)) cb = emptyFn;\n-        if(!checkPlatform(cb)) return;\n+    checkPlatform() {\n+        if(platform !== 'win32') \n+            throw new Error('windows-cpu only works on Windows platforms.');\n         \n-        var cmd = \"wmic path Win32_PerfFormattedData_PerfProc_Process get Name,PercentProcessorTime,IDProcess | findstr \/i \/c:\" + arg;\n-        exec(cmd, function (error, res, stderr) {\n-            if(error !== null || stderr) return cb(error || stderr);\n-            if(!res) return cb('Cannot find results for provided arg: ' + arg, { load: 0, results: [] });\n-            \n-            var found = res.replace(\/[^\\S\\n]+\/g, ':').replace(\/\\:\\s\/g, '|').split('|').filter(function(v) {\n-                return !!v;\n-            }).map(function(v) {\n-                var data = v.split(':');\n-                return {\n-                    pid: +data[0],\n-                    process: data[1],\n-                    load: +data[2]\n-                };\n-            });\n-            \n-            var totalLoad = 0;\n-            \n-            found.forEach(function(obj) {\n-                totalLoad += obj.load;\n-            });\n-            \n-            var output = {\n-                load: totalLoad,\n-                found: found\n-            };\n-            \n-            cb(null, output);\n-        });\n-    };\n+        try {\n+            fs.accessSync(wmic);\n+        } catch(e) {\n+            throw new Error('windows-cpu is not supported on your version of Windows or you are not running as administrator.');\n+        }\n+        \n+        return true;\n+    }\n     \n     \/**\n      * Gets the total load in percent for all processes running on the current machine per CPU.\n-     * @param {function} cb A callback function to handle the results (error, results).\n+     * @param  {Function} cb Callback to call with results (error, results)\n+     * @return {WindowsCPU}  Instance of the WindowsCPU class\n      * @example\n      *\n-     * var cpu = require('windows-cpu');\n+     * const cpu = require('windows-cpu');\n      *\n      * \/\/ Get total load on server for each CPU\n      * cpu.totalLoad(function(error, results) {\n@@ -129,27 +101,27 @@\n      *      \/\/ [3, 10]\n      * });\n      *\/\n-    exports.totalLoad = function totalLoad(cb) {\n-        if (!isFunction(cb)) cb = emptyFn;\n-        if (!checkPlatform(cb)) return;\n-        \n-        execFile(wmic, ['cpu', 'get', 'loadpercentage'], function (error, res, stderr) {\n+    totalLoad(cb) {\n+        execFile(wmic, ['cpu', 'get', 'loadpercentage'], function(error, res, stderr) {\n             if(error !== null || stderr) return cb(error || stderr);\n             \n-            var cpus = (res.match(\/\\d+\/g) || []).map(function(x) { \n+            let cpus = (res.match(\/\\d+\/g) || []).map(function(x) { \n                 return +(x.trim()); \n             });\n             \n             cb(null, cpus);\n         });\n-    };\n+        \n+        return this;\n+    }\n     \n     \/**\n-     * Gets the total load in percent for all Node.js processes running on the current machine.\n-     * @param {function} cb A callback function to handle the results (error, results).\n+     * Retrieves the current cpu load for all node processes running on the current machine\n+     * @param  {Function} cb Callback to call with results (error, results)\n+     * @return {WindowsCPU}  Instance of the WindowsCPU class\n      * @example\n      *\n-     * var cpu = require('windows-cpu');\n+     * const cpu = require('windows-cpu');\n      *\n      * \/\/ Get total load for all node processes\n      * cpu.nodeLoad(function(error, results) {\n@@ -167,19 +139,21 @@\n      *      \/\/    ]\n      *      \/\/ }\n      *\n-     *      console.log('Total Node.js Load: ' + results.load);\n+     *      console.log(`Total Node.js Load: ${results.load}%`);\n      * });\n      *\/\n-    exports.nodeLoad = function nodeLoad(cb) {\n+    nodeLoad(cb) {\n         findLoad('node', cb);\n-    };\n+        return this;\n+    }\n     \n     \/**\n-     * Gets the total load in percent for all processes running on the current machine per CPU.\n-     * @param {function} cb A callback function to handle the results (error, results).\n+     * Retrieves the current cpu load for this process.\n+     * @param  {Function} cb Callback to call with results (error, results)\n+     * @return {WindowsCPU}  Instance of the WindowsCPU class\n      * @example\n      *\n-     * var cpu = require('windows-cpu');\n+     * const cpu = require('windows-cpu');\n      *\n      * \/\/ Get load for current running node process\n      * cpu.processLoad(function(error, results) {\n@@ -195,19 +169,21 @@\n      *      \/\/    ]\n      *      \/\/ }\n      *\n-     *      console.log('Total Process Load: ' + results.load);\n+     *      console.log(`Total Process Load: ${results.load}%`);\n      * });\n      *\/\n-    exports.processLoad = function processLoad(cb) {\n+    processLoad(cb) {\n         findLoad(process.pid, cb);\n-    };\n+        return this;\n+    }\n     \n     \/**\n-     * Gets the name of each processor in the machine.\n-     * @param {function} cb A callback function to handle the results (error, results).\n+     * Gets list of all processors in the current machine.\n+     * @param  {Function} cb Callback to call with results (error, results)\n+     * @return {WindowsCPU}  Instance of the WindowsCPU class\n      * @example\n      *\n-     * var cpu = require('windows-cpu');\n+     * const cpu = require('windows-cpu');\n      *\n      * \/\/ Get listing of processors\n      * cpu.cpuInfo(function(error, results) {\n@@ -224,28 +200,28 @@\n      *      console.log('Installed Processors: ', results);\n      * });\n      *\/\n-    exports.cpuInfo = function cpuInfo(cb) {\n-        if(!isFunction(cb)) cb = emptyFn;\n-        if(!checkPlatform(cb)) return;\n-        \n-        execFile(wmic, ['cpu', 'get', 'Name'], function (error, res, stderr) {\n+    cpuInfo(cb) {\n+        execFile(wmic, ['cpu', 'get', 'Name'], function(error, res, stderr) {\n             if(error !== null || stderr) return cb(error || stderr);\n             \n-            var cpus = res.match(\/[^\\r\\n]+\/g).map(function(v) {\n+            let cpus = res.match(\/[^\\r\\n]+\/g).map(function(v) {\n                 return v.trim();\n             });\n             \n             cpus.shift();\n             cb(null, cpus);\n         });\n-    };\n-\n+        \n+        return this;\n+    }\n+    \n     \/**\n-     * Gets the total memory usage value in KB , MB and GB .\n-     * @param {function} cb A callback function to handle the result (error, results).\n+     * Gets the total memory usage on the machine in KB, MB and GB.\n+     * @param  {Function} cb Callback to call with results (error, results)\n+     * @return {WindowsCPU}  Instance of the WindowsCPU class\n      * @example\n      *\n-     * var cpu = require('windows-cpu');\n+     * const cpu = require('windows-cpu');\n      *\n      * \/\/ Get the memory usage\n      * cpu.totalMemoryUsage(function(error, results) {\n@@ -263,17 +239,14 @@\n      *      console.log('Total Memory Usage: ', result);\n      * });\n      *\/\n-    exports.totalMemoryUsage = function totalMemoryUsage(cb) {\n-        if (!isFunction(cb)) cb = emptyFn;\n-        if (!checkPlatform(cb)) return;\n-        \n-        var cmd = \"tasklist \/FO csv \/nh\";\n-        exec(cmd, function (error, res, stderr) {\n+    totalMemoryUsage(cb) {\n+        let cmd = 'tasklist \/FO csv \/nh';\n+        exec(cmd, function(error, res, stderr) {\n             if(error !== null || stderr) return cb(error || stderr);\n-            var results = { usageInKb: 0 , usageInMb: 0 , usageInGb: 0 };\n+            let results = { usageInKb: 0 , usageInMb: 0 , usageInGb: 0 };\n             \n             results.usageInKb = res.match(\/[^\\r\\n]+\/g).map(function(v) {\n-                var amt = +v.split('\",\"')[4].replace(\/[^\\d]\/g, '');\n+                let amt = +v.split('\",\"')[4].replace(\/[^\\d]\/g, '');\n                 return (!isNaN(amt) && typeof amt === 'number')? amt : 0;\n             }).reduce(function(prev, current) {\n                 return prev + current;\n@@ -284,5 +257,9 @@\n             \n             cb(null, results);\n         });\n-    };\n-}());\n+        \n+        return this;\n+    }\n+}\n+\n+module.exports = new WindowsCPU();"
        },
        {
            "index":909,
            "vuln_id":"GHSA-hc6c-75p4-hmq4",
            "cwe_id":"{'CWE-476'}",
            "score":2.5,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/a7116dd3913c4a4afd2a3a938573aa7c785fdfc6'}",
            "dataset":"osv",
            "summary":"Reference binding to null pointer in `MatrixDiag*` ops ### Impact\nThe implementation of [`MatrixDiag*` operations](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/4c4f420e68f1cfaf8f4b6e8e3eb857e9e4c3ff33\/tensorflow\/core\/kernels\/linalg\/matrix_diag_op.cc#L195-L197) does not validate that the tensor arguments are non-empty:\n\n```cc\n      num_rows = context->input(2).flat<int32>()(0);\n      num_cols = context->input(3).flat<int32>()(0);\n      padding_value = context->input(4).flat<T>()(0); \n``` \n\nThus, users can trigger null pointer dereferences if any of the above tensors are null:\n\n```python\nimport tensorflow as tf\n\nd = tf.convert_to_tensor([],dtype=tf.float32)\np = tf.convert_to_tensor([],dtype=tf.float32)\ntf.raw_ops.MatrixDiagV2(diagonal=d, k=0, num_rows=0, num_cols=0, padding_value=p)\n```\n\nChanging from `tf.raw_ops.MatrixDiagV2` to `tf.raw_ops.MatrixDiagV3` still reproduces the issue.\n\n### Patches\nWe have patched the issue in GitHub commit [a7116dd3913c4a4afd2a3a938573aa7c785fdfc6](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/a7116dd3913c4a4afd2a3a938573aa7c785fdfc6).\n\nThe fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by Ye Zhang and Yakun Zhang of Baidu X-Team.",
            "published_date":"2021-05-21",
            "chain_len":1,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/a7116dd3913c4a4afd2a3a938573aa7c785fdfc6",
            "commit_sha":"a7116dd3913c4a4afd2a3a938573aa7c785fdfc6",
            "patch":"SINGLE",
            "chain_ord":"['a7116dd3913c4a4afd2a3a938573aa7c785fdfc6']",
            "before_first_fix_commit":"{'4c4f420e68f1cfaf8f4b6e8e3eb857e9e4c3ff33'}",
            "last_fix_commit":"a7116dd3913c4a4afd2a3a938573aa7c785fdfc6",
            "chain_ord_pos":1.0,
            "commit_datetime":"04\/18\/2021, 03:55:53",
            "message":"Validate `MatrixDiagV{2,3}` arguments to prevent breakage.\n\nPiperOrigin-RevId: 369056033\nChange-Id: Ic2018c297d3dd6f252dc1dd3667f1ed5cb1eaa42",
            "author":"Mihai Maruseac",
            "comments":null,
            "stats":"{'additions': 16, 'deletions': 3, 'total': 19}",
            "files":"{'tensorflow\/core\/kernels\/linalg\/matrix_diag_op.cc': {'additions': 16, 'deletions': 3, 'changes': 19, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/a7116dd3913c4a4afd2a3a938573aa7c785fdfc6\/tensorflow%2Fcore%2Fkernels%2Flinalg%2Fmatrix_diag_op.cc', 'patch': '@@ -192,9 +192,22 @@ class MatrixDiagOp : public OpKernel {\\n           upper_diag_index = diag_index.flat<int32>()(1);\\n         }\\n       }\\n-      num_rows = context->input(2).flat<int32>()(0);\\n-      num_cols = context->input(3).flat<int32>()(0);\\n-      padding_value = context->input(4).flat<T>()(0);\\n+\\n+      auto& num_rows_tensor = context->input(2);\\n+      OP_REQUIRES(context, TensorShapeUtils::IsScalar(num_rows_tensor.shape()),\\n+                  errors::InvalidArgument(\"num_rows must be a scalar\"));\\n+      num_rows = num_rows_tensor.flat<int32>()(0);\\n+\\n+      auto& num_cols_tensor = context->input(3);\\n+      OP_REQUIRES(context, TensorShapeUtils::IsScalar(num_cols_tensor.shape()),\\n+                  errors::InvalidArgument(\"num_cols must be a scalar\"));\\n+      num_cols = num_cols_tensor.flat<int32>()(0);\\n+\\n+      auto& padding_value_tensor = context->input(4);\\n+      OP_REQUIRES(context,\\n+                  TensorShapeUtils::IsScalar(padding_value_tensor.shape()),\\n+                  errors::InvalidArgument(\"padding_value must be a scalar\"));\\n+      padding_value = padding_value_tensor.flat<T>()(0);\\n     }\\n \\n     \/\/ Size validations.'}}",
            "message_norm":"validate `matrixdiagv{2,3}` arguments to prevent breakage.\n\npiperorigin-revid: 369056033\nchange-id: ic2018c297d3dd6f252dc1dd3667f1ed5cb1eaa42",
            "language":"en",
            "entities":"[('validate', 'ACTION', ''), ('prevent', 'ACTION', ''), ('369056033', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/core\/kernels\/linalg\/matrix_diag_op.cc'])",
            "num_files":1.0,
            "patch_content":"From a7116dd3913c4a4afd2a3a938573aa7c785fdfc6 Mon Sep 17 00:00:00 2001\nFrom: Mihai Maruseac <mihaimaruseac@google.com>\nDate: Sat, 17 Apr 2021 20:55:53 -0700\nSubject: [PATCH] Validate `MatrixDiagV{2,3}` arguments to prevent breakage.\n\nPiperOrigin-RevId: 369056033\nChange-Id: Ic2018c297d3dd6f252dc1dd3667f1ed5cb1eaa42\n---\n ...\/core\/kernels\/linalg\/matrix_diag_op.cc     | 19 ++++++++++++++++---\n 1 file changed, 16 insertions(+), 3 deletions(-)\n\ndiff --git a\/tensorflow\/core\/kernels\/linalg\/matrix_diag_op.cc b\/tensorflow\/core\/kernels\/linalg\/matrix_diag_op.cc\nindex 69cc8170793ae3..d4eb589836a859 100644\n--- a\/tensorflow\/core\/kernels\/linalg\/matrix_diag_op.cc\n+++ b\/tensorflow\/core\/kernels\/linalg\/matrix_diag_op.cc\n@@ -192,9 +192,22 @@ class MatrixDiagOp : public OpKernel {\n           upper_diag_index = diag_index.flat<int32>()(1);\n         }\n       }\n-      num_rows = context->input(2).flat<int32>()(0);\n-      num_cols = context->input(3).flat<int32>()(0);\n-      padding_value = context->input(4).flat<T>()(0);\n+\n+      auto& num_rows_tensor = context->input(2);\n+      OP_REQUIRES(context, TensorShapeUtils::IsScalar(num_rows_tensor.shape()),\n+                  errors::InvalidArgument(\"num_rows must be a scalar\"));\n+      num_rows = num_rows_tensor.flat<int32>()(0);\n+\n+      auto& num_cols_tensor = context->input(3);\n+      OP_REQUIRES(context, TensorShapeUtils::IsScalar(num_cols_tensor.shape()),\n+                  errors::InvalidArgument(\"num_cols must be a scalar\"));\n+      num_cols = num_cols_tensor.flat<int32>()(0);\n+\n+      auto& padding_value_tensor = context->input(4);\n+      OP_REQUIRES(context,\n+                  TensorShapeUtils::IsScalar(padding_value_tensor.shape()),\n+                  errors::InvalidArgument(\"padding_value must be a scalar\"));\n+      padding_value = padding_value_tensor.flat<T>()(0);\n     }\n \n     \/\/ Size validations."
        },
        {
            "index":533,
            "vuln_id":"GHSA-p2vw-f87c-q597",
            "cwe_id":"{'CWE-863'}",
            "score":6.5,
            "chain":"{'https:\/\/github.com\/snipe\/snipe-it\/commit\/2e9cf8fa87a025c0eac9f79f4864b3fdd33a950c'}",
            "dataset":"osv",
            "summary":"Improper Access Control in snipe\/snipe-it Improper Access Control in GitHub repository snipe\/snipe-it prior to 5.4.4.",
            "published_date":"2022-04-29",
            "chain_len":1,
            "project":"https:\/\/github.com\/snipe\/snipe-it",
            "commit_href":"https:\/\/github.com\/snipe\/snipe-it\/commit\/2e9cf8fa87a025c0eac9f79f4864b3fdd33a950c",
            "commit_sha":"2e9cf8fa87a025c0eac9f79f4864b3fdd33a950c",
            "patch":"SINGLE",
            "chain_ord":"['2e9cf8fa87a025c0eac9f79f4864b3fdd33a950c']",
            "before_first_fix_commit":"{'126bb486b5146975f562d51b8f75dd2e30bee74d'}",
            "last_fix_commit":"2e9cf8fa87a025c0eac9f79f4864b3fdd33a950c",
            "chain_ord_pos":1.0,
            "commit_datetime":"04\/28\/2022, 14:45:37",
            "message":"Added access gate to the requested assets index\n\nSigned-off-by: snipe <snipe@snipe.net>",
            "author":"snipe",
            "comments":null,
            "stats":"{'additions': 1, 'deletions': 0, 'total': 1}",
            "files":"{'app\/Http\/Controllers\/Assets\/AssetsController.php': {'additions': 1, 'deletions': 0, 'changes': 1, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/snipe\/snipe-it\/raw\/2e9cf8fa87a025c0eac9f79f4864b3fdd33a950c\/app%2FHttp%2FControllers%2FAssets%2FAssetsController.php', 'patch': \"@@ -861,6 +861,7 @@ public function auditStore(Request $request, $id)\\n \\n     public function getRequestedIndex($user_id = null)\\n     {\\n+        $this->authorize('index', Asset::class);\\n         $requestedItems = CheckoutRequest::with('user', 'requestedItem')->whereNull('canceled_at')->with('user', 'requestedItem');\\n \\n         if ($user_id) {\"}}",
            "message_norm":"added access gate to the requested assets index\n\nsigned-off-by: snipe <snipe@snipe.net>",
            "language":"en",
            "entities":"[('added', 'ACTION', ''), ('snipe@snipe.net', 'EMAIL', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['app\/Http\/Controllers\/Assets\/AssetsController.php'])",
            "num_files":1.0,
            "patch_content":"From 2e9cf8fa87a025c0eac9f79f4864b3fdd33a950c Mon Sep 17 00:00:00 2001\nFrom: snipe <snipe@snipe.net>\nDate: Thu, 28 Apr 2022 15:45:37 +0100\nSubject: [PATCH] Added access gate to the requested assets index\n\nSigned-off-by: snipe <snipe@snipe.net>\n---\n app\/Http\/Controllers\/Assets\/AssetsController.php | 1 +\n 1 file changed, 1 insertion(+)\n\ndiff --git a\/app\/Http\/Controllers\/Assets\/AssetsController.php b\/app\/Http\/Controllers\/Assets\/AssetsController.php\nindex 069664a7f68c..d0f90f9fa8fa 100755\n--- a\/app\/Http\/Controllers\/Assets\/AssetsController.php\n+++ b\/app\/Http\/Controllers\/Assets\/AssetsController.php\n@@ -861,6 +861,7 @@ public function auditStore(Request $request, $id)\n \n     public function getRequestedIndex($user_id = null)\n     {\n+        $this->authorize('index', Asset::class);\n         $requestedItems = CheckoutRequest::with('user', 'requestedItem')->whereNull('canceled_at')->with('user', 'requestedItem');\n \n         if ($user_id) {"
        },
        {
            "index":217,
            "vuln_id":"GHSA-7wv8-g97r-432h",
            "cwe_id":"{'CWE-200'}",
            "score":7.5,
            "chain":"{'https:\/\/github.com\/microweber\/microweber\/commit\/e680e134a4215c979bfd2eaf58336be34c8fc6e6'}",
            "dataset":"osv",
            "summary":"Exposure of Sensitive Information to an Unauthorized Actor in microweber Exposure of Sensitive Information to an Unauthorized Actor in Packagist microweber\/microweber prior to 1.2.11.",
            "published_date":"2022-01-21",
            "chain_len":1,
            "project":"https:\/\/github.com\/microweber\/microweber",
            "commit_href":"https:\/\/github.com\/microweber\/microweber\/commit\/e680e134a4215c979bfd2eaf58336be34c8fc6e6",
            "commit_sha":"e680e134a4215c979bfd2eaf58336be34c8fc6e6",
            "patch":"SINGLE",
            "chain_ord":"['e680e134a4215c979bfd2eaf58336be34c8fc6e6']",
            "before_first_fix_commit":"{'62aa09ed44ff63f5fffc5addbf000423d7c38e44'}",
            "last_fix_commit":"e680e134a4215c979bfd2eaf58336be34c8fc6e6",
            "chain_ord_pos":1.0,
            "commit_datetime":"01\/19\/2022, 09:35:10",
            "message":"search_authors only admins",
            "author":"Bozhidar Slaveykov",
            "comments":null,
            "stats":"{'additions': 2, 'deletions': 2, 'total': 4}",
            "files":"{'src\/MicroweberPackages\/User\/helpers\/api_user.php': {'additions': 2, 'deletions': 2, 'changes': 4, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/microweber\/microweber\/raw\/e680e134a4215c979bfd2eaf58336be34c8fc6e6\/src%2FMicroweberPackages%2FUser%2Fhelpers%2Fapi_user.php', 'patch': \"@@ -63,9 +63,9 @@\\n \\n });\\n \\n-api_expose('users\/search_authors', function ($params = false) {\\n+api_expose_admin('users\/search_authors', function ($params = false) {\\n \\n-    $return = array();\\n+    $return = array(); \\n \\n     $kw = false;\\n     if (isset($params['kw'])) {\"}}",
            "message_norm":"search_authors only admins",
            "language":"en",
            "entities":"[('admins', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['src\/MicroweberPackages\/User\/helpers\/api_user.php'])",
            "num_files":1.0,
            "patch_content":"From e680e134a4215c979bfd2eaf58336be34c8fc6e6 Mon Sep 17 00:00:00 2001\nFrom: Bozhidar Slaveykov <bobi@microweber.com>\nDate: Wed, 19 Jan 2022 11:35:10 +0200\nSubject: [PATCH] search_authors only admins\n\n---\n src\/MicroweberPackages\/User\/helpers\/api_user.php | 4 ++--\n 1 file changed, 2 insertions(+), 2 deletions(-)\n\ndiff --git a\/src\/MicroweberPackages\/User\/helpers\/api_user.php b\/src\/MicroweberPackages\/User\/helpers\/api_user.php\nindex 2278395c345..ad6bb9572b3 100644\n--- a\/src\/MicroweberPackages\/User\/helpers\/api_user.php\n+++ b\/src\/MicroweberPackages\/User\/helpers\/api_user.php\n@@ -63,9 +63,9 @@\n \n });\n \n-api_expose('users\/search_authors', function ($params = false) {\n+api_expose_admin('users\/search_authors', function ($params = false) {\n \n-    $return = array();\n+    $return = array(); \n \n     $kw = false;\n     if (isset($params['kw'])) {"
        },
        {
            "index":457,
            "vuln_id":"GHSA-6445-fm66-fvq2",
            "cwe_id":"{'CWE-190'}",
            "score":6.5,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/b51b82fe65ebace4475e3c54eb089c18a4403f1c', 'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/a68f68061e263a88321c104a6c911fe5598050a8'}",
            "dataset":"osv",
            "summary":"Integer overflows in Tensorflow ### Impact \nThe [implementation of `AddManySparseToTensorsMap`](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/5100e359aef5c8021f2e71c7b986420b85ce7b3d\/tensorflow\/core\/kernels\/sparse_tensors_map_ops.cc) is vulnerable to an integer overflow which results in a `CHECK`-fail when building new `TensorShape` objects (so, an assert failure based denial of service):\n\n```python\nimport tensorflow as tf\nimport numpy as np\n\ntf.raw_ops.AddManySparseToTensorsMap(\n    sparse_indices=[(0,0),(0,1),(0,2),(4,3),(5,0),(5,1)],\n    sparse_values=[1,1,1,1,1,1],\n    sparse_shape=[2**32,2**32],\n    container='',\n    shared_name='',\n    name=None)\n```\n\nWe are missing some validation on the shapes of the input tensors as well as directly constructing a large `TensorShape` with user-provided dimensions. The latter is an instance of [TFSA-2021-198](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/tensorflow\/security\/advisory\/tfsa-2021-198.md) (CVE-2021-41197) and is easily fixed by replacing a call to `TensorShape` constructor with a call to `BuildTensorShape` static helper factory.\n### Patches\nWe have patched the issue in GitHub commits [b51b82fe65ebace4475e3c54eb089c18a4403f1c](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/b51b82fe65ebace4475e3c54eb089c18a4403f1c) and [a68f68061e263a88321c104a6c911fe5598050a8](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/a68f68061e263a88321c104a6c911fe5598050a8).\n\nThe fix will be included in TensorFlow 2.8.0. We will also cherrypick this commit on TensorFlow 2.7.1, TensorFlow 2.6.3, and TensorFlow 2.5.3, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by Faysal Hossain Shezan from University of Virginia.",
            "published_date":"2022-02-09",
            "chain_len":2,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/a68f68061e263a88321c104a6c911fe5598050a8",
            "commit_sha":"a68f68061e263a88321c104a6c911fe5598050a8",
            "patch":"MULTI",
            "chain_ord":"['b51b82fe65ebace4475e3c54eb089c18a4403f1c', 'a68f68061e263a88321c104a6c911fe5598050a8']",
            "before_first_fix_commit":"{'e8f4be7958736823b9f56090611ec2fb09824d51'}",
            "last_fix_commit":"a68f68061e263a88321c104a6c911fe5598050a8",
            "chain_ord_pos":2.0,
            "commit_datetime":"12\/10\/2021, 00:17:26",
            "message":"Replace faulty overflow check with a builder for `TensorShape`.\n\nPrevents an integer overflow that was not caught before.\n\nPiperOrigin-RevId: 415381595\nChange-Id: I76585ddedc912bd9f4a390aeafa8e2ced1a28863",
            "author":"Mihai Maruseac",
            "comments":null,
            "stats":"{'additions': 3, 'deletions': 15, 'total': 18}",
            "files":"{'tensorflow\/core\/kernels\/sparse_tensors_map_ops.cc': {'additions': 3, 'deletions': 15, 'changes': 18, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/a68f68061e263a88321c104a6c911fe5598050a8\/tensorflow%2Fcore%2Fkernels%2Fsparse_tensors_map_ops.cc', 'patch': '@@ -263,22 +263,10 @@ class AddManySparseToTensorsMapOp : public SparseTensorAccessingOp {\\n             \"Rank of input SparseTensor should be > 1, but saw rank: \", rank));\\n \\n     auto input_shape_vec = input_shape->vec<int64_t>();\\n-    int new_num_elements = 1;\\n-    bool overflow_ocurred = false;\\n-    for (int i = 0; i < input_shape_vec.size(); i++) {\\n-      new_num_elements =\\n-          MultiplyWithoutOverflow(new_num_elements, input_shape_vec(i));\\n-      if (new_num_elements < 0) {\\n-        overflow_ocurred = true;\\n-        break;\\n-      }\\n-    }\\n-\\n-    OP_REQUIRES(\\n-        context, !overflow_ocurred,\\n-        errors::Internal(\"Encountered overflow from large input shape.\"));\\n \\n-    TensorShape tensor_input_shape(input_shape_vec);\\n+    TensorShape tensor_input_shape;\\n+    OP_REQUIRES_OK(context, TensorShape::BuildTensorShape(input_shape_vec,\\n+                                                          &tensor_input_shape));\\n     gtl::InlinedVector<int64_t, 8> std_order(rank);\\n     std::iota(std_order.begin(), std_order.end(), 0);\\n     SparseTensor input_st;'}}",
            "message_norm":"replace faulty overflow check with a builder for `tensorshape`.\n\nprevents an integer overflow that was not caught before.\n\npiperorigin-revid: 415381595\nchange-id: i76585ddedc912bd9f4a390aeafa8e2ced1a28863",
            "language":"en",
            "entities":"[('overflow', 'SECWORD', ''), ('prevents', 'ACTION', ''), ('integer overflow', 'SECWORD', ''), ('415381595', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/core\/kernels\/sparse_tensors_map_ops.cc'])",
            "num_files":1.0,
            "patch_content":"From a68f68061e263a88321c104a6c911fe5598050a8 Mon Sep 17 00:00:00 2001\nFrom: Mihai Maruseac <mihaimaruseac@google.com>\nDate: Thu, 9 Dec 2021 16:17:26 -0800\nSubject: [PATCH] Replace faulty overflow check with a builder for\n `TensorShape`.\n\nPrevents an integer overflow that was not caught before.\n\nPiperOrigin-RevId: 415381595\nChange-Id: I76585ddedc912bd9f4a390aeafa8e2ced1a28863\n---\n ...\/core\/kernels\/sparse_tensors_map_ops.cc     | 18 +++---------------\n 1 file changed, 3 insertions(+), 15 deletions(-)\n\ndiff --git a\/tensorflow\/core\/kernels\/sparse_tensors_map_ops.cc b\/tensorflow\/core\/kernels\/sparse_tensors_map_ops.cc\nindex 5fa690743b05c1..b486a2b4dc3c92 100644\n--- a\/tensorflow\/core\/kernels\/sparse_tensors_map_ops.cc\n+++ b\/tensorflow\/core\/kernels\/sparse_tensors_map_ops.cc\n@@ -263,22 +263,10 @@ class AddManySparseToTensorsMapOp : public SparseTensorAccessingOp {\n             \"Rank of input SparseTensor should be > 1, but saw rank: \", rank));\n \n     auto input_shape_vec = input_shape->vec<int64_t>();\n-    int new_num_elements = 1;\n-    bool overflow_ocurred = false;\n-    for (int i = 0; i < input_shape_vec.size(); i++) {\n-      new_num_elements =\n-          MultiplyWithoutOverflow(new_num_elements, input_shape_vec(i));\n-      if (new_num_elements < 0) {\n-        overflow_ocurred = true;\n-        break;\n-      }\n-    }\n-\n-    OP_REQUIRES(\n-        context, !overflow_ocurred,\n-        errors::Internal(\"Encountered overflow from large input shape.\"));\n \n-    TensorShape tensor_input_shape(input_shape_vec);\n+    TensorShape tensor_input_shape;\n+    OP_REQUIRES_OK(context, TensorShape::BuildTensorShape(input_shape_vec,\n+                                                          &tensor_input_shape));\n     gtl::InlinedVector<int64_t, 8> std_order(rank);\n     std::iota(std_order.begin(), std_order.end(), 0);\n     SparseTensor input_st;"
        },
        {
            "index":133,
            "vuln_id":"GHSA-mr6r-82x4-f4jj",
            "cwe_id":"{'CWE-203'}",
            "score":7.4,
            "chain":"{'https:\/\/github.com\/simplito\/elliptic-php\/commit\/15652609aa55968d56685c2a9120535ccdc00fd9'}",
            "dataset":"osv",
            "summary":"Timing attacks might allow practical recovery of the long-term private key In elliptic-php versions priot to 1.0.6, Timing attacks might be possible which can result in practical recovery of the long-term private key generated by the library under certain conditions. Leakage of a bit-length of the scalar during scalar multiplication is possible on an elliptic curve which might allow practical recovery of the long-term private key.",
            "published_date":"2019-11-20",
            "chain_len":1,
            "project":"https:\/\/github.com\/simplito\/elliptic-php",
            "commit_href":"https:\/\/github.com\/simplito\/elliptic-php\/commit\/15652609aa55968d56685c2a9120535ccdc00fd9",
            "commit_sha":"15652609aa55968d56685c2a9120535ccdc00fd9",
            "patch":"SINGLE",
            "chain_ord":"['15652609aa55968d56685c2a9120535ccdc00fd9']",
            "before_first_fix_commit":"{'03a8dbc6514a1c8e6b00b967bca388d36ab73169'}",
            "last_fix_commit":"15652609aa55968d56685c2a9120535ccdc00fd9",
            "chain_ord_pos":1.0,
            "commit_datetime":"11\/14\/2019, 13:43:07",
            "message":"ecdsa: Apply nonce bit-length mitigation to stop timing leakage.\n\nPorted from elliptic-js: https:\/\/github.com\/indutny\/elliptic\/pull\/203",
            "author":"Sebastian Smyczy\u0144ski",
            "comments":null,
            "stats":"{'additions': 11, 'deletions': 1, 'total': 12}",
            "files":"{'lib\/EC.php': {'additions': 11, 'deletions': 1, 'changes': 12, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/simplito\/elliptic-php\/raw\/15652609aa55968d56685c2a9120535ccdc00fd9\/lib%2FEC.php', 'patch': \"@@ -136,7 +136,17 @@ public function sign($msg, $key, $enc = null, $options = null)\\n             if( $k->cmpn(1) <= 0 || $k->cmp($ns1) >= 0 )\\n                 continue;\\n \\n-            $kp = $this->g->mul($k);\\n+            \/\/ Fix the bit-length of the random nonce,\\n+            \/\/ so that it doesn't leak via timing.\\n+            \/\/ This does not change that ks = k mod k\\n+            $ks = $k->add($this->n);\\n+            $kt = $ks->add($this->n);\\n+            if ($ks->bitLength() === $this->n->bitLength()) {\\n+                $kp = $this->g->mul($kt);\\n+            } else {\\n+                $kp = $this->g->mul($ks);\\n+            }\\n+\\n             if( $kp->isInfinity() )\\n                 continue;\"}}",
            "message_norm":"ecdsa: apply nonce bit-length mitigation to stop timing leakage.\n\nported from elliptic-js: https:\/\/github.com\/indutny\/elliptic\/pull\/203",
            "language":"en",
            "entities":"[('ecdsa', 'SECWORD', ''), ('nonce', 'SECWORD', ''), ('timing leakage', 'SECWORD', ''), ('https:\/\/github.com\/indutny\/elliptic\/pull\/203', 'URL', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['lib\/EC.php'])",
            "num_files":1.0,
            "patch_content":"From 15652609aa55968d56685c2a9120535ccdc00fd9 Mon Sep 17 00:00:00 2001\nFrom: =?UTF-8?q?Sebastian=20Smyczy=C5=84ski?= <s.smyczynski@simplito.com>\nDate: Thu, 14 Nov 2019 14:43:07 +0100\nSubject: [PATCH] ecdsa: Apply nonce bit-length mitigation to stop timing\n leakage.\n\nPorted from elliptic-js: https:\/\/github.com\/indutny\/elliptic\/pull\/203\n---\n lib\/EC.php | 12 +++++++++++-\n 1 file changed, 11 insertions(+), 1 deletion(-)\n\ndiff --git a\/lib\/EC.php b\/lib\/EC.php\nindex f8a6af7..286b72e 100644\n--- a\/lib\/EC.php\n+++ b\/lib\/EC.php\n@@ -136,7 +136,17 @@ public function sign($msg, $key, $enc = null, $options = null)\n             if( $k->cmpn(1) <= 0 || $k->cmp($ns1) >= 0 )\n                 continue;\n \n-            $kp = $this->g->mul($k);\n+            \/\/ Fix the bit-length of the random nonce,\n+            \/\/ so that it doesn't leak via timing.\n+            \/\/ This does not change that ks = k mod k\n+            $ks = $k->add($this->n);\n+            $kt = $ks->add($this->n);\n+            if ($ks->bitLength() === $this->n->bitLength()) {\n+                $kp = $this->g->mul($kt);\n+            } else {\n+                $kp = $this->g->mul($ks);\n+            }\n+\n             if( $kp->isInfinity() )\n                 continue;"
        },
        {
            "index":579,
            "vuln_id":"GHSA-2wc6-2rcj-8v76",
            "cwe_id":"{'CWE-1240'}",
            "score":6.5,
            "chain":"{'https:\/\/github.com\/sodiumoxide\/sodiumoxide\/commit\/24c7a5550807ac8a09648b5878f19d14c3a69135'}",
            "dataset":"osv",
            "summary":"scalarmult() vulnerable to degenerate public keys The `scalarmult()` function included in previous versions of this crate\naccepted all-zero public keys, for which the resulting Diffie-Hellman shared\nsecret will always be zero regardless of the private key used.\n\nThis issue was fixed by checking for this class of keys and rejecting them\nif they are used.",
            "published_date":"2021-08-25",
            "chain_len":1,
            "project":"https:\/\/github.com\/sodiumoxide\/sodiumoxide",
            "commit_href":"https:\/\/github.com\/sodiumoxide\/sodiumoxide\/commit\/24c7a5550807ac8a09648b5878f19d14c3a69135",
            "commit_sha":"24c7a5550807ac8a09648b5878f19d14c3a69135",
            "patch":"SINGLE",
            "chain_ord":"['24c7a5550807ac8a09648b5878f19d14c3a69135']",
            "before_first_fix_commit":"{'12d49e8ed1b53821465f24312695376eb86c89d2'}",
            "last_fix_commit":"24c7a5550807ac8a09648b5878f19d14c3a69135",
            "chain_ord_pos":1.0,
            "commit_datetime":"01\/26\/2017, 19:24:31",
            "message":"Check the return value of `scalarmult()`. Closes #154",
            "author":"Daniel Ashhami",
            "comments":null,
            "stats":"{'additions': 55, 'deletions': 46, 'total': 101}",
            "files":"{'src\/crypto\/scalarmult\/curve25519.rs': {'additions': 55, 'deletions': 46, 'changes': 101, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/sodiumoxide\/sodiumoxide\/raw\/24c7a5550807ac8a09648b5878f19d14c3a69135\/src%2Fcrypto%2Fscalarmult%2Fcurve25519.rs', 'patch': '@@ -23,14 +23,20 @@ new_type! {\\n \\n \/\/\/ `scalarmult()` multiplies a group element `p`\\n \/\/\/ by an integer `n`. It returns the resulting group element\\n-\/\/\/ `q`.\\n+\/\/\/ `Ok(q)`.\\n+\/\/\/ If the the `GroupElement` is all zero, `scalarmult()` returns `Err(())` since\\n+\/\/\/ the resulting `GroupElement` would be all zero, no matter the `Scalar`.\\n pub fn scalarmult(&Scalar(ref n): &Scalar,\\n-                  &GroupElement(ref p): &GroupElement) -> GroupElement {\\n+                  &GroupElement(ref p): &GroupElement)\\n+                  -> Result<GroupElement, ()> {\\n     let mut q = [0; GROUPELEMENTBYTES];\\n     unsafe {\\n-        ffi::crypto_scalarmult_curve25519(&mut q, n, p);\\n+        if ffi::crypto_scalarmult_curve25519(&mut q, n, p) != 0 {\\n+            Err(())\\n+        } else {\\n+            Ok(GroupElement(q))\\n+        }\\n     }\\n-    GroupElement(q)\\n }\\n \\n \/\/\/ `scalarmult_base()` computes the scalar product of a standard\\n@@ -47,74 +53,77 @@ pub fn scalarmult_base(&Scalar(ref n): &Scalar) -> GroupElement {\\n #[cfg(test)]\\n mod test {\\n     use super::*;\\n+    use randombytes::randombytes_into;\\n \\n     #[test]\\n     fn test_vector_1() {\\n         \/\/ corresponding to tests\/scalarmult.c and tests\/scalarmult3.cpp from NaCl\\n-        let alicesk = Scalar([0x77,0x07,0x6d,0x0a,0x73,0x18,0xa5,0x7d\\n-                             ,0x3c,0x16,0xc1,0x72,0x51,0xb2,0x66,0x45\\n-                             ,0xdf,0x4c,0x2f,0x87,0xeb,0xc0,0x99,0x2a\\n-                             ,0xb1,0x77,0xfb,0xa5,0x1d,0xb9,0x2c,0x2a]);\\n-        let alicepk_expected = [0x85,0x20,0xf0,0x09,0x89,0x30,0xa7,0x54\\n-                               ,0x74,0x8b,0x7d,0xdc,0xb4,0x3e,0xf7,0x5a\\n-                               ,0x0d,0xbf,0x3a,0x0d,0x26,0x38,0x1a,0xf4\\n-                               ,0xeb,0xa4,0xa9,0x8e,0xaa,0x9b,0x4e,0x6a];\\n+        let alicesk = Scalar([0x77, 0x07, 0x6d, 0x0a, 0x73, 0x18, 0xa5, 0x7d, 0x3c, 0x16, 0xc1,\\n+                              0x72, 0x51, 0xb2, 0x66, 0x45, 0xdf, 0x4c, 0x2f, 0x87, 0xeb, 0xc0,\\n+                              0x99, 0x2a, 0xb1, 0x77, 0xfb, 0xa5, 0x1d, 0xb9, 0x2c, 0x2a]);\\n+        let alicepk_expected = [0x85, 0x20, 0xf0, 0x09, 0x89, 0x30, 0xa7, 0x54, 0x74, 0x8b, 0x7d,\\n+                                0xdc, 0xb4, 0x3e, 0xf7, 0x5a, 0x0d, 0xbf, 0x3a, 0x0d, 0x26, 0x38,\\n+                                0x1a, 0xf4, 0xeb, 0xa4, 0xa9, 0x8e, 0xaa, 0x9b, 0x4e, 0x6a];\\n         let GroupElement(alicepk) = scalarmult_base(&alicesk);\\n         assert!(alicepk == alicepk_expected);\\n     }\\n \\n     #[test]\\n     fn test_vector_2() {\\n         \/\/ corresponding to tests\/scalarmult2.c and tests\/scalarmult4.cpp from NaCl\\n-        let bobsk = Scalar([0x5d,0xab,0x08,0x7e,0x62,0x4a,0x8a,0x4b\\n-                           ,0x79,0xe1,0x7f,0x8b,0x83,0x80,0x0e,0xe6\\n-                           ,0x6f,0x3b,0xb1,0x29,0x26,0x18,0xb6,0xfd\\n-                           ,0x1c,0x2f,0x8b,0x27,0xff,0x88,0xe0,0xeb]);\\n-        let bobpk_expected = [0xde,0x9e,0xdb,0x7d,0x7b,0x7d,0xc1,0xb4\\n-                             ,0xd3,0x5b,0x61,0xc2,0xec,0xe4,0x35,0x37\\n-                             ,0x3f,0x83,0x43,0xc8,0x5b,0x78,0x67,0x4d\\n-                             ,0xad,0xfc,0x7e,0x14,0x6f,0x88,0x2b,0x4f];\\n+        let bobsk = Scalar([0x5d, 0xab, 0x08, 0x7e, 0x62, 0x4a, 0x8a, 0x4b, 0x79, 0xe1, 0x7f,\\n+                            0x8b, 0x83, 0x80, 0x0e, 0xe6, 0x6f, 0x3b, 0xb1, 0x29, 0x26, 0x18,\\n+                            0xb6, 0xfd, 0x1c, 0x2f, 0x8b, 0x27, 0xff, 0x88, 0xe0, 0xeb]);\\n+        let bobpk_expected = [0xde, 0x9e, 0xdb, 0x7d, 0x7b, 0x7d, 0xc1, 0xb4, 0xd3, 0x5b, 0x61,\\n+                              0xc2, 0xec, 0xe4, 0x35, 0x37, 0x3f, 0x83, 0x43, 0xc8, 0x5b, 0x78,\\n+                              0x67, 0x4d, 0xad, 0xfc, 0x7e, 0x14, 0x6f, 0x88, 0x2b, 0x4f];\\n         let GroupElement(bobpk) = scalarmult_base(&bobsk);\\n         assert!(bobpk == bobpk_expected);\\n     }\\n \\n     #[test]\\n     fn test_vector_3() {\\n         \/\/ corresponding to tests\/scalarmult5.c and tests\/scalarmult7.cpp from NaCl\\n-        let alicesk = Scalar([0x77,0x07,0x6d,0x0a,0x73,0x18,0xa5,0x7d\\n-                             ,0x3c,0x16,0xc1,0x72,0x51,0xb2,0x66,0x45\\n-                             ,0xdf,0x4c,0x2f,0x87,0xeb,0xc0,0x99,0x2a\\n-                             ,0xb1,0x77,0xfb,0xa5,0x1d,0xb9,0x2c,0x2a]);\\n-        let bobpk = GroupElement([0xde,0x9e,0xdb,0x7d,0x7b,0x7d,0xc1,0xb4\\n-                                 ,0xd3,0x5b,0x61,0xc2,0xec,0xe4,0x35,0x37\\n-                                 ,0x3f,0x83,0x43,0xc8,0x5b,0x78,0x67,0x4d\\n-                                 ,0xad,0xfc,0x7e,0x14,0x6f,0x88,0x2b,0x4f]);\\n-        let k_expected = [0x4a,0x5d,0x9d,0x5b,0xa4,0xce,0x2d,0xe1\\n-                         ,0x72,0x8e,0x3b,0xf4,0x80,0x35,0x0f,0x25\\n-                         ,0xe0,0x7e,0x21,0xc9,0x47,0xd1,0x9e,0x33\\n-                         ,0x76,0xf0,0x9b,0x3c,0x1e,0x16,0x17,0x42];\\n-        let GroupElement(k) = scalarmult(&alicesk, &bobpk);\\n+        let alicesk = Scalar([0x77, 0x07, 0x6d, 0x0a, 0x73, 0x18, 0xa5, 0x7d, 0x3c, 0x16, 0xc1,\\n+                              0x72, 0x51, 0xb2, 0x66, 0x45, 0xdf, 0x4c, 0x2f, 0x87, 0xeb, 0xc0,\\n+                              0x99, 0x2a, 0xb1, 0x77, 0xfb, 0xa5, 0x1d, 0xb9, 0x2c, 0x2a]);\\n+        let bobpk = GroupElement([0xde, 0x9e, 0xdb, 0x7d, 0x7b, 0x7d, 0xc1, 0xb4, 0xd3, 0x5b,\\n+                                  0x61, 0xc2, 0xec, 0xe4, 0x35, 0x37, 0x3f, 0x83, 0x43, 0xc8,\\n+                                  0x5b, 0x78, 0x67, 0x4d, 0xad, 0xfc, 0x7e, 0x14, 0x6f, 0x88,\\n+                                  0x2b, 0x4f]);\\n+        let k_expected = [0x4a, 0x5d, 0x9d, 0x5b, 0xa4, 0xce, 0x2d, 0xe1, 0x72, 0x8e, 0x3b, 0xf4,\\n+                          0x80, 0x35, 0x0f, 0x25, 0xe0, 0x7e, 0x21, 0xc9, 0x47, 0xd1, 0x9e, 0x33,\\n+                          0x76, 0xf0, 0x9b, 0x3c, 0x1e, 0x16, 0x17, 0x42];\\n+        let GroupElement(k) = scalarmult(&alicesk, &bobpk).unwrap();\\n         assert!(k == k_expected);\\n     }\\n \\n     #[test]\\n     fn test_vector_4() {\\n         \/\/ corresponding to tests\/scalarmult6.c from NaCl\\n-        let bobsk = Scalar([0x5d,0xab,0x08,0x7e,0x62,0x4a,0x8a,0x4b\\n-                           ,0x79,0xe1,0x7f,0x8b,0x83,0x80,0x0e,0xe6\\n-                           ,0x6f,0x3b,0xb1,0x29,0x26,0x18,0xb6,0xfd\\n-                           ,0x1c,0x2f,0x8b,0x27,0xff,0x88,0xe0,0xeb]);\\n-        let alicepk = GroupElement([0x85,0x20,0xf0,0x09,0x89,0x30,0xa7,0x54\\n-                                   ,0x74,0x8b,0x7d,0xdc,0xb4,0x3e,0xf7,0x5a\\n-                                   ,0x0d,0xbf,0x3a,0x0d,0x26,0x38,0x1a,0xf4\\n-                                   ,0xeb,0xa4,0xa9,0x8e,0xaa,0x9b,0x4e,0x6a]);\\n-        let k_expected = [0x4a,0x5d,0x9d,0x5b,0xa4,0xce,0x2d,0xe1\\n-                         ,0x72,0x8e,0x3b,0xf4,0x80,0x35,0x0f,0x25\\n-                         ,0xe0,0x7e,0x21,0xc9,0x47,0xd1,0x9e,0x33\\n-                         ,0x76,0xf0,0x9b,0x3c,0x1e,0x16,0x17,0x42];\\n-        let GroupElement(k) = scalarmult(&bobsk, &alicepk);\\n+        let bobsk = Scalar([0x5d, 0xab, 0x08, 0x7e, 0x62, 0x4a, 0x8a, 0x4b, 0x79, 0xe1, 0x7f,\\n+                            0x8b, 0x83, 0x80, 0x0e, 0xe6, 0x6f, 0x3b, 0xb1, 0x29, 0x26, 0x18,\\n+                            0xb6, 0xfd, 0x1c, 0x2f, 0x8b, 0x27, 0xff, 0x88, 0xe0, 0xeb]);\\n+        let alicepk = GroupElement([0x85, 0x20, 0xf0, 0x09, 0x89, 0x30, 0xa7, 0x54, 0x74, 0x8b,\\n+                                    0x7d, 0xdc, 0xb4, 0x3e, 0xf7, 0x5a, 0x0d, 0xbf, 0x3a, 0x0d,\\n+                                    0x26, 0x38, 0x1a, 0xf4, 0xeb, 0xa4, 0xa9, 0x8e, 0xaa, 0x9b,\\n+                                    0x4e, 0x6a]);\\n+        let k_expected = [0x4a, 0x5d, 0x9d, 0x5b, 0xa4, 0xce, 0x2d, 0xe1, 0x72, 0x8e, 0x3b, 0xf4,\\n+                          0x80, 0x35, 0x0f, 0x25, 0xe0, 0x7e, 0x21, 0xc9, 0x47, 0xd1, 0x9e, 0x33,\\n+                          0x76, 0xf0, 0x9b, 0x3c, 0x1e, 0x16, 0x17, 0x42];\\n+        let GroupElement(k) = scalarmult(&bobsk, &alicepk).unwrap();\\n         assert!(k == k_expected);\\n     }\\n+\\n+    #[test]\\n+    #[should_panic]\\n+    fn test_all_zero() {\\n+        let mut sk = [0; SCALARBYTES];\\n+        randombytes_into(&mut sk);\\n+        let sk = Scalar(sk);\\n+        let pk = GroupElement([0; GROUPELEMENTBYTES]);\\n+        let _ = scalarmult(&sk, &pk).unwrap();\\n+    }\\n }\\n \\n #[cfg(feature = \"benchmarks\")]'}}",
            "message_norm":"check the return value of `scalarmult()`. closes #154",
            "language":"en",
            "entities":"[('#154', 'ISSUE', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['src\/crypto\/scalarmult\/curve25519.rs'])",
            "num_files":1.0,
            "patch_content":"From 24c7a5550807ac8a09648b5878f19d14c3a69135 Mon Sep 17 00:00:00 2001\nFrom: Daniel Ashhami <dnaq@users.noreply.github.com>\nDate: Thu, 26 Jan 2017 20:24:31 +0100\nSubject: [PATCH] Check the return value of `scalarmult()`. Closes #154\n\n---\n src\/crypto\/scalarmult\/curve25519.rs | 101 +++++++++++++++-------------\n 1 file changed, 55 insertions(+), 46 deletions(-)\n\ndiff --git a\/src\/crypto\/scalarmult\/curve25519.rs b\/src\/crypto\/scalarmult\/curve25519.rs\nindex cd26ef1da3..2bd0a902d2 100644\n--- a\/src\/crypto\/scalarmult\/curve25519.rs\n+++ b\/src\/crypto\/scalarmult\/curve25519.rs\n@@ -23,14 +23,20 @@ new_type! {\n \n \/\/\/ `scalarmult()` multiplies a group element `p`\n \/\/\/ by an integer `n`. It returns the resulting group element\n-\/\/\/ `q`.\n+\/\/\/ `Ok(q)`.\n+\/\/\/ If the the `GroupElement` is all zero, `scalarmult()` returns `Err(())` since\n+\/\/\/ the resulting `GroupElement` would be all zero, no matter the `Scalar`.\n pub fn scalarmult(&Scalar(ref n): &Scalar,\n-                  &GroupElement(ref p): &GroupElement) -> GroupElement {\n+                  &GroupElement(ref p): &GroupElement)\n+                  -> Result<GroupElement, ()> {\n     let mut q = [0; GROUPELEMENTBYTES];\n     unsafe {\n-        ffi::crypto_scalarmult_curve25519(&mut q, n, p);\n+        if ffi::crypto_scalarmult_curve25519(&mut q, n, p) != 0 {\n+            Err(())\n+        } else {\n+            Ok(GroupElement(q))\n+        }\n     }\n-    GroupElement(q)\n }\n \n \/\/\/ `scalarmult_base()` computes the scalar product of a standard\n@@ -47,18 +53,17 @@ pub fn scalarmult_base(&Scalar(ref n): &Scalar) -> GroupElement {\n #[cfg(test)]\n mod test {\n     use super::*;\n+    use randombytes::randombytes_into;\n \n     #[test]\n     fn test_vector_1() {\n         \/\/ corresponding to tests\/scalarmult.c and tests\/scalarmult3.cpp from NaCl\n-        let alicesk = Scalar([0x77,0x07,0x6d,0x0a,0x73,0x18,0xa5,0x7d\n-                             ,0x3c,0x16,0xc1,0x72,0x51,0xb2,0x66,0x45\n-                             ,0xdf,0x4c,0x2f,0x87,0xeb,0xc0,0x99,0x2a\n-                             ,0xb1,0x77,0xfb,0xa5,0x1d,0xb9,0x2c,0x2a]);\n-        let alicepk_expected = [0x85,0x20,0xf0,0x09,0x89,0x30,0xa7,0x54\n-                               ,0x74,0x8b,0x7d,0xdc,0xb4,0x3e,0xf7,0x5a\n-                               ,0x0d,0xbf,0x3a,0x0d,0x26,0x38,0x1a,0xf4\n-                               ,0xeb,0xa4,0xa9,0x8e,0xaa,0x9b,0x4e,0x6a];\n+        let alicesk = Scalar([0x77, 0x07, 0x6d, 0x0a, 0x73, 0x18, 0xa5, 0x7d, 0x3c, 0x16, 0xc1,\n+                              0x72, 0x51, 0xb2, 0x66, 0x45, 0xdf, 0x4c, 0x2f, 0x87, 0xeb, 0xc0,\n+                              0x99, 0x2a, 0xb1, 0x77, 0xfb, 0xa5, 0x1d, 0xb9, 0x2c, 0x2a]);\n+        let alicepk_expected = [0x85, 0x20, 0xf0, 0x09, 0x89, 0x30, 0xa7, 0x54, 0x74, 0x8b, 0x7d,\n+                                0xdc, 0xb4, 0x3e, 0xf7, 0x5a, 0x0d, 0xbf, 0x3a, 0x0d, 0x26, 0x38,\n+                                0x1a, 0xf4, 0xeb, 0xa4, 0xa9, 0x8e, 0xaa, 0x9b, 0x4e, 0x6a];\n         let GroupElement(alicepk) = scalarmult_base(&alicesk);\n         assert!(alicepk == alicepk_expected);\n     }\n@@ -66,14 +71,12 @@ mod test {\n     #[test]\n     fn test_vector_2() {\n         \/\/ corresponding to tests\/scalarmult2.c and tests\/scalarmult4.cpp from NaCl\n-        let bobsk = Scalar([0x5d,0xab,0x08,0x7e,0x62,0x4a,0x8a,0x4b\n-                           ,0x79,0xe1,0x7f,0x8b,0x83,0x80,0x0e,0xe6\n-                           ,0x6f,0x3b,0xb1,0x29,0x26,0x18,0xb6,0xfd\n-                           ,0x1c,0x2f,0x8b,0x27,0xff,0x88,0xe0,0xeb]);\n-        let bobpk_expected = [0xde,0x9e,0xdb,0x7d,0x7b,0x7d,0xc1,0xb4\n-                             ,0xd3,0x5b,0x61,0xc2,0xec,0xe4,0x35,0x37\n-                             ,0x3f,0x83,0x43,0xc8,0x5b,0x78,0x67,0x4d\n-                             ,0xad,0xfc,0x7e,0x14,0x6f,0x88,0x2b,0x4f];\n+        let bobsk = Scalar([0x5d, 0xab, 0x08, 0x7e, 0x62, 0x4a, 0x8a, 0x4b, 0x79, 0xe1, 0x7f,\n+                            0x8b, 0x83, 0x80, 0x0e, 0xe6, 0x6f, 0x3b, 0xb1, 0x29, 0x26, 0x18,\n+                            0xb6, 0xfd, 0x1c, 0x2f, 0x8b, 0x27, 0xff, 0x88, 0xe0, 0xeb]);\n+        let bobpk_expected = [0xde, 0x9e, 0xdb, 0x7d, 0x7b, 0x7d, 0xc1, 0xb4, 0xd3, 0x5b, 0x61,\n+                              0xc2, 0xec, 0xe4, 0x35, 0x37, 0x3f, 0x83, 0x43, 0xc8, 0x5b, 0x78,\n+                              0x67, 0x4d, 0xad, 0xfc, 0x7e, 0x14, 0x6f, 0x88, 0x2b, 0x4f];\n         let GroupElement(bobpk) = scalarmult_base(&bobsk);\n         assert!(bobpk == bobpk_expected);\n     }\n@@ -81,40 +84,46 @@ mod test {\n     #[test]\n     fn test_vector_3() {\n         \/\/ corresponding to tests\/scalarmult5.c and tests\/scalarmult7.cpp from NaCl\n-        let alicesk = Scalar([0x77,0x07,0x6d,0x0a,0x73,0x18,0xa5,0x7d\n-                             ,0x3c,0x16,0xc1,0x72,0x51,0xb2,0x66,0x45\n-                             ,0xdf,0x4c,0x2f,0x87,0xeb,0xc0,0x99,0x2a\n-                             ,0xb1,0x77,0xfb,0xa5,0x1d,0xb9,0x2c,0x2a]);\n-        let bobpk = GroupElement([0xde,0x9e,0xdb,0x7d,0x7b,0x7d,0xc1,0xb4\n-                                 ,0xd3,0x5b,0x61,0xc2,0xec,0xe4,0x35,0x37\n-                                 ,0x3f,0x83,0x43,0xc8,0x5b,0x78,0x67,0x4d\n-                                 ,0xad,0xfc,0x7e,0x14,0x6f,0x88,0x2b,0x4f]);\n-        let k_expected = [0x4a,0x5d,0x9d,0x5b,0xa4,0xce,0x2d,0xe1\n-                         ,0x72,0x8e,0x3b,0xf4,0x80,0x35,0x0f,0x25\n-                         ,0xe0,0x7e,0x21,0xc9,0x47,0xd1,0x9e,0x33\n-                         ,0x76,0xf0,0x9b,0x3c,0x1e,0x16,0x17,0x42];\n-        let GroupElement(k) = scalarmult(&alicesk, &bobpk);\n+        let alicesk = Scalar([0x77, 0x07, 0x6d, 0x0a, 0x73, 0x18, 0xa5, 0x7d, 0x3c, 0x16, 0xc1,\n+                              0x72, 0x51, 0xb2, 0x66, 0x45, 0xdf, 0x4c, 0x2f, 0x87, 0xeb, 0xc0,\n+                              0x99, 0x2a, 0xb1, 0x77, 0xfb, 0xa5, 0x1d, 0xb9, 0x2c, 0x2a]);\n+        let bobpk = GroupElement([0xde, 0x9e, 0xdb, 0x7d, 0x7b, 0x7d, 0xc1, 0xb4, 0xd3, 0x5b,\n+                                  0x61, 0xc2, 0xec, 0xe4, 0x35, 0x37, 0x3f, 0x83, 0x43, 0xc8,\n+                                  0x5b, 0x78, 0x67, 0x4d, 0xad, 0xfc, 0x7e, 0x14, 0x6f, 0x88,\n+                                  0x2b, 0x4f]);\n+        let k_expected = [0x4a, 0x5d, 0x9d, 0x5b, 0xa4, 0xce, 0x2d, 0xe1, 0x72, 0x8e, 0x3b, 0xf4,\n+                          0x80, 0x35, 0x0f, 0x25, 0xe0, 0x7e, 0x21, 0xc9, 0x47, 0xd1, 0x9e, 0x33,\n+                          0x76, 0xf0, 0x9b, 0x3c, 0x1e, 0x16, 0x17, 0x42];\n+        let GroupElement(k) = scalarmult(&alicesk, &bobpk).unwrap();\n         assert!(k == k_expected);\n     }\n \n     #[test]\n     fn test_vector_4() {\n         \/\/ corresponding to tests\/scalarmult6.c from NaCl\n-        let bobsk = Scalar([0x5d,0xab,0x08,0x7e,0x62,0x4a,0x8a,0x4b\n-                           ,0x79,0xe1,0x7f,0x8b,0x83,0x80,0x0e,0xe6\n-                           ,0x6f,0x3b,0xb1,0x29,0x26,0x18,0xb6,0xfd\n-                           ,0x1c,0x2f,0x8b,0x27,0xff,0x88,0xe0,0xeb]);\n-        let alicepk = GroupElement([0x85,0x20,0xf0,0x09,0x89,0x30,0xa7,0x54\n-                                   ,0x74,0x8b,0x7d,0xdc,0xb4,0x3e,0xf7,0x5a\n-                                   ,0x0d,0xbf,0x3a,0x0d,0x26,0x38,0x1a,0xf4\n-                                   ,0xeb,0xa4,0xa9,0x8e,0xaa,0x9b,0x4e,0x6a]);\n-        let k_expected = [0x4a,0x5d,0x9d,0x5b,0xa4,0xce,0x2d,0xe1\n-                         ,0x72,0x8e,0x3b,0xf4,0x80,0x35,0x0f,0x25\n-                         ,0xe0,0x7e,0x21,0xc9,0x47,0xd1,0x9e,0x33\n-                         ,0x76,0xf0,0x9b,0x3c,0x1e,0x16,0x17,0x42];\n-        let GroupElement(k) = scalarmult(&bobsk, &alicepk);\n+        let bobsk = Scalar([0x5d, 0xab, 0x08, 0x7e, 0x62, 0x4a, 0x8a, 0x4b, 0x79, 0xe1, 0x7f,\n+                            0x8b, 0x83, 0x80, 0x0e, 0xe6, 0x6f, 0x3b, 0xb1, 0x29, 0x26, 0x18,\n+                            0xb6, 0xfd, 0x1c, 0x2f, 0x8b, 0x27, 0xff, 0x88, 0xe0, 0xeb]);\n+        let alicepk = GroupElement([0x85, 0x20, 0xf0, 0x09, 0x89, 0x30, 0xa7, 0x54, 0x74, 0x8b,\n+                                    0x7d, 0xdc, 0xb4, 0x3e, 0xf7, 0x5a, 0x0d, 0xbf, 0x3a, 0x0d,\n+                                    0x26, 0x38, 0x1a, 0xf4, 0xeb, 0xa4, 0xa9, 0x8e, 0xaa, 0x9b,\n+                                    0x4e, 0x6a]);\n+        let k_expected = [0x4a, 0x5d, 0x9d, 0x5b, 0xa4, 0xce, 0x2d, 0xe1, 0x72, 0x8e, 0x3b, 0xf4,\n+                          0x80, 0x35, 0x0f, 0x25, 0xe0, 0x7e, 0x21, 0xc9, 0x47, 0xd1, 0x9e, 0x33,\n+                          0x76, 0xf0, 0x9b, 0x3c, 0x1e, 0x16, 0x17, 0x42];\n+        let GroupElement(k) = scalarmult(&bobsk, &alicepk).unwrap();\n         assert!(k == k_expected);\n     }\n+\n+    #[test]\n+    #[should_panic]\n+    fn test_all_zero() {\n+        let mut sk = [0; SCALARBYTES];\n+        randombytes_into(&mut sk);\n+        let sk = Scalar(sk);\n+        let pk = GroupElement([0; GROUPELEMENTBYTES]);\n+        let _ = scalarmult(&sk, &pk).unwrap();\n+    }\n }\n \n #[cfg(feature = \"benchmarks\")]"
        },
        {
            "index":391,
            "vuln_id":"GHSA-hhrj-wp42-32v3",
            "cwe_id":"{'CWE-209'}",
            "score":7.5,
            "chain":"{'https:\/\/github.com\/microweber\/microweber\/commit\/2417bd2eda2aa2868c1dad1abf62341f22bfc20a'}",
            "dataset":"osv",
            "summary":"Generation of Error Message Containing Sensitive Information in microweber Generation of Error Message Containing Sensitive Information in Packagist microweber\/microweber prior to 1.2.11.",
            "published_date":"2022-02-19",
            "chain_len":1,
            "project":"https:\/\/github.com\/microweber\/microweber",
            "commit_href":"https:\/\/github.com\/microweber\/microweber\/commit\/2417bd2eda2aa2868c1dad1abf62341f22bfc20a",
            "commit_sha":"2417bd2eda2aa2868c1dad1abf62341f22bfc20a",
            "patch":"SINGLE",
            "chain_ord":"['2417bd2eda2aa2868c1dad1abf62341f22bfc20a']",
            "before_first_fix_commit":"{'8adc9da7307c89d7cb7b309f69d0c2922a56aa2e'}",
            "last_fix_commit":"2417bd2eda2aa2868c1dad1abf62341f22bfc20a",
            "chain_ord_pos":1.0,
            "commit_datetime":"02\/17\/2022, 09:15:14",
            "message":"Update Comment.php",
            "author":"Bozhidar Slaveykov",
            "comments":null,
            "stats":"{'additions': 9, 'deletions': 0, 'total': 9}",
            "files":"{'src\/MicroweberPackages\/Comment\/Models\/Comment.php': {'additions': 9, 'deletions': 0, 'changes': 9, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/microweber\/microweber\/raw\/2417bd2eda2aa2868c1dad1abf62341f22bfc20a\/src%2FMicroweberPackages%2FComment%2FModels%2FComment.php', 'patch': \"@@ -5,10 +5,12 @@\\n use EloquentFilter\\\\Filterable;\\n use Illuminate\\\\Database\\\\Eloquent\\\\Model;\\n use MicroweberPackages\\\\Content\\\\Models\\\\ModelFilters\\\\ContentFilter;\\n+use MicroweberPackages\\\\Core\\\\Models\\\\HasSearchableTrait;\\n \\n class Comment extends Model\\n {\\n     use Filterable;\\n+    use HasSearchableTrait;\\n \\n     public $table = 'comments';\\n \\n@@ -21,6 +23,13 @@ class Comment extends Model\\n         'comment_body',\\n     ];\\n \\n+    protected $searchable = [\\n+        'comment_name',\\n+        'comment_email',\\n+        'comment_website',\\n+        'comment_body',\\n+    ];\\n+\\n     public function modelFilter()\\n     {\\n         return $this->provideFilter(ContentFilter::class);\"}}",
            "message_norm":"update comment.php",
            "language":"fr",
            "entities":"[('update', 'ACTION', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['src\/MicroweberPackages\/Comment\/Models\/Comment.php'])",
            "num_files":1.0,
            "patch_content":"From 2417bd2eda2aa2868c1dad1abf62341f22bfc20a Mon Sep 17 00:00:00 2001\nFrom: Bozhidar Slaveykov <bobi@microweber.com>\nDate: Thu, 17 Feb 2022 11:15:14 +0200\nSubject: [PATCH] Update Comment.php\n\n---\n src\/MicroweberPackages\/Comment\/Models\/Comment.php | 9 +++++++++\n 1 file changed, 9 insertions(+)\n\ndiff --git a\/src\/MicroweberPackages\/Comment\/Models\/Comment.php b\/src\/MicroweberPackages\/Comment\/Models\/Comment.php\nindex fa76a64345f..6e543a876fd 100644\n--- a\/src\/MicroweberPackages\/Comment\/Models\/Comment.php\n+++ b\/src\/MicroweberPackages\/Comment\/Models\/Comment.php\n@@ -5,10 +5,12 @@\n use EloquentFilter\\Filterable;\n use Illuminate\\Database\\Eloquent\\Model;\n use MicroweberPackages\\Content\\Models\\ModelFilters\\ContentFilter;\n+use MicroweberPackages\\Core\\Models\\HasSearchableTrait;\n \n class Comment extends Model\n {\n     use Filterable;\n+    use HasSearchableTrait;\n \n     public $table = 'comments';\n \n@@ -21,6 +23,13 @@ class Comment extends Model\n         'comment_body',\n     ];\n \n+    protected $searchable = [\n+        'comment_name',\n+        'comment_email',\n+        'comment_website',\n+        'comment_body',\n+    ];\n+\n     public function modelFilter()\n     {\n         return $this->provideFilter(ContentFilter::class);"
        },
        {
            "index":366,
            "vuln_id":"GHSA-pgcq-h79j-2f69",
            "cwe_id":"{'CWE-354'}",
            "score":7.0,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/4d74d8a00b07441cba090a02e0dd9ed385145bf4', 'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/da4aad5946be30e5f049920fa076e1f7ef021261', 'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/e7f497570abb6b4ae5af4970620cd880e4c0c904', 'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/4dddb2fd0b01cdd196101afbba6518658a2c9e07', 'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/579261dcd446385831fe4f7457d802a59685121d', 'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/68422b215e618df5ad375bcdc6d2052e9fd3080a'}",
            "dataset":"osv",
            "summary":"Incomplete validation of shapes in multiple TF ops ### Impact\nSeveral TensorFlow operations are missing validation for the shapes of the tensor arguments involved in the call. Depending on the API, this can result in undefined behavior and segfault or `CHECK`-fail related crashes but in some scenarios writes and reads from heap populated arrays are also possible.\n\nWe have discovered these issues internally via tooling while working on improving\/testing GPU op determinism. As such, we don't have reproducers and there will be multiple fixes for these issues.\n\n### Patches\nWe have patched the issue in GitHub commits [68422b215e618df5ad375bcdc6d2052e9fd3080a](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/68422b215e618df5ad375bcdc6d2052e9fd3080a), [4d74d8a00b07441cba090a02e0dd9ed385145bf4](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/4d74d8a00b07441cba090a02e0dd9ed385145bf4), [579261dcd446385831fe4f7457d802a59685121d](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/579261dcd446385831fe4f7457d802a59685121d), [da4aad5946be30e5f049920fa076e1f7ef021261](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/da4aad5946be30e5f049920fa076e1f7ef021261), [4dddb2fd0b01cdd196101afbba6518658a2c9e07](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/4dddb2fd0b01cdd196101afbba6518658a2c9e07), and [e7f497570abb6b4ae5af4970620cd880e4c0c904](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/e7f497570abb6b4ae5af4970620cd880e4c0c904).\n\nThese fixes will be included in TensorFlow 2.7.0. We will also cherrypick these commits on TensorFlow 2.6.1, TensorFlow 2.5.2, and TensorFlow 2.4.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.",
            "published_date":"2021-11-10",
            "chain_len":6,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/4dddb2fd0b01cdd196101afbba6518658a2c9e07",
            "commit_sha":"4dddb2fd0b01cdd196101afbba6518658a2c9e07",
            "patch":"MULTI",
            "chain_ord":"['579261dcd446385831fe4f7457d802a59685121d', '4d74d8a00b07441cba090a02e0dd9ed385145bf4', '68422b215e618df5ad375bcdc6d2052e9fd3080a', 'da4aad5946be30e5f049920fa076e1f7ef021261', '4dddb2fd0b01cdd196101afbba6518658a2c9e07', 'e7f497570abb6b4ae5af4970620cd880e4c0c904']",
            "before_first_fix_commit":"{'e0214528739cad3bd02fbf2696a793dc342ffb94'}",
            "last_fix_commit":"e7f497570abb6b4ae5af4970620cd880e4c0c904",
            "chain_ord_pos":5.0,
            "commit_datetime":"10\/20\/2021, 21:53:58",
            "message":"Fix segfault in pools on empty shapes when certain dimension were very large.\n\nPooling ops multiply certain components of the input shape, e.g. by multiplying input.shape[1] * input.shape[2] * input.shape[3]. This multiplication could overflow an int64 value if shape[0] was 0 but shape[1], shape[2], and shape[3] were very large, e.g. by passing an input with shape (0, 2**25, 2**25, 2**25).\n\nPiperOrigin-RevId: 404644978\nChange-Id: Ic79f89c970357ca2962b1f231449066db9403146",
            "author":"Reed Wanderman-Milne",
            "comments":null,
            "stats":"{'additions': 9, 'deletions': 0, 'total': 9}",
            "files":"{'tensorflow\/core\/kernels\/pooling_ops_common.h': {'additions': 9, 'deletions': 0, 'changes': 9, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/4dddb2fd0b01cdd196101afbba6518658a2c9e07\/tensorflow%2Fcore%2Fkernels%2Fpooling_ops_common.h', 'patch': \"@@ -189,6 +189,9 @@ class MaxPoolingOp : public OpKernel {\\n   void SpatialMaxPool(OpKernelContext* context, Tensor* output,\\n                       const Tensor& tensor_in, const PoolParameters& params,\\n                       const Padding& padding) {\\n+    if (output->NumElements() == 0) {\\n+      return;\\n+    }\\n     \/\/ On GPU, use Eigen's Spatial Max Pooling.  On CPU, use an\\n     \/\/ EigenMatrix version that is currently faster than Eigen's\\n     \/\/ Spatial MaxPooling implementation.\\n@@ -443,6 +446,9 @@ class MaxPoolingV2Op : public OpKernel {\\n   void SpatialMaxPool(OpKernelContext* context, Tensor* output,\\n                       const Tensor& tensor_in, const PoolParameters& params,\\n                       const Padding& padding) {\\n+    if (output->NumElements() == 0) {\\n+      return;\\n+    }\\n     \/\/ On GPU, use Eigen's Spatial Max Pooling.  On CPU, use an\\n     \/\/ EigenMatrix version that is currently faster than Eigen's\\n     \/\/ Spatial MaxPooling implementation.\\n@@ -561,6 +567,9 @@ template <typename Device, typename T>\\n void SpatialAvgPool(OpKernelContext* context, Tensor* output,\\n                     const Tensor& input, const PoolParameters& params,\\n                     const Padding& padding) {\\n+  if (output->NumElements() == 0) {\\n+    return;\\n+  }\\n   typedef Eigen::Map<const Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\\n       ConstEigenMatrixMap;\\n   typedef Eigen::Map<Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\"}}",
            "message_norm":"fix segfault in pools on empty shapes when certain dimension were very large.\n\npooling ops multiply certain components of the input shape, e.g. by multiplying input.shape[1] * input.shape[2] * input.shape[3]. this multiplication could overflow an int64 value if shape[0] was 0 but shape[1], shape[2], and shape[3] were very large, e.g. by passing an input with shape (0, 2**25, 2**25, 2**25).\n\npiperorigin-revid: 404644978\nchange-id: ic79f89c970357ca2962b1f231449066db9403146",
            "language":"en",
            "entities":"[('fix', 'ACTION', ''), ('segfault', 'SECWORD', ''), ('overflow', 'SECWORD', ''), ('404644978', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/core\/kernels\/pooling_ops_common.h'])",
            "num_files":1.0,
            "patch_content":"From 4dddb2fd0b01cdd196101afbba6518658a2c9e07 Mon Sep 17 00:00:00 2001\nFrom: Reed Wanderman-Milne <reedwm@google.com>\nDate: Wed, 20 Oct 2021 14:53:58 -0700\nSubject: [PATCH] Fix segfault in pools on empty shapes when certain dimension\n were very large.\n\nPooling ops multiply certain components of the input shape, e.g. by multiplying input.shape[1] * input.shape[2] * input.shape[3]. This multiplication could overflow an int64 value if shape[0] was 0 but shape[1], shape[2], and shape[3] were very large, e.g. by passing an input with shape (0, 2**25, 2**25, 2**25).\n\nPiperOrigin-RevId: 404644978\nChange-Id: Ic79f89c970357ca2962b1f231449066db9403146\n---\n tensorflow\/core\/kernels\/pooling_ops_common.h | 9 +++++++++\n 1 file changed, 9 insertions(+)\n\ndiff --git a\/tensorflow\/core\/kernels\/pooling_ops_common.h b\/tensorflow\/core\/kernels\/pooling_ops_common.h\nindex 1a41a5adca5d29..8890e24a32ad97 100644\n--- a\/tensorflow\/core\/kernels\/pooling_ops_common.h\n+++ b\/tensorflow\/core\/kernels\/pooling_ops_common.h\n@@ -189,6 +189,9 @@ class MaxPoolingOp : public OpKernel {\n   void SpatialMaxPool(OpKernelContext* context, Tensor* output,\n                       const Tensor& tensor_in, const PoolParameters& params,\n                       const Padding& padding) {\n+    if (output->NumElements() == 0) {\n+      return;\n+    }\n     \/\/ On GPU, use Eigen's Spatial Max Pooling.  On CPU, use an\n     \/\/ EigenMatrix version that is currently faster than Eigen's\n     \/\/ Spatial MaxPooling implementation.\n@@ -443,6 +446,9 @@ class MaxPoolingV2Op : public OpKernel {\n   void SpatialMaxPool(OpKernelContext* context, Tensor* output,\n                       const Tensor& tensor_in, const PoolParameters& params,\n                       const Padding& padding) {\n+    if (output->NumElements() == 0) {\n+      return;\n+    }\n     \/\/ On GPU, use Eigen's Spatial Max Pooling.  On CPU, use an\n     \/\/ EigenMatrix version that is currently faster than Eigen's\n     \/\/ Spatial MaxPooling implementation.\n@@ -561,6 +567,9 @@ template <typename Device, typename T>\n void SpatialAvgPool(OpKernelContext* context, Tensor* output,\n                     const Tensor& input, const PoolParameters& params,\n                     const Padding& padding) {\n+  if (output->NumElements() == 0) {\n+    return;\n+  }\n   typedef Eigen::Map<const Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n       ConstEigenMatrixMap;\n   typedef Eigen::Map<Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>"
        },
        {
            "index":667,
            "vuln_id":"GHSA-c697-r227-pq6h",
            "cwe_id":"{'CWE-434'}",
            "score":7.8,
            "chain":"{'https:\/\/github.com\/pimcore\/pimcore\/commit\/35d1853baf64d6a1d90fd8803e52439da53a3911'}",
            "dataset":"osv",
            "summary":"Unrestricted Upload of File with Dangerous Type in pimcore Unrestricted Upload of File with Dangerous Type in Packagist pimcore\/pimcore",
            "published_date":"2022-01-21",
            "chain_len":1,
            "project":"https:\/\/github.com\/pimcore\/pimcore",
            "commit_href":"https:\/\/github.com\/pimcore\/pimcore\/commit\/35d1853baf64d6a1d90fd8803e52439da53a3911",
            "commit_sha":"35d1853baf64d6a1d90fd8803e52439da53a3911",
            "patch":"SINGLE",
            "chain_ord":"['35d1853baf64d6a1d90fd8803e52439da53a3911']",
            "before_first_fix_commit":"{'d8377fc752dc3a42ca72cb49650481191f14ec63'}",
            "last_fix_commit":"35d1853baf64d6a1d90fd8803e52439da53a3911",
            "chain_ord_pos":1.0,
            "commit_datetime":"01\/17\/2022, 15:52:05",
            "message":"[Settings] Validate SVG uploads for branding",
            "author":"Bernhard Rusch",
            "comments":null,
            "stats":"{'additions': 6, 'deletions': 0, 'total': 6}",
            "files":"{'bundles\/AdminBundle\/Controller\/Admin\/SettingsController.php': {'additions': 6, 'deletions': 0, 'changes': 6, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/pimcore\/pimcore\/raw\/35d1853baf64d6a1d90fd8803e52439da53a3911\/bundles%2FAdminBundle%2FController%2FAdmin%2FSettingsController.php', 'patch': \"@@ -109,6 +109,12 @@ public function uploadCustomLogoAction(Request $request)\\n             throw new \\\\Exception('Unsupported file format');\\n         }\\n \\n+        if($fileExt === 'svg') {\\n+            if(strpos(file_get_contents($_FILES['Filedata']['tmp_name']), '<script')) {\\n+                throw new \\\\Exception('Scripts in SVG files are not supported');\\n+            }\\n+        }\\n+\\n         $storage = Tool\\\\Storage::get('admin');\\n         $storage->writeStream(self::CUSTOM_LOGO_PATH, fopen($_FILES['Filedata']['tmp_name'], 'rb'));\"}}",
            "message_norm":"[settings] validate svg uploads for branding",
            "language":"sv",
            "entities":"[('validate', 'ACTION', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['bundles\/AdminBundle\/Controller\/Admin\/SettingsController.php'])",
            "num_files":1.0,
            "patch_content":"From 35d1853baf64d6a1d90fd8803e52439da53a3911 Mon Sep 17 00:00:00 2001\nFrom: Bernhard Rusch <bernhard.rusch@elements.at>\nDate: Mon, 17 Jan 2022 16:52:05 +0100\nSubject: [PATCH] [Settings] Validate SVG uploads for branding\n\n---\n bundles\/AdminBundle\/Controller\/Admin\/SettingsController.php | 6 ++++++\n 1 file changed, 6 insertions(+)\n\ndiff --git a\/bundles\/AdminBundle\/Controller\/Admin\/SettingsController.php b\/bundles\/AdminBundle\/Controller\/Admin\/SettingsController.php\nindex b706d5eb1bd..eeaec96a597 100644\n--- a\/bundles\/AdminBundle\/Controller\/Admin\/SettingsController.php\n+++ b\/bundles\/AdminBundle\/Controller\/Admin\/SettingsController.php\n@@ -109,6 +109,12 @@ public function uploadCustomLogoAction(Request $request)\n             throw new \\Exception('Unsupported file format');\n         }\n \n+        if($fileExt === 'svg') {\n+            if(strpos(file_get_contents($_FILES['Filedata']['tmp_name']), '<script')) {\n+                throw new \\Exception('Scripts in SVG files are not supported');\n+            }\n+        }\n+\n         $storage = Tool\\Storage::get('admin');\n         $storage->writeStream(self::CUSTOM_LOGO_PATH, fopen($_FILES['Filedata']['tmp_name'], 'rb'));"
        },
        {
            "index":398,
            "vuln_id":"GHSA-4h66-vghf-xg5x",
            "cwe_id":"{'CWE-77'}",
            "score":9.8,
            "chain":"{'https:\/\/github.com\/hoperyy\/get-npm-package-version\/commit\/49459d4a3ce68587d48ffa8dead86fc9ed58e965', 'https:\/\/github.com\/hoperyy\/get-npm-package-version\/commit\/40b1cf31a0607ea66f9e30a0c3af1383b52b2dec'}",
            "dataset":"osv",
            "summary":"get-npm-package-version Command Injection vulnerability The package get-npm-package-version before 1.0.7 is vulnerable to Command Injection via the `main` function in index.js.",
            "published_date":"2022-08-03",
            "chain_len":2,
            "project":"https:\/\/github.com\/hoperyy\/get-npm-package-version",
            "commit_href":"https:\/\/github.com\/hoperyy\/get-npm-package-version\/commit\/40b1cf31a0607ea66f9e30a0c3af1383b52b2dec",
            "commit_sha":"40b1cf31a0607ea66f9e30a0c3af1383b52b2dec",
            "patch":"MULTI",
            "chain_ord":"['40b1cf31a0607ea66f9e30a0c3af1383b52b2dec', '49459d4a3ce68587d48ffa8dead86fc9ed58e965']",
            "before_first_fix_commit":"{'52797864df09049ea28d65d14620774257a965b0'}",
            "last_fix_commit":"49459d4a3ce68587d48ffa8dead86fc9ed58e965",
            "chain_ord_pos":1.0,
            "commit_datetime":"01\/26\/2021, 03:48:05",
            "message":"feat: add defence to Command Injection\n\nadd defence to Command Injection",
            "author":"DuLinRain",
            "comments":null,
            "stats":"{'additions': 3, 'deletions': 0, 'total': 3}",
            "files":"{'index.js': {'additions': 3, 'deletions': 0, 'changes': 3, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/hoperyy\/get-npm-package-version\/raw\/40b1cf31a0607ea66f9e30a0c3af1383b52b2dec\/index.js', 'patch': \"@@ -1,5 +1,8 @@\\n module.exports = function (packageName, { registry = '', timeout = null } = {}) {\\n     try {\\n+        if (\/[`$&{}[;|]\/g.test(packageName) || \/[`$&{}[;|]\/g.test(registry)) {\\n+            return null\\n+        }\\n         let version;\\n \\n         const config = {\"}}",
            "message_norm":"feat: add defence to command injection\n\nadd defence to command injection",
            "language":"en",
            "entities":"[('add', 'ACTION', ''), ('defence', 'SECWORD', ''), ('command injection', 'SECWORD', ''), ('add', 'ACTION', ''), ('defence', 'SECWORD', ''), ('command injection', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['index.js'])",
            "num_files":1.0,
            "patch_content":"From 40b1cf31a0607ea66f9e30a0c3af1383b52b2dec Mon Sep 17 00:00:00 2001\nFrom: DuLinRain <dulinrain@mail.hfut.edu.cn>\nDate: Tue, 26 Jan 2021 11:48:05 +0800\nSubject: [PATCH] feat: add defence to Command Injection\n\nadd defence to Command Injection\n---\n index.js | 3 +++\n 1 file changed, 3 insertions(+)\n\ndiff --git a\/index.js b\/index.js\nindex a207f28..2e4b490 100644\n--- a\/index.js\n+++ b\/index.js\n@@ -1,5 +1,8 @@\n module.exports = function (packageName, { registry = '', timeout = null } = {}) {\n     try {\n+        if (\/[`$&{}[;|]\/g.test(packageName) || \/[`$&{}[;|]\/g.test(registry)) {\n+            return null\n+        }\n         let version;\n \n         const config = {"
        },
        {
            "index":124,
            "vuln_id":"GHSA-mv78-g7wq-mhp4",
            "cwe_id":"{'CWE-369'}",
            "score":2.5,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/49847ae69a4e1a97ae7f2db5e217c77721e37948'}",
            "dataset":"osv",
            "summary":"Division by zero in padding computation in TFLite ### Impact\nThe TFLite computation for size of output after padding, [`ComputeOutSize`](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/0c9692ae7b1671c983569e5d3de5565843d500cf\/tensorflow\/lite\/kernels\/padding.h#L43-L55), does not check that the `stride` argument is not 0 before doing the division.\n\n```cc\ninline int ComputeOutSize(TfLitePadding padding, int image_size,\n                          int filter_size, int stride, int dilation_rate = 1) {\n  int effective_filter_size = (filter_size - 1) * dilation_rate + 1;\n  switch (padding) {\n    case kTfLitePaddingSame:\n      return (image_size + stride - 1) \/ stride;\n    case kTfLitePaddingValid:\n      return (image_size + stride - effective_filter_size) \/ stride;\n    default:\n      return 0;\n  }\n}\n```\n  \nUsers can craft special models such that `ComputeOutSize` is called with `stride` set to 0.\n\n### Patches\nWe have patched the issue in GitHub commit [49847ae69a4e1a97ae7f2db5e217c77721e37948](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/49847ae69a4e1a97ae7f2db5e217c77721e37948).\n\nThe fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by members of the Aivul Team from Qihoo 360.",
            "published_date":"2021-05-21",
            "chain_len":1,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/49847ae69a4e1a97ae7f2db5e217c77721e37948",
            "commit_sha":"49847ae69a4e1a97ae7f2db5e217c77721e37948",
            "patch":"SINGLE",
            "chain_ord":"['49847ae69a4e1a97ae7f2db5e217c77721e37948']",
            "before_first_fix_commit":"{'b0e85b5b3859d060a42364c79fe664b07299a0e9'}",
            "last_fix_commit":"49847ae69a4e1a97ae7f2db5e217c77721e37948",
            "chain_ord_pos":1.0,
            "commit_datetime":"04\/27\/2021, 22:37:08",
            "message":"Fix division by zero in TFLite padding.\n\nPiperOrigin-RevId: 370777494\nChange-Id: Ic1331e4a1603b9e4c8aa183012a6c8237410aa0f",
            "author":"Mihai Maruseac",
            "comments":null,
            "stats":"{'additions': 5, 'deletions': 0, 'total': 5}",
            "files":"{'tensorflow\/lite\/kernels\/padding.h': {'additions': 5, 'deletions': 0, 'changes': 5, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/49847ae69a4e1a97ae7f2db5e217c77721e37948\/tensorflow%2Flite%2Fkernels%2Fpadding.h', 'patch': '@@ -44,6 +44,11 @@ inline int ComputePaddingWithOffset(int stride, int dilation_rate, int in_size,\\n inline int ComputeOutSize(TfLitePadding padding, int image_size,\\n                           int filter_size, int stride, int dilation_rate = 1) {\\n   int effective_filter_size = (filter_size - 1) * dilation_rate + 1;\\n+\\n+  \/\/ TODO(b\/186448822): This uses 0 since the function has no other way to\\n+  \/\/ report error case\\n+  if (stride == 0) return 0;\\n+\\n   switch (padding) {\\n     case kTfLitePaddingSame:\\n       return (image_size + stride - 1) \/ stride;'}}",
            "message_norm":"fix division by zero in tflite padding.\n\npiperorigin-revid: 370777494\nchange-id: ic1331e4a1603b9e4c8aa183012a6c8237410aa0f",
            "language":"en",
            "entities":"[('fix', 'ACTION', ''), ('division by zero', 'SECWORD', ''), ('370777494', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/lite\/kernels\/padding.h'])",
            "num_files":1.0,
            "patch_content":"From 49847ae69a4e1a97ae7f2db5e217c77721e37948 Mon Sep 17 00:00:00 2001\nFrom: Mihai Maruseac <mihaimaruseac@google.com>\nDate: Tue, 27 Apr 2021 15:37:08 -0700\nSubject: [PATCH] Fix division by zero in TFLite padding.\n\nPiperOrigin-RevId: 370777494\nChange-Id: Ic1331e4a1603b9e4c8aa183012a6c8237410aa0f\n---\n tensorflow\/lite\/kernels\/padding.h | 5 +++++\n 1 file changed, 5 insertions(+)\n\ndiff --git a\/tensorflow\/lite\/kernels\/padding.h b\/tensorflow\/lite\/kernels\/padding.h\nindex 85950eaf34bb92..d9cca3ea135152 100644\n--- a\/tensorflow\/lite\/kernels\/padding.h\n+++ b\/tensorflow\/lite\/kernels\/padding.h\n@@ -44,6 +44,11 @@ inline int ComputePaddingWithOffset(int stride, int dilation_rate, int in_size,\n inline int ComputeOutSize(TfLitePadding padding, int image_size,\n                           int filter_size, int stride, int dilation_rate = 1) {\n   int effective_filter_size = (filter_size - 1) * dilation_rate + 1;\n+\n+  \/\/ TODO(b\/186448822): This uses 0 since the function has no other way to\n+  \/\/ report error case\n+  if (stride == 0) return 0;\n+\n   switch (padding) {\n     case kTfLitePaddingSame:\n       return (image_size + stride - 1) \/ stride;"
        },
        {
            "index":831,
            "vuln_id":"GHSA-763g-fqq7-48wg",
            "cwe_id":"{'CWE-611'}",
            "score":5.3,
            "chain":"{'https:\/\/github.com\/checkstyle\/checkstyle\/commit\/c46a16d177e6797895b195c288ae9a9a096254b8'}",
            "dataset":"osv",
            "summary":"XML external entity (XXE) processing ('external-parameter-entities' feature was not fully disabled)) Due to an incomplete fix for [CVE-2019-9658](https:\/\/cve.mitre.org\/cgi-bin\/cvename.cgi?name=CVE-2019-9658), checkstyle was still vulnerable to XML External Entity (XXE) Processing.\n\n### Impact\n\n#### User: Build Maintainers\n\nThis vulnerability probably doesn't impact Maven\/Gradle users as, in most cases, these builds are processing files that are trusted, or pre-vetted by a pull request reviewer before being run on internal CI infrastructure.\n\n#### User: Static Analysis as a Service\n\nIf you operate a site\/service that parses \"untrusted\" Checkstyle XML configuration files, you are vulnerable to this and should patch.\n\nNote from the discoverer of the original CVE-2019-9658:\n\n> While looking at a few companies that run Checkstyle\/PMD\/ect... as a service I notice that it's a common pattern to run the static code analysis tool inside of a Docker container with the following flags:\n> ```\n> --net=none \\\n> --privileged=false \\\n> --cap-drop=ALL\n> ```\n> Running the analysis in Docker has the advantage that there should be no sensitive local file information that XXE can exfiltrate from the container. Additionally, these flags prevent vulnerabilities in static analysis tools like Checkstyle from being used to exfiltrate data via XXE or to perform SSRF.\n> \\- [Jonathan Leitschuh](https:\/\/twitter.com\/jlleitschuh)\n\n### Patches\n_Has the problem been patched? What versions should users upgrade to?_\n\nPatched, will be released with version 8.29 at 26 Jan 2020.\n\n### Workarounds\n_Is there a way for users to fix or remediate the vulnerability without upgrading?_\n\nNo workaround are available\n\n### References\n\n - [CWE-611: Improper Restriction of XML External Entity Reference](https:\/\/cwe.mitre.org\/data\/definitions\/611.html)\n - GitHub Issue https:\/\/github.com\/checkstyle\/checkstyle\/issues\/7468\n\n### For more information\n\nIf you have any questions or comments about this advisory:\n* Open an issue in https:\/\/github.com\/checkstyle\/checkstyle\/issues",
            "published_date":"2020-01-31",
            "chain_len":1,
            "project":"https:\/\/github.com\/checkstyle\/checkstyle",
            "commit_href":"https:\/\/github.com\/checkstyle\/checkstyle\/commit\/c46a16d177e6797895b195c288ae9a9a096254b8",
            "commit_sha":"c46a16d177e6797895b195c288ae9a9a096254b8",
            "patch":"SINGLE",
            "chain_ord":"['c46a16d177e6797895b195c288ae9a9a096254b8']",
            "before_first_fix_commit":"{'dfed7949c3714dceb0318e4381f00e7fb5d5dfe3'}",
            "last_fix_commit":"c46a16d177e6797895b195c288ae9a9a096254b8",
            "chain_ord_pos":1.0,
            "commit_datetime":"01\/19\/2020, 01:32:28",
            "message":"Issue #7468: disable 'external-parameter-entities' feature by default",
            "author":"Roman Ivanov",
            "comments":null,
            "stats":"{'additions': 4, 'deletions': 0, 'total': 4}",
            "files":"{'src\/main\/java\/com\/puppycrawl\/tools\/checkstyle\/XmlLoader.java': {'additions': 4, 'deletions': 0, 'changes': 4, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/checkstyle\/checkstyle\/raw\/c46a16d177e6797895b195c288ae9a9a096254b8\/src%2Fmain%2Fjava%2Fcom%2Fpuppycrawl%2Ftools%2Fcheckstyle%2FXmlLoader.java', 'patch': '@@ -124,6 +124,9 @@ public static final class LoadExternalDtdFeatureProvider {\\n         \/** Feature that enables including external general entities in XML files. *\/\\n         public static final String EXTERNAL_GENERAL_ENTITIES =\\n                 \"http:\/\/xml.org\/sax\/features\/external-general-entities\";\\n+        \/** Feature that enables including external parameter entities in XML files. *\/\\n+        public static final String EXTERNAL_PARAMETER_ENTITIES =\\n+                \"http:\/\/xml.org\/sax\/features\/external-parameter-entities\";\\n \\n         \/** Stop instances being created. **\/\\n         private LoadExternalDtdFeatureProvider() {\\n@@ -146,6 +149,7 @@ public static void setFeaturesBySystemProperty(SAXParserFactory factory)\\n \\n             factory.setFeature(LOAD_EXTERNAL_DTD, enableExternalDtdLoad);\\n             factory.setFeature(EXTERNAL_GENERAL_ENTITIES, enableExternalDtdLoad);\\n+            factory.setFeature(EXTERNAL_PARAMETER_ENTITIES, enableExternalDtdLoad);\\n         }\\n \\n     }'}}",
            "message_norm":"issue #7468: disable 'external-parameter-entities' feature by default",
            "language":"fr",
            "entities":"[('#7468', 'ISSUE', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['src\/main\/java\/com\/puppycrawl\/tools\/checkstyle\/XmlLoader.java'])",
            "num_files":1.0,
            "patch_content":"From c46a16d177e6797895b195c288ae9a9a096254b8 Mon Sep 17 00:00:00 2001\nFrom: Roman Ivanov <ivanov-jr@mail.ru>\nDate: Sat, 18 Jan 2020 17:32:28 -0800\nSubject: [PATCH] Issue #7468: disable 'external-parameter-entities' feature by\n default\n\n---\n src\/main\/java\/com\/puppycrawl\/tools\/checkstyle\/XmlLoader.java | 4 ++++\n 1 file changed, 4 insertions(+)\n\ndiff --git a\/src\/main\/java\/com\/puppycrawl\/tools\/checkstyle\/XmlLoader.java b\/src\/main\/java\/com\/puppycrawl\/tools\/checkstyle\/XmlLoader.java\nindex 350034dc842..cc8d456a8c7 100644\n--- a\/src\/main\/java\/com\/puppycrawl\/tools\/checkstyle\/XmlLoader.java\n+++ b\/src\/main\/java\/com\/puppycrawl\/tools\/checkstyle\/XmlLoader.java\n@@ -124,6 +124,9 @@ public static final class LoadExternalDtdFeatureProvider {\n         \/** Feature that enables including external general entities in XML files. *\/\n         public static final String EXTERNAL_GENERAL_ENTITIES =\n                 \"http:\/\/xml.org\/sax\/features\/external-general-entities\";\n+        \/** Feature that enables including external parameter entities in XML files. *\/\n+        public static final String EXTERNAL_PARAMETER_ENTITIES =\n+                \"http:\/\/xml.org\/sax\/features\/external-parameter-entities\";\n \n         \/** Stop instances being created. **\/\n         private LoadExternalDtdFeatureProvider() {\n@@ -146,6 +149,7 @@ public static void setFeaturesBySystemProperty(SAXParserFactory factory)\n \n             factory.setFeature(LOAD_EXTERNAL_DTD, enableExternalDtdLoad);\n             factory.setFeature(EXTERNAL_GENERAL_ENTITIES, enableExternalDtdLoad);\n+            factory.setFeature(EXTERNAL_PARAMETER_ENTITIES, enableExternalDtdLoad);\n         }\n \n     }"
        },
        {
            "index":282,
            "vuln_id":"GHSA-3p92-886g-qxpq",
            "cwe_id":"{'CWE-201'}",
            "score":5.1,
            "chain":"{'https:\/\/github.com\/soldair\/node-floody\/commit\/6c44722312131f4ac8a1af40f0f861c85efe01b0'}",
            "dataset":"osv",
            "summary":"Remote Memory Exposure in floody Versions of `floody` before 0.1.1 are vulnerable to remote memory exposure.\n\n.write(number)` in the affected `floody` versions passes a number to Buffer constructor, appending a chunk of uninitialized memory.\n\nProof of Concept: \n\n```\nvar f = require('floody')(process.stdout); \nf.write(USERSUPPLIEDINPUT); \n'f.stop();\n\n\n## Recommendation\n\nUpdate to version 0.1.1 or later.",
            "published_date":"2019-06-04",
            "chain_len":1,
            "project":"https:\/\/github.com\/soldair\/node-floody",
            "commit_href":"https:\/\/github.com\/soldair\/node-floody\/commit\/6c44722312131f4ac8a1af40f0f861c85efe01b0",
            "commit_sha":"6c44722312131f4ac8a1af40f0f861c85efe01b0",
            "patch":"SINGLE",
            "chain_ord":"['6c44722312131f4ac8a1af40f0f861c85efe01b0']",
            "before_first_fix_commit":"{'2a150c5552b8ce2f2a12ae4a3fd33882d5827afd'}",
            "last_fix_commit":"6c44722312131f4ac8a1af40f0f861c85efe01b0",
            "chain_ord_pos":1.0,
            "commit_datetime":"01\/15\/2016, 13:27:13",
            "message":"adding fix for exposing uninitalized memory found by @chalker",
            "author":"Ryan Day",
            "comments":"{'com_1': {'author': 'ChALkeR', 'datetime': '01\/15\/2016, 20:42:58', 'body': \"POC: `var f = require('floody')(process.stdout); f.write(1000); f.stop();`.\"}}",
            "stats":"{'additions': 1, 'deletions': 1, 'total': 2}",
            "files":"{'index.js': {'additions': 1, 'deletions': 1, 'changes': 2, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/soldair\/node-floody\/raw\/6c44722312131f4ac8a1af40f0f861c85efe01b0\/index.js', 'patch': \"@@ -28,7 +28,7 @@ module.exports = function(options){\\n \\n     if(writes.length > windowSize) writes.shift();\\n \\n-    data = data instanceof Buffer ? data : new Buffer(data);\\n+    data = data instanceof Buffer ? data : new Buffer(data+'');\\n     bufLen += data.length;\\n \\n     buf.push(data);\"}}",
            "message_norm":"adding fix for exposing uninitalized memory found by @chalker",
            "language":"en",
            "entities":"[('adding', 'ACTION', ''), ('uninitalized memory', 'SECWORD', ''), ('found', 'ACTION', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['index.js'])",
            "num_files":1.0,
            "patch_content":"From 6c44722312131f4ac8a1af40f0f861c85efe01b0 Mon Sep 17 00:00:00 2001\nFrom: Ryan Day <soldair@gmail.com>\nDate: Fri, 15 Jan 2016 05:27:13 -0800\nSubject: [PATCH] adding fix for exposing uninitalized memory found by @chalker\n\n---\n index.js | 2 +-\n 1 file changed, 1 insertion(+), 1 deletion(-)\n\ndiff --git a\/index.js b\/index.js\nindex 83ba366..eb22e84 100644\n--- a\/index.js\n+++ b\/index.js\n@@ -28,7 +28,7 @@ module.exports = function(options){\n \n     if(writes.length > windowSize) writes.shift();\n \n-    data = data instanceof Buffer ? data : new Buffer(data);\n+    data = data instanceof Buffer ? data : new Buffer(data+'');\n     bufLen += data.length;\n \n     buf.push(data);"
        },
        {
            "index":355,
            "vuln_id":"GHSA-f7f4-hqp2-7prc",
            "cwe_id":"{'CWE-20'}",
            "score":7.5,
            "chain":"{'https:\/\/github.com\/balderdashy\/sails-hook-sockets\/commit\/ff02114eaec090ee51db48435cc32d451662606e', 'https:\/\/github.com\/balderdashy\/sails-hook-sockets\/commit\/0533a4864b1920fd8fbb5287bc0889193c5faf44'}",
            "dataset":"osv",
            "summary":"Improper Input Validation in sails-hook-sockets Sails.js before v1.0.0-46 allows attackers to cause a denial of service with a single request because there is no error handler in sails-hook-sockets to handle an empty pathname in a WebSocket request.",
            "published_date":"2020-07-24",
            "chain_len":2,
            "project":"https:\/\/github.com\/balderdashy\/sails-hook-sockets",
            "commit_href":"https:\/\/github.com\/balderdashy\/sails-hook-sockets\/commit\/ff02114eaec090ee51db48435cc32d451662606e",
            "commit_sha":"ff02114eaec090ee51db48435cc32d451662606e",
            "patch":"MULTI",
            "chain_ord":"['ff02114eaec090ee51db48435cc32d451662606e', '0533a4864b1920fd8fbb5287bc0889193c5faf44']",
            "before_first_fix_commit":"{'4f78b7946f7a7ac4c762936d7633298606c1e4a7'}",
            "last_fix_commit":"0533a4864b1920fd8fbb5287bc0889193c5faf44",
            "chain_ord_pos":1.0,
            "commit_datetime":"09\/23\/2018, 21:18:38",
            "message":"Define req.path for socket requests.",
            "author":"Mike McNeil",
            "comments":"{'com_1': {'author': 'mikermcneil', 'datetime': '10\/01\/2018, 15:34:06', 'body': 'Thanks Ali!'}}",
            "stats":"{'additions': 3, 'deletions': 0, 'total': 3}",
            "files":"{'lib\/receive-incoming-sails-io-msg.js': {'additions': 3, 'deletions': 0, 'changes': 3, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/balderdashy\/sails-hook-sockets\/raw\/ff02114eaec090ee51db48435cc32d451662606e\/lib%2Freceive-incoming-sails-io-msg.js', 'patch': \"@@ -3,6 +3,7 @@\\n  *\/\\n \\n var util = require('util');\\n+var url = require('url');\\n var _ = require('@sailshq\/lodash');\\n var semver = require('semver');\\n var parseSdkMetadata = require('.\/parse-sdk-metadata');\\n@@ -105,6 +106,8 @@ module.exports = function ToReceiveIncomingSailsIOMsg(app) {\\n \\n       url     : options.incomingSailsIOMsg.url,\\n \\n+      path    : url.parse(options.incomingSailsIOMsg.url).pathname,\\n+\\n       method  : options.eventName,\\n \\n       \/\/ Attached data becomes simulated HTTP body (`req.body`)\"}}",
            "message_norm":"define req.path for socket requests.",
            "language":"en",
            "entities":null,
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['lib\/receive-incoming-sails-io-msg.js'])",
            "num_files":1.0,
            "patch_content":"From ff02114eaec090ee51db48435cc32d451662606e Mon Sep 17 00:00:00 2001\nFrom: Mike McNeil <git+mikermcneil@sailsjs.com>\nDate: Sun, 23 Sep 2018 16:18:38 -0500\nSubject: [PATCH] Define req.path for socket requests.\n\n---\n lib\/receive-incoming-sails-io-msg.js | 3 +++\n 1 file changed, 3 insertions(+)\n\ndiff --git a\/lib\/receive-incoming-sails-io-msg.js b\/lib\/receive-incoming-sails-io-msg.js\nindex 2ddca3b..5be5704 100644\n--- a\/lib\/receive-incoming-sails-io-msg.js\n+++ b\/lib\/receive-incoming-sails-io-msg.js\n@@ -3,6 +3,7 @@\n  *\/\n \n var util = require('util');\n+var url = require('url');\n var _ = require('@sailshq\/lodash');\n var semver = require('semver');\n var parseSdkMetadata = require('.\/parse-sdk-metadata');\n@@ -105,6 +106,8 @@ module.exports = function ToReceiveIncomingSailsIOMsg(app) {\n \n       url     : options.incomingSailsIOMsg.url,\n \n+      path    : url.parse(options.incomingSailsIOMsg.url).pathname,\n+\n       method  : options.eventName,\n \n       \/\/ Attached data becomes simulated HTTP body (`req.body`)"
        },
        {
            "index":630,
            "vuln_id":"GHSA-27j5-4p9v-pp67",
            "cwe_id":"{'CWE-617'}",
            "score":5.5,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/8a6e874437670045e6c7dc6154c7412b4a2135e2'}",
            "dataset":"osv",
            "summary":"`std::abort` raised from `TensorListReserve` ### Impact\nProviding a negative element to `num_elements` list argument of  `tf.raw_ops.TensorListReserve` causes the runtime to abort the process due to reallocating a `std::vector` to have a negative number of elements:\n\n```python\nimport tensorflow as tf\n\ntf.raw_ops.TensorListReserve(\n  element_shape = tf.constant([1]),\n  num_elements=tf.constant([-1]),\n  element_dtype = tf.int32)\n```\n\nThe [implementation](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/8d72537c6abf5a44103b57b9c2e22c14f5f49698\/tensorflow\/core\/kernels\/list_kernels.cc#L312) calls `std::vector.resize()` with the new size controlled by input given by the user, without checking that this input is valid.\n\n### Patches\nWe have patched the issue in GitHub commit [8a6e874437670045e6c7dc6154c7412b4a2135e2](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/8a6e874437670045e6c7dc6154c7412b4a2135e2).\n\nThe fix will be included in TensorFlow 2.6.0. We will also cherrypick this commit on TensorFlow 2.5.1, TensorFlow 2.4.3, and TensorFlow 2.3.4, as these are also affected and still in supported range.\n\n### For more information \nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by members of the Aivul Team from Qihoo 360.",
            "published_date":"2021-08-25",
            "chain_len":1,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/8a6e874437670045e6c7dc6154c7412b4a2135e2",
            "commit_sha":"8a6e874437670045e6c7dc6154c7412b4a2135e2",
            "patch":"SINGLE",
            "chain_ord":"['8a6e874437670045e6c7dc6154c7412b4a2135e2']",
            "before_first_fix_commit":"{'3e23241a7f330f62c701f5ceb10f6594cd735f70'}",
            "last_fix_commit":"8a6e874437670045e6c7dc6154c7412b4a2135e2",
            "chain_ord_pos":1.0,
            "commit_datetime":"07\/10\/2021, 00:32:55",
            "message":"Validate num_elements input in tf.raw_ops.TensorListReserve\n\nPiperOrigin-RevId: 383954564\nChange-Id: I454bd78eff85bc4f16ddb7e608596971cca47f8f",
            "author":"Laura Pak",
            "comments":null,
            "stats":"{'additions': 4, 'deletions': 0, 'total': 4}",
            "files":"{'tensorflow\/core\/kernels\/list_kernels.cc': {'additions': 4, 'deletions': 0, 'changes': 4, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/8a6e874437670045e6c7dc6154c7412b4a2135e2\/tensorflow%2Fcore%2Fkernels%2Flist_kernels.cc', 'patch': '@@ -302,6 +302,10 @@ class TensorListReserve : public OpKernel {\\n     PartialTensorShape element_shape;\\n     OP_REQUIRES_OK(c, TensorShapeFromTensor(c->input(0), &element_shape));\\n     int32 num_elements = c->input(1).scalar<int32>()();\\n+    OP_REQUIRES(c, num_elements >= 0,\\n+                errors::InvalidArgument(\"The num_elements to reserve must be a \"\\n+                                        \"non negative number, but got \",\\n+                                        num_elements));\\n     TensorList output;\\n     output.element_shape = element_shape;\\n     output.element_dtype = element_dtype_;'}}",
            "message_norm":"validate num_elements input in tf.raw_ops.tensorlistreserve\n\npiperorigin-revid: 383954564\nchange-id: i454bd78eff85bc4f16ddb7e608596971cca47f8f",
            "language":"en",
            "entities":"[('validate', 'ACTION', ''), ('383954564', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/core\/kernels\/list_kernels.cc'])",
            "num_files":1.0,
            "patch_content":"From 8a6e874437670045e6c7dc6154c7412b4a2135e2 Mon Sep 17 00:00:00 2001\nFrom: Laura Pak <lpak@google.com>\nDate: Fri, 9 Jul 2021 17:32:55 -0700\nSubject: [PATCH] Validate num_elements input in tf.raw_ops.TensorListReserve\n\nPiperOrigin-RevId: 383954564\nChange-Id: I454bd78eff85bc4f16ddb7e608596971cca47f8f\n---\n tensorflow\/core\/kernels\/list_kernels.cc | 4 ++++\n 1 file changed, 4 insertions(+)\n\ndiff --git a\/tensorflow\/core\/kernels\/list_kernels.cc b\/tensorflow\/core\/kernels\/list_kernels.cc\nindex 9a2f373f5ce0cf..488e02337f707b 100644\n--- a\/tensorflow\/core\/kernels\/list_kernels.cc\n+++ b\/tensorflow\/core\/kernels\/list_kernels.cc\n@@ -302,6 +302,10 @@ class TensorListReserve : public OpKernel {\n     PartialTensorShape element_shape;\n     OP_REQUIRES_OK(c, TensorShapeFromTensor(c->input(0), &element_shape));\n     int32 num_elements = c->input(1).scalar<int32>()();\n+    OP_REQUIRES(c, num_elements >= 0,\n+                errors::InvalidArgument(\"The num_elements to reserve must be a \"\n+                                        \"non negative number, but got \",\n+                                        num_elements));\n     TensorList output;\n     output.element_shape = element_shape;\n     output.element_dtype = element_dtype_;"
        },
        {
            "index":927,
            "vuln_id":"GHSA-vmjw-c2vp-p33c",
            "cwe_id":"{'CWE-681'}",
            "score":5.5,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/b5cdbf12ffcaaffecf98f22a6be5a64bb96e4f58', 'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/3a7362750d5c372420aa8f0caf7bf5b5c3d0f52d'}",
            "dataset":"osv",
            "summary":"Crash in NMS ops caused by integer conversion to unsigned ### Impact\nAn attacker can cause denial of service in applications serving models using `tf.raw_ops.NonMaxSuppressionV5` by triggering a division by 0:\n\n```python\nimport tensorflow as tf\n\ntf.raw_ops.NonMaxSuppressionV5(\n  boxes=[[0.1,0.1,0.1,0.1],[0.2,0.2,0.2,0.2],[0.3,0.3,0.3,0.3]],\n  scores=[1.0,2.0,3.0],\n  max_output_size=-1,\n  iou_threshold=0.5,\n  score_threshold=0.5,\n  soft_nms_sigma=1.0,\n  pad_to_max_output_size=True)\n```\n  \nThe [implementation](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/460e000de3a83278fb00b61a16d161b1964f15f4\/tensorflow\/core\/kernels\/image\/non_max_suppression_op.cc#L170-L271) uses a user controlled argument to resize a `std::vector`:\n\n```cc\n  const int output_size = max_output_size.scalar<int>()();\n  \/\/ ...\n  std::vector<int> selected;\n  \/\/ ...\n  if (pad_to_max_output_size) {\n    selected.resize(output_size, 0);\n    \/\/ ...\n  }\n```\n    \nHowever, as `std::vector::resize` takes the size argument as a `size_t` and `output_size` is an `int`, there is an implicit conversion to usigned. If the attacker supplies a negative value, this conversion results in a crash.\n\nA similar issue occurs in `CombinedNonMaxSuppression`:\n\n```python\nimport tensorflow as tf\n\ntf.raw_ops.NonMaxSuppressionV5(\n  boxes=[[[[0.1,0.1,0.1,0.1],[0.2,0.2,0.2,0.2],[0.3,0.3,0.3,0.3]],[[0.1,0.1,0.1,0.1],[0.2,0.2,0.2,0.2],[0.3,0.3,0.3,0.3]],[[0.1,0.1,0.1,0.1],[0.2,0.2,0.2,0.2],[0.3,0.3,0.3,0.3]]]],\n  scores=[[[1.0,2.0,3.0],[1.0,2.0,3.0],[1.0,2.0,3.0]]],\n  max_output_size_per_class=-1,\n  max_total_size=10,\n  iou_threshold=score_threshold=0.5,\n  pad_per_class=True,\n  clip_boxes=True)\n```\n  \n### Patches\nWe have patched the issue in GitHub commit [3a7362750d5c372420aa8f0caf7bf5b5c3d0f52d](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/3a7362750d5c372420aa8f0caf7bf5b5c3d0f52d) and commit [b5cdbf12ffcaaffecf98f22a6be5a64bb96e4f58](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/b5cdbf12ffcaaffecf98f22a6be5a64bb96e4f58).\n\nThe fix will be included in TensorFlow 2.6.0. We will also cherrypick this commit on TensorFlow 2.5.1, TensorFlow 2.4.3, and TensorFlow 2.3.4, as these are also affected and still in supported range.\n\n### For more information \nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by members of the Aivul Team from Qihoo 360.",
            "published_date":"2021-08-25",
            "chain_len":2,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/3a7362750d5c372420aa8f0caf7bf5b5c3d0f52d",
            "commit_sha":"3a7362750d5c372420aa8f0caf7bf5b5c3d0f52d",
            "patch":"MULTI",
            "chain_ord":"['b5cdbf12ffcaaffecf98f22a6be5a64bb96e4f58', '3a7362750d5c372420aa8f0caf7bf5b5c3d0f52d']",
            "before_first_fix_commit":"{'a87fa31dc3becc97c7e945b9b8c8711acb92fc12'}",
            "last_fix_commit":"3a7362750d5c372420aa8f0caf7bf5b5c3d0f52d",
            "chain_ord_pos":2.0,
            "commit_datetime":"07\/31\/2021, 05:02:22",
            "message":"Prevent crash\/heap OOB due to integer conversion to unsigned in NMS kernels\n\nPiperOrigin-RevId: 387938262\nChange-Id: Id361a715307e7179977cf5c64391c199a966f2ad",
            "author":"Mihai Maruseac",
            "comments":null,
            "stats":"{'additions': 8, 'deletions': 0, 'total': 8}",
            "files":"{'tensorflow\/core\/kernels\/image\/non_max_suppression_op.cc': {'additions': 8, 'deletions': 0, 'changes': 8, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/3a7362750d5c372420aa8f0caf7bf5b5c3d0f52d\/tensorflow%2Fcore%2Fkernels%2Fimage%2Fnon_max_suppression_op.cc', 'patch': '@@ -169,6 +169,8 @@ void DoNonMaxSuppressionOp(OpKernelContext* context, const Tensor& scores,\\n                            bool pad_to_max_output_size = false,\\n                            int* ptr_num_valid_outputs = nullptr) {\\n   const int output_size = max_output_size.scalar<int>()();\\n+  OP_REQUIRES(context, output_size >= 0,\\n+              errors::InvalidArgument(\"output size must be non-negative\"));\\n \\n   std::vector<T> scores_data(num_boxes);\\n   std::copy_n(scores.flat<T>().data(), num_boxes, scores_data.begin());\\n@@ -768,6 +770,9 @@ class NonMaxSuppressionV4Op : public OpKernel {\\n         context, scores, num_boxes, max_output_size, iou_threshold_val,\\n         score_threshold_val, dummy_soft_nms_sigma, similarity_fn,\\n         return_scores_tensor_, pad_to_max_output_size_, &num_valid_outputs);\\n+    if (!context->status().ok()) {\\n+      return;\\n+    }\\n \\n     \/\/ Allocate scalar output tensor for number of indices computed.\\n     Tensor* num_outputs_t = nullptr;\\n@@ -845,6 +850,9 @@ class NonMaxSuppressionV5Op : public OpKernel {\\n         context, scores, num_boxes, max_output_size, iou_threshold_val,\\n         score_threshold_val, soft_nms_sigma_val, similarity_fn,\\n         return_scores_tensor_, pad_to_max_output_size_, &num_valid_outputs);\\n+    if (!context->status().ok()) {\\n+      return;\\n+    }\\n \\n     \/\/ Allocate scalar output tensor for number of indices computed.\\n     Tensor* num_outputs_t = nullptr;'}}",
            "message_norm":"prevent crash\/heap oob due to integer conversion to unsigned in nms kernels\n\npiperorigin-revid: 387938262\nchange-id: id361a715307e7179977cf5c64391c199a966f2ad",
            "language":"en",
            "entities":"[('prevent', 'ACTION', ''), ('heap oob', 'SECWORD', ''), ('387938262', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/core\/kernels\/image\/non_max_suppression_op.cc'])",
            "num_files":1.0,
            "patch_content":"From 3a7362750d5c372420aa8f0caf7bf5b5c3d0f52d Mon Sep 17 00:00:00 2001\nFrom: Mihai Maruseac <mihaimaruseac@google.com>\nDate: Fri, 30 Jul 2021 22:02:22 -0700\nSubject: [PATCH] Prevent crash\/heap OOB due to integer conversion to unsigned\n in NMS kernels\n\nPiperOrigin-RevId: 387938262\nChange-Id: Id361a715307e7179977cf5c64391c199a966f2ad\n---\n tensorflow\/core\/kernels\/image\/non_max_suppression_op.cc | 8 ++++++++\n 1 file changed, 8 insertions(+)\n\ndiff --git a\/tensorflow\/core\/kernels\/image\/non_max_suppression_op.cc b\/tensorflow\/core\/kernels\/image\/non_max_suppression_op.cc\nindex 69b05cc9d84f83..1ec4c853f5f5b6 100644\n--- a\/tensorflow\/core\/kernels\/image\/non_max_suppression_op.cc\n+++ b\/tensorflow\/core\/kernels\/image\/non_max_suppression_op.cc\n@@ -169,6 +169,8 @@ void DoNonMaxSuppressionOp(OpKernelContext* context, const Tensor& scores,\n                            bool pad_to_max_output_size = false,\n                            int* ptr_num_valid_outputs = nullptr) {\n   const int output_size = max_output_size.scalar<int>()();\n+  OP_REQUIRES(context, output_size >= 0,\n+              errors::InvalidArgument(\"output size must be non-negative\"));\n \n   std::vector<T> scores_data(num_boxes);\n   std::copy_n(scores.flat<T>().data(), num_boxes, scores_data.begin());\n@@ -768,6 +770,9 @@ class NonMaxSuppressionV4Op : public OpKernel {\n         context, scores, num_boxes, max_output_size, iou_threshold_val,\n         score_threshold_val, dummy_soft_nms_sigma, similarity_fn,\n         return_scores_tensor_, pad_to_max_output_size_, &num_valid_outputs);\n+    if (!context->status().ok()) {\n+      return;\n+    }\n \n     \/\/ Allocate scalar output tensor for number of indices computed.\n     Tensor* num_outputs_t = nullptr;\n@@ -845,6 +850,9 @@ class NonMaxSuppressionV5Op : public OpKernel {\n         context, scores, num_boxes, max_output_size, iou_threshold_val,\n         score_threshold_val, soft_nms_sigma_val, similarity_fn,\n         return_scores_tensor_, pad_to_max_output_size_, &num_valid_outputs);\n+    if (!context->status().ok()) {\n+      return;\n+    }\n \n     \/\/ Allocate scalar output tensor for number of indices computed.\n     Tensor* num_outputs_t = nullptr;"
        },
        {
            "index":648,
            "vuln_id":"GHSA-hjgp-8ffr-hwwr",
            "cwe_id":"{'CWE-311'}",
            "score":0.0,
            "chain":"{'https:\/\/github.com\/dcodeIO\/ClosureCompiler.js\/commit\/e59848f5975e5b15279c044daf9cff8ff192bae6'}",
            "dataset":"osv",
            "summary":"Downloads Resources over HTTP in closurecompiler Affected versions of `closurecompiler` insecurely download an executable over an unencrypted HTTP connection. \n\nIn scenarios where an attacker has a privileged network position, it is possible to intercept the response and replace the executable with a malicious one, resulting in code execution on the system running `closurecompiler`.\n\n\n## Recommendation\n\nUpdate to version 1.6.1 or later.",
            "published_date":"2019-02-18",
            "chain_len":1,
            "project":"https:\/\/github.com\/dcodeIO\/ClosureCompiler.js",
            "commit_href":"https:\/\/github.com\/dcodeIO\/ClosureCompiler.js\/commit\/e59848f5975e5b15279c044daf9cff8ff192bae6",
            "commit_sha":"e59848f5975e5b15279c044daf9cff8ff192bae6",
            "patch":"SINGLE",
            "chain_ord":"['e59848f5975e5b15279c044daf9cff8ff192bae6']",
            "before_first_fix_commit":"{'a896952c01f25a5317b6619723fe1ebeabaeb468', '923250af8c94154bdbc48f61230af1adf2543173'}",
            "last_fix_commit":"e59848f5975e5b15279c044daf9cff8ff192bae6",
            "chain_ord_pos":1.0,
            "commit_datetime":"11\/01\/2016, 14:20:53",
            "message":"Merge pull request #51 from Greenek\/master\n\nUpdate link to bundled-openjdk-jre",
            "author":"Daniel Wirtz",
            "comments":null,
            "stats":"{'additions': 4, 'deletions': 4, 'total': 8}",
            "files":"{'scripts\/configure.js': {'additions': 4, 'deletions': 4, 'changes': 8, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/dcodeIO\/ClosureCompiler.js\/raw\/e59848f5975e5b15279c044daf9cff8ff192bae6\/scripts%2Fconfigure.js', 'patch': '@@ -48,7 +48,7 @@ function platformPostfix() {\\n }\\n \\n \/\/ Bundled JRE download url\\n-var jrePrefix = \"http:\/\/bundled-openjdk-jre.googlecode.com\/files\/OpenJDK-JRE-7u6_24-\";\\n+var jrePrefix = \"https:\/\/storage.googleapis.com\/google-code-archive-downloads\/v2\/code.google.com\/bundled-openjdk-jre\/OpenJDK-JRE-7u6_24-\";\\n var jrePostfix = \".tar.gz\";\\n var jreUrl = jrePrefix+platformPostfix()+jrePostfix;\\n \\n@@ -205,13 +205,13 @@ function download(downloadUrl, filename, callback, ondata) {\\n  * @param {function(?Error)} callback\\n  * @param {function(Object)=} entryCallback\\n  *\/\\n-function unpack(filename, callback, entryCallback) {   \\n+function unpack(filename, callback, entryCallback) {\\n     var input = fs.createReadStream(filename, { flags: \\'r\\', encoding: null }),\\n         files = {},\\n         dir = path.dirname(filename),\\n         returned = false,\\n         to = null;\\n-    \\n+\\n     \/\/ Finishs the unpack if all files are done\\n     function maybeFinish() {\\n         if (to !== null) clearTimeout(to);\\n@@ -230,7 +230,7 @@ function unpack(filename, callback, entryCallback) {\\n             }\\n         }, 1000);\\n     }\\n-    \\n+\\n     input.pipe(zlib.createGunzip()).pipe(tar.Parse()).on(\"entry\", function(entry) {\\n         if (entryCallback) entryCallback(entry);\\n         if (entry[\"type\"] == \\'File\\') {'}}",
            "message_norm":"merge pull request #51 from greenek\/master\n\nupdate link to bundled-openjdk-jre",
            "language":"no",
            "entities":"[('#51', 'ISSUE', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['scripts\/configure.js'])",
            "num_files":1.0,
            "patch_content":"From a896952c01f25a5317b6619723fe1ebeabaeb468 Mon Sep 17 00:00:00 2001\nFrom: =?UTF-8?q?Pawe=C5=82=20Golonko?= <pgolonko@gmail.com>\nDate: Sat, 3 Sep 2016 19:10:35 +0200\nSubject: [PATCH] Update link to bundled-openjdk-jre\n\n---\n scripts\/configure.js | 8 ++++----\n 1 file changed, 4 insertions(+), 4 deletions(-)\n\ndiff --git a\/scripts\/configure.js b\/scripts\/configure.js\nindex 5c34a1e..4f2f540 100644\n--- a\/scripts\/configure.js\n+++ b\/scripts\/configure.js\n@@ -48,7 +48,7 @@ function platformPostfix() {\n }\n \n \/\/ Bundled JRE download url\n-var jrePrefix = \"http:\/\/bundled-openjdk-jre.googlecode.com\/files\/OpenJDK-JRE-7u6_24-\";\n+var jrePrefix = \"https:\/\/storage.googleapis.com\/google-code-archive-downloads\/v2\/code.google.com\/bundled-openjdk-jre\/OpenJDK-JRE-7u6_24-\";\n var jrePostfix = \".tar.gz\";\n var jreUrl = jrePrefix+platformPostfix()+jrePostfix;\n \n@@ -205,13 +205,13 @@ function download(downloadUrl, filename, callback, ondata) {\n  * @param {function(?Error)} callback\n  * @param {function(Object)=} entryCallback\n  *\/\n-function unpack(filename, callback, entryCallback) {   \n+function unpack(filename, callback, entryCallback) {\n     var input = fs.createReadStream(filename, { flags: 'r', encoding: null }),\n         files = {},\n         dir = path.dirname(filename),\n         returned = false,\n         to = null;\n-    \n+\n     \/\/ Finishs the unpack if all files are done\n     function maybeFinish() {\n         if (to !== null) clearTimeout(to);\n@@ -230,7 +230,7 @@ function unpack(filename, callback, entryCallback) {\n             }\n         }, 1000);\n     }\n-    \n+\n     input.pipe(zlib.createGunzip()).pipe(tar.Parse()).on(\"entry\", function(entry) {\n         if (entryCallback) entryCallback(entry);\n         if (entry[\"type\"] == 'File') {"
        },
        {
            "index":427,
            "vuln_id":"GHSA-m3xv-x3ph-mq22",
            "cwe_id":"{'CWE-94'}",
            "score":9.8,
            "chain":"{'https:\/\/github.com\/nystudio107\/craft-seomatic\/commit\/0c5c0c0e0cb61000d12ec55ebf174745a5bf6469'}",
            "dataset":"osv",
            "summary":"Server-side Template Injection in nystudio107\/craft-seomatic A Server-side Template Injection (SSTI) vulnerability exists in Nystudio107 Seomatic prior to 3.4.12 in src\/helpers\/UrlHelper.php via the host header.",
            "published_date":"2022-03-12",
            "chain_len":1,
            "project":"https:\/\/github.com\/nystudio107\/craft-seomatic",
            "commit_href":"https:\/\/github.com\/nystudio107\/craft-seomatic\/commit\/0c5c0c0e0cb61000d12ec55ebf174745a5bf6469",
            "commit_sha":"0c5c0c0e0cb61000d12ec55ebf174745a5bf6469",
            "patch":"SINGLE",
            "chain_ord":"['0c5c0c0e0cb61000d12ec55ebf174745a5bf6469']",
            "before_first_fix_commit":"{'1a47702db9d2df3fa3e12b1c4be09b55d2b6166f'}",
            "last_fix_commit":"0c5c0c0e0cb61000d12ec55ebf174745a5bf6469",
            "chain_ord_pos":1.0,
            "commit_datetime":"09\/25\/2021, 04:44:33",
            "message":"Sanitize all URLs\n\nSigned-off-by: Andrew Welch <andrew@nystudio107.com>",
            "author":"Andrew Welch",
            "comments":null,
            "stats":"{'additions': 2, 'deletions': 2, 'total': 4}",
            "files":"{'src\/helpers\/UrlHelper.php': {'additions': 2, 'deletions': 2, 'changes': 4, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/nystudio107\/craft-seomatic\/raw\/0c5c0c0e0cb61000d12ec55ebf174745a5bf6469\/src%2Fhelpers%2FUrlHelper.php', 'patch': \"@@ -56,7 +56,7 @@ public static function siteUrl(string $path = '', $params = null, string $scheme\\n             return $url;\\n         }\\n \\n-        return parent::siteUrl($path, $params, $scheme, $siteId);\\n+        return DynamicMeta::sanitizeUrl(parent::siteUrl($path, $params, $scheme, $siteId));\\n     }\\n \\n     \/**\\n@@ -130,7 +130,7 @@ public static function absoluteUrlWithProtocol($url): string\\n             $url = rtrim($url, '\/');\\n         }\\n \\n-        return $url;\\n+        return DynamicMeta::sanitizeUrl($url);\\n     }\\n \\n     \/**\"}}",
            "message_norm":"sanitize all urls\n\nsigned-off-by: andrew welch <andrew@nystudio107.com>",
            "language":"en",
            "entities":"[('sanitize', 'SECWORD', ''), ('andrew@nystudio107.com', 'EMAIL', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['src\/helpers\/UrlHelper.php'])",
            "num_files":1.0,
            "patch_content":"From 0c5c0c0e0cb61000d12ec55ebf174745a5bf6469 Mon Sep 17 00:00:00 2001\nFrom: Andrew Welch <andrew@nystudio107.com>\nDate: Sat, 25 Sep 2021 00:44:33 -0400\nSubject: [PATCH] Sanitize all URLs\n\nSigned-off-by: Andrew Welch <andrew@nystudio107.com>\n---\n src\/helpers\/UrlHelper.php | 4 ++--\n 1 file changed, 2 insertions(+), 2 deletions(-)\n\ndiff --git a\/src\/helpers\/UrlHelper.php b\/src\/helpers\/UrlHelper.php\nindex 250a921ee..4f0f1c353 100644\n--- a\/src\/helpers\/UrlHelper.php\n+++ b\/src\/helpers\/UrlHelper.php\n@@ -56,7 +56,7 @@ public static function siteUrl(string $path = '', $params = null, string $scheme\n             return $url;\n         }\n \n-        return parent::siteUrl($path, $params, $scheme, $siteId);\n+        return DynamicMeta::sanitizeUrl(parent::siteUrl($path, $params, $scheme, $siteId));\n     }\n \n     \/**\n@@ -130,7 +130,7 @@ public static function absoluteUrlWithProtocol($url): string\n             $url = rtrim($url, '\/');\n         }\n \n-        return $url;\n+        return DynamicMeta::sanitizeUrl($url);\n     }\n \n     \/**"
        },
        {
            "index":266,
            "vuln_id":"GHSA-57f3-gghm-9mhc",
            "cwe_id":"{'CWE-400'}",
            "score":7.5,
            "chain":"{'https:\/\/github.com\/MrRio\/jsPDF\/commit\/d8bb3b39efcd129994f7a3b01b632164144ec43e'}",
            "dataset":"osv",
            "summary":"Regular Expression Denial of Service (ReDoS) This affects the package jspdf before 2.3.1. ReDoS is possible via the addImage function.",
            "published_date":"2021-03-12",
            "chain_len":1,
            "project":"https:\/\/github.com\/MrRio\/jsPDF",
            "commit_href":"https:\/\/github.com\/MrRio\/jsPDF\/commit\/d8bb3b39efcd129994f7a3b01b632164144ec43e",
            "commit_sha":"d8bb3b39efcd129994f7a3b01b632164144ec43e",
            "patch":"SINGLE",
            "chain_ord":"['d8bb3b39efcd129994f7a3b01b632164144ec43e']",
            "before_first_fix_commit":"{'c91995de97c598deaf6fda7109ea886a50f50109'}",
            "last_fix_commit":"d8bb3b39efcd129994f7a3b01b632164144ec43e",
            "chain_ord_pos":1.0,
            "commit_datetime":"02\/11\/2021, 15:50:17",
            "message":"fix ReDoS-vulnerable regexp in addImage (#3091)",
            "author":"Yeting Li",
            "comments":null,
            "stats":"{'additions': 1, 'deletions': 1, 'total': 2}",
            "files":"{'src\/modules\/addimage.js': {'additions': 1, 'deletions': 1, 'changes': 2, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/parallax\/jsPDF\/raw\/d8bb3b39efcd129994f7a3b01b632164144ec43e\/src%2Fmodules%2Faddimage.js', 'patch': '@@ -630,7 +630,7 @@ import { atob, btoa } from \"..\/libs\/AtobBtoa.js\";\\n     var result = null;\\n \\n     if (dataUrlParts.length === 2) {\\n-      var extractedInfo = \/^data:(\\\\w*\\\\\/\\\\w*);*(charset=[\\\\w=-]*)*;*$\/.exec(\\n+      var extractedInfo = \/^data:(\\\\w*\\\\\/\\\\w*);*(charset=(?!charset=)[\\\\w=-]*)*;*$\/.exec(\\n         dataUrlParts[0]\\n       );\\n       if (Array.isArray(extractedInfo)) {'}}",
            "message_norm":"fix redos-vulnerable regexp in addimage (#3091)",
            "language":"en",
            "entities":"[('fix', 'ACTION', ''), ('redos', 'SECWORD', ''), ('vulnerable', 'SECWORD', ''), ('#3091', 'ISSUE', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['src\/modules\/addimage.js'])",
            "num_files":1.0,
            "patch_content":"From d8bb3b39efcd129994f7a3b01b632164144ec43e Mon Sep 17 00:00:00 2001\nFrom: Yeting Li <liyt@ios.ac.cn>\nDate: Thu, 11 Feb 2021 23:50:17 +0800\nSubject: [PATCH] fix ReDoS-vulnerable regexp in addImage (#3091)\n\n---\n src\/modules\/addimage.js | 2 +-\n 1 file changed, 1 insertion(+), 1 deletion(-)\n\ndiff --git a\/src\/modules\/addimage.js b\/src\/modules\/addimage.js\nindex 5d206606f..d0295d0ca 100644\n--- a\/src\/modules\/addimage.js\n+++ b\/src\/modules\/addimage.js\n@@ -630,7 +630,7 @@ import { atob, btoa } from \"..\/libs\/AtobBtoa.js\";\n     var result = null;\n \n     if (dataUrlParts.length === 2) {\n-      var extractedInfo = \/^data:(\\w*\\\/\\w*);*(charset=[\\w=-]*)*;*$\/.exec(\n+      var extractedInfo = \/^data:(\\w*\\\/\\w*);*(charset=(?!charset=)[\\w=-]*)*;*$\/.exec(\n         dataUrlParts[0]\n       );\n       if (Array.isArray(extractedInfo)) {"
        },
        {
            "index":319,
            "vuln_id":"GHSA-hrg5-737c-2p56",
            "cwe_id":"{'CWE-20'}",
            "score":5.5,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/13d38a07ce9143e044aa737cfd7bb759d0e9b400'}",
            "dataset":"osv",
            "summary":"Missing validation causes denial of service via `UnsortedSegmentJoin` ### Impact\nThe implementation of [`tf.raw_ops.UnsortedSegmentJoin`](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/f3b9bf4c3c0597563b289c0512e98d4ce81f886e\/tensorflow\/core\/kernels\/unsorted_segment_join_op.cc#L92-L95) does not fully validate the input arguments. This results in a `CHECK`-failure which can be used to trigger a denial of service attack:\n\n```python\nimport tensorflow as tf\n\ntf.raw_ops.UnsortedSegmentJoin(\n  inputs=tf.constant(\"this\", shape=[12], dtype=tf.string),\n  segment_ids=tf.constant(0, shape=[12], dtype=tf.int64),\n  num_segments=tf.constant(0, shape=[12], dtype=tf.int64))\n``` \n  \nThe code assumes `num_segments` is a scalar but there is no validation for this before accessing its value:\n\n```cc\nconst Tensor& num_segments_tensor = context->input(2);\nOP_REQUIRES(context, num_segments_tensor.NumElements() != 0,\n            errors::InvalidArgument(\"Number of segments cannot be empty.\"));\nauto num_segments = num_segments_tensor.scalar<NUM_SEGMENTS_TYPE>()();\n``` \n\n### Patches\nWe have patched the issue in GitHub commit [13d38a07ce9143e044aa737cfd7bb759d0e9b400](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/13d38a07ce9143e044aa737cfd7bb759d0e9b400).\n\nThe fix will be included in TensorFlow 2.9.0. We will also cherrypick this commit on TensorFlow 2.8.1, TensorFlow 2.7.2, and TensorFlow 2.6.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by Neophytos Christou from Secure Systems Lab at Brown University.",
            "published_date":"2022-05-24",
            "chain_len":1,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/13d38a07ce9143e044aa737cfd7bb759d0e9b400",
            "commit_sha":"13d38a07ce9143e044aa737cfd7bb759d0e9b400",
            "patch":"SINGLE",
            "chain_ord":"['13d38a07ce9143e044aa737cfd7bb759d0e9b400']",
            "before_first_fix_commit":"{'fa57990ccca1fb2b9a1f296183985931746914d3'}",
            "last_fix_commit":"13d38a07ce9143e044aa737cfd7bb759d0e9b400",
            "chain_ord_pos":1.0,
            "commit_datetime":"04\/28\/2022, 18:37:31",
            "message":"Fix tf.raw_ops.UnsortedSegmentJoin vulnerability with invalid num_segments.\n\nCheck that input is actually a scalar before treating it as such.\n\nPiperOrigin-RevId: 445206880",
            "author":"Alan Liu",
            "comments":null,
            "stats":"{'additions': 3, 'deletions': 0, 'total': 3}",
            "files":"{'tensorflow\/core\/kernels\/unsorted_segment_join_op.cc': {'additions': 3, 'deletions': 0, 'changes': 3, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/13d38a07ce9143e044aa737cfd7bb759d0e9b400\/tensorflow%2Fcore%2Fkernels%2Funsorted_segment_join_op.cc', 'patch': '@@ -92,6 +92,9 @@ class UnsortedSegmentJoinOp : public OpKernel {\\n     const Tensor& num_segments_tensor = context->input(2);\\n     OP_REQUIRES(context, num_segments_tensor.NumElements() != 0,\\n                 errors::InvalidArgument(\"Number of segments cannot be empty.\"));\\n+    OP_REQUIRES(context,\\n+                TensorShapeUtils::IsScalar(num_segments_tensor.shape()),\\n+                errors::InvalidArgument(\"Number of segments must be a scalar\"));\\n     auto num_segments = num_segments_tensor.scalar<NUM_SEGMENTS_TYPE>()();\\n \\n     OP_REQUIRES('}}",
            "message_norm":"fix tf.raw_ops.unsortedsegmentjoin vulnerability with invalid num_segments.\n\ncheck that input is actually a scalar before treating it as such.\n\npiperorigin-revid: 445206880",
            "language":"en",
            "entities":"[('fix', 'ACTION', ''), ('vulnerability', 'SECWORD', ''), ('445206880', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/core\/kernels\/unsorted_segment_join_op.cc'])",
            "num_files":1.0,
            "patch_content":"From 13d38a07ce9143e044aa737cfd7bb759d0e9b400 Mon Sep 17 00:00:00 2001\nFrom: Alan Liu <liualan@google.com>\nDate: Thu, 28 Apr 2022 11:37:31 -0700\nSubject: [PATCH] Fix tf.raw_ops.UnsortedSegmentJoin vulnerability with invalid\n num_segments.\n\nCheck that input is actually a scalar before treating it as such.\n\nPiperOrigin-RevId: 445206880\n---\n tensorflow\/core\/kernels\/unsorted_segment_join_op.cc | 3 +++\n 1 file changed, 3 insertions(+)\n\ndiff --git a\/tensorflow\/core\/kernels\/unsorted_segment_join_op.cc b\/tensorflow\/core\/kernels\/unsorted_segment_join_op.cc\nindex c8445ca4c4c596..9529c6e8b74ea9 100644\n--- a\/tensorflow\/core\/kernels\/unsorted_segment_join_op.cc\n+++ b\/tensorflow\/core\/kernels\/unsorted_segment_join_op.cc\n@@ -92,6 +92,9 @@ class UnsortedSegmentJoinOp : public OpKernel {\n     const Tensor& num_segments_tensor = context->input(2);\n     OP_REQUIRES(context, num_segments_tensor.NumElements() != 0,\n                 errors::InvalidArgument(\"Number of segments cannot be empty.\"));\n+    OP_REQUIRES(context,\n+                TensorShapeUtils::IsScalar(num_segments_tensor.shape()),\n+                errors::InvalidArgument(\"Number of segments must be a scalar\"));\n     auto num_segments = num_segments_tensor.scalar<NUM_SEGMENTS_TYPE>()();\n \n     OP_REQUIRES("
        },
        {
            "index":726,
            "vuln_id":"GHSA-9697-98pf-4rw7",
            "cwe_id":"{'CWE-125'}",
            "score":5.5,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/42459e4273c2e47a3232cc16c4f4fff3b3a35c38'}",
            "dataset":"osv",
            "summary":"Heap OOB in `UpperBound` and `LowerBound` ### Impact\nAn attacker can read from outside of bounds of heap allocated data by sending specially crafted illegal arguments to `tf.raw_ops.UpperBound`:\n\n```python\nimport tensorflow as tf\n  \ntf.raw_ops.UpperBound(\n  sorted_input=[1,2,3],\n  values=tf.constant(value=[[0,0,0],[1,1,1],[2,2,2]],dtype=tf.int64),\n  out_type=tf.int64)\n```\n  \nThe [implementation](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/460e000de3a83278fb00b61a16d161b1964f15f4\/tensorflow\/core\/kernels\/searchsorted_op.cc#L85-L104) does not validate the rank of `sorted_input` argument:\n\n```cc\n  void Compute(OpKernelContext* ctx) override {\n    const Tensor& sorted_inputs_t = ctx->input(0);\n    \/\/ ...\n    OP_REQUIRES(ctx, sorted_inputs_t.dim_size(0) == values_t.dim_size(0),\n                Status(error::INVALID_ARGUMENT,\n                       \"Leading dim_size of both tensors must match.\"));\n    \/\/ ...\n    if (output_t->dtype() == DT_INT32) {\n      OP_REQUIRES(ctx,\n                  FastBoundsCheck(sorted_inputs_t.dim_size(1), ...));\n      \/\/ ...\n    }\n```\n\nAs we access the first two dimensions of `sorted_inputs_t` tensor, it must have rank at least 2.\n\nA similar issue occurs in `tf.raw_ops.LowerBound`.\n\n### Patches\nWe have patched the issue in GitHub commit [42459e4273c2e47a3232cc16c4f4fff3b3a35c38](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/42459e4273c2e47a3232cc16c4f4fff3b3a35c38).\n  \nThe fix will be included in TensorFlow 2.6.0. We will also cherrypick this commit on TensorFlow 2.5.1, TensorFlow 2.4.3, and TensorFlow 2.3.4, as these are also affected and still in supported range.\n  \n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by members of the Aivul Team from Qihoo 360.",
            "published_date":"2021-08-25",
            "chain_len":1,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/42459e4273c2e47a3232cc16c4f4fff3b3a35c38",
            "commit_sha":"42459e4273c2e47a3232cc16c4f4fff3b3a35c38",
            "patch":"SINGLE",
            "chain_ord":"['42459e4273c2e47a3232cc16c4f4fff3b3a35c38']",
            "before_first_fix_commit":"{'b5cdbf12ffcaaffecf98f22a6be5a64bb96e4f58'}",
            "last_fix_commit":"42459e4273c2e47a3232cc16c4f4fff3b3a35c38",
            "chain_ord_pos":1.0,
            "commit_datetime":"07\/30\/2021, 05:25:05",
            "message":"Prevent CHECK-fail\/heap OOB in UpperBound and LowerBound\n\nPiperOrigin-RevId: 387738073\nChange-Id: Iee74de95ddad18440d052a75a5a1cb67544f490a",
            "author":"Mihai Maruseac",
            "comments":null,
            "stats":"{'additions': 8, 'deletions': 0, 'total': 8}",
            "files":"{'tensorflow\/core\/kernels\/searchsorted_op.cc': {'additions': 8, 'deletions': 0, 'changes': 8, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/42459e4273c2e47a3232cc16c4f4fff3b3a35c38\/tensorflow%2Fcore%2Fkernels%2Fsearchsorted_op.cc', 'patch': '@@ -86,6 +86,10 @@ class UpperBoundOp : public OpKernel {\\n     const Tensor& sorted_inputs_t = ctx->input(0);\\n     const Tensor& values_t = ctx->input(1);\\n \\n+    \/\/ inputs must be at least a matrix\\n+    OP_REQUIRES(\\n+        ctx, sorted_inputs_t.shape().dims() >= 2,\\n+        errors::InvalidArgument(\"sorted input argument must be a matrix\"));\\n     \/\/ must have same batch dim_size for both\\n     OP_REQUIRES(ctx, sorted_inputs_t.dim_size(0) == values_t.dim_size(0),\\n                 Status(error::INVALID_ARGUMENT,\\n@@ -127,6 +131,10 @@ class LowerBoundOp : public OpKernel {\\n     const Tensor& sorted_inputs_t = ctx->input(0);\\n     const Tensor& values_t = ctx->input(1);\\n \\n+    \/\/ inputs must be at least a matrix\\n+    OP_REQUIRES(\\n+        ctx, sorted_inputs_t.shape().dims() >= 2,\\n+        errors::InvalidArgument(\"sorted input argument must be a matrix\"));\\n     \/\/ must have same batch dim_size for both\\n     OP_REQUIRES(ctx, sorted_inputs_t.dim_size(0) == values_t.dim_size(0),\\n                 Status(error::INVALID_ARGUMENT,'}}",
            "message_norm":"prevent check-fail\/heap oob in upperbound and lowerbound\n\npiperorigin-revid: 387738073\nchange-id: iee74de95ddad18440d052a75a5a1cb67544f490a",
            "language":"en",
            "entities":"[('prevent', 'ACTION', ''), ('heap oob', 'SECWORD', ''), ('387738073', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/core\/kernels\/searchsorted_op.cc'])",
            "num_files":1.0,
            "patch_content":"From 42459e4273c2e47a3232cc16c4f4fff3b3a35c38 Mon Sep 17 00:00:00 2001\nFrom: Mihai Maruseac <mihaimaruseac@google.com>\nDate: Thu, 29 Jul 2021 22:25:05 -0700\nSubject: [PATCH] Prevent CHECK-fail\/heap OOB in UpperBound and LowerBound\n\nPiperOrigin-RevId: 387738073\nChange-Id: Iee74de95ddad18440d052a75a5a1cb67544f490a\n---\n tensorflow\/core\/kernels\/searchsorted_op.cc | 8 ++++++++\n 1 file changed, 8 insertions(+)\n\ndiff --git a\/tensorflow\/core\/kernels\/searchsorted_op.cc b\/tensorflow\/core\/kernels\/searchsorted_op.cc\nindex 01e221dc471c4d..5f075a6a540e9f 100644\n--- a\/tensorflow\/core\/kernels\/searchsorted_op.cc\n+++ b\/tensorflow\/core\/kernels\/searchsorted_op.cc\n@@ -86,6 +86,10 @@ class UpperBoundOp : public OpKernel {\n     const Tensor& sorted_inputs_t = ctx->input(0);\n     const Tensor& values_t = ctx->input(1);\n \n+    \/\/ inputs must be at least a matrix\n+    OP_REQUIRES(\n+        ctx, sorted_inputs_t.shape().dims() >= 2,\n+        errors::InvalidArgument(\"sorted input argument must be a matrix\"));\n     \/\/ must have same batch dim_size for both\n     OP_REQUIRES(ctx, sorted_inputs_t.dim_size(0) == values_t.dim_size(0),\n                 Status(error::INVALID_ARGUMENT,\n@@ -127,6 +131,10 @@ class LowerBoundOp : public OpKernel {\n     const Tensor& sorted_inputs_t = ctx->input(0);\n     const Tensor& values_t = ctx->input(1);\n \n+    \/\/ inputs must be at least a matrix\n+    OP_REQUIRES(\n+        ctx, sorted_inputs_t.shape().dims() >= 2,\n+        errors::InvalidArgument(\"sorted input argument must be a matrix\"));\n     \/\/ must have same batch dim_size for both\n     OP_REQUIRES(ctx, sorted_inputs_t.dim_size(0) == values_t.dim_size(0),\n                 Status(error::INVALID_ARGUMENT,"
        },
        {
            "index":96,
            "vuln_id":"GHSA-q7q9-w24q-cpgh",
            "cwe_id":"{'CWE-79'}",
            "score":6.1,
            "chain":"{'https:\/\/github.com\/apache\/ambari\/pull\/3040\/commits\/a9cfdb9dcce63a3494c07c81ebb2cf8da218a210'}",
            "dataset":"osv",
            "summary":"Cross-site Scripting (XSS) in Apache Ambari Views A cross-site scripting issue was found in Apache Ambari Views. This was addressed in Apache Ambari 2.7.4.",
            "published_date":"2022-01-06",
            "chain_len":1,
            "project":"https:\/\/github.com\/apache\/ambari",
            "commit_href":"https:\/\/github.com\/apache\/ambari\/pull\/3040\/commits\/a9cfdb9dcce63a3494c07c81ebb2cf8da218a210",
            "commit_sha":"a9cfdb9dcce63a3494c07c81ebb2cf8da218a210",
            "patch":"SINGLE",
            "chain_ord":"['a9cfdb9dcce63a3494c07c81ebb2cf8da218a210']",
            "before_first_fix_commit":"{'9e9ddf752f36aa9340d6dfdc23ecccfcd646fc53'}",
            "last_fix_commit":"a9cfdb9dcce63a3494c07c81ebb2cf8da218a210",
            "chain_ord_pos":1.0,
            "commit_datetime":"07\/01\/2019, 11:43:05",
            "message":"AMBARI-25329. Ambari breadcrumbs xss vulnerability",
            "author":"Alex Antonenko",
            "comments":null,
            "stats":"{'additions': 12, 'deletions': 2, 'total': 14}",
            "files":"{'ambari-web\/app\/views\/common\/breadcrumbs_view.js': {'additions': 12, 'deletions': 2, 'changes': 14, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/apache\/ambari\/raw\/a9cfdb9dcce63a3494c07c81ebb2cf8da218a210\/ambari-web%2Fapp%2Fviews%2Fcommon%2Fbreadcrumbs_view.js', 'patch': \"@@ -149,8 +149,16 @@ App.BreadcrumbItem = Em.Object.extend({\\n   createLabel() {\\n     let label = this.get('label');\\n     let labelBindingPath = this.get('labelBindingPath');\\n+    let formattedLabel;\\n+\\n+    if (labelBindingPath) {\\n+      formattedLabel = Ember.Handlebars.Utils.escapeExpression(App.get(_getLabelPathWithoutApp(labelBindingPath)));\\n+    } else{\\n+      formattedLabel = label;\\n+    }\\n+\\n+\\n \\n-    let formattedLabel = labelBindingPath ? App.get(_getLabelPathWithoutApp(labelBindingPath)) : label;\\n     this.set('formattedLabel', this.labelPostFormat(formattedLabel));\\n   },\\n \\n@@ -216,7 +224,9 @@ App.BreadcrumbsView = Em.View.extend({\\n       }\\n       currentState = currentState.get('parentState');\\n     }\\n-    items = items.reverse().map(item => App.BreadcrumbItem.extend(item).create());\\n+    items.reverse();\\n+    items.slice(1).forEach(item => item.label = Ember.Handlebars.Utils.escapeExpression(item.label));\\n+    items = items.map(item => App.BreadcrumbItem.extend(item).create());\\n     if (items.length) {\\n       items.get('lastObject').setProperties({\\n         disabled: true,\"}}",
            "message_norm":"ambari-25329. ambari breadcrumbs xss vulnerability",
            "language":"ca",
            "entities":"[('xss', 'SECWORD', ''), ('vulnerability', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['ambari-web\/app\/views\/common\/breadcrumbs_view.js'])",
            "num_files":1.0,
            "patch_content":"From a9cfdb9dcce63a3494c07c81ebb2cf8da218a210 Mon Sep 17 00:00:00 2001\nFrom: Alex Antonenko <aantonenko@hortonworks.com>\nDate: Mon, 1 Jul 2019 14:43:05 +0300\nSubject: [PATCH] AMBARI-25329. Ambari breadcrumbs xss vulnerability\n\n---\n ambari-web\/app\/views\/common\/breadcrumbs_view.js | 14 ++++++++++++--\n 1 file changed, 12 insertions(+), 2 deletions(-)\n\ndiff --git a\/ambari-web\/app\/views\/common\/breadcrumbs_view.js b\/ambari-web\/app\/views\/common\/breadcrumbs_view.js\nindex ec6e6a64efb..31190c5faf2 100644\n--- a\/ambari-web\/app\/views\/common\/breadcrumbs_view.js\n+++ b\/ambari-web\/app\/views\/common\/breadcrumbs_view.js\n@@ -149,8 +149,16 @@ App.BreadcrumbItem = Em.Object.extend({\n   createLabel() {\n     let label = this.get('label');\n     let labelBindingPath = this.get('labelBindingPath');\n+    let formattedLabel;\n+\n+    if (labelBindingPath) {\n+      formattedLabel = Ember.Handlebars.Utils.escapeExpression(App.get(_getLabelPathWithoutApp(labelBindingPath)));\n+    } else{\n+      formattedLabel = label;\n+    }\n+\n+\n \n-    let formattedLabel = labelBindingPath ? App.get(_getLabelPathWithoutApp(labelBindingPath)) : label;\n     this.set('formattedLabel', this.labelPostFormat(formattedLabel));\n   },\n \n@@ -216,7 +224,9 @@ App.BreadcrumbsView = Em.View.extend({\n       }\n       currentState = currentState.get('parentState');\n     }\n-    items = items.reverse().map(item => App.BreadcrumbItem.extend(item).create());\n+    items.reverse();\n+    items.slice(1).forEach(item => item.label = Ember.Handlebars.Utils.escapeExpression(item.label));\n+    items = items.map(item => App.BreadcrumbItem.extend(item).create());\n     if (items.length) {\n       items.get('lastObject').setProperties({\n         disabled: true,"
        },
        {
            "index":80,
            "vuln_id":"GHSA-pc58-wgmc-hfjr",
            "cwe_id":"{'CWE-1321'}",
            "score":7.5,
            "chain":"{'https:\/\/github.com\/mout\/mout\/commit\/3fecf1333e6d71ae72edf48c71dc665e40df7605'}",
            "dataset":"osv",
            "summary":"Prototype Pollution in mout This affects all versions of package mout. The deepFillIn function can be used to 'fill missing properties recursively', while the deepMixIn 'mixes objects into the target object, recursively mixing existing child objects as well'. In both cases, the key used to access the target object recursively is not checked, leading to a Prototype Pollution.",
            "published_date":"2022-02-09",
            "chain_len":1,
            "project":"https:\/\/github.com\/mout\/mout",
            "commit_href":"https:\/\/github.com\/mout\/mout\/commit\/3fecf1333e6d71ae72edf48c71dc665e40df7605",
            "commit_sha":"3fecf1333e6d71ae72edf48c71dc665e40df7605",
            "patch":"SINGLE",
            "chain_ord":"['3fecf1333e6d71ae72edf48c71dc665e40df7605']",
            "before_first_fix_commit":"{'397fa131ec8090e305397d2db2d607c04440c2f3', '2189378ed476d34b3cd39ba7f1199dbf12b8e51b'}",
            "last_fix_commit":"3fecf1333e6d71ae72edf48c71dc665e40df7605",
            "chain_ord_pos":1.0,
            "commit_datetime":"07\/15\/2021, 18:58:13",
            "message":"Merge pull request #270 from 418sec\/1-npm-mout\n\nSecurity Fix for Prototype Pollution - huntr.dev",
            "author":"Mathias Paumgarten",
            "comments":"{'com_1': {'author': 'andrew-itscript', 'datetime': '08\/31\/2021, 08:47:19', 'body': '@roboshoes when do you plan to publish these changes to npm?'}, 'com_2': {'author': 'roboshoes', 'datetime': '09\/01\/2021, 17:49:50', 'body': 'Sorry for the delay. Has been released under [`v1.2.3`](https:\/\/www.npmjs.com\/package\/mout\/v\/1.2.3)'}}",
            "stats":"{'additions': 4, 'deletions': 0, 'total': 4}",
            "files":"{'src\/object\/set.js': {'additions': 4, 'deletions': 0, 'changes': 4, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/mout\/mout\/raw\/3fecf1333e6d71ae72edf48c71dc665e40df7605\/src%2Fobject%2Fset.js', 'patch': '@@ -4,6 +4,10 @@ define([\\'.\/namespace\\'], function (namespace) {\\n      * set \"nested\" object property\\n      *\/\\n     function set(obj, prop, val){\\n+        \/\/ prototype pollution mitigation\\n+        if(prop.includes(\\'__proto__\\') || prop.includes(\\'prototype\\') || prop.includes(\\'constructor\\')) {\\n+            return false;\\n+        }\\n         var parts = (\/^(.+)\\\\.(.+)$\/).exec(prop);\\n         if (parts){\\n             namespace(obj, parts[1])[parts[2]] = val;'}}",
            "message_norm":"merge pull request #270 from 418sec\/1-npm-mout\n\nsecurity fix for prototype pollution - huntr.dev",
            "language":"en",
            "entities":"[('#270', 'ISSUE', ''), ('security', 'SECWORD', ''), ('prototype pollution', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['src\/object\/set.js'])",
            "num_files":1.0,
            "patch_content":"From bd1821223989e815bc7dac1217bc7859777ce0bf Mon Sep 17 00:00:00 2001\nFrom: Asjid Kalam <asjid.kalam@gmail.com>\nDate: Mon, 14 Dec 2020 21:04:27 +0530\nSubject: [PATCH] fixed prototype pollution\n\n---\n src\/object\/set.js | 4 ++++\n 1 file changed, 4 insertions(+)\n\ndiff --git a\/src\/object\/set.js b\/src\/object\/set.js\nindex b8fa25a3..a38f332e 100644\n--- a\/src\/object\/set.js\n+++ b\/src\/object\/set.js\n@@ -4,6 +4,10 @@ define(['.\/namespace'], function (namespace) {\n      * set \"nested\" object property\n      *\/\n     function set(obj, prop, val){\n+        \/\/ prototype pollution mitigation\n+        if(prop.includes('__proto__') || prop.includes('prototype') || prop.includes('constructor')) {\n+            return false;\n+        }\n         var parts = (\/^(.+)\\.(.+)$\/).exec(prop);\n         if (parts){\n             namespace(obj, parts[1])[parts[2]] = val;"
        },
        {
            "index":851,
            "vuln_id":"GHSA-vcfc-9wcp-j623",
            "cwe_id":"{'CWE-79'}",
            "score":6.1,
            "chain":"{'https:\/\/github.com\/ServiceStack\/ServiceStack\/commit\/a0e0d7de20f5d1712f1793f925496def4383c610'}",
            "dataset":"osv",
            "summary":"Cross site scripting attack in ServiceStack Framework ServiceStack ServiceStack Framework 4.5.14 is affected by: Cross Site Scripting (XSS). The impact is: JavaScrpit is reflected in the server response, hence executed by the browser. The component is: the query used in the GET request is prone. The attack vector is: Since there is no server-side validation and If Browser encoding is bypassed, the victim is affected when opening a crafted URL. The fixed version is: 5.2.0.",
            "published_date":"2022-05-24",
            "chain_len":1,
            "project":"https:\/\/github.com\/ServiceStack\/ServiceStack",
            "commit_href":"https:\/\/github.com\/ServiceStack\/ServiceStack\/commit\/a0e0d7de20f5d1712f1793f925496def4383c610",
            "commit_sha":"a0e0d7de20f5d1712f1793f925496def4383c610",
            "patch":"SINGLE",
            "chain_ord":"['a0e0d7de20f5d1712f1793f925496def4383c610']",
            "before_first_fix_commit":"{'2c6fbe4554014ead15b7b188bb748b655b30807c'}",
            "last_fix_commit":"a0e0d7de20f5d1712f1793f925496def4383c610",
            "chain_ord_pos":1.0,
            "commit_datetime":"08\/17\/2018, 15:05:59",
            "message":"HtmlEncode Raw URL in HtmlFormat snapshot",
            "author":"Demis Bellot",
            "comments":null,
            "stats":"{'additions': 3, 'deletions': 2, 'total': 5}",
            "files":"{'src\/ServiceStack\/Formats\/HtmlFormat.cs': {'additions': 3, 'deletions': 2, 'changes': 5, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/ServiceStack\/ServiceStack\/raw\/a0e0d7de20f5d1712f1793f925496def4383c610\/src%2FServiceStack%2FFormats%2FHtmlFormat.cs', 'patch': '@@ -90,12 +90,13 @@ public async Task SerializeToStreamAsync(IRequest req, object response, Stream o\\n             {\\n                 \/\/ Serialize then escape any potential script tags to avoid XSS when displaying as HTML\\n                 var json = JsonDataContractSerializer.Instance.SerializeToString(dto) ?? \"null\";\\n-                json = json.Replace(\"<\", \"&lt;\").Replace(\">\", \"&gt;\");\\n+                json = json.HtmlEncode();\\n \\n                 var url = req.ResolveAbsoluteUrl()\\n                     .Replace(\"format=html\", \"\")\\n                     .Replace(\"format=shtm\", \"\")\\n-                    .TrimEnd(\\'?\\', \\'&\\');\\n+                    .TrimEnd(\\'?\\', \\'&\\')\\n+                    .HtmlEncode();\\n \\n                 url += url.Contains(\"?\") ? \"&\" : \"?\";'}}",
            "message_norm":"htmlencode raw url in htmlformat snapshot",
            "language":"en",
            "entities":"[('htmlencode', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['src\/ServiceStack\/Formats\/HtmlFormat.cs'])",
            "num_files":1.0,
            "patch_content":"From a0e0d7de20f5d1712f1793f925496def4383c610 Mon Sep 17 00:00:00 2001\nFrom: Demis Bellot <demis.bellot@gmail.com>\nDate: Fri, 17 Aug 2018 11:05:59 -0400\nSubject: [PATCH] HtmlEncode Raw URL in HtmlFormat snapshot\n\n---\n src\/ServiceStack\/Formats\/HtmlFormat.cs | 5 +++--\n 1 file changed, 3 insertions(+), 2 deletions(-)\n\ndiff --git a\/src\/ServiceStack\/Formats\/HtmlFormat.cs b\/src\/ServiceStack\/Formats\/HtmlFormat.cs\nindex e62df99e576..482955e38fc 100644\n--- a\/src\/ServiceStack\/Formats\/HtmlFormat.cs\n+++ b\/src\/ServiceStack\/Formats\/HtmlFormat.cs\n@@ -90,12 +90,13 @@ public async Task SerializeToStreamAsync(IRequest req, object response, Stream o\n             {\n                 \/\/ Serialize then escape any potential script tags to avoid XSS when displaying as HTML\n                 var json = JsonDataContractSerializer.Instance.SerializeToString(dto) ?? \"null\";\n-                json = json.Replace(\"<\", \"&lt;\").Replace(\">\", \"&gt;\");\n+                json = json.HtmlEncode();\n \n                 var url = req.ResolveAbsoluteUrl()\n                     .Replace(\"format=html\", \"\")\n                     .Replace(\"format=shtm\", \"\")\n-                    .TrimEnd('?', '&');\n+                    .TrimEnd('?', '&')\n+                    .HtmlEncode();\n \n                 url += url.Contains(\"?\") ? \"&\" : \"?\";"
        },
        {
            "index":784,
            "vuln_id":"GHSA-g6vq-wc8w-4g69",
            "cwe_id":"{'CWE-352'}",
            "score":4.3,
            "chain":"{'https:\/\/github.com\/firefly-iii\/firefly-iii\/commit\/518b4ba5a7a56760902758ae0a2c6a392c2f4d37'}",
            "dataset":"osv",
            "summary":"firefly-iii is vulnerable to Cross-Site Request Forgery (CSRF) firefly-iii is vulnerable to Cross-Site Request Forgery (CSRF).",
            "published_date":"2021-12-06",
            "chain_len":1,
            "project":"https:\/\/github.com\/firefly-iii\/firefly-iii",
            "commit_href":"https:\/\/github.com\/firefly-iii\/firefly-iii\/commit\/518b4ba5a7a56760902758ae0a2c6a392c2f4d37",
            "commit_sha":"518b4ba5a7a56760902758ae0a2c6a392c2f4d37",
            "patch":"SINGLE",
            "chain_ord":"['518b4ba5a7a56760902758ae0a2c6a392c2f4d37']",
            "before_first_fix_commit":"{'0f9c1b9427b946b5eb580112edfcb3ed6a812970'}",
            "last_fix_commit":"518b4ba5a7a56760902758ae0a2c6a392c2f4d37",
            "chain_ord_pos":1.0,
            "commit_datetime":"11\/24\/2021, 18:22:07",
            "message":"Fix CSRF issues",
            "author":"James Cole",
            "comments":null,
            "stats":"{'additions': 6, 'deletions': 5, 'total': 11}",
            "files":"{'routes\/web.php': {'additions': 6, 'deletions': 5, 'changes': 11, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/firefly-iii\/firefly-iii\/raw\/518b4ba5a7a56760902758ae0a2c6a392c2f4d37\/routes%2Fweb.php', 'patch': \"@@ -213,7 +213,7 @@ static function () {\\n     ['middleware' => 'user-full-auth', 'namespace' => 'FireflyIII\\\\Http\\\\Controllers', 'prefix' => 'subscriptions', 'as' => 'subscriptions.'],\\n     static function () {\\n         Route::get('', ['uses' => 'Bill\\\\IndexController@index', 'as' => 'index']);\\n-        Route::get('rescan\/{bill}', ['uses' => 'Bill\\\\ShowController@rescan', 'as' => 'rescan']);\\n+        Route::post('rescan\/{bill}', ['uses' => 'Bill\\\\ShowController@rescan', 'as' => 'rescan']);\\n         Route::get('create', ['uses' => 'Bill\\\\CreateController@create', 'as' => 'create']);\\n         Route::get('edit\/{bill}', ['uses' => 'Bill\\\\EditController@edit', 'as' => 'edit']);\\n         Route::get('delete\/{bill}', ['uses' => 'Bill\\\\DeleteController@delete', 'as' => 'delete']);\\n@@ -649,7 +649,7 @@ static function () {\\n         Route::get('rate\/{fromCurrencyCode}\/{toCurrencyCode}\/{date}', ['uses' => 'Json\\\\ExchangeController@getRate', 'as' => 'rate']);\\n \\n         \/\/ intro things:\\n-        Route::any('intro\/finished\/{route}\/{specificPage?}', ['uses' => 'Json\\\\IntroController@postFinished', 'as' => 'intro.finished']);\\n+        Route::post('intro\/finished\/{route}\/{specificPage?}', ['uses' => 'Json\\\\IntroController@postFinished', 'as' => 'intro.finished']);\\n         Route::post('intro\/enable\/{route}\/{specificPage?}', ['uses' => 'Json\\\\IntroController@postEnable', 'as' => 'intro.enable']);\\n         Route::get('intro\/{route}\/{specificPage?}', ['uses' => 'Json\\\\IntroController@getIntroSteps', 'as' => 'intro']);\\n     }\\n@@ -726,14 +726,15 @@ static function () {\\n         Route::post('enable2FA', ['uses' => 'ProfileController@enable2FA', 'as' => 'enable2FA']);\\n         Route::get('2fa\/code', ['uses' => 'ProfileController@code', 'as' => 'code']);\\n         Route::post('2fa\/code', ['uses' => 'ProfileController@postCode', 'as' => 'code.store']);\\n-        Route::get('\/delete-code', ['uses' => 'ProfileController@deleteCode', 'as' => 'delete-code']);\\n-        Route::get('2fa\/new-codes', ['uses' => 'ProfileController@newBackupCodes', 'as' => 'new-backup-codes']);\\n+        Route::post('\/delete-code', ['uses' => 'ProfileController@deleteCode', 'as' => 'delete-code']);\\n+        Route::post('2fa\/new-codes', ['uses' => 'ProfileController@newBackupCodes', 'as' => 'new-backup-codes']);\\n \\n     }\\n );\\n \\n \/**\\n  * Recurring Transactions Controller.\\n+ * \\n  *\/\\n Route::group(\\n     ['middleware' => 'user-full-auth', 'namespace' => 'FireflyIII\\\\Http\\\\Controllers', 'prefix' => 'recurring', 'as' => 'recurring.'],\\n@@ -1078,7 +1079,7 @@ static function () {\\n \/\/ See reference nr. 6\\n         Route::post('store\/{tj}', ['uses' => 'LinkController@store', 'as' => 'store']);\\n         Route::get('delete\/{journalLink}', ['uses' => 'LinkController@delete', 'as' => 'delete']);\\n-        Route::get('switch\/{journalLink}', ['uses' => 'LinkController@switchLink', 'as' => 'switch']);\\n+        Route::post('switch\/{journalLink}', ['uses' => 'LinkController@switchLink', 'as' => 'switch']);\\n \\n         Route::post('destroy\/{journalLink}', ['uses' => 'LinkController@destroy', 'as' => 'destroy']);\\n     }\"}}",
            "message_norm":"fix csrf issues",
            "language":"en",
            "entities":"[('fix', 'ACTION', ''), ('csrf', 'SECWORD', ''), ('issues', 'FLAW', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['routes\/web.php'])",
            "num_files":1.0,
            "patch_content":"From 518b4ba5a7a56760902758ae0a2c6a392c2f4d37 Mon Sep 17 00:00:00 2001\nFrom: James Cole <james@firefly-iii.org>\nDate: Wed, 24 Nov 2021 19:22:07 +0100\nSubject: [PATCH] Fix CSRF issues\n\n---\n routes\/web.php | 11 ++++++-----\n 1 file changed, 6 insertions(+), 5 deletions(-)\n\ndiff --git a\/routes\/web.php b\/routes\/web.php\nindex 6eb7ddb5143..1c293863662 100644\n--- a\/routes\/web.php\n+++ b\/routes\/web.php\n@@ -213,7 +213,7 @@ static function () {\n     ['middleware' => 'user-full-auth', 'namespace' => 'FireflyIII\\Http\\Controllers', 'prefix' => 'subscriptions', 'as' => 'subscriptions.'],\n     static function () {\n         Route::get('', ['uses' => 'Bill\\IndexController@index', 'as' => 'index']);\n-        Route::get('rescan\/{bill}', ['uses' => 'Bill\\ShowController@rescan', 'as' => 'rescan']);\n+        Route::post('rescan\/{bill}', ['uses' => 'Bill\\ShowController@rescan', 'as' => 'rescan']);\n         Route::get('create', ['uses' => 'Bill\\CreateController@create', 'as' => 'create']);\n         Route::get('edit\/{bill}', ['uses' => 'Bill\\EditController@edit', 'as' => 'edit']);\n         Route::get('delete\/{bill}', ['uses' => 'Bill\\DeleteController@delete', 'as' => 'delete']);\n@@ -649,7 +649,7 @@ static function () {\n         Route::get('rate\/{fromCurrencyCode}\/{toCurrencyCode}\/{date}', ['uses' => 'Json\\ExchangeController@getRate', 'as' => 'rate']);\n \n         \/\/ intro things:\n-        Route::any('intro\/finished\/{route}\/{specificPage?}', ['uses' => 'Json\\IntroController@postFinished', 'as' => 'intro.finished']);\n+        Route::post('intro\/finished\/{route}\/{specificPage?}', ['uses' => 'Json\\IntroController@postFinished', 'as' => 'intro.finished']);\n         Route::post('intro\/enable\/{route}\/{specificPage?}', ['uses' => 'Json\\IntroController@postEnable', 'as' => 'intro.enable']);\n         Route::get('intro\/{route}\/{specificPage?}', ['uses' => 'Json\\IntroController@getIntroSteps', 'as' => 'intro']);\n     }\n@@ -726,14 +726,15 @@ static function () {\n         Route::post('enable2FA', ['uses' => 'ProfileController@enable2FA', 'as' => 'enable2FA']);\n         Route::get('2fa\/code', ['uses' => 'ProfileController@code', 'as' => 'code']);\n         Route::post('2fa\/code', ['uses' => 'ProfileController@postCode', 'as' => 'code.store']);\n-        Route::get('\/delete-code', ['uses' => 'ProfileController@deleteCode', 'as' => 'delete-code']);\n-        Route::get('2fa\/new-codes', ['uses' => 'ProfileController@newBackupCodes', 'as' => 'new-backup-codes']);\n+        Route::post('\/delete-code', ['uses' => 'ProfileController@deleteCode', 'as' => 'delete-code']);\n+        Route::post('2fa\/new-codes', ['uses' => 'ProfileController@newBackupCodes', 'as' => 'new-backup-codes']);\n \n     }\n );\n \n \/**\n  * Recurring Transactions Controller.\n+ * \n  *\/\n Route::group(\n     ['middleware' => 'user-full-auth', 'namespace' => 'FireflyIII\\Http\\Controllers', 'prefix' => 'recurring', 'as' => 'recurring.'],\n@@ -1078,7 +1079,7 @@ static function () {\n \/\/ See reference nr. 6\n         Route::post('store\/{tj}', ['uses' => 'LinkController@store', 'as' => 'store']);\n         Route::get('delete\/{journalLink}', ['uses' => 'LinkController@delete', 'as' => 'delete']);\n-        Route::get('switch\/{journalLink}', ['uses' => 'LinkController@switchLink', 'as' => 'switch']);\n+        Route::post('switch\/{journalLink}', ['uses' => 'LinkController@switchLink', 'as' => 'switch']);\n \n         Route::post('destroy\/{journalLink}', ['uses' => 'LinkController@destroy', 'as' => 'destroy']);\n     }"
        },
        {
            "index":40,
            "vuln_id":"GHSA-whv6-rj84-2vh2",
            "cwe_id":"{'CWE-79'}",
            "score":0.0,
            "chain":"{'https:\/\/github.com\/juliushaertl\/nextcloud-vue-collections\/commit\/8ec1fca214f003538cec4137792ede928f25f583'}",
            "dataset":"osv",
            "summary":"Cross-Site Scripting in nextcloud-vue-collections Versions of `nextcloud-vue-collections` prior to 0.4.2 are vulnerable to Cross-Site Scripting (XSS).  The `v-tooltip` component has an insecure `defaultHTML` configuration that allows arbitrary JavaScript to be injected in the tooltip of a collection item. This allows attackers to execute arbitrary code in a victim's browser.\n\n\n## Recommendation\n\nUpgrade to version 0.4.2 or later.",
            "published_date":"2020-09-04",
            "chain_len":1,
            "project":"https:\/\/github.com\/juliushaertl\/nextcloud-vue-collections",
            "commit_href":"https:\/\/github.com\/juliushaertl\/nextcloud-vue-collections\/commit\/8ec1fca214f003538cec4137792ede928f25f583",
            "commit_sha":"8ec1fca214f003538cec4137792ede928f25f583",
            "patch":"SINGLE",
            "chain_ord":"['8ec1fca214f003538cec4137792ede928f25f583']",
            "before_first_fix_commit":"{'1d55cc3b462bc344de6cfbe45d590d0c2f99fc1a'}",
            "last_fix_commit":"8ec1fca214f003538cec4137792ede928f25f583",
            "chain_ord_pos":1.0,
            "commit_datetime":"07\/29\/2019, 13:41:42",
            "message":"Force defaultHtml setting of v-tooltip to be disabled\n\nSigned-off-by: Julius H\u00e4rtl <jus@bitgrid.net>",
            "author":"Julius H\u00e4rtl",
            "comments":null,
            "stats":"{'additions': 3, 'deletions': 0, 'total': 3}",
            "files":"{'src\/components\/CollectionListItem.vue': {'additions': 3, 'deletions': 0, 'changes': 3, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/nextcloud\/nextcloud-vue-collections\/raw\/8ec1fca214f003538cec4137792ede928f25f583\/src%2Fcomponents%2FCollectionListItem.vue', 'patch': \"@@ -60,6 +60,9 @@ import Action from 'nextcloud-vue\/dist\/Components\/Action'\\n import Avatar from 'nextcloud-vue\/dist\/Components\/Avatar'\\n import Tooltip from 'nextcloud-vue\/dist\/Directives\/Tooltip'\\n \\n+Tooltip.options.defaultHtml = false\\n+\\n+\\n export default {\\n \\tname: 'CollectionListItem',\\n \\tcomponents: {\"}}",
            "message_norm":"force defaulthtml setting of v-tooltip to be disabled\n\nsigned-off-by: julius h\u00e4rtl <jus@bitgrid.net>",
            "language":"en",
            "entities":"[('jus@bitgrid.net', 'EMAIL', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['src\/components\/CollectionListItem.vue'])",
            "num_files":1.0,
            "patch_content":"From 8ec1fca214f003538cec4137792ede928f25f583 Mon Sep 17 00:00:00 2001\nFrom: =?UTF-8?q?Julius=20H=C3=A4rtl?= <jus@bitgrid.net>\nDate: Mon, 29 Jul 2019 15:41:42 +0200\nSubject: [PATCH] Force defaultHtml setting of v-tooltip to be disabled\nMIME-Version: 1.0\nContent-Type: text\/plain; charset=UTF-8\nContent-Transfer-Encoding: 8bit\n\nSigned-off-by: Julius H\u00e4rtl <jus@bitgrid.net>\n---\n src\/components\/CollectionListItem.vue | 3 +++\n 1 file changed, 3 insertions(+)\n\ndiff --git a\/src\/components\/CollectionListItem.vue b\/src\/components\/CollectionListItem.vue\nindex 146e1742..21de93cd 100644\n--- a\/src\/components\/CollectionListItem.vue\n+++ b\/src\/components\/CollectionListItem.vue\n@@ -60,6 +60,9 @@ import Action from 'nextcloud-vue\/dist\/Components\/Action'\n import Avatar from 'nextcloud-vue\/dist\/Components\/Avatar'\n import Tooltip from 'nextcloud-vue\/dist\/Directives\/Tooltip'\n \n+Tooltip.options.defaultHtml = false\n+\n+\n export default {\n \tname: 'CollectionListItem',\n \tcomponents: {"
        },
        {
            "index":498,
            "vuln_id":"GHSA-xw79-hhv6-578c",
            "cwe_id":"{'CWE-79'}",
            "score":0.0,
            "chain":"{'https:\/\/github.com\/zeit\/serve-handler\/commit\/65b4d4183a31a8076c78c40118acb0ca1b64f620'}",
            "dataset":"osv",
            "summary":"Cross-Site Scripting in serve Versions of `serve` prior to 10.0.2 are vulnerable to Cross-Site Scripting (XSS). The package does not encode output, allowing attackers to execute arbitrary JavaScript in the victim's browser if user-supplied input is rendered.\n\n\n## Recommendation\n\nUpgrade to version 10.0.2 or later.",
            "published_date":"2020-09-11",
            "chain_len":1,
            "project":"https:\/\/github.com\/zeit\/serve-handler",
            "commit_href":"https:\/\/github.com\/zeit\/serve-handler\/commit\/65b4d4183a31a8076c78c40118acb0ca1b64f620",
            "commit_sha":"65b4d4183a31a8076c78c40118acb0ca1b64f620",
            "patch":"SINGLE",
            "chain_ord":"['65b4d4183a31a8076c78c40118acb0ca1b64f620']",
            "before_first_fix_commit":"{'2b3be81a46e09fc5f8bc2c69a5311d439dac74af'}",
            "last_fix_commit":"65b4d4183a31a8076c78c40118acb0ca1b64f620",
            "chain_ord_pos":1.0,
            "commit_datetime":"09\/24\/2018, 17:05:10",
            "message":"Interpolate template variables correctly (#64)",
            "author":"Leo Lamprecht",
            "comments":null,
            "stats":"{'additions': 4, 'deletions': 4, 'total': 8}",
            "files":"{'src\/directory.jst': {'additions': 4, 'deletions': 4, 'changes': 8, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/vercel\/serve-handler\/raw\/65b4d4183a31a8076c78c40118acb0ca1b64f620\/src%2Fdirectory.jst', 'patch': '@@ -4,7 +4,7 @@\\n     <meta charset=\"utf-8\">\\n     <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\\n \\n-    <title>Files within {{=it.directory}}<\/title>\\n+    <title>Files within {{!it.directory}}<\/title>\\n \\n \\t<style>\\n \\t\\tbody {\\n@@ -187,7 +187,7 @@\\n           <i>Index of&nbsp;<\/i>\\n \\n           {{~it.paths :value:index}}\\n-            <a href=\"\/{{=value.url}}\">{{=value.name}}<\/a>\\n+            <a href=\"\/{{!value.url}}\">{{!value.name}}<\/a>\\n           {{~}}\\n         <\/h1>\\n \\n@@ -197,9 +197,9 @@\\n       <ul id=\"files\">\\n         {{~it.files :value:index}}\\n           <li>\\n-            <a href=\"{{=value.relative}}\" title=\"{{=value.title}}\" class=\"{{=value.ext}}\">{{=value.base}}<\/a>\\n+            <a href=\"{{!value.relative}}\" title=\"{{!value.title}}\" class=\"{{!value.ext}}\">{{!value.base}}<\/a>\\n \\t\\t\\t{{? value.size}}\\n-\\t\\t\\t\\t<i>{{=value.size}}<\/i>\\n+\\t\\t\\t\\t<i>{{!value.size}}<\/i>\\n \\t\\t\\t{{?}}\\n           <\/li>\\n         {{~}}'}}",
            "message_norm":"interpolate template variables correctly (#64)",
            "language":"en",
            "entities":"[('#64', 'ISSUE', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['src\/directory.jst'])",
            "num_files":1.0,
            "patch_content":"From 65b4d4183a31a8076c78c40118acb0ca1b64f620 Mon Sep 17 00:00:00 2001\nFrom: Leo Lamprecht <mindrun@icloud.com>\nDate: Mon, 24 Sep 2018 19:05:10 +0200\nSubject: [PATCH] Interpolate template variables correctly (#64)\n\n---\n src\/directory.jst | 8 ++++----\n 1 file changed, 4 insertions(+), 4 deletions(-)\n\ndiff --git a\/src\/directory.jst b\/src\/directory.jst\nindex 31c25ec..aa3021a 100644\n--- a\/src\/directory.jst\n+++ b\/src\/directory.jst\n@@ -4,7 +4,7 @@\n     <meta charset=\"utf-8\">\n     <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n \n-    <title>Files within {{=it.directory}}<\/title>\n+    <title>Files within {{!it.directory}}<\/title>\n \n \t<style>\n \t\tbody {\n@@ -187,7 +187,7 @@\n           <i>Index of&nbsp;<\/i>\n \n           {{~it.paths :value:index}}\n-            <a href=\"\/{{=value.url}}\">{{=value.name}}<\/a>\n+            <a href=\"\/{{!value.url}}\">{{!value.name}}<\/a>\n           {{~}}\n         <\/h1>\n \n@@ -197,9 +197,9 @@\n       <ul id=\"files\">\n         {{~it.files :value:index}}\n           <li>\n-            <a href=\"{{=value.relative}}\" title=\"{{=value.title}}\" class=\"{{=value.ext}}\">{{=value.base}}<\/a>\n+            <a href=\"{{!value.relative}}\" title=\"{{!value.title}}\" class=\"{{!value.ext}}\">{{!value.base}}<\/a>\n \t\t\t{{? value.size}}\n-\t\t\t\t<i>{{=value.size}}<\/i>\n+\t\t\t\t<i>{{!value.size}}<\/i>\n \t\t\t{{?}}\n           <\/li>\n         {{~}}"
        },
        {
            "index":836,
            "vuln_id":"GHSA-cwv3-863g-39vx",
            "cwe_id":"{'CWE-835', 'CWE-674'}",
            "score":7.3,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/c6173f5fe66cdbab74f4f869311fe6aae2ba35f4', 'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/9c1dc920d8ffb4893d6c9d27d1f039607b326743'}",
            "dataset":"osv",
            "summary":"Stack overflow due to looping TFLite subgraph ### Impact\nTFlite graphs must not have loops between nodes. However, this condition was not checked and an attacker could craft models that would result in infinite loop during evaluation. In certain cases, the infinite loop would be replaced by stack overflow due to too many recursive calls.\n\nFor example, the [`While` implementation](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/106d8f4fb89335a2c52d7c895b7a7485465ca8d9\/tensorflow\/lite\/kernels\/while.cc) could be tricked into a scneario where both the body and the loop subgraphs are the same. Evaluating one of the subgraphs means calling the `Eval` function for the other and this quickly exhaust all stack space.\n    \n### Patches \nWe have patched the issue in GitHub commit [9c1dc920d8ffb4893d6c9d27d1f039607b326743](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/9c1dc920d8ffb4893d6c9d27d1f039607b326743) (for the `While` operator) and in GitHub commit [c6173f5fe66cdbab74f4f869311fe6aae2ba35f4](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/c6173f5fe66cdbab74f4f869311fe6aae2ba35f4) (in general).\n    \nThe fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution \nThis vulnerability has been reported by members of the Aivul Team from Qihoo 360.",
            "published_date":"2021-05-21",
            "chain_len":2,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/9c1dc920d8ffb4893d6c9d27d1f039607b326743",
            "commit_sha":"9c1dc920d8ffb4893d6c9d27d1f039607b326743",
            "patch":"MULTI",
            "chain_ord":"['9c1dc920d8ffb4893d6c9d27d1f039607b326743', 'c6173f5fe66cdbab74f4f869311fe6aae2ba35f4']",
            "before_first_fix_commit":"{'46b80bd2a8943d5976dc83bd5c0322c0023255a7'}",
            "last_fix_commit":"c6173f5fe66cdbab74f4f869311fe6aae2ba35f4",
            "chain_ord_pos":1.0,
            "commit_datetime":"04\/28\/2021, 00:47:46",
            "message":"Prevent infinite loop\/stack overflow in TFLite `while` op.\n\nPiperOrigin-RevId: 370800333\nChange-Id: I6a2e4ff849da339545c449db2af7e11ce6ff02c3",
            "author":"Mihai Maruseac",
            "comments":null,
            "stats":"{'additions': 2, 'deletions': 0, 'total': 2}",
            "files":"{'tensorflow\/lite\/kernels\/while.cc': {'additions': 2, 'deletions': 0, 'changes': 2, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/9c1dc920d8ffb4893d6c9d27d1f039607b326743\/tensorflow%2Flite%2Fkernels%2Fwhile.cc', 'patch': '@@ -138,6 +138,8 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\\n   auto* subgraphs = this_subgraph->GetSubgraphs();\\n   TF_LITE_ENSURE(context, op_data->cond_subgraph_index < subgraphs->size());\\n   TF_LITE_ENSURE(context, op_data->body_subgraph_index < subgraphs->size());\\n+  TF_LITE_ENSURE(context,\\n+                 op_data->cond_subgraph_index != op_data->body_subgraph_index);\\n \\n   Subgraph* cond_subgraph = (*subgraphs)[op_data->cond_subgraph_index].get();\\n   Subgraph* body_subgraph = (*subgraphs)[op_data->body_subgraph_index].get();'}}",
            "message_norm":"prevent infinite loop\/stack overflow in tflite `while` op.\n\npiperorigin-revid: 370800333\nchange-id: i6a2e4ff849da339545c449db2af7e11ce6ff02c3",
            "language":"en",
            "entities":"[('prevent', 'ACTION', ''), ('infinite loop', 'SECWORD', ''), ('overflow', 'SECWORD', ''), ('370800333', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/lite\/kernels\/while.cc'])",
            "num_files":1.0,
            "patch_content":"From 9c1dc920d8ffb4893d6c9d27d1f039607b326743 Mon Sep 17 00:00:00 2001\nFrom: Mihai Maruseac <mihaimaruseac@google.com>\nDate: Tue, 27 Apr 2021 17:47:46 -0700\nSubject: [PATCH] Prevent infinite loop\/stack overflow in TFLite `while` op.\n\nPiperOrigin-RevId: 370800333\nChange-Id: I6a2e4ff849da339545c449db2af7e11ce6ff02c3\n---\n tensorflow\/lite\/kernels\/while.cc | 2 ++\n 1 file changed, 2 insertions(+)\n\ndiff --git a\/tensorflow\/lite\/kernels\/while.cc b\/tensorflow\/lite\/kernels\/while.cc\nindex e05959fe2a6825..74ab81c2a95993 100644\n--- a\/tensorflow\/lite\/kernels\/while.cc\n+++ b\/tensorflow\/lite\/kernels\/while.cc\n@@ -138,6 +138,8 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n   auto* subgraphs = this_subgraph->GetSubgraphs();\n   TF_LITE_ENSURE(context, op_data->cond_subgraph_index < subgraphs->size());\n   TF_LITE_ENSURE(context, op_data->body_subgraph_index < subgraphs->size());\n+  TF_LITE_ENSURE(context,\n+                 op_data->cond_subgraph_index != op_data->body_subgraph_index);\n \n   Subgraph* cond_subgraph = (*subgraphs)[op_data->cond_subgraph_index].get();\n   Subgraph* body_subgraph = (*subgraphs)[op_data->body_subgraph_index].get();"
        },
        {
            "index":548,
            "vuln_id":"GHSA-5pg2-qg87-vmj7",
            "cwe_id":"{'CWE-79'}",
            "score":5.4,
            "chain":"{'https:\/\/github.com\/microweber\/microweber\/commit\/9ebbb4dd35da74025ab6965f722829a7f8f86566'}",
            "dataset":"osv",
            "summary":"Cross-site Scripting in microweber Cross-site Scripting (XSS) - Stored in GitHub repository microweber\/microweber prior to 1.2.19.",
            "published_date":"2022-07-02",
            "chain_len":1,
            "project":"https:\/\/github.com\/microweber\/microweber",
            "commit_href":"https:\/\/github.com\/microweber\/microweber\/commit\/9ebbb4dd35da74025ab6965f722829a7f8f86566",
            "commit_sha":"9ebbb4dd35da74025ab6965f722829a7f8f86566",
            "patch":"SINGLE",
            "chain_ord":"['9ebbb4dd35da74025ab6965f722829a7f8f86566']",
            "before_first_fix_commit":"{'c2991b3a44896320a834a4b611257db587129645'}",
            "last_fix_commit":"9ebbb4dd35da74025ab6965f722829a7f8f86566",
            "chain_ord_pos":1.0,
            "commit_datetime":"07\/01\/2022, 08:07:47",
            "message":"update",
            "author":"Peter Ivanov",
            "comments":null,
            "stats":"{'additions': 4, 'deletions': 1, 'total': 5}",
            "files":"{'src\/MicroweberPackages\/App\/functions\/plupload.php': {'additions': 4, 'deletions': 1, 'changes': 5, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/microweber\/microweber\/raw\/9ebbb4dd35da74025ab6965f722829a7f8f86566\/src%2FMicroweberPackages%2FApp%2Ffunctions%2Fplupload.php', 'patch': \"@@ -336,9 +336,12 @@\\n \/\/ Make sure the fileName is unique but only if chunking is disabled\\n if ($chunks < 2 && file_exists($targetDir . DIRECTORY_SEPARATOR . $fileName)) {\\n     $ext = strrpos($fileName, '.');\\n+\\n     $fileName_a = substr($fileName, 0, $ext);\\n     $fileName_b = substr($fileName, $ext);\\n \\n+    $fileName_b = strtolower($fileName_b);\\n+\\n     $count = 1;\\n     while (file_exists($targetDir . DIRECTORY_SEPARATOR . $fileName_a . '_' . $count . $fileName_b)) {\\n         ++$count;\\n@@ -500,7 +503,7 @@\\n \\n     if (is_file($filePath) and !$chunks || $chunk == $chunks - 1) {\\n         $ext = get_file_extension($filePath);\\n-\\n+        $ext = strtolower($ext);\\n         if (function_exists('finfo_open') and function_exists('finfo_file')) {\\n             $finfo = finfo_open(FILEINFO_MIME_TYPE); \/\/ return mime type ala mimetype extension\\n             $mime = @finfo_file($finfo, $filePath);\"}}",
            "message_norm":"update",
            "language":"ro",
            "entities":"[('update', 'ACTION', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['src\/MicroweberPackages\/App\/functions\/plupload.php'])",
            "num_files":1.0,
            "patch_content":"From 9ebbb4dd35da74025ab6965f722829a7f8f86566 Mon Sep 17 00:00:00 2001\nFrom: Peter Ivanov <peter@microweber.com>\nDate: Fri, 1 Jul 2022 11:07:47 +0300\nSubject: [PATCH] update\n\n---\n src\/MicroweberPackages\/App\/functions\/plupload.php | 5 ++++-\n 1 file changed, 4 insertions(+), 1 deletion(-)\n\ndiff --git a\/src\/MicroweberPackages\/App\/functions\/plupload.php b\/src\/MicroweberPackages\/App\/functions\/plupload.php\nindex 3b6e31707cb..244980968ce 100644\n--- a\/src\/MicroweberPackages\/App\/functions\/plupload.php\n+++ b\/src\/MicroweberPackages\/App\/functions\/plupload.php\n@@ -336,9 +336,12 @@\n \/\/ Make sure the fileName is unique but only if chunking is disabled\n if ($chunks < 2 && file_exists($targetDir . DIRECTORY_SEPARATOR . $fileName)) {\n     $ext = strrpos($fileName, '.');\n+\n     $fileName_a = substr($fileName, 0, $ext);\n     $fileName_b = substr($fileName, $ext);\n \n+    $fileName_b = strtolower($fileName_b);\n+\n     $count = 1;\n     while (file_exists($targetDir . DIRECTORY_SEPARATOR . $fileName_a . '_' . $count . $fileName_b)) {\n         ++$count;\n@@ -500,7 +503,7 @@\n \n     if (is_file($filePath) and !$chunks || $chunk == $chunks - 1) {\n         $ext = get_file_extension($filePath);\n-\n+        $ext = strtolower($ext);\n         if (function_exists('finfo_open') and function_exists('finfo_file')) {\n             $finfo = finfo_open(FILEINFO_MIME_TYPE); \/\/ return mime type ala mimetype extension\n             $mime = @finfo_file($finfo, $filePath);"
        },
        {
            "index":816,
            "vuln_id":"GHSA-7ggw-h8pp-r95r",
            "cwe_id":"{'CWE-613'}",
            "score":0.0,
            "chain":"{'https:\/\/github.com\/octobercms\/library\/commit\/642f597489e6f644d4bd9a0c267e864cabead024'}",
            "dataset":"osv",
            "summary":"Session ID not invalidated after logout ### Impact\nWhen logging out, the session ID was not invalidated. This is not a problem while the user is logged out, but as soon as the user logs back in the old session ID would be valid again; which means that anyone that gained access to the old session cookie would be able to act as the logged in user. This is not a major concern for the majority of cases, since it requires a malicious party gaining access to the session cookie in the first place, but nevertheless has been fixed.\n\n### Patches\nIssue has been patched in Build 472 (v1.0.472) and v1.1.2.\n\n### Workarounds\nApply https:\/\/github.com\/octobercms\/library\/commit\/642f597489e6f644d4bd9a0c267e864cabead024 to your installation manually if unable to upgrade to Build 472 or v1.1.2.\n\n### References\n- Reported by Anisio (Brazilian Information Security Analyst)\n- http:\/\/cve.circl.lu\/cve\/CVE-2021-3311\n\n### For more information\nIf you have any questions or comments about this advisory:\n* Email us at [hello@octobercms.com](mailto:hello@octobercms.com)\n\n### Threat assessment:\n<img width=\"699\" alt=\"Screen Shot 2021-02-07 at 11 50 35 PM\" src=\"https:\/\/user-images.githubusercontent.com\/7253840\/107180881-51eaf000-699f-11eb-8828-333128faf2a6.png\">",
            "published_date":"2021-02-10",
            "chain_len":1,
            "project":"https:\/\/github.com\/octobercms\/library",
            "commit_href":"https:\/\/github.com\/octobercms\/library\/commit\/642f597489e6f644d4bd9a0c267e864cabead024",
            "commit_sha":"642f597489e6f644d4bd9a0c267e864cabead024",
            "patch":"SINGLE",
            "chain_ord":"['642f597489e6f644d4bd9a0c267e864cabead024']",
            "before_first_fix_commit":"{'e292d79ef2090f4d67a7d913d89c9d3597b0d334'}",
            "last_fix_commit":"642f597489e6f644d4bd9a0c267e864cabead024",
            "chain_ord_pos":1.0,
            "commit_datetime":"01\/30\/2021, 00:47:39",
            "message":"Invalidate the session ID to prevent reuse\n\n1. Good logs in\n2. Bad captures Good's session cookie\n3. Good logs out\n4. Session cookie no longer works\n5. Good logs in a second time\n6. ORIGINAL session cookie works (Bad is also signed in)",
            "author":"Samuel Georges",
            "comments":null,
            "stats":"{'additions': 1, 'deletions': 1, 'total': 2}",
            "files":"{'src\/Auth\/Manager.php': {'additions': 1, 'deletions': 1, 'changes': 2, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/octobercms\/library\/raw\/642f597489e6f644d4bd9a0c267e864cabead024\/src%2FAuth%2FManager.php', 'patch': '@@ -686,7 +686,7 @@ public function logout()\\n \\n         $this->user = null;\\n \\n-        Session::flush();\\n+        Session::invalidate();\\n         Cookie::queue(Cookie::forget($this->sessionKey));\\n     }'}}",
            "message_norm":"invalidate the session id to prevent reuse\n\n1. good logs in\n2. bad captures good's session cookie\n3. good logs out\n4. session cookie no longer works\n5. good logs in a second time\n6. original session cookie works (bad is also signed in)",
            "language":"en",
            "entities":"[('invalidate', 'ACTION', ''), ('prevent', 'ACTION', ''), ('cookie', 'SECWORD', ''), ('cookie', 'SECWORD', ''), ('cookie', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['src\/Auth\/Manager.php'])",
            "num_files":1.0,
            "patch_content":"From 642f597489e6f644d4bd9a0c267e864cabead024 Mon Sep 17 00:00:00 2001\nFrom: Samuel Georges <sam@daftspunk.com>\nDate: Sat, 30 Jan 2021 11:47:39 +1100\nSubject: [PATCH] Invalidate the session ID to prevent reuse\n\n1. Good logs in\n2. Bad captures Good's session cookie\n3. Good logs out\n4. Session cookie no longer works\n5. Good logs in a second time\n6. ORIGINAL session cookie works (Bad is also signed in)\n---\n src\/Auth\/Manager.php | 2 +-\n 1 file changed, 1 insertion(+), 1 deletion(-)\n\ndiff --git a\/src\/Auth\/Manager.php b\/src\/Auth\/Manager.php\nindex 1bb65baf6..a8dc26a28 100644\n--- a\/src\/Auth\/Manager.php\n+++ b\/src\/Auth\/Manager.php\n@@ -686,7 +686,7 @@ public function logout()\n \n         $this->user = null;\n \n-        Session::flush();\n+        Session::invalidate();\n         Cookie::queue(Cookie::forget($this->sessionKey));\n     }"
        },
        {
            "index":871,
            "vuln_id":"GHSA-fq6p-6334-8gr4",
            "cwe_id":"{'CWE-401'}",
            "score":4.3,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/ab51e5b813573dc9f51efa335aebcf2994125ee9'}",
            "dataset":"osv",
            "summary":"Memory leak in decoding PNG images ### Impact\nWhen [decoding PNG images](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/a1320ec1eac186da1d03f033109191f715b2b130\/tensorflow\/core\/kernels\/image\/decode_image_op.cc#L322-L416) TensorFlow can produce a memory leak if the image is invalid.\nAfter calling `png::CommonInitDecode(..., &decode)`, the `decode` value contains allocated buffers which can only be freed by calling `png::CommonFreeDecode(&decode)`. However, several error case in the function implementation invoke the `OP_REQUIRES` macro which immediately terminates the execution of the function, without allowing for the memory free to occur.\n  \n### Patches   \nWe have patched the issue in GitHub commit [ab51e5b813573dc9f51efa335aebcf2994125ee9](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/ab51e5b813573dc9f51efa335aebcf2994125ee9).\n\nThe fix will be included in TensorFlow 2.8.0. We will also cherrypick this commit on TensorFlow 2.7.1, TensorFlow 2.6.3, and TensorFlow 2.5.3, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.",
            "published_date":"2022-02-09",
            "chain_len":1,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/ab51e5b813573dc9f51efa335aebcf2994125ee9",
            "commit_sha":"ab51e5b813573dc9f51efa335aebcf2994125ee9",
            "patch":"SINGLE",
            "chain_ord":"['ab51e5b813573dc9f51efa335aebcf2994125ee9']",
            "before_first_fix_commit":"{'fb5ce99505358985ace9e811fd25a57047471d6f'}",
            "last_fix_commit":"ab51e5b813573dc9f51efa335aebcf2994125ee9",
            "chain_ord_pos":1.0,
            "commit_datetime":"11\/12\/2021, 03:24:32",
            "message":"Prevent memory leak in decoding PNG images.\n\nPiperOrigin-RevId: 409300653\nChange-Id: I6182124c545989cef80cefd439b659095920763b",
            "author":"Mihai Maruseac",
            "comments":null,
            "stats":"{'additions': 12, 'deletions': 0, 'total': 12}",
            "files":"{'tensorflow\/core\/kernels\/image\/decode_image_op.cc': {'additions': 12, 'deletions': 0, 'changes': 12, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/ab51e5b813573dc9f51efa335aebcf2994125ee9\/tensorflow%2Fcore%2Fkernels%2Fimage%2Fdecode_image_op.cc', 'patch': '@@ -18,6 +18,8 @@ limitations under the License.\\n #include <cstdint>\\n #include <memory>\\n \\n+#include \"tensorflow\/core\/lib\/gtl\/cleanup.h\"\\n+\\n #define EIGEN_USE_THREADS\\n \\n #include \"absl\/strings\/escaping.h\"\\n@@ -326,6 +328,16 @@ class DecodeImageV2Op : public OpKernel {\\n         context, png::CommonInitDecode(input, channels_, channel_bits, &decode),\\n         errors::InvalidArgument(\"Invalid PNG. Failed to initialize decoder.\"));\\n \\n+    \/\/ If we reach this point, then there is data in `decode` which must be\\n+    \/\/ freed by the time we end execution in this function. We cannot call\\n+    \/\/ `png::CommonFreeDecode()` before an `OP_REQUIRES` because if\\n+    \/\/ `OP_REQUIRES` constraint is satisfied then the data would be freed\\n+    \/\/ prematurely. Instead, let\\'s use a `Cleanup` object.\\n+    auto cleanup = gtl::MakeCleanup([&decode]() {\\n+      std::cerr << \"Cleanup called...\\\\n\";\\n+      png::CommonFreeDecode(&decode);\\n+    });\\n+\\n     \/\/ Verify that width and height are not too large:\\n     \/\/ - verify width and height don\\'t overflow int.\\n     \/\/ - width can later be multiplied by channels_ and sizeof(uint16), so'}}",
            "message_norm":"prevent memory leak in decoding png images.\n\npiperorigin-revid: 409300653\nchange-id: i6182124c545989cef80cefd439b659095920763b",
            "language":"en",
            "entities":"[('prevent', 'ACTION', ''), ('memory leak', 'SECWORD', ''), ('decoding', 'SECWORD', ''), ('409300653', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/core\/kernels\/image\/decode_image_op.cc'])",
            "num_files":1.0,
            "patch_content":"From ab51e5b813573dc9f51efa335aebcf2994125ee9 Mon Sep 17 00:00:00 2001\nFrom: Mihai Maruseac <mihaimaruseac@google.com>\nDate: Thu, 11 Nov 2021 19:24:32 -0800\nSubject: [PATCH] Prevent memory leak in decoding PNG images.\n\nPiperOrigin-RevId: 409300653\nChange-Id: I6182124c545989cef80cefd439b659095920763b\n---\n tensorflow\/core\/kernels\/image\/decode_image_op.cc | 12 ++++++++++++\n 1 file changed, 12 insertions(+)\n\ndiff --git a\/tensorflow\/core\/kernels\/image\/decode_image_op.cc b\/tensorflow\/core\/kernels\/image\/decode_image_op.cc\nindex eef98dd2d83400..ee0ae957203b37 100644\n--- a\/tensorflow\/core\/kernels\/image\/decode_image_op.cc\n+++ b\/tensorflow\/core\/kernels\/image\/decode_image_op.cc\n@@ -18,6 +18,8 @@ limitations under the License.\n #include <cstdint>\n #include <memory>\n \n+#include \"tensorflow\/core\/lib\/gtl\/cleanup.h\"\n+\n #define EIGEN_USE_THREADS\n \n #include \"absl\/strings\/escaping.h\"\n@@ -326,6 +328,16 @@ class DecodeImageV2Op : public OpKernel {\n         context, png::CommonInitDecode(input, channels_, channel_bits, &decode),\n         errors::InvalidArgument(\"Invalid PNG. Failed to initialize decoder.\"));\n \n+    \/\/ If we reach this point, then there is data in `decode` which must be\n+    \/\/ freed by the time we end execution in this function. We cannot call\n+    \/\/ `png::CommonFreeDecode()` before an `OP_REQUIRES` because if\n+    \/\/ `OP_REQUIRES` constraint is satisfied then the data would be freed\n+    \/\/ prematurely. Instead, let's use a `Cleanup` object.\n+    auto cleanup = gtl::MakeCleanup([&decode]() {\n+      std::cerr << \"Cleanup called...\\n\";\n+      png::CommonFreeDecode(&decode);\n+    });\n+\n     \/\/ Verify that width and height are not too large:\n     \/\/ - verify width and height don't overflow int.\n     \/\/ - width can later be multiplied by channels_ and sizeof(uint16), so"
        },
        {
            "index":616,
            "vuln_id":"GHSA-5vr6-hm68-5j9p",
            "cwe_id":"{'CWE-79'}",
            "score":6.1,
            "chain":"{'https:\/\/github.com\/librenms\/librenms\/pull\/13554\/commits\/4f231a0f49b6c953d506913364ffd7fb3a660630'}",
            "dataset":"osv",
            "summary":"Cross-site Scripting in LibreNMS LibreNMS 21.11.0 is affected by is affected by a Cross Site Scripting (XSS) vulnerability in includes\/html\/forms\/poller-groups.inc.php.",
            "published_date":"2021-12-03",
            "chain_len":1,
            "project":"https:\/\/github.com\/librenms\/librenms",
            "commit_href":"https:\/\/github.com\/librenms\/librenms\/pull\/13554\/commits\/4f231a0f49b6c953d506913364ffd7fb3a660630",
            "commit_sha":"4f231a0f49b6c953d506913364ffd7fb3a660630",
            "patch":"SINGLE",
            "chain_ord":"['4f231a0f49b6c953d506913364ffd7fb3a660630']",
            "before_first_fix_commit":"{'fff7b45a7599f8f13a55250dc5f2b957f3394194'}",
            "last_fix_commit":"4f231a0f49b6c953d506913364ffd7fb3a660630",
            "chain_ord_pos":1.0,
            "commit_datetime":"11\/22\/2021, 22:31:45",
            "message":"fix XSS vulnerability in poller-groups.inc.php",
            "author":"AL-KASSAR",
            "comments":null,
            "stats":"{'additions': 1, 'deletions': 1, 'total': 2}",
            "files":"{'includes\/html\/forms\/poller-groups.inc.php': {'additions': 1, 'deletions': 1, 'changes': 2, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/librenms\/librenms\/raw\/4f231a0f49b6c953d506913364ffd7fb3a660630\/includes%2Fhtml%2Fforms%2Fpoller-groups.inc.php', 'patch': '@@ -44,5 +44,5 @@\\n if (! empty($ok)) {\\n     exit(\"$ok\");\\n } else {\\n-    exit(\"ERROR: $error\");\\n+    exit(\"ERROR: \".htmlspecialchars($error));;\\n }'}}",
            "message_norm":"fix xss vulnerability in poller-groups.inc.php",
            "language":"en",
            "entities":"[('fix', 'ACTION', ''), ('xss', 'SECWORD', ''), ('vulnerability', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['includes\/html\/forms\/poller-groups.inc.php'])",
            "num_files":1.0,
            "patch_content":"From 4f231a0f49b6c953d506913364ffd7fb3a660630 Mon Sep 17 00:00:00 2001\nFrom: AL-KASSAR <feras.al-kassar@sap.com>\nDate: Mon, 22 Nov 2021 23:31:45 +0100\nSubject: [PATCH] fix XSS vulnerability in poller-groups.inc.php\n\n---\n includes\/html\/forms\/poller-groups.inc.php | 2 +-\n 1 file changed, 1 insertion(+), 1 deletion(-)\n\ndiff --git a\/includes\/html\/forms\/poller-groups.inc.php b\/includes\/html\/forms\/poller-groups.inc.php\nindex f7bec45bef2f..82b12b239a80 100644\n--- a\/includes\/html\/forms\/poller-groups.inc.php\n+++ b\/includes\/html\/forms\/poller-groups.inc.php\n@@ -44,5 +44,5 @@\n if (! empty($ok)) {\n     exit(\"$ok\");\n } else {\n-    exit(\"ERROR: $error\");\n+    exit(\"ERROR: \".htmlspecialchars($error));;\n }"
        },
        {
            "index":439,
            "vuln_id":"GHSA-grmf-4fq6-2r79",
            "cwe_id":"{'CWE-119'}",
            "score":9.8,
            "chain":"{'https:\/\/github.com\/aubio\/aubio\/commit\/b1559f4c9ce2b304d8d27ffdc7128b6795ca82e5'}",
            "dataset":"osv",
            "summary":"Improper Restriction of Operations within the Bounds of a Memory Buffer in aubio aubio v0.4.0 to v0.4.8 has a Buffer Overflow in new_aubio_tempo.",
            "published_date":"2019-07-26",
            "chain_len":1,
            "project":"https:\/\/github.com\/aubio\/aubio",
            "commit_href":"https:\/\/github.com\/aubio\/aubio\/commit\/b1559f4c9ce2b304d8d27ffdc7128b6795ca82e5",
            "commit_sha":"b1559f4c9ce2b304d8d27ffdc7128b6795ca82e5",
            "patch":"SINGLE",
            "chain_ord":"['b1559f4c9ce2b304d8d27ffdc7128b6795ca82e5']",
            "before_first_fix_commit":"{'c4a8bc138e49de8b43fcd2221ef84dfa5073208f'}",
            "last_fix_commit":"b1559f4c9ce2b304d8d27ffdc7128b6795ca82e5",
            "chain_ord_pos":1.0,
            "commit_datetime":"11\/24\/2018, 16:17:29",
            "message":"[tempo] fix buffer overflow in method parser",
            "author":"Paul Brossier",
            "comments":null,
            "stats":"{'additions': 4, 'deletions': 3, 'total': 7}",
            "files":"{'src\/tempo\/tempo.c': {'additions': 4, 'deletions': 3, 'changes': 7, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/aubio\/aubio\/raw\/b1559f4c9ce2b304d8d27ffdc7128b6795ca82e5\/src%2Ftempo%2Ftempo.c', 'patch': '@@ -168,7 +168,7 @@ aubio_tempo_t * new_aubio_tempo (const char_t * tempo_mode,\\n     uint_t buf_size, uint_t hop_size, uint_t samplerate)\\n {\\n   aubio_tempo_t * o = AUBIO_NEW(aubio_tempo_t);\\n-  char_t specdesc_func[20];\\n+  char_t specdesc_func[PATH_MAX];\\n   o->samplerate = samplerate;\\n   \/\/ check parameters are valid\\n   if ((sint_t)hop_size < 1) {\\n@@ -203,9 +203,10 @@ aubio_tempo_t * new_aubio_tempo (const char_t * tempo_mode,\\n   o->pp       = new_aubio_peakpicker();\\n   aubio_peakpicker_set_threshold (o->pp, o->threshold);\\n   if ( strcmp(tempo_mode, \"default\") == 0 ) {\\n-    strcpy(specdesc_func, \"specflux\");\\n+    strncpy(specdesc_func, \"specflux\", PATH_MAX - 1);\\n   } else {\\n-    strcpy(specdesc_func, tempo_mode);\\n+    strncpy(specdesc_func, tempo_mode, PATH_MAX - 1);\\n+    specdesc_func[PATH_MAX - 1] = \\'\\\\0\\';\\n   }\\n   o->od       = new_aubio_specdesc(specdesc_func,buf_size);\\n   o->of       = new_fvec(1);'}}",
            "message_norm":"[tempo] fix buffer overflow in method parser",
            "language":"en",
            "entities":"[('buffer overflow', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['src\/tempo\/tempo.c'])",
            "num_files":1.0,
            "patch_content":"From b1559f4c9ce2b304d8d27ffdc7128b6795ca82e5 Mon Sep 17 00:00:00 2001\nFrom: Paul Brossier <piem@piem.org>\nDate: Sat, 24 Nov 2018 17:17:29 +0100\nSubject: [PATCH] [tempo] fix buffer overflow in method parser\n\n---\n src\/tempo\/tempo.c | 7 ++++---\n 1 file changed, 4 insertions(+), 3 deletions(-)\n\ndiff --git a\/src\/tempo\/tempo.c b\/src\/tempo\/tempo.c\nindex 80c89e99c..a52beaf5f 100644\n--- a\/src\/tempo\/tempo.c\n+++ b\/src\/tempo\/tempo.c\n@@ -168,7 +168,7 @@ aubio_tempo_t * new_aubio_tempo (const char_t * tempo_mode,\n     uint_t buf_size, uint_t hop_size, uint_t samplerate)\n {\n   aubio_tempo_t * o = AUBIO_NEW(aubio_tempo_t);\n-  char_t specdesc_func[20];\n+  char_t specdesc_func[PATH_MAX];\n   o->samplerate = samplerate;\n   \/\/ check parameters are valid\n   if ((sint_t)hop_size < 1) {\n@@ -203,9 +203,10 @@ aubio_tempo_t * new_aubio_tempo (const char_t * tempo_mode,\n   o->pp       = new_aubio_peakpicker();\n   aubio_peakpicker_set_threshold (o->pp, o->threshold);\n   if ( strcmp(tempo_mode, \"default\") == 0 ) {\n-    strcpy(specdesc_func, \"specflux\");\n+    strncpy(specdesc_func, \"specflux\", PATH_MAX - 1);\n   } else {\n-    strcpy(specdesc_func, tempo_mode);\n+    strncpy(specdesc_func, tempo_mode, PATH_MAX - 1);\n+    specdesc_func[PATH_MAX - 1] = '\\0';\n   }\n   o->od       = new_aubio_specdesc(specdesc_func,buf_size);\n   o->of       = new_fvec(1);"
        },
        {
            "index":545,
            "vuln_id":"GHSA-hwv5-w8gm-fq9f",
            "cwe_id":"{'CWE-22'}",
            "score":3.5,
            "chain":"{'https:\/\/github.com\/horazont\/xmpp-http-upload\/commit\/82056540191e89f0cd697c81f57714c00962ed75'}",
            "dataset":"osv",
            "summary":"Directory Traversal vulnerability in GET\/PUT allows attackers to Disclose Information or Write Files via a crafted GET\/PUT request ### Impact\n\n#### Information Disclosure\n\nWhen the GET method is attacked, attackers can read files which have a `.data` suffix and which are accompanied by a JSON file with the `.meta` suffix. This can lead to Information Disclosure and in some shared-hosting scenarios also to circumvention of authentication or other limitations on the outbound (GET) traffic.\n\nFor example, in a scenario where a single server has multiple instances of the application running (with separate DATA_ROOT settings), an attacker who has knowledge about the directory structure is able to read files from any other instance to which the process has read access.\n\nIf instances have individual authentication (for example, HTTP authentication via a reverse proxy, source IP based filtering) or other restrictions (such as quotas), attackers may circumvent those limits in such a scenario by using the Directory Traversal to retrieve data from the other instances.\n\n#### File Write\n\nIf the associated XMPP server (or anyone knowing the SECRET_KEY) is malicious, they can write files outside the DATA_ROOT. The files which are written are constrained to have the `.meta` and the `.data` suffixes; the `.meta` file will contain the JSON with the Content-Type of the original request and the `.data` file will contain the payload.\n\n### Patches\n\nPR #12 fixes the issue. The PR has been merged into version 0.4.0 and 0.4.0 has been released and pushed to PyPI. Users are advised to upgrade immediately.\n\n### Workarounds\n\n- Apache can apparently be configured to filter such malicious paths when reverse-proxying. \n- There are no other workarounds known.\n\n### References\n\n- [Pull Request #12](https:\/\/github.com\/horazont\/xmpp-http-upload\/pull\/12)",
            "published_date":"2020-10-06",
            "chain_len":1,
            "project":"https:\/\/github.com\/horazont\/xmpp-http-upload",
            "commit_href":"https:\/\/github.com\/horazont\/xmpp-http-upload\/commit\/82056540191e89f0cd697c81f57714c00962ed75",
            "commit_sha":"82056540191e89f0cd697c81f57714c00962ed75",
            "patch":"SINGLE",
            "chain_ord":"['82056540191e89f0cd697c81f57714c00962ed75']",
            "before_first_fix_commit":"{'f0fc7443c06a0e8aecb5696fc2bd513a2cc8b611'}",
            "last_fix_commit":"82056540191e89f0cd697c81f57714c00962ed75",
            "chain_ord_pos":1.0,
            "commit_datetime":"10\/05\/2020, 23:06:21",
            "message":"Simplify path handling, use safe_join\n\nThe current implementation of sanitized_join did not handle\n\"..\" properly. The problem is, that .absolute() does not do\nwhat .resolve() does, but .resolve() does not work on non\nexistant paths.\n\nAnyway, flask has a function exactly for this: safe_join.\n\nSo let's use that one.\n\nWhile at it, simplified the whole path handling a bit.",
            "author":"Christian Tacke",
            "comments":null,
            "stats":"{'additions': 15, 'deletions': 34, 'total': 49}",
            "files":"{'xhu.py': {'additions': 15, 'deletions': 34, 'changes': 49, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/horazont\/xmpp-http-upload\/raw\/82056540191e89f0cd697c81f57714c00962ed75\/xhu.py', 'patch': '@@ -29,6 +29,7 @@\\n import typing\\n \\n import flask\\n+import werkzeug.exceptions\\n \\n app = flask.Flask(\"xmpp-http-upload\")\\n app.config.from_envvar(\"XMPP_HTTP_UPLOAD_CONFIG\")\\n@@ -39,16 +40,11 @@\\n     CORS(app)\\n \\n \\n-def sanitized_join(path: str, root: pathlib.Path) -> pathlib.Path:\\n-    result = (root \/ path).absolute()\\n-    if not str(result).startswith(str(root) + \"\/\"):\\n-        raise ValueError(\"resulting path is outside root\")\\n-    return result\\n-\\n-\\n-def get_paths(base_path: pathlib.Path):\\n-    data_file = pathlib.Path(str(base_path) + \".data\")\\n-    metadata_file = pathlib.Path(str(base_path) + \".meta\")\\n+def get_paths(root: str, sub_path: str) \\\\\\n+        -> typing.Tuple[pathlib.Path, pathlib.Path]:\\n+    base_path = flask.safe_join(root, sub_path)\\n+    data_file = pathlib.Path(base_path + \".data\")\\n+    metadata_file = pathlib.Path(base_path + \".meta\")\\n \\n     return data_file, metadata_file\\n \\n@@ -58,15 +54,10 @@ def load_metadata(metadata_file):\\n         return json.load(f)\\n \\n \\n-def get_info(path: str, root: pathlib.Path) -> typing.Tuple[\\n+def get_info(path: str) -> typing.Tuple[\\n         pathlib.Path,\\n         dict]:\\n-    dest_path = sanitized_join(\\n-        path,\\n-        pathlib.Path(app.config[\"DATA_ROOT\"]),\\n-    )\\n-\\n-    data_file, metadata_file = get_paths(dest_path)\\n+    data_file, metadata_file = get_paths(app.config[\"DATA_ROOT\"], path)\\n \\n     return data_file, load_metadata(metadata_file)\\n \\n@@ -104,11 +95,8 @@ def stream_file(src, dest, nbytes):\\n @app.route(\"\/<path:path>\", methods=[\"PUT\"])\\n def put_file(path):\\n     try:\\n-        dest_path = sanitized_join(\\n-            path,\\n-            pathlib.Path(app.config[\"DATA_ROOT\"]),\\n-        )\\n-    except ValueError:\\n+        data_file, metadata_file = get_paths(app.config[\"DATA_ROOT\"], path)\\n+    except werkzeug.exceptions.NotFound:\\n         return flask.Response(\\n             \"Not Found\",\\n             404,\\n@@ -134,8 +122,7 @@ def put_file(path):\\n         \"application\/octet-stream\",\\n     )\\n \\n-    dest_path.parent.mkdir(parents=True, exist_ok=True, mode=0o770)\\n-    data_file, metadata_file = get_paths(dest_path)\\n+    data_file.parent.mkdir(parents=True, exist_ok=True, mode=0o770)\\n \\n     try:\\n         with write_file(data_file) as fout:\\n@@ -189,13 +176,10 @@ def generate_headers(response_headers, metadata_headers):\\n @app.route(\"\/<path:path>\", methods=[\"HEAD\"])\\n def head_file(path):\\n     try:\\n-        data_file, metadata = get_info(\\n-            path,\\n-            pathlib.Path(app.config[\"DATA_ROOT\"])\\n-        )\\n+        data_file, metadata = get_info(path)\\n \\n         stat = data_file.stat()\\n-    except (OSError, ValueError):\\n+    except (OSError, werkzeug.exceptions.NotFound):\\n         return flask.Response(\\n             \"Not Found\",\\n             404,\\n@@ -214,11 +198,8 @@ def head_file(path):\\n @app.route(\"\/<path:path>\", methods=[\"GET\"])\\n def get_file(path):\\n     try:\\n-        data_file, metadata = get_info(\\n-            path,\\n-            pathlib.Path(app.config[\"DATA_ROOT\"])\\n-        )\\n-    except (OSError, ValueError):\\n+        data_file, metadata = get_info(path)\\n+    except (OSError, werkzeug.exceptions.NotFound):\\n         return flask.Response(\\n             \"Not Found\",\\n             404,'}}",
            "message_norm":"simplify path handling, use safe_join\n\nthe current implementation of sanitized_join did not handle\n\"..\" properly. the problem is, that .absolute() does not do\nwhat .resolve() does, but .resolve() does not work on non\nexistant paths.\n\nanyway, flask has a function exactly for this: safe_join.\n\nso let's use that one.\n\nwhile at it, simplified the whole path handling a bit.",
            "language":"en",
            "entities":"[('sanitized_join', 'SECWORD', ''), ('problem', 'FLAW', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['xhu.py'])",
            "num_files":1.0,
            "patch_content":"From 82056540191e89f0cd697c81f57714c00962ed75 Mon Sep 17 00:00:00 2001\nFrom: Christian Tacke <8560110+ChristianTacke@users.noreply.github.com>\nDate: Tue, 6 Oct 2020 01:06:21 +0200\nSubject: [PATCH] Simplify path handling, use safe_join\n\nThe current implementation of sanitized_join did not handle\n\"..\" properly. The problem is, that .absolute() does not do\nwhat .resolve() does, but .resolve() does not work on non\nexistant paths.\n\nAnyway, flask has a function exactly for this: safe_join.\n\nSo let's use that one.\n\nWhile at it, simplified the whole path handling a bit.\n---\n xhu.py | 49 +++++++++++++++----------------------------------\n 1 file changed, 15 insertions(+), 34 deletions(-)\n\ndiff --git a\/xhu.py b\/xhu.py\nindex a28c5a8..b999957 100644\n--- a\/xhu.py\n+++ b\/xhu.py\n@@ -29,6 +29,7 @@\n import typing\n \n import flask\n+import werkzeug.exceptions\n \n app = flask.Flask(\"xmpp-http-upload\")\n app.config.from_envvar(\"XMPP_HTTP_UPLOAD_CONFIG\")\n@@ -39,16 +40,11 @@\n     CORS(app)\n \n \n-def sanitized_join(path: str, root: pathlib.Path) -> pathlib.Path:\n-    result = (root \/ path).absolute()\n-    if not str(result).startswith(str(root) + \"\/\"):\n-        raise ValueError(\"resulting path is outside root\")\n-    return result\n-\n-\n-def get_paths(base_path: pathlib.Path):\n-    data_file = pathlib.Path(str(base_path) + \".data\")\n-    metadata_file = pathlib.Path(str(base_path) + \".meta\")\n+def get_paths(root: str, sub_path: str) \\\n+        -> typing.Tuple[pathlib.Path, pathlib.Path]:\n+    base_path = flask.safe_join(root, sub_path)\n+    data_file = pathlib.Path(base_path + \".data\")\n+    metadata_file = pathlib.Path(base_path + \".meta\")\n \n     return data_file, metadata_file\n \n@@ -58,15 +54,10 @@ def load_metadata(metadata_file):\n         return json.load(f)\n \n \n-def get_info(path: str, root: pathlib.Path) -> typing.Tuple[\n+def get_info(path: str) -> typing.Tuple[\n         pathlib.Path,\n         dict]:\n-    dest_path = sanitized_join(\n-        path,\n-        pathlib.Path(app.config[\"DATA_ROOT\"]),\n-    )\n-\n-    data_file, metadata_file = get_paths(dest_path)\n+    data_file, metadata_file = get_paths(app.config[\"DATA_ROOT\"], path)\n \n     return data_file, load_metadata(metadata_file)\n \n@@ -104,11 +95,8 @@ def stream_file(src, dest, nbytes):\n @app.route(\"\/<path:path>\", methods=[\"PUT\"])\n def put_file(path):\n     try:\n-        dest_path = sanitized_join(\n-            path,\n-            pathlib.Path(app.config[\"DATA_ROOT\"]),\n-        )\n-    except ValueError:\n+        data_file, metadata_file = get_paths(app.config[\"DATA_ROOT\"], path)\n+    except werkzeug.exceptions.NotFound:\n         return flask.Response(\n             \"Not Found\",\n             404,\n@@ -134,8 +122,7 @@ def put_file(path):\n         \"application\/octet-stream\",\n     )\n \n-    dest_path.parent.mkdir(parents=True, exist_ok=True, mode=0o770)\n-    data_file, metadata_file = get_paths(dest_path)\n+    data_file.parent.mkdir(parents=True, exist_ok=True, mode=0o770)\n \n     try:\n         with write_file(data_file) as fout:\n@@ -189,13 +176,10 @@ def generate_headers(response_headers, metadata_headers):\n @app.route(\"\/<path:path>\", methods=[\"HEAD\"])\n def head_file(path):\n     try:\n-        data_file, metadata = get_info(\n-            path,\n-            pathlib.Path(app.config[\"DATA_ROOT\"])\n-        )\n+        data_file, metadata = get_info(path)\n \n         stat = data_file.stat()\n-    except (OSError, ValueError):\n+    except (OSError, werkzeug.exceptions.NotFound):\n         return flask.Response(\n             \"Not Found\",\n             404,\n@@ -214,11 +198,8 @@ def head_file(path):\n @app.route(\"\/<path:path>\", methods=[\"GET\"])\n def get_file(path):\n     try:\n-        data_file, metadata = get_info(\n-            path,\n-            pathlib.Path(app.config[\"DATA_ROOT\"])\n-        )\n-    except (OSError, ValueError):\n+        data_file, metadata = get_info(path)\n+    except (OSError, werkzeug.exceptions.NotFound):\n         return flask.Response(\n             \"Not Found\",\n             404,"
        },
        {
            "index":27,
            "vuln_id":"GHSA-5q5w-mqp6-g2gh",
            "cwe_id":"{'CWE-434', 'CWE-79'}",
            "score":5.4,
            "chain":"{'https:\/\/github.com\/jsdecena\/laracom\/commit\/256026193ce994dc4c1365e02f414d8a0cd77ae8'}",
            "dataset":"osv",
            "summary":"Unrestricted Upload of File with Dangerous Type in jsdecena\/laracom jsdecena\/laracom prior to version 2.0.9 is vulnerable to Unrestricted Upload of File with Dangerous Type.",
            "published_date":"2022-02-06",
            "chain_len":1,
            "project":"https:\/\/github.com\/jsdecena\/laracom",
            "commit_href":"https:\/\/github.com\/jsdecena\/laracom\/commit\/256026193ce994dc4c1365e02f414d8a0cd77ae8",
            "commit_sha":"256026193ce994dc4c1365e02f414d8a0cd77ae8",
            "patch":"SINGLE",
            "chain_ord":"['256026193ce994dc4c1365e02f414d8a0cd77ae8']",
            "before_first_fix_commit":"{'8ef50622cb9c696d79a7a6b37e9b43266da6545d'}",
            "last_fix_commit":"256026193ce994dc4c1365e02f414d8a0cd77ae8",
            "chain_ord_pos":1.0,
            "commit_datetime":"02\/01\/2022, 22:16:55",
            "message":"Fix vulnerability report from hunter.dev",
            "author":"Roland Jeffrey Decena",
            "comments":null,
            "stats":"{'additions': 3, 'deletions': 1, 'total': 4}",
            "files":"{'project\/app\/Shop\/Products\/Requests\/UpdateProductRequest.php': {'additions': 3, 'deletions': 1, 'changes': 4, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/jsdecena\/laracom\/raw\/256026193ce994dc4c1365e02f414d8a0cd77ae8\/project%2Fapp%2FShop%2FProducts%2FRequests%2FUpdateProductRequest.php', 'patch': \"@@ -20,7 +20,9 @@ public function rules()\\n             'quantity' => ['required', 'integer', 'min:0'],\\n             'price' => ['required', 'numeric', 'min:0'],\\n             'sale_price' => ['nullable', 'numeric'],\\n-            'weight' => ['nullable', 'numeric', 'min:0']\\n+            'weight' => ['nullable', 'numeric', 'min:0'],\\n+            'image' => 'image|mimes:jpeg,png,jpg,gif,svg|max:2048',\\n+            'cover' => 'image|mimes:jpeg,png,jpg,gif,svg|max:2048',\\n         ];\\n     }\\n }\"}}",
            "message_norm":"fix vulnerability report from hunter.dev",
            "language":"en",
            "entities":"[('fix', 'ACTION', ''), ('vulnerability', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['project\/app\/Shop\/Products\/Requests\/UpdateProductRequest.php'])",
            "num_files":1.0,
            "patch_content":"From 256026193ce994dc4c1365e02f414d8a0cd77ae8 Mon Sep 17 00:00:00 2001\nFrom: Roland Jeffrey Decena <jeff.decena@publictrust.co.nz>\nDate: Wed, 2 Feb 2022 11:16:55 +1300\nSubject: [PATCH] Fix vulnerability report from hunter.dev\n\n---\n project\/app\/Shop\/Products\/Requests\/UpdateProductRequest.php | 4 +++-\n 1 file changed, 3 insertions(+), 1 deletion(-)\n\ndiff --git a\/project\/app\/Shop\/Products\/Requests\/UpdateProductRequest.php b\/project\/app\/Shop\/Products\/Requests\/UpdateProductRequest.php\nindex 2ef53f04c..15278341c 100644\n--- a\/project\/app\/Shop\/Products\/Requests\/UpdateProductRequest.php\n+++ b\/project\/app\/Shop\/Products\/Requests\/UpdateProductRequest.php\n@@ -20,7 +20,9 @@ public function rules()\n             'quantity' => ['required', 'integer', 'min:0'],\n             'price' => ['required', 'numeric', 'min:0'],\n             'sale_price' => ['nullable', 'numeric'],\n-            'weight' => ['nullable', 'numeric', 'min:0']\n+            'weight' => ['nullable', 'numeric', 'min:0'],\n+            'image' => 'image|mimes:jpeg,png,jpg,gif,svg|max:2048',\n+            'cover' => 'image|mimes:jpeg,png,jpg,gif,svg|max:2048',\n         ];\n     }\n }"
        },
        {
            "index":234,
            "vuln_id":"GHSA-2wwc-w2gw-4329",
            "cwe_id":"{'CWE-787'}",
            "score":7.5,
            "chain":"{'https:\/\/github.com\/chakra-core\/ChakraCore\/commit\/94181502091b7c22eb86ab1b45ce80bf7ff03aaf', 'https:\/\/github.com\/chakra-core\/ChakraCore\/commit\/cc871514deeaeaedb5b757c2ca8cd4ab9abccb5d'}",
            "dataset":"osv",
            "summary":"Out-of-bounds write A remote code execution vulnerability exists in the way that the Chakra scripting engine handles objects in memory in Microsoft Edge, aka 'Chakra Scripting Engine Memory Corruption Vulnerability'. This CVE ID is unique from CVE-2019-1307, CVE-2019-1308, CVE-2019-1335.",
            "published_date":"2021-03-29",
            "chain_len":2,
            "project":"https:\/\/github.com\/chakra-core\/ChakraCore",
            "commit_href":"https:\/\/github.com\/chakra-core\/ChakraCore\/commit\/94181502091b7c22eb86ab1b45ce80bf7ff03aaf",
            "commit_sha":"94181502091b7c22eb86ab1b45ce80bf7ff03aaf",
            "patch":"MULTI",
            "chain_ord":"['94181502091b7c22eb86ab1b45ce80bf7ff03aaf', 'cc871514deeaeaedb5b757c2ca8cd4ab9abccb5d']",
            "before_first_fix_commit":"{'7e9a2ee60baa95ceb4f48f522f823c812ca90c80', '5989c6e038d80f92dcd8e10d725cdf45396201bb'}",
            "last_fix_commit":"cc871514deeaeaedb5b757c2ca8cd4ab9abccb5d",
            "chain_ord_pos":1.0,
            "commit_datetime":"08\/30\/2019, 22:55:27",
            "message":"CVE-2019-1366",
            "author":"Paul Leathers",
            "comments":null,
            "stats":"{'additions': 1, 'deletions': 1, 'total': 2}",
            "files":"{'lib\/Backend\/GlobOpt.cpp': {'additions': 1, 'deletions': 1, 'changes': 2, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/chakra-core\/ChakraCore\/raw\/94181502091b7c22eb86ab1b45ce80bf7ff03aaf\/lib%2FBackend%2FGlobOpt.cpp', 'patch': '@@ -3603,7 +3603,7 @@ GlobOpt::OptSrc(IR::Opnd *opnd, IR::Instr * *pInstr, Value **indirIndexValRef, I\\n \\n         opnd->SetValueType(valueType);\\n \\n-        if(!IsLoopPrePass() && opnd->IsSymOpnd() && valueType.IsDefinite())\\n+        if(!IsLoopPrePass() && opnd->IsSymOpnd() && (valueType.IsDefinite() || valueType.IsNotTaggedValue()))\\n         {\\n             if (opnd->AsSymOpnd()->m_sym->IsPropertySym())\\n             {'}}",
            "message_norm":"cve-2019-1366",
            "language":"ro",
            "entities":"[('cve-2019-1366', 'VULNID', 'CVE')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['lib\/Backend\/GlobOpt.cpp'])",
            "num_files":1.0,
            "patch_content":"From 94181502091b7c22eb86ab1b45ce80bf7ff03aaf Mon Sep 17 00:00:00 2001\nFrom: Paul Leathers <pleath@microsoft.com>\nDate: Fri, 30 Aug 2019 15:55:27 -0700\nSubject: [PATCH] CVE-2019-1366\n\n---\n lib\/Backend\/GlobOpt.cpp | 2 +-\n 1 file changed, 1 insertion(+), 1 deletion(-)\n\ndiff --git a\/lib\/Backend\/GlobOpt.cpp b\/lib\/Backend\/GlobOpt.cpp\nindex 1a7530499c2..43b52dba7e9 100644\n--- a\/lib\/Backend\/GlobOpt.cpp\n+++ b\/lib\/Backend\/GlobOpt.cpp\n@@ -3603,7 +3603,7 @@ GlobOpt::OptSrc(IR::Opnd *opnd, IR::Instr * *pInstr, Value **indirIndexValRef, I\n \n         opnd->SetValueType(valueType);\n \n-        if(!IsLoopPrePass() && opnd->IsSymOpnd() && valueType.IsDefinite())\n+        if(!IsLoopPrePass() && opnd->IsSymOpnd() && (valueType.IsDefinite() || valueType.IsNotTaggedValue()))\n         {\n             if (opnd->AsSymOpnd()->m_sym->IsPropertySym())\n             {"
        },
        {
            "index":475,
            "vuln_id":"GHSA-9p47-w5xp-f4xr",
            "cwe_id":"{'CWE-311'}",
            "score":0.0,
            "chain":"{'https:\/\/github.com\/felixrieseberg\/windows-build-tools\/commit\/9835d33e68f2cb5e4d148e954bb3ed0221d98e90'}",
            "dataset":"osv",
            "summary":"Downloads Resources over HTTP in windows-build-tools Affected versions of `windows-build-tools` insecurely download an executable over an unencrypted HTTP connection. \n\nIn scenarios where an attacker has a privileged network position, it is possible to intercept the response and replace the executable with a malicious one, resulting in code execution on the system running `windows-build-tools`.\n\n\n## Recommendation\n\nUpdate to version 1.0.0 or later.",
            "published_date":"2018-11-09",
            "chain_len":1,
            "project":"https:\/\/github.com\/felixrieseberg\/windows-build-tools",
            "commit_href":"https:\/\/github.com\/felixrieseberg\/windows-build-tools\/commit\/9835d33e68f2cb5e4d148e954bb3ed0221d98e90",
            "commit_sha":"9835d33e68f2cb5e4d148e954bb3ed0221d98e90",
            "patch":"SINGLE",
            "chain_ord":"['9835d33e68f2cb5e4d148e954bb3ed0221d98e90']",
            "before_first_fix_commit":"{'3a97ea156d07aaccca101ed8f0a1606eea8dfc2d'}",
            "last_fix_commit":"9835d33e68f2cb5e4d148e954bb3ed0221d98e90",
            "chain_ord_pos":1.0,
            "commit_datetime":"12\/22\/2016, 17:19:25",
            "message":":wrench: Use HTTPS - Thanks to @grander",
            "author":"Felix Rieseberg",
            "comments":null,
            "stats":"{'additions': 1, 'deletions': 1, 'total': 2}",
            "files":"{'src\/constants.js': {'additions': 1, 'deletions': 1, 'changes': 2, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/felixrieseberg\/windows-build-tools\/raw\/9835d33e68f2cb5e4d148e954bb3ed0221d98e90\/src%2Fconstants.js', 'patch': \"@@ -2,7 +2,7 @@ var pythonMirror = process.env.npm_config_python_mirror || process.env.PYTHON_MI\\n \\n var buildTools = {\\n   installerName: 'BuildTools_Full.exe',\\n-  installerUrl: 'http:\/\/download.microsoft.com\/download\/5\/f\/7\/5f7acaeb-8363-451f-9425-68a90f98b238\/visualcppbuildtools_full.exe',\\n+  installerUrl: 'https:\/\/download.microsoft.com\/download\/5\/f\/7\/5f7acaeb-8363-451f-9425-68a90f98b238\/visualcppbuildtools_full.exe',\\n   logName: 'build-tools-log.txt'\\n }\"}}",
            "message_norm":":wrench: use https - thanks to @grander",
            "language":"en",
            "entities":null,
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['src\/constants.js'])",
            "num_files":1.0,
            "patch_content":"From 9835d33e68f2cb5e4d148e954bb3ed0221d98e90 Mon Sep 17 00:00:00 2001\nFrom: Felix Rieseberg <felix@felixrieseberg.com>\nDate: Thu, 22 Dec 2016 09:19:25 -0800\nSubject: [PATCH] :wrench: Use HTTPS - Thanks to @grander\n\n---\n src\/constants.js | 2 +-\n 1 file changed, 1 insertion(+), 1 deletion(-)\n\ndiff --git a\/src\/constants.js b\/src\/constants.js\nindex a8e388c..bdba14c 100644\n--- a\/src\/constants.js\n+++ b\/src\/constants.js\n@@ -2,7 +2,7 @@ var pythonMirror = process.env.npm_config_python_mirror || process.env.PYTHON_MI\n \n var buildTools = {\n   installerName: 'BuildTools_Full.exe',\n-  installerUrl: 'http:\/\/download.microsoft.com\/download\/5\/f\/7\/5f7acaeb-8363-451f-9425-68a90f98b238\/visualcppbuildtools_full.exe',\n+  installerUrl: 'https:\/\/download.microsoft.com\/download\/5\/f\/7\/5f7acaeb-8363-451f-9425-68a90f98b238\/visualcppbuildtools_full.exe',\n   logName: 'build-tools-log.txt'\n }"
        },
        {
            "index":43,
            "vuln_id":"GHSA-394c-5j6w-4xmx",
            "cwe_id":"{'CWE-400'}",
            "score":7.5,
            "chain":"{'https:\/\/github.com\/faisalman\/ua-parser-js\/commit\/6d1f26df051ba681463ef109d36c9cf0f7e32b18'}",
            "dataset":"osv",
            "summary":"Regular Expression Denial of Service (ReDoS) in ua-parser-js The package ua-parser-js before 0.7.23 are vulnerable to Regular Expression Denial of Service (ReDoS) in multiple regexes (see linked commit for more info).",
            "published_date":"2022-02-09",
            "chain_len":1,
            "project":"https:\/\/github.com\/faisalman\/ua-parser-js",
            "commit_href":"https:\/\/github.com\/faisalman\/ua-parser-js\/commit\/6d1f26df051ba681463ef109d36c9cf0f7e32b18",
            "commit_sha":"6d1f26df051ba681463ef109d36c9cf0f7e32b18",
            "patch":"SINGLE",
            "chain_ord":"['6d1f26df051ba681463ef109d36c9cf0f7e32b18']",
            "before_first_fix_commit":"{'86471ad7e24724757e6147cd449cc2af4fab6280'}",
            "last_fix_commit":"6d1f26df051ba681463ef109d36c9cf0f7e32b18",
            "chain_ord_pos":1.0,
            "commit_datetime":"11\/30\/2020, 17:50:19",
            "message":"Fix ReDoS vulnerabilities reported by Snyk",
            "author":"Faisal Salman",
            "comments":null,
            "stats":"{'additions': 14, 'deletions': 14, 'total': 28}",
            "files":"{'src\/ua-parser.js': {'additions': 14, 'deletions': 14, 'changes': 28, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/faisalman\/ua-parser-js\/raw\/6d1f26df051ba681463ef109d36c9cf0f7e32b18\/src%2Fua-parser.js', 'patch': \"@@ -222,7 +222,7 @@\\n \\n             \/\/ Presto based\\n             \/(opera\\\\smini)\\\\\/([\\\\w\\\\.-]+)\/i,                                       \/\/ Opera Mini\\n-            \/(opera\\\\s[mobiletab]+).+version\\\\\/([\\\\w\\\\.-]+)\/i,                      \/\/ Opera Mobi\/Tablet\\n+            \/(opera\\\\s[mobiletab]{3,6}).+version\\\\\/([\\\\w\\\\.-]+)\/i,                  \/\/ Opera Mobi\/Tablet\\n             \/(opera).+version\\\\\/([\\\\w\\\\.]+)\/i,                                     \/\/ Opera > 9.80\\n             \/(opera)[\\\\\/\\\\s]+([\\\\w\\\\.]+)\/i                                          \/\/ Opera < 9.80\\n             ], [NAME, VERSION], [\\n@@ -252,7 +252,7 @@\\n             \/(konqueror)\\\\\/([\\\\w\\\\.]+)\/i                                           \/\/ Konqueror\\n             ], [[NAME, 'Konqueror'], VERSION], [\\n \\n-            \/(trident).+rv[:\\\\s]([\\\\w\\\\.]+).+like\\\\sgecko\/i                         \/\/ IE11\\n+            \/(trident).+rv[:\\\\s]([\\\\w\\\\.]{1,9}).+like\\\\sgecko\/i                     \/\/ IE11\\n             ], [[NAME, 'IE'], VERSION], [\\n \\n             \/(edge|edgios|edga|edg)\\\\\/((\\\\d+)?[\\\\w\\\\.]+)\/i                          \/\/ Microsoft Edge\\n@@ -362,13 +362,13 @@\\n             \/fxios\\\\\/([\\\\w\\\\.-]+)\/i                                                \/\/ Firefox for iOS\\n             ], [VERSION, [NAME, 'Firefox']], [\\n \\n-            \/version\\\\\/([\\\\w\\\\.]+).+?mobile\\\\\/\\\\w+\\\\s(safari)\/i                       \/\/ Mobile Safari\\n+            \/version\\\\\/([\\\\w\\\\.]+)\\\\s.*mobile\\\\\/\\\\w+\\\\s(safari)\/i                      \/\/ Mobile Safari\\n             ], [VERSION, [NAME, 'Mobile Safari']], [\\n \\n-            \/version\\\\\/([\\\\w\\\\.]+).+?(mobile\\\\s?safari|safari)\/i                    \/\/ Safari & Safari Mobile\\n+            \/version\\\\\/([\\\\w\\\\.]+)\\\\s.*(mobile\\\\s?safari|safari)\/i                   \/\/ Safari & Safari Mobile\\n             ], [VERSION, NAME], [\\n \\n-            \/webkit.+?(gsa)\\\\\/([\\\\w\\\\.]+).+?(mobile\\\\s?safari|safari)(\\\\\/[\\\\w\\\\.]+)\/i  \/\/ Google Search Appliance on iOS\\n+            \/webkit.+?(gsa)\\\\\/([\\\\w\\\\.]+)\\\\s.*(mobile\\\\s?safari|safari)(\\\\\/[\\\\w\\\\.]+)\/i \/\/ Google Search Appliance on iOS\\n             ], [[NAME, 'GSA'], VERSION], [\\n \\n             \/webkit.+?(mobile\\\\s?safari|safari)(\\\\\/[\\\\w\\\\.]+)\/i                     \/\/ Safari < 3.0\\n@@ -387,7 +387,7 @@\\n \\n                                                                                 \/\/ Firefox\/SeaMonkey\/K-Meleon\/IceCat\/IceApe\/Firebird\/Phoenix\\n             \/(firefox)\\\\\/([\\\\w\\\\.]+)\\\\s[\\\\w\\\\s\\\\-]+\\\\\/[\\\\w\\\\.]+$\/i,                       \/\/ Other Firefox-based\\n-            \/(mozilla)\\\\\/([\\\\w\\\\.]+).+rv\\\\:.+gecko\\\\\/\\\\d+\/i,                          \/\/ Mozilla\\n+            \/(mozilla)\\\\\/([\\\\w\\\\.]+)\\\\s.+rv\\\\:.+gecko\\\\\/\\\\d+\/i,                        \/\/ Mozilla\\n \\n             \/\/ Other\\n             \/(polaris|lynx|dillo|icab|doris|amaya|w3m|netsurf|sleipnir)[\\\\\/\\\\s]?([\\\\w\\\\.]+)\/i,\\n@@ -487,7 +487,7 @@\\n             \/(sprint\\\\s(\\\\w+))\/i                                                  \/\/ Sprint Phones\\n             ], [[VENDOR, mapper.str, maps.device.sprint.vendor], [MODEL, mapper.str, maps.device.sprint.model], [TYPE, MOBILE]], [\\n \\n-            \/(htc)[;_\\\\s-]+([\\\\w\\\\s]+(?=\\\\)|\\\\sbuild)|\\\\w+)\/i,                        \/\/ HTC\\n+            \/(htc)[;_\\\\s-]{1,2}([\\\\w\\\\s]+(?=\\\\)|\\\\sbuild)|\\\\w+)\/i,                    \/\/ HTC\\n             \/(zte)-(\\\\w*)\/i,                                                     \/\/ ZTE\\n             \/(alcatel|geeksphone|nexian|panasonic|(?=;\\\\s)sony)[_\\\\s-]?([\\\\w-]*)\/i\\n                                                                                 \/\/ Alcatel\/GeeksPhone\/Nexian\/Panasonic\/Sony\\n@@ -591,13 +591,13 @@\\n             ], [MODEL, [VENDOR, 'Google'], [TYPE, MOBILE]], [\\n \\n             \/android.+;\\\\s(\\\\w+)\\\\s+build\\\\\/hm\\\\1\/i,                                 \/\/ Xiaomi Hongmi 'numeric' models\\n-            \/android.+(hm[\\\\s\\\\-_]*note?[\\\\s_]*(?:\\\\d\\\\w)?)\\\\s+build\/i,               \/\/ Xiaomi Hongmi\\n-            \/android.+(redmi[\\\\s\\\\-_]*(?:note|k)?(?:[\\\\s_]?[\\\\w\\\\s]+))(?:\\\\s+build|\\\\))\/i,      \\n+            \/android.+(hm[\\\\s\\\\-_]?note?[\\\\s_]?(?:\\\\d\\\\w)?)\\\\sbuild\/i,                \/\/ Xiaomi Hongmi\\n+            \/android.+(redmi[\\\\s\\\\-_]?(?:note|k)?(?:[\\\\s_]?[\\\\w\\\\s]+))(?:\\\\sbuild|\\\\))\/i,      \\n                                                                                 \/\/ Xiaomi Redmi\\n-            \/android.+(mi[\\\\s\\\\-_]*(?:a\\\\d|one|one[\\\\s_]plus|note lte)?[\\\\s_]?(?:\\\\d?\\\\w?)[\\\\s_]*(?:plus)?)\\\\s+build\/i    \\n+            \/android.+(mi[\\\\s\\\\-_]?(?:a\\\\d|one|one[\\\\s_]plus|note lte)?[\\\\s_]?(?:\\\\d?\\\\w?)[\\\\s_]?(?:plus)?)\\\\sbuild\/i    \\n                                                                                 \/\/ Xiaomi Mi\\n             ], [[MODEL, \/_\/g, ' '], [VENDOR, 'Xiaomi'], [TYPE, MOBILE]], [\\n-            \/android.+(mi[\\\\s\\\\-_]*(?:pad)(?:[\\\\s_]?[\\\\w\\\\s]+))(?:\\\\s+build|\\\\))\/i     \/\/ Mi Pad tablets\\n+            \/android.+(mi[\\\\s\\\\-_]?(?:pad)(?:[\\\\s_]?[\\\\w\\\\s]+))(?:\\\\sbuild|\\\\))\/i     \/\/ Mi Pad tablets\\n             ],[[MODEL, \/_\/g, ' '], [VENDOR, 'Xiaomi'], [TYPE, TABLET]], [\\n             \/android.+;\\\\s(m[1-5]\\\\snote)\\\\sbuild\/i                                \/\/ Meizu\\n             ], [MODEL, [VENDOR, 'Meizu'], [TYPE, MOBILE]], [\\n@@ -611,7 +611,7 @@\\n             \/android.+[;\\\\\/]\\\\s*(RCT[\\\\d\\\\w]+)\\\\s+build\/i                            \/\/ RCA Tablets\\n             ], [MODEL, [VENDOR, 'RCA'], [TYPE, TABLET]], [\\n \\n-            \/android.+[;\\\\\/\\\\s]+(Venue[\\\\d\\\\s]{2,7})\\\\s+build\/i                      \/\/ Dell Venue Tablets\\n+            \/android.+[;\\\\\/\\\\s](Venue[\\\\d\\\\s]{2,7})\\\\s+build\/i                       \/\/ Dell Venue Tablets\\n             ], [MODEL, [VENDOR, 'Dell'], [TYPE, TABLET]], [\\n \\n             \/android.+[;\\\\\/]\\\\s*(Q[T|M][\\\\d\\\\w]+)\\\\s+build\/i                         \/\/ Verizon Tablet\\n@@ -669,8 +669,8 @@\\n             \/android.+[;\\\\\/]\\\\s*TU_(1491)\\\\s+build\/i                               \/\/ Rotor Tablets\\n             ], [MODEL, [VENDOR, 'Rotor'], [TYPE, TABLET]], [\\n \\n-            \/android.+(KS(.+))\\\\s+build\/i                                        \/\/ Amazon Kindle Tablets\\n-            ], [MODEL, [VENDOR, 'Amazon'], [TYPE, TABLET]], [\\n+            \/\/android.+(KS(.+))\\\\s+build\/i                                        \/\/ Amazon Kindle Tablets\\n+            \/\/], [MODEL, [VENDOR, 'Amazon'], [TYPE, TABLET]], [\\n \\n             \/android.+(Gigaset)[\\\\s\\\\-]+(Q\\\\w{1,9})\\\\s+build\/i                      \/\/ Gigaset Tablets\\n             ], [VENDOR, MODEL, [TYPE, TABLET]], [\"}}",
            "message_norm":"fix redos vulnerabilities reported by snyk",
            "language":"en",
            "entities":"[('fix', 'ACTION', ''), ('redos', 'SECWORD', ''), ('vulnerabilities', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['src\/ua-parser.js'])",
            "num_files":1.0,
            "patch_content":"From 6d1f26df051ba681463ef109d36c9cf0f7e32b18 Mon Sep 17 00:00:00 2001\nFrom: Faisal Salman <f@faisalman.com>\nDate: Tue, 1 Dec 2020 00:50:19 +0700\nSubject: [PATCH] Fix ReDoS vulnerabilities reported by Snyk\n\n---\n src\/ua-parser.js | 28 ++++++++++++++--------------\n 1 file changed, 14 insertions(+), 14 deletions(-)\n\ndiff --git a\/src\/ua-parser.js b\/src\/ua-parser.js\nindex e522d9317..5ea799f83 100755\n--- a\/src\/ua-parser.js\n+++ b\/src\/ua-parser.js\n@@ -222,7 +222,7 @@\n \n             \/\/ Presto based\n             \/(opera\\smini)\\\/([\\w\\.-]+)\/i,                                       \/\/ Opera Mini\n-            \/(opera\\s[mobiletab]+).+version\\\/([\\w\\.-]+)\/i,                      \/\/ Opera Mobi\/Tablet\n+            \/(opera\\s[mobiletab]{3,6}).+version\\\/([\\w\\.-]+)\/i,                  \/\/ Opera Mobi\/Tablet\n             \/(opera).+version\\\/([\\w\\.]+)\/i,                                     \/\/ Opera > 9.80\n             \/(opera)[\\\/\\s]+([\\w\\.]+)\/i                                          \/\/ Opera < 9.80\n             ], [NAME, VERSION], [\n@@ -252,7 +252,7 @@\n             \/(konqueror)\\\/([\\w\\.]+)\/i                                           \/\/ Konqueror\n             ], [[NAME, 'Konqueror'], VERSION], [\n \n-            \/(trident).+rv[:\\s]([\\w\\.]+).+like\\sgecko\/i                         \/\/ IE11\n+            \/(trident).+rv[:\\s]([\\w\\.]{1,9}).+like\\sgecko\/i                     \/\/ IE11\n             ], [[NAME, 'IE'], VERSION], [\n \n             \/(edge|edgios|edga|edg)\\\/((\\d+)?[\\w\\.]+)\/i                          \/\/ Microsoft Edge\n@@ -362,13 +362,13 @@\n             \/fxios\\\/([\\w\\.-]+)\/i                                                \/\/ Firefox for iOS\n             ], [VERSION, [NAME, 'Firefox']], [\n \n-            \/version\\\/([\\w\\.]+).+?mobile\\\/\\w+\\s(safari)\/i                       \/\/ Mobile Safari\n+            \/version\\\/([\\w\\.]+)\\s.*mobile\\\/\\w+\\s(safari)\/i                      \/\/ Mobile Safari\n             ], [VERSION, [NAME, 'Mobile Safari']], [\n \n-            \/version\\\/([\\w\\.]+).+?(mobile\\s?safari|safari)\/i                    \/\/ Safari & Safari Mobile\n+            \/version\\\/([\\w\\.]+)\\s.*(mobile\\s?safari|safari)\/i                   \/\/ Safari & Safari Mobile\n             ], [VERSION, NAME], [\n \n-            \/webkit.+?(gsa)\\\/([\\w\\.]+).+?(mobile\\s?safari|safari)(\\\/[\\w\\.]+)\/i  \/\/ Google Search Appliance on iOS\n+            \/webkit.+?(gsa)\\\/([\\w\\.]+)\\s.*(mobile\\s?safari|safari)(\\\/[\\w\\.]+)\/i \/\/ Google Search Appliance on iOS\n             ], [[NAME, 'GSA'], VERSION], [\n \n             \/webkit.+?(mobile\\s?safari|safari)(\\\/[\\w\\.]+)\/i                     \/\/ Safari < 3.0\n@@ -387,7 +387,7 @@\n \n                                                                                 \/\/ Firefox\/SeaMonkey\/K-Meleon\/IceCat\/IceApe\/Firebird\/Phoenix\n             \/(firefox)\\\/([\\w\\.]+)\\s[\\w\\s\\-]+\\\/[\\w\\.]+$\/i,                       \/\/ Other Firefox-based\n-            \/(mozilla)\\\/([\\w\\.]+).+rv\\:.+gecko\\\/\\d+\/i,                          \/\/ Mozilla\n+            \/(mozilla)\\\/([\\w\\.]+)\\s.+rv\\:.+gecko\\\/\\d+\/i,                        \/\/ Mozilla\n \n             \/\/ Other\n             \/(polaris|lynx|dillo|icab|doris|amaya|w3m|netsurf|sleipnir)[\\\/\\s]?([\\w\\.]+)\/i,\n@@ -487,7 +487,7 @@\n             \/(sprint\\s(\\w+))\/i                                                  \/\/ Sprint Phones\n             ], [[VENDOR, mapper.str, maps.device.sprint.vendor], [MODEL, mapper.str, maps.device.sprint.model], [TYPE, MOBILE]], [\n \n-            \/(htc)[;_\\s-]+([\\w\\s]+(?=\\)|\\sbuild)|\\w+)\/i,                        \/\/ HTC\n+            \/(htc)[;_\\s-]{1,2}([\\w\\s]+(?=\\)|\\sbuild)|\\w+)\/i,                    \/\/ HTC\n             \/(zte)-(\\w*)\/i,                                                     \/\/ ZTE\n             \/(alcatel|geeksphone|nexian|panasonic|(?=;\\s)sony)[_\\s-]?([\\w-]*)\/i\n                                                                                 \/\/ Alcatel\/GeeksPhone\/Nexian\/Panasonic\/Sony\n@@ -591,13 +591,13 @@\n             ], [MODEL, [VENDOR, 'Google'], [TYPE, MOBILE]], [\n \n             \/android.+;\\s(\\w+)\\s+build\\\/hm\\1\/i,                                 \/\/ Xiaomi Hongmi 'numeric' models\n-            \/android.+(hm[\\s\\-_]*note?[\\s_]*(?:\\d\\w)?)\\s+build\/i,               \/\/ Xiaomi Hongmi\n-            \/android.+(redmi[\\s\\-_]*(?:note|k)?(?:[\\s_]?[\\w\\s]+))(?:\\s+build|\\))\/i,      \n+            \/android.+(hm[\\s\\-_]?note?[\\s_]?(?:\\d\\w)?)\\sbuild\/i,                \/\/ Xiaomi Hongmi\n+            \/android.+(redmi[\\s\\-_]?(?:note|k)?(?:[\\s_]?[\\w\\s]+))(?:\\sbuild|\\))\/i,      \n                                                                                 \/\/ Xiaomi Redmi\n-            \/android.+(mi[\\s\\-_]*(?:a\\d|one|one[\\s_]plus|note lte)?[\\s_]?(?:\\d?\\w?)[\\s_]*(?:plus)?)\\s+build\/i    \n+            \/android.+(mi[\\s\\-_]?(?:a\\d|one|one[\\s_]plus|note lte)?[\\s_]?(?:\\d?\\w?)[\\s_]?(?:plus)?)\\sbuild\/i    \n                                                                                 \/\/ Xiaomi Mi\n             ], [[MODEL, \/_\/g, ' '], [VENDOR, 'Xiaomi'], [TYPE, MOBILE]], [\n-            \/android.+(mi[\\s\\-_]*(?:pad)(?:[\\s_]?[\\w\\s]+))(?:\\s+build|\\))\/i     \/\/ Mi Pad tablets\n+            \/android.+(mi[\\s\\-_]?(?:pad)(?:[\\s_]?[\\w\\s]+))(?:\\sbuild|\\))\/i     \/\/ Mi Pad tablets\n             ],[[MODEL, \/_\/g, ' '], [VENDOR, 'Xiaomi'], [TYPE, TABLET]], [\n             \/android.+;\\s(m[1-5]\\snote)\\sbuild\/i                                \/\/ Meizu\n             ], [MODEL, [VENDOR, 'Meizu'], [TYPE, MOBILE]], [\n@@ -611,7 +611,7 @@\n             \/android.+[;\\\/]\\s*(RCT[\\d\\w]+)\\s+build\/i                            \/\/ RCA Tablets\n             ], [MODEL, [VENDOR, 'RCA'], [TYPE, TABLET]], [\n \n-            \/android.+[;\\\/\\s]+(Venue[\\d\\s]{2,7})\\s+build\/i                      \/\/ Dell Venue Tablets\n+            \/android.+[;\\\/\\s](Venue[\\d\\s]{2,7})\\s+build\/i                       \/\/ Dell Venue Tablets\n             ], [MODEL, [VENDOR, 'Dell'], [TYPE, TABLET]], [\n \n             \/android.+[;\\\/]\\s*(Q[T|M][\\d\\w]+)\\s+build\/i                         \/\/ Verizon Tablet\n@@ -669,8 +669,8 @@\n             \/android.+[;\\\/]\\s*TU_(1491)\\s+build\/i                               \/\/ Rotor Tablets\n             ], [MODEL, [VENDOR, 'Rotor'], [TYPE, TABLET]], [\n \n-            \/android.+(KS(.+))\\s+build\/i                                        \/\/ Amazon Kindle Tablets\n-            ], [MODEL, [VENDOR, 'Amazon'], [TYPE, TABLET]], [\n+            \/\/android.+(KS(.+))\\s+build\/i                                        \/\/ Amazon Kindle Tablets\n+            \/\/], [MODEL, [VENDOR, 'Amazon'], [TYPE, TABLET]], [\n \n             \/android.+(Gigaset)[\\s\\-]+(Q\\w{1,9})\\s+build\/i                      \/\/ Gigaset Tablets\n             ], [VENDOR, MODEL, [TYPE, TABLET]], ["
        },
        {
            "index":810,
            "vuln_id":"GHSA-f8m6-h2c7-8h9x",
            "cwe_id":"{'CWE-400'}",
            "score":7.5,
            "chain":"{'https:\/\/github.com\/nltk\/nltk\/commit\/1405aad979c6b8080dbbc8e0858f89b2e3690341'}",
            "dataset":"osv",
            "summary":"Inefficient Regular Expression Complexity in nltk (word_tokenize, sent_tokenize) ### Impact\nThe vulnerability is present in [`PunktSentenceTokenizer`](https:\/\/www.nltk.org\/api\/nltk.tokenize.punkt.html#nltk.tokenize.punkt.PunktSentenceTokenizer), [`sent_tokenize`](https:\/\/www.nltk.org\/api\/nltk.tokenize.html#nltk.tokenize.sent_tokenize)  and [`word_tokenize`](https:\/\/www.nltk.org\/api\/nltk.tokenize.html#nltk.tokenize.word_tokenize). Any users of this class, or these two functions, are vulnerable to a Regular Expression Denial of Service (ReDoS) attack. \nIn short, a specifically crafted long input to any of these vulnerable functions will cause them to take a significant amount of execution time. The effect of this vulnerability is noticeable with the following example:\n```python\nfrom nltk.tokenize import word_tokenize\n\nn = 8\nfor length in [10**i for i in range(2, n)]:\n    # Prepare a malicious input\n    text = \"a\" * length\n    start_t = time.time()\n    # Call `word_tokenize` and naively measure the execution time\n    word_tokenize(text)\n    print(f\"A length of {length:<{n}} takes {time.time() - start_t:.4f}s\")\n```\nWhich gave the following output during testing:\n```python\nA length of 100      takes 0.0060s\nA length of 1000     takes 0.0060s\nA length of 10000    takes 0.6320s\nA length of 100000   takes 56.3322s\n...\n```\nI canceled the execution of the program after running it for several hours.\n\nIf your program relies on any of the vulnerable functions for tokenizing unpredictable user input, then we would strongly recommend upgrading to a version of NLTK without the vulnerability, or applying the workaround described below.\n\n### Patches\nThe problem has been patched in NLTK 3.6.6. After the fix, running the above program gives the following result:\n```python\nA length of 100      takes 0.0070s\nA length of 1000     takes 0.0010s\nA length of 10000    takes 0.0060s\nA length of 100000   takes 0.0400s\nA length of 1000000  takes 0.3520s\nA length of 10000000 takes 3.4641s\n```\nThis output shows a linear relationship in execution time versus input length, which is desirable for regular expressions.\nWe recommend updating to NLTK 3.6.6+ if possible.\n\n### Workarounds\nThe execution time of the vulnerable functions is exponential to the length of a malicious input. With other words, the execution time can be bounded by limiting the maximum length of an input to any of the vulnerable functions. Our recommendation is to implement such a limit.\n\n### References\n* The issue showcasing the vulnerability: https:\/\/github.com\/nltk\/nltk\/issues\/2866\n* The pull request containing considerably more information on the vulnerability, and the fix: https:\/\/github.com\/nltk\/nltk\/pull\/2869\n* The commit containing the fix: 1405aad979c6b8080dbbc8e0858f89b2e3690341\n* Information on CWE-1333: Inefficient Regular Expression Complexity: https:\/\/cwe.mitre.org\/data\/definitions\/1333.html\n\n### For more information\nIf you have any questions or comments about this advisory:\n* Open an issue in [github.com\/nltk\/nltk](https:\/\/github.com\/nltk\/nltk)\n* Email us at [nltk.team@gmail.com](mailto:nltk.team@gmail.com)",
            "published_date":"2022-01-06",
            "chain_len":1,
            "project":"https:\/\/github.com\/nltk\/nltk",
            "commit_href":"https:\/\/github.com\/nltk\/nltk\/commit\/1405aad979c6b8080dbbc8e0858f89b2e3690341",
            "commit_sha":"1405aad979c6b8080dbbc8e0858f89b2e3690341",
            "patch":"SINGLE",
            "chain_ord":"['1405aad979c6b8080dbbc8e0858f89b2e3690341']",
            "before_first_fix_commit":"{'0b7b076247ec41f9b6b8a94400d48ea299e4b507'}",
            "last_fix_commit":"1405aad979c6b8080dbbc8e0858f89b2e3690341",
            "chain_ord_pos":1.0,
            "commit_datetime":"11\/26\/2021, 11:58:19",
            "message":"Resolved serious ReDoS in PunktSentenceTokenizer (#2869)\n\n* Resolved serious ReDOS in PunktSentenceTokenizer\r\n\r\n* Improve performance by relying on string split instead of re.search\r\n\r\n* Solved issue if sentence contains just one token",
            "author":"Tom Aarsen",
            "comments":null,
            "stats":"{'additions': 61, 'deletions': 5, 'total': 66}",
            "files":"{'nltk\/tokenize\/punkt.py': {'additions': 61, 'deletions': 5, 'changes': 66, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/nltk\/nltk\/raw\/1405aad979c6b8080dbbc8e0858f89b2e3690341\/nltk%2Ftokenize%2Fpunkt.py', 'patch': '@@ -266,7 +266,6 @@ def word_tokenize(self, s):\\n         return self._word_tokenizer_re().findall(s)\\n \\n     _period_context_fmt = r\"\"\"\\n-        \\\\S*                          # some word material\\n         %(SentEndChars)s             # a potential sentence ending\\n         (?=(?P<after_tok>\\n             %(NonWord)s              # either other punctuation\\n@@ -1284,8 +1283,7 @@ def debug_decisions(self, text):\\n         See format_debug_decision() to help make this output readable.\\n         \"\"\"\\n \\n-        for match in self._lang_vars.period_context_re().finditer(text):\\n-            decision_text = match.group() + match.group(\"after_tok\")\\n+        for match, decision_text in self._match_potential_end_contexts(text):\\n             tokens = self._tokenize_words(decision_text)\\n             tokens = list(self._annotate_first_pass(tokens))\\n             while tokens and not tokens[0].tok.endswith(self._lang_vars.sent_end_chars):\\n@@ -1333,10 +1331,68 @@ def sentences_from_text(self, text, realign_boundaries=True):\\n         \"\"\"\\n         return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]\\n \\n+    def _match_potential_end_contexts(self, text):\\n+        \"\"\"\\n+        Given a text, find the matches of potential sentence breaks,\\n+        alongside the contexts surrounding these sentence breaks.\\n+\\n+        Since the fix for the ReDOS discovered in issue #2866, we no longer match\\n+        the word before a potential end of sentence token. Instead, we use a separate\\n+        regex for this. As a consequence, `finditer`\\'s desire to find non-overlapping\\n+        matches no longer aids us in finding the single longest match.\\n+        Where previously, we could use::\\n+\\n+            >>> pst = PunktSentenceTokenizer()\\n+            >>> text = \"Very bad acting!!! I promise.\"\\n+            >>> list(pst._lang_vars.period_context_re().finditer(text)) # doctest: +SKIP\\n+            [<re.Match object; span=(9, 18), match=\\'acting!!!\\'>]\\n+\\n+        Now we have to find the word before (i.e. \\'acting\\') separately, and `finditer`\\n+        returns::\\n+\\n+            >>> pst = PunktSentenceTokenizer()\\n+            >>> text = \"Very bad acting!!! I promise.\"\\n+            >>> list(pst._lang_vars.period_context_re().finditer(text)) # doctest: +NORMALIZE_WHITESPACE\\n+            [<re.Match object; span=(15, 16), match=\\'!\\'>,\\n+            <re.Match object; span=(16, 17), match=\\'!\\'>,\\n+            <re.Match object; span=(17, 18), match=\\'!\\'>]\\n+\\n+        So, we need to find the word before the match from right to left, and then manually remove\\n+        the overlaps. That is what this method does::\\n+\\n+            >>> pst = PunktSentenceTokenizer()\\n+            >>> text = \"Very bad acting!!! I promise.\"\\n+            >>> pst._match_potential_end_contexts(text)\\n+            [(<re.Match object; span=(17, 18), match=\\'!\\'>, \\'acting!!! I\\')]\\n+\\n+        :param text: String of one or more sentences\\n+        :type text: str\\n+        :return: List of match-context tuples.\\n+        :rtype: List[Tuple[re.Match, str]]\\n+        \"\"\"\\n+        before_words = {}\\n+        matches = []\\n+        for match in reversed(list(self._lang_vars.period_context_re().finditer(text))):\\n+            # Ignore matches that have already been captured by matches to the right of this match\\n+            if matches and match.end() > before_start:\\n+                continue\\n+            # Find the word before the current match\\n+            split = text[: match.start()].rsplit(maxsplit=1)\\n+            before_start = len(split[0]) if len(split) == 2 else 0\\n+            before_words[match] = split[-1]\\n+            matches.append(match)\\n+\\n+        return [\\n+            (\\n+                match,\\n+                before_words[match] + match.group() + match.group(\"after_tok\"),\\n+            )\\n+            for match in matches[::-1]\\n+        ]\\n+\\n     def _slices_from_text(self, text):\\n         last_break = 0\\n-        for match in self._lang_vars.period_context_re().finditer(text):\\n-            context = match.group() + match.group(\"after_tok\")\\n+        for match, context in self._match_potential_end_contexts(text):\\n             if self.text_contains_sentbreak(context):\\n                 yield slice(last_break, match.end())\\n                 if match.group(\"next_tok\"):'}}",
            "message_norm":"resolved serious redos in punktsentencetokenizer (#2869)\n\n* resolved serious redos in punktsentencetokenizer\r\n\r\n* improve performance by relying on string split instead of re.search\r\n\r\n* solved issue if sentence contains just one token",
            "language":"en",
            "entities":"[('redos', 'SECWORD', ''), ('#2869', 'ISSUE', ''), ('redos', 'SECWORD', ''), ('improve', 'ACTION', ''), ('issue', 'FLAW', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['nltk\/tokenize\/punkt.py'])",
            "num_files":1.0,
            "patch_content":"From 1405aad979c6b8080dbbc8e0858f89b2e3690341 Mon Sep 17 00:00:00 2001\nFrom: Tom Aarsen <37621491+tomaarsen@users.noreply.github.com>\nDate: Fri, 26 Nov 2021 12:58:19 +0100\nSubject: [PATCH] Resolved serious ReDoS in PunktSentenceTokenizer (#2869)\n\n* Resolved serious ReDOS in PunktSentenceTokenizer\n\n* Improve performance by relying on string split instead of re.search\n\n* Solved issue if sentence contains just one token\n---\n nltk\/tokenize\/punkt.py | 66 ++++++++++++++++++++++++++++++++++++++----\n 1 file changed, 61 insertions(+), 5 deletions(-)\n\ndiff --git a\/nltk\/tokenize\/punkt.py b\/nltk\/tokenize\/punkt.py\nindex a08ff4c903..54937b9ecd 100644\n--- a\/nltk\/tokenize\/punkt.py\n+++ b\/nltk\/tokenize\/punkt.py\n@@ -266,7 +266,6 @@ def word_tokenize(self, s):\n         return self._word_tokenizer_re().findall(s)\n \n     _period_context_fmt = r\"\"\"\n-        \\S*                          # some word material\n         %(SentEndChars)s             # a potential sentence ending\n         (?=(?P<after_tok>\n             %(NonWord)s              # either other punctuation\n@@ -1284,8 +1283,7 @@ def debug_decisions(self, text):\n         See format_debug_decision() to help make this output readable.\n         \"\"\"\n \n-        for match in self._lang_vars.period_context_re().finditer(text):\n-            decision_text = match.group() + match.group(\"after_tok\")\n+        for match, decision_text in self._match_potential_end_contexts(text):\n             tokens = self._tokenize_words(decision_text)\n             tokens = list(self._annotate_first_pass(tokens))\n             while tokens and not tokens[0].tok.endswith(self._lang_vars.sent_end_chars):\n@@ -1333,10 +1331,68 @@ def sentences_from_text(self, text, realign_boundaries=True):\n         \"\"\"\n         return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]\n \n+    def _match_potential_end_contexts(self, text):\n+        \"\"\"\n+        Given a text, find the matches of potential sentence breaks,\n+        alongside the contexts surrounding these sentence breaks.\n+\n+        Since the fix for the ReDOS discovered in issue #2866, we no longer match\n+        the word before a potential end of sentence token. Instead, we use a separate\n+        regex for this. As a consequence, `finditer`'s desire to find non-overlapping\n+        matches no longer aids us in finding the single longest match.\n+        Where previously, we could use::\n+\n+            >>> pst = PunktSentenceTokenizer()\n+            >>> text = \"Very bad acting!!! I promise.\"\n+            >>> list(pst._lang_vars.period_context_re().finditer(text)) # doctest: +SKIP\n+            [<re.Match object; span=(9, 18), match='acting!!!'>]\n+\n+        Now we have to find the word before (i.e. 'acting') separately, and `finditer`\n+        returns::\n+\n+            >>> pst = PunktSentenceTokenizer()\n+            >>> text = \"Very bad acting!!! I promise.\"\n+            >>> list(pst._lang_vars.period_context_re().finditer(text)) # doctest: +NORMALIZE_WHITESPACE\n+            [<re.Match object; span=(15, 16), match='!'>,\n+            <re.Match object; span=(16, 17), match='!'>,\n+            <re.Match object; span=(17, 18), match='!'>]\n+\n+        So, we need to find the word before the match from right to left, and then manually remove\n+        the overlaps. That is what this method does::\n+\n+            >>> pst = PunktSentenceTokenizer()\n+            >>> text = \"Very bad acting!!! I promise.\"\n+            >>> pst._match_potential_end_contexts(text)\n+            [(<re.Match object; span=(17, 18), match='!'>, 'acting!!! I')]\n+\n+        :param text: String of one or more sentences\n+        :type text: str\n+        :return: List of match-context tuples.\n+        :rtype: List[Tuple[re.Match, str]]\n+        \"\"\"\n+        before_words = {}\n+        matches = []\n+        for match in reversed(list(self._lang_vars.period_context_re().finditer(text))):\n+            # Ignore matches that have already been captured by matches to the right of this match\n+            if matches and match.end() > before_start:\n+                continue\n+            # Find the word before the current match\n+            split = text[: match.start()].rsplit(maxsplit=1)\n+            before_start = len(split[0]) if len(split) == 2 else 0\n+            before_words[match] = split[-1]\n+            matches.append(match)\n+\n+        return [\n+            (\n+                match,\n+                before_words[match] + match.group() + match.group(\"after_tok\"),\n+            )\n+            for match in matches[::-1]\n+        ]\n+\n     def _slices_from_text(self, text):\n         last_break = 0\n-        for match in self._lang_vars.period_context_re().finditer(text):\n-            context = match.group() + match.group(\"after_tok\")\n+        for match, context in self._match_potential_end_contexts(text):\n             if self.text_contains_sentbreak(context):\n                 yield slice(last_break, match.end())\n                 if match.group(\"next_tok\"):"
        },
        {
            "index":419,
            "vuln_id":"GHSA-35g4-qx3c-vjhx",
            "cwe_id":"{'CWE-306'}",
            "score":6.5,
            "chain":"{'https:\/\/github.com\/matrix-org\/matrix-appservice-bridge\/commit\/b69e745584a34fcfd858df33e4631e420da07b9f'}",
            "dataset":"osv",
            "summary":"Automatic room upgrade handling can be used maliciously to bridge a room non-consentually  ### Impact\n\nIf a bridge has room upgrade handling turned on in the configuration (the `roomUpgradeOpts` key when instantiating a new `Bridge` instance.), any `m.room.tombstone` event it encounters will be used to unbridge the current room and bridge into the target room. However, the target room `m.room.create` event is not checked to verify if the `predecessor` field contains the previous room. This means that any mailcious admin of a bridged room can repoint the traffic to a different room without the new room being aware.\n\n\n### Patches\n\nVersions 2.6.1 and greater are patched.\n\n### Workarounds\n\nDisabling the automatic room upgrade handling can be done by removing the `roomUpgradeOpts` key from the `Bridge` class options. \n\n### References\n\nThe issue is patched by https:\/\/github.com\/matrix-org\/matrix-appservice-bridge\/pull\/330\n\n### For more information]\n\nIf you have any questions or comments about this advisory, email us at security@matrix.org.",
            "published_date":"2021-06-21",
            "chain_len":1,
            "project":"https:\/\/github.com\/matrix-org\/matrix-appservice-bridge",
            "commit_href":"https:\/\/github.com\/matrix-org\/matrix-appservice-bridge\/commit\/b69e745584a34fcfd858df33e4631e420da07b9f",
            "commit_sha":"b69e745584a34fcfd858df33e4631e420da07b9f",
            "patch":"SINGLE",
            "chain_ord":"['b69e745584a34fcfd858df33e4631e420da07b9f']",
            "before_first_fix_commit":"{'1b587e2455cbd30779f5052d2d2d46d58463a3e2'}",
            "last_fix_commit":"b69e745584a34fcfd858df33e4631e420da07b9f",
            "chain_ord_pos":1.0,
            "commit_datetime":"05\/28\/2021, 09:32:25",
            "message":"Check m.room.create event on room upgrade",
            "author":"Will Hunt",
            "comments":null,
            "stats":"{'additions': 7, 'deletions': 0, 'total': 7}",
            "files":"{'src\/components\/room-upgrade-handler.ts': {'additions': 7, 'deletions': 0, 'changes': 7, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/matrix-org\/matrix-appservice-bridge\/raw\/b69e745584a34fcfd858df33e4631e420da07b9f\/src%2Fcomponents%2Froom-upgrade-handler.ts', 'patch': \"@@ -145,6 +145,13 @@ export class RoomUpgradeHandler {\\n     private async onJoinedNewRoom(oldRoomId: string, newRoomId: string) {\\n         log.debug(`Joined ${newRoomId}`);\\n         const intent = this.bridge.getIntent();\\n+        const { predecessor } = await intent.getStateEvent(newRoomId, 'm.room.create');\\n+        if (predecessor.room_id !== oldRoomId) {\\n+            log.error(\\n+    `Room doesn't have a matching predecessor (expected: ${oldRoomId}, got: ${predecessor.room_id}), not bridging.`\\n+            );\\n+            return false;\\n+        }\\n         const asBot = this.bridge.getBot();\\n         if (this.opts.migrateStoreEntries) {\\n             const success = await this.migrateStoreEntries(oldRoomId, newRoomId);\"}}",
            "message_norm":"check m.room.create event on room upgrade",
            "language":"en",
            "entities":null,
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['src\/components\/room-upgrade-handler.ts'])",
            "num_files":1.0,
            "patch_content":"From b69e745584a34fcfd858df33e4631e420da07b9f Mon Sep 17 00:00:00 2001\nFrom: Will Hunt <will@half-shot.uk>\nDate: Fri, 28 May 2021 10:32:25 +0100\nSubject: [PATCH] Check m.room.create event on room upgrade\n\n---\n src\/components\/room-upgrade-handler.ts | 7 +++++++\n 1 file changed, 7 insertions(+)\n\ndiff --git a\/src\/components\/room-upgrade-handler.ts b\/src\/components\/room-upgrade-handler.ts\nindex 2a4237b9..7f62ca71 100644\n--- a\/src\/components\/room-upgrade-handler.ts\n+++ b\/src\/components\/room-upgrade-handler.ts\n@@ -145,6 +145,13 @@ export class RoomUpgradeHandler {\n     private async onJoinedNewRoom(oldRoomId: string, newRoomId: string) {\n         log.debug(`Joined ${newRoomId}`);\n         const intent = this.bridge.getIntent();\n+        const { predecessor } = await intent.getStateEvent(newRoomId, 'm.room.create');\n+        if (predecessor.room_id !== oldRoomId) {\n+            log.error(\n+    `Room doesn't have a matching predecessor (expected: ${oldRoomId}, got: ${predecessor.room_id}), not bridging.`\n+            );\n+            return false;\n+        }\n         const asBot = this.bridge.getBot();\n         if (this.opts.migrateStoreEntries) {\n             const success = await this.migrateStoreEntries(oldRoomId, newRoomId);"
        },
        {
            "index":115,
            "vuln_id":"GHSA-wcv5-qrj6-9pfm",
            "cwe_id":"{'CWE-787', 'CWE-120'}",
            "score":2.5,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/8f37b52e1320d8d72a9529b2468277791a261197'}",
            "dataset":"osv",
            "summary":"Heap buffer overflow in `Conv3DBackprop*` ### Impact\nMissing validation between arguments to `tf.raw_ops.Conv3DBackprop*` operations can result in heap buffer overflows:\n\n```python\nimport tensorflow as tf\n\ninput_sizes = tf.constant([1, 1, 1, 1, 2], shape=[5], dtype=tf.int32)\nfilter_tensor = tf.constant([734.6274508233133, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0,\n                            -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0,\n                            -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0], shape=[4, 1, 6, 1, 1], dtype=tf.float32)\nout_backprop = tf.constant([-10.0], shape=[1, 1, 1, 1, 1], dtype=tf.float32)\n\ntf.raw_ops.Conv3DBackpropInputV2(input_sizes=input_sizes, filter=filter_tensor, out_backprop=out_backprop, strides=[1, 89, 29, 89, 1], padding='SAME', data_format='NDHWC', dilations=[1, 1, 1, 1, 1])\n```\n```python\nimport tensorflow as tf\n\ninput_values = [-10.0] * (7 * 7 * 7 * 7 * 7)\ninput_values[0] = 429.6491056791816\ninput_sizes = tf.constant(input_values, shape=[7, 7, 7, 7, 7], dtype=tf.float32)\nfilter_tensor = tf.constant([7, 7, 7, 1, 1], shape=[5], dtype=tf.int32)\nout_backprop = tf.constant([-10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0], shape=[7, 1, 1, 1, 1], dtype=tf.float32)\n  \ntf.raw_ops.Conv3DBackpropFilterV2(input=input_sizes, filter_sizes=filter_tensor, out_backprop=out_backprop, strides=[1, 37, 65, 93, 1], padding='VALID', data_format='NDHWC', dilations=[1, 1, 1, 1, 1])\n```\n\nThis is because the [implementation](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/4814fafb0ca6b5ab58a09411523b2193fed23fed\/tensorflow\/core\/kernels\/conv_grad_shape_utils.cc#L94-L153) assumes that the `input`, `filter_sizes` and `out_backprop` tensors have the same shape, as they are accessed in parallel.\n\n### Patches\nWe have patched the issue in GitHub commit [8f37b52e1320d8d72a9529b2468277791a261197](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/8f37b52e1320d8d72a9529b2468277791a261197).\n\nThe fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our securityguide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by Yakun Zhang and Ying Wang of Baidu X-Team.",
            "published_date":"2021-05-21",
            "chain_len":1,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/8f37b52e1320d8d72a9529b2468277791a261197",
            "commit_sha":"8f37b52e1320d8d72a9529b2468277791a261197",
            "patch":"SINGLE",
            "chain_ord":"['8f37b52e1320d8d72a9529b2468277791a261197']",
            "before_first_fix_commit":"{'4814fafb0ca6b5ab58a09411523b2193fed23fed'}",
            "last_fix_commit":"8f37b52e1320d8d72a9529b2468277791a261197",
            "chain_ord_pos":1.0,
            "commit_datetime":"04\/19\/2021, 20:46:32",
            "message":"Validate some shape requirements for `Conv3DBackpropFilter*` and `Conv3DBackpropInput*` ops.\n\nOlder versions of Eigen might otherwise crash \/ produce OOB read on specially crafted inputs.\n\nPiperOrigin-RevId: 369293977\nChange-Id: I58f51445a93936d7cf8e616f75de17677df36718",
            "author":"Mihai Maruseac",
            "comments":null,
            "stats":"{'additions': 56, 'deletions': 0, 'total': 56}",
            "files":"{'tensorflow\/core\/kernels\/conv_grad_ops_3d.cc': {'additions': 56, 'deletions': 0, 'changes': 56, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/8f37b52e1320d8d72a9529b2468277791a261197\/tensorflow%2Fcore%2Fkernels%2Fconv_grad_ops_3d.cc', 'patch': '@@ -239,6 +239,20 @@ class Conv3DBackpropInputOp : public OpKernel {\\n       input_shape = context->input(0).shape();\\n     }\\n \\n+    OP_REQUIRES(\\n+        context, input_shape.dim_size(4) == filter_shape.dim_size(3),\\n+        errors::InvalidArgument(\"input and filter_sizes must have the same \"\\n+                                \"number of channels. Got \",\\n+                                input_shape.dim_size(4), \" for input and \",\\n+                                filter_shape.dim_size(3), \" for filter_sizes\"));\\n+    OP_REQUIRES(\\n+        context, out_backprop_shape.dim_size(4) == filter_shape.dim_size(4),\\n+        errors::InvalidArgument(\"out_backprop and filter_sizes must have the \"\\n+                                \"same number of channels. Got \",\\n+                                out_backprop_shape.dim_size(4),\\n+                                \" for out_backprop and \",\\n+                                filter_shape.dim_size(4), \" for filter_sizes\"));\\n+\\n     ConvBackpropDimensions dims;\\n     OP_REQUIRES_OK(context, ConvBackpropComputeDimensions(\\n                                 \"Conv3DBackpropInputOp\", \/*num_spatial_dims=*\/3,\\n@@ -346,6 +360,20 @@ class Conv3DCustomBackpropInputOp : public OpKernel {\\n       input_shape = context->input(0).shape();\\n     }\\n \\n+    OP_REQUIRES(\\n+        context, input_shape.dim_size(4) == filter_shape.dim_size(3),\\n+        errors::InvalidArgument(\"input and filter_sizes must have the same \"\\n+                                \"number of channels. Got \",\\n+                                input_shape.dim_size(4), \" for input and \",\\n+                                filter_shape.dim_size(3), \" for filter_sizes\"));\\n+    OP_REQUIRES(\\n+        context, out_backprop_shape.dim_size(4) == filter_shape.dim_size(4),\\n+        errors::InvalidArgument(\"out_backprop and filter_sizes must have the \"\\n+                                \"same number of channels. Got \",\\n+                                out_backprop_shape.dim_size(4),\\n+                                \" for out_backprop and \",\\n+                                filter_shape.dim_size(4), \" for filter_sizes\"));\\n+\\n     ConvBackpropDimensions dims;\\n     OP_REQUIRES_OK(context, ConvBackpropComputeDimensions(\\n                                 \"Conv3DBackpropInputOp\", \/*num_spatial_dims=*\/3,\\n@@ -696,6 +724,20 @@ class Conv3DBackpropFilterOp : public OpKernel {\\n       filter_shape = context->input(1).shape();\\n     }\\n \\n+    OP_REQUIRES(\\n+        context, input_shape.dim_size(4) == filter_shape.dim_size(3),\\n+        errors::InvalidArgument(\"input and filter_sizes must have the same \"\\n+                                \"number of channels. Got \",\\n+                                input_shape.dim_size(4), \" for input and \",\\n+                                filter_shape.dim_size(3), \" for filter_sizes\"));\\n+    OP_REQUIRES(\\n+        context, out_backprop_shape.dim_size(4) == filter_shape.dim_size(4),\\n+        errors::InvalidArgument(\"out_backprop and filter_sizes must have the \"\\n+                                \"same number of channels. Got \",\\n+                                out_backprop_shape.dim_size(4),\\n+                                \" for out_backprop and \",\\n+                                filter_shape.dim_size(4), \" for filter_sizes\"));\\n+\\n     ConvBackpropDimensions dims;\\n     OP_REQUIRES_OK(context,\\n                    ConvBackpropComputeDimensions(\\n@@ -808,6 +850,20 @@ class Conv3DCustomBackpropFilterOp : public OpKernel {\\n       filter_shape = context->input(1).shape();\\n     }\\n \\n+    OP_REQUIRES(\\n+        context, input_shape.dim_size(4) == filter_shape.dim_size(3),\\n+        errors::InvalidArgument(\"input and filter_sizes must have the same \"\\n+                                \"number of channels. Got \",\\n+                                input_shape.dim_size(4), \" for input and \",\\n+                                filter_shape.dim_size(3), \" for filter_sizes\"));\\n+    OP_REQUIRES(\\n+        context, out_backprop_shape.dim_size(4) == filter_shape.dim_size(4),\\n+        errors::InvalidArgument(\"out_backprop and filter_sizes must have the \"\\n+                                \"same number of channels. Got \",\\n+                                out_backprop_shape.dim_size(4),\\n+                                \" for out_backprop and \",\\n+                                filter_shape.dim_size(4), \" for filter_sizes\"));\\n+\\n     ConvBackpropDimensions dims;\\n     OP_REQUIRES_OK(context,\\n                    ConvBackpropComputeDimensions('}}",
            "message_norm":"validate some shape requirements for `conv3dbackpropfilter*` and `conv3dbackpropinput*` ops.\n\nolder versions of eigen might otherwise crash \/ produce oob read on specially crafted inputs.\n\npiperorigin-revid: 369293977\nchange-id: i58f51445a93936d7cf8e616f75de17677df36718",
            "language":"en",
            "entities":"[('validate', 'ACTION', ''), ('oob', 'SECWORD', ''), ('369293977', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/core\/kernels\/conv_grad_ops_3d.cc'])",
            "num_files":1.0,
            "patch_content":"From 8f37b52e1320d8d72a9529b2468277791a261197 Mon Sep 17 00:00:00 2001\nFrom: Mihai Maruseac <mihaimaruseac@google.com>\nDate: Mon, 19 Apr 2021 13:46:32 -0700\nSubject: [PATCH] Validate some shape requirements for `Conv3DBackpropFilter*`\n and `Conv3DBackpropInput*` ops.\n\nOlder versions of Eigen might otherwise crash \/ produce OOB read on specially crafted inputs.\n\nPiperOrigin-RevId: 369293977\nChange-Id: I58f51445a93936d7cf8e616f75de17677df36718\n---\n tensorflow\/core\/kernels\/conv_grad_ops_3d.cc | 56 +++++++++++++++++++++\n 1 file changed, 56 insertions(+)\n\ndiff --git a\/tensorflow\/core\/kernels\/conv_grad_ops_3d.cc b\/tensorflow\/core\/kernels\/conv_grad_ops_3d.cc\nindex f736a12fb1ca39..8c72d01578d6d1 100644\n--- a\/tensorflow\/core\/kernels\/conv_grad_ops_3d.cc\n+++ b\/tensorflow\/core\/kernels\/conv_grad_ops_3d.cc\n@@ -239,6 +239,20 @@ class Conv3DBackpropInputOp : public OpKernel {\n       input_shape = context->input(0).shape();\n     }\n \n+    OP_REQUIRES(\n+        context, input_shape.dim_size(4) == filter_shape.dim_size(3),\n+        errors::InvalidArgument(\"input and filter_sizes must have the same \"\n+                                \"number of channels. Got \",\n+                                input_shape.dim_size(4), \" for input and \",\n+                                filter_shape.dim_size(3), \" for filter_sizes\"));\n+    OP_REQUIRES(\n+        context, out_backprop_shape.dim_size(4) == filter_shape.dim_size(4),\n+        errors::InvalidArgument(\"out_backprop and filter_sizes must have the \"\n+                                \"same number of channels. Got \",\n+                                out_backprop_shape.dim_size(4),\n+                                \" for out_backprop and \",\n+                                filter_shape.dim_size(4), \" for filter_sizes\"));\n+\n     ConvBackpropDimensions dims;\n     OP_REQUIRES_OK(context, ConvBackpropComputeDimensions(\n                                 \"Conv3DBackpropInputOp\", \/*num_spatial_dims=*\/3,\n@@ -346,6 +360,20 @@ class Conv3DCustomBackpropInputOp : public OpKernel {\n       input_shape = context->input(0).shape();\n     }\n \n+    OP_REQUIRES(\n+        context, input_shape.dim_size(4) == filter_shape.dim_size(3),\n+        errors::InvalidArgument(\"input and filter_sizes must have the same \"\n+                                \"number of channels. Got \",\n+                                input_shape.dim_size(4), \" for input and \",\n+                                filter_shape.dim_size(3), \" for filter_sizes\"));\n+    OP_REQUIRES(\n+        context, out_backprop_shape.dim_size(4) == filter_shape.dim_size(4),\n+        errors::InvalidArgument(\"out_backprop and filter_sizes must have the \"\n+                                \"same number of channels. Got \",\n+                                out_backprop_shape.dim_size(4),\n+                                \" for out_backprop and \",\n+                                filter_shape.dim_size(4), \" for filter_sizes\"));\n+\n     ConvBackpropDimensions dims;\n     OP_REQUIRES_OK(context, ConvBackpropComputeDimensions(\n                                 \"Conv3DBackpropInputOp\", \/*num_spatial_dims=*\/3,\n@@ -696,6 +724,20 @@ class Conv3DBackpropFilterOp : public OpKernel {\n       filter_shape = context->input(1).shape();\n     }\n \n+    OP_REQUIRES(\n+        context, input_shape.dim_size(4) == filter_shape.dim_size(3),\n+        errors::InvalidArgument(\"input and filter_sizes must have the same \"\n+                                \"number of channels. Got \",\n+                                input_shape.dim_size(4), \" for input and \",\n+                                filter_shape.dim_size(3), \" for filter_sizes\"));\n+    OP_REQUIRES(\n+        context, out_backprop_shape.dim_size(4) == filter_shape.dim_size(4),\n+        errors::InvalidArgument(\"out_backprop and filter_sizes must have the \"\n+                                \"same number of channels. Got \",\n+                                out_backprop_shape.dim_size(4),\n+                                \" for out_backprop and \",\n+                                filter_shape.dim_size(4), \" for filter_sizes\"));\n+\n     ConvBackpropDimensions dims;\n     OP_REQUIRES_OK(context,\n                    ConvBackpropComputeDimensions(\n@@ -808,6 +850,20 @@ class Conv3DCustomBackpropFilterOp : public OpKernel {\n       filter_shape = context->input(1).shape();\n     }\n \n+    OP_REQUIRES(\n+        context, input_shape.dim_size(4) == filter_shape.dim_size(3),\n+        errors::InvalidArgument(\"input and filter_sizes must have the same \"\n+                                \"number of channels. Got \",\n+                                input_shape.dim_size(4), \" for input and \",\n+                                filter_shape.dim_size(3), \" for filter_sizes\"));\n+    OP_REQUIRES(\n+        context, out_backprop_shape.dim_size(4) == filter_shape.dim_size(4),\n+        errors::InvalidArgument(\"out_backprop and filter_sizes must have the \"\n+                                \"same number of channels. Got \",\n+                                out_backprop_shape.dim_size(4),\n+                                \" for out_backprop and \",\n+                                filter_shape.dim_size(4), \" for filter_sizes\"));\n+\n     ConvBackpropDimensions dims;\n     OP_REQUIRES_OK(context,\n                    ConvBackpropComputeDimensions("
        },
        {
            "index":90,
            "vuln_id":"GHSA-56cx-wf47-hx7w",
            "cwe_id":"{'CWE-307'}",
            "score":5.3,
            "chain":"{'https:\/\/github.com\/firefly-iii\/firefly-iii\/commit\/afc9f4b7ebc8a240c85864a6e1abda62bfeefae8'}",
            "dataset":"osv",
            "summary":" Improper Restriction of Excessive Authentication Attempts firefly-iii is vulnerable to Improper Restriction of Excessive Authentication Attempts",
            "published_date":"2021-08-09",
            "chain_len":1,
            "project":"https:\/\/github.com\/firefly-iii\/firefly-iii",
            "commit_href":"https:\/\/github.com\/firefly-iii\/firefly-iii\/commit\/afc9f4b7ebc8a240c85864a6e1abda62bfeefae8",
            "commit_sha":"afc9f4b7ebc8a240c85864a6e1abda62bfeefae8",
            "patch":"SINGLE",
            "chain_ord":"['afc9f4b7ebc8a240c85864a6e1abda62bfeefae8']",
            "before_first_fix_commit":"{'cb759e5c21118cf18b29cb4619d7a2a540a9c76f'}",
            "last_fix_commit":"afc9f4b7ebc8a240c85864a6e1abda62bfeefae8",
            "chain_ord_pos":1.0,
            "commit_datetime":"07\/23\/2021, 04:26:42",
            "message":"Add missing rate limiter.",
            "author":"James Cole",
            "comments":null,
            "stats":"{'additions': 2, 'deletions': 1, 'total': 3}",
            "files":"{'app\/Http\/Controllers\/Auth\/LoginController.php': {'additions': 2, 'deletions': 1, 'changes': 3, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/firefly-iii\/firefly-iii\/raw\/afc9f4b7ebc8a240c85864a6e1abda62bfeefae8\/app%2FHttp%2FControllers%2FAuth%2FLoginController.php', 'patch': '@@ -29,6 +29,7 @@\\n use FireflyIII\\\\Providers\\\\RouteServiceProvider;\\n use Illuminate\\\\Contracts\\\\View\\\\Factory;\\n use Illuminate\\\\Foundation\\\\Auth\\\\AuthenticatesUsers;\\n+use Illuminate\\\\Foundation\\\\Auth\\\\ThrottlesLogins;\\n use Illuminate\\\\Http\\\\JsonResponse;\\n use Illuminate\\\\Http\\\\RedirectResponse;\\n use Illuminate\\\\Http\\\\Request;\\n@@ -47,7 +48,7 @@\\n  *\/\\n class LoginController extends Controller\\n {\\n-    use AuthenticatesUsers;\\n+    use AuthenticatesUsers, ThrottlesLogins;\\n \\n     \/**\\n      * Where to redirect users after login.'}}",
            "message_norm":"add missing rate limiter.",
            "language":"et",
            "entities":"[('add', 'ACTION', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['app\/Http\/Controllers\/Auth\/LoginController.php'])",
            "num_files":1.0,
            "patch_content":"From afc9f4b7ebc8a240c85864a6e1abda62bfeefae8 Mon Sep 17 00:00:00 2001\nFrom: James Cole <james@firefly-iii.org>\nDate: Fri, 23 Jul 2021 06:26:42 +0200\nSubject: [PATCH] Add missing rate limiter.\n\n---\n app\/Http\/Controllers\/Auth\/LoginController.php | 3 ++-\n 1 file changed, 2 insertions(+), 1 deletion(-)\n\ndiff --git a\/app\/Http\/Controllers\/Auth\/LoginController.php b\/app\/Http\/Controllers\/Auth\/LoginController.php\nindex 13afb3a4661..27a8cb866d3 100644\n--- a\/app\/Http\/Controllers\/Auth\/LoginController.php\n+++ b\/app\/Http\/Controllers\/Auth\/LoginController.php\n@@ -29,6 +29,7 @@\n use FireflyIII\\Providers\\RouteServiceProvider;\n use Illuminate\\Contracts\\View\\Factory;\n use Illuminate\\Foundation\\Auth\\AuthenticatesUsers;\n+use Illuminate\\Foundation\\Auth\\ThrottlesLogins;\n use Illuminate\\Http\\JsonResponse;\n use Illuminate\\Http\\RedirectResponse;\n use Illuminate\\Http\\Request;\n@@ -47,7 +48,7 @@\n  *\/\n class LoginController extends Controller\n {\n-    use AuthenticatesUsers;\n+    use AuthenticatesUsers, ThrottlesLogins;\n \n     \/**\n      * Where to redirect users after login."
        },
        {
            "index":808,
            "vuln_id":"GHSA-v6vg-pxvv-g5cq",
            "cwe_id":"{'CWE-269'}",
            "score":6.5,
            "chain":"{'https:\/\/github.com\/snipe\/snipe-it\/commit\/db0c0e790892db874573d95f8ae4268b8a011ab1'}",
            "dataset":"osv",
            "summary":"Improper Privilege Management in Snipe-IT Snipe-IT prior to 5.3.9 is vulnerable to improper privilege management. A user who does not have access to the supplier module may view supplier content.",
            "published_date":"2022-02-15",
            "chain_len":1,
            "project":"https:\/\/github.com\/snipe\/snipe-it",
            "commit_href":"https:\/\/github.com\/snipe\/snipe-it\/commit\/db0c0e790892db874573d95f8ae4268b8a011ab1",
            "commit_sha":"db0c0e790892db874573d95f8ae4268b8a011ab1",
            "patch":"SINGLE",
            "chain_ord":"['db0c0e790892db874573d95f8ae4268b8a011ab1']",
            "before_first_fix_commit":"{'05c0819776b07425b2831cd31a8a0f4e7ac30c09', 'd77a47765ea1fd112a9b0731a88de1e26ed24256'}",
            "last_fix_commit":"db0c0e790892db874573d95f8ae4268b8a011ab1",
            "chain_ord_pos":1.0,
            "commit_datetime":"02\/13\/2022, 18:56:55",
            "message":"Merge pull request #10665 from snipe\/fixes\/adds_gate_to_supplier_view\n\nAdds gate to supplier",
            "author":"snipe",
            "comments":null,
            "stats":"{'additions': 1, 'deletions': 0, 'total': 1}",
            "files":"{'app\/Http\/Controllers\/SuppliersController.php': {'additions': 1, 'deletions': 0, 'changes': 1, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/snipe\/snipe-it\/raw\/db0c0e790892db874573d95f8ae4268b8a011ab1\/app%2FHttp%2FControllers%2FSuppliersController.php', 'patch': \"@@ -184,6 +184,7 @@ public function destroy($supplierId)\\n      *\/\\n     public function show($supplierId = null)\\n     {\\n+        $this->authorize('view', Supplier::class);\\n         $supplier = Supplier::find($supplierId);\\n \\n         if (isset($supplier->id)) {\"}}",
            "message_norm":"merge pull request #10665 from snipe\/fixes\/adds_gate_to_supplier_view\n\nadds gate to supplier",
            "language":"en",
            "entities":"[('#10665', 'ISSUE', ''), ('adds_gate_to_supplier_view', 'ACTION', ''), ('adds', 'ACTION', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['app\/Http\/Controllers\/SuppliersController.php'])",
            "num_files":1.0,
            "patch_content":"From d77a47765ea1fd112a9b0731a88de1e26ed24256 Mon Sep 17 00:00:00 2001\nFrom: snipe <snipe@snipe.net>\nDate: Sun, 13 Feb 2022 11:53:45 -0700\nSubject: [PATCH] Adds gate to supplier\n\nSigned-off-by: snipe <snipe@snipe.net>\n---\n app\/Http\/Controllers\/SuppliersController.php | 1 +\n 1 file changed, 1 insertion(+)\n\ndiff --git a\/app\/Http\/Controllers\/SuppliersController.php b\/app\/Http\/Controllers\/SuppliersController.php\nindex 41ff45679190..7fcaa11ddf27 100755\n--- a\/app\/Http\/Controllers\/SuppliersController.php\n+++ b\/app\/Http\/Controllers\/SuppliersController.php\n@@ -184,6 +184,7 @@ public function destroy($supplierId)\n      *\/\n     public function show($supplierId = null)\n     {\n+        $this->authorize('view', Supplier::class);\n         $supplier = Supplier::find($supplierId);\n \n         if (isset($supplier->id)) {"
        },
        {
            "index":255,
            "vuln_id":"GHSA-vpwq-6cp4-ffqc",
            "cwe_id":"{'CWE-79'}",
            "score":5.4,
            "chain":"{'https:\/\/github.com\/star7th\/showdoc\/commit\/ba45d19e1d77a7eea866dab30eff5da552694891'}",
            "dataset":"osv",
            "summary":"Stored Cross-site Scripting in ShowDoc ShowDoc prior to version 2.10.4 is vulnerable to stored cross-site scripting viva `axd` and `cshtml` file upload.",
            "published_date":"2022-03-16",
            "chain_len":1,
            "project":"https:\/\/github.com\/star7th\/showdoc",
            "commit_href":"https:\/\/github.com\/star7th\/showdoc\/commit\/ba45d19e1d77a7eea866dab30eff5da552694891",
            "commit_sha":"ba45d19e1d77a7eea866dab30eff5da552694891",
            "patch":"SINGLE",
            "chain_ord":"['ba45d19e1d77a7eea866dab30eff5da552694891']",
            "before_first_fix_commit":"{'4e6b321c0d63ee7c4480409c7a68ae116096c4bc'}",
            "last_fix_commit":"ba45d19e1d77a7eea866dab30eff5da552694891",
            "chain_ord_pos":1.0,
            "commit_datetime":"03\/14\/2022, 02:52:48",
            "message":"bug",
            "author":"star7th",
            "comments":null,
            "stats":"{'additions': 2, 'deletions': 0, 'total': 2}",
            "files":"{'server\/Application\/Api\/Model\/AttachmentModel.class.php': {'additions': 2, 'deletions': 0, 'changes': 2, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/star7th\/showdoc\/raw\/ba45d19e1d77a7eea866dab30eff5da552694891\/server%2FApplication%2FApi%2FModel%2FAttachmentModel.class.php', 'patch': '@@ -309,6 +309,8 @@ public function isDangerFilename($filename){\\n \\t\\t\\t|| $isDangerStr($filename , \".aspx\")\\n \\t\\t\\t|| $isDangerStr($filename , \".xsd\")\\n \\t\\t\\t|| $isDangerStr($filename , \".asa\")\\n+\\t\\t\\t|| $isDangerStr($filename , \".cshtml\")\\n+\\t\\t\\t|| $isDangerStr($filename , \".axd\")\\n \\t\\t) {\\n \\t\\t\\treturn true;\\n \\t\\t}'}}",
            "message_norm":"bug",
            "language":"id",
            "entities":"[('bug', 'FLAW', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['server\/Application\/Api\/Model\/AttachmentModel.class.php'])",
            "num_files":1.0,
            "patch_content":"From ba45d19e1d77a7eea866dab30eff5da552694891 Mon Sep 17 00:00:00 2001\nFrom: star7th <xing7th@gmail.com>\nDate: Mon, 14 Mar 2022 10:52:48 +0800\nSubject: [PATCH] bug\n\n---\n server\/Application\/Api\/Model\/AttachmentModel.class.php | 2 ++\n 1 file changed, 2 insertions(+)\n\ndiff --git a\/server\/Application\/Api\/Model\/AttachmentModel.class.php b\/server\/Application\/Api\/Model\/AttachmentModel.class.php\nindex 3a41ecd99..b518b9b57 100644\n--- a\/server\/Application\/Api\/Model\/AttachmentModel.class.php\n+++ b\/server\/Application\/Api\/Model\/AttachmentModel.class.php\n@@ -309,6 +309,8 @@ public function isDangerFilename($filename){\n \t\t\t|| $isDangerStr($filename , \".aspx\")\n \t\t\t|| $isDangerStr($filename , \".xsd\")\n \t\t\t|| $isDangerStr($filename , \".asa\")\n+\t\t\t|| $isDangerStr($filename , \".cshtml\")\n+\t\t\t|| $isDangerStr($filename , \".axd\")\n \t\t) {\n \t\t\treturn true;\n \t\t}"
        },
        {
            "index":511,
            "vuln_id":"GHSA-8rmh-55h4-93h5",
            "cwe_id":"{'CWE-22'}",
            "score":7.2,
            "chain":"{'https:\/\/github.com\/DSpace\/DSpace\/commit\/7af52a0883a9dbc475cf3001f04ed11b24c8a4c0', 'https:\/\/github.com\/DSpace\/DSpace\/commit\/56e76049185bbd87c994128a9d77735ad7af0199'}",
            "dataset":"osv",
            "summary":"DSpace ItemImportService API Vulnerable to Path Traversal in Simple Archive Format Package Import ### Impact\nItemImportServiceImpl is vulnerable to a path traversal vulnerability. This means a malicious SAF (simple archive format) package could cause a file\/directory to be created anywhere the Tomcat\/DSpace user can write to on the server.  However, this path traversal vulnerability is only possible by a user with special privileges (either Administrators or someone with command-line access to the server).  This vulnerability impacts the XMLUI, JSPUI and command-line.\n\n_This vulnerability does NOT impact 7.x._\n\n### Patches\n\n_DSpace 6.x:_ \n* Fixed in 6.4 via commit: https:\/\/github.com\/DSpace\/DSpace\/commit\/7af52a0883a9dbc475cf3001f04ed11b24c8a4c0\n* 6.x patch file: https:\/\/github.com\/DSpace\/DSpace\/commit\/7af52a0883a9dbc475cf3001f04ed11b24c8a4c0.patch (may be applied manually if an immediate upgrade to 6.4 or 7.x is not possible)\n\n_DSpace 5.x:_\n* Fixed in 5.11 via commit: https:\/\/github.com\/DSpace\/DSpace\/commit\/56e76049185bbd87c994128a9d77735ad7af0199\n* 5.x patch file: https:\/\/github.com\/DSpace\/DSpace\/commit\/56e76049185bbd87c994128a9d77735ad7af0199.patch (may be applied manually if an immediate upgrade to 5.11 or 6.4 or 7.x is not possible)\n\n#### Apply the patch to your DSpace\nIf at all possible, we recommend upgrading your DSpace site based on the upgrade instructions. However, if you are unable to do so, you can manually apply the above patches as follows:\n1. Download the appropriate patch file to the machine where DSpace is running\n2. From the `[dspace-src]` folder, apply the patch, e.g. `git apply [name-of-file].patch`\n3. Now, update your DSpace site (based loosely on the Upgrade instructions). This generally involves three steps:\n    1. Rebuild DSpace, e.g. `mvn -U clean package`  (This will recompile all DSpace code)\n    2. Redeploy DSpace, e.g. `ant update`  (This will copy all updated WARs \/ configs to your installation directory). Depending on your setup you also may need to copy the updated WARs over to your Tomcat webapps folder.\n    3. Restart Tomcat\n\n### Workarounds\n\nAs a basic workaround, you may block all access to the following URL paths:\n* If you are using the XMLUI, block all access to `\/admin\/batchimport` path (this is the URL of the Admin Batch Import tool). Keep in mind, if your site uses the path \"\/xmlui\", then you'd need to block access to `\/xmlui\/admin\/batchimport`.\n* If you are using the JSPUI, block all access to `\/dspace-admin\/batchimport` path (this is the URL of the Admin Batch Import tool).  Keep in mind, if your site uses the path \"\/jspui\", then you'd need to block access to `\/jspui\/dspace-admin\/batchimport`.\n\nKeep in mind, only an Administrative user or a user with command-line access to the server is able to import\/upload SAF packages. Therefore, assuming those users do not blindly upload untrusted SAF packages, then it is unlikely your site could be impacted by this vulnerability.\n\n\n### For more information\nIf you have any questions or comments about this advisory:\n* Email us at security@dspace.org",
            "published_date":"2022-08-06",
            "chain_len":2,
            "project":"https:\/\/github.com\/DSpace\/DSpace",
            "commit_href":"https:\/\/github.com\/DSpace\/DSpace\/commit\/56e76049185bbd87c994128a9d77735ad7af0199",
            "commit_sha":"56e76049185bbd87c994128a9d77735ad7af0199",
            "patch":"MULTI",
            "chain_ord":"['7af52a0883a9dbc475cf3001f04ed11b24c8a4c0', '56e76049185bbd87c994128a9d77735ad7af0199']",
            "before_first_fix_commit":"{'73cdff26fdc40bb022e21dcfdeefebf28057cde7'}",
            "last_fix_commit":"56e76049185bbd87c994128a9d77735ad7af0199",
            "chain_ord_pos":2.0,
            "commit_datetime":"01\/14\/2022, 00:37:25",
            "message":"[DS-4131] Better path handling in ItemImport zips",
            "author":"Kim Shepherd",
            "comments":null,
            "stats":"{'additions': 30, 'deletions': 6, 'total': 36}",
            "files":"{'dspace-api\/src\/main\/java\/org\/dspace\/app\/itemimport\/ItemImport.java': {'additions': 30, 'deletions': 6, 'changes': 36, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/DSpace\/DSpace\/raw\/56e76049185bbd87c994128a9d77735ad7af0199\/dspace-api%2Fsrc%2Fmain%2Fjava%2Forg%2Fdspace%2Fapp%2Fitemimport%2FItemImport.java', 'patch': '@@ -2003,22 +2003,30 @@ public static String unzip(File zipfile, String destDir) throws IOException {\\n         if (destinationDir == null){\\n         \\tdestinationDir = tempWorkDir;\\n         }\\n+        log.debug(\"Using directory \" + destinationDir + \" for zip extraction. (destDir arg is \" + destDir +\\n+                \", tempWorkDir is \" + tempWorkDir + \")\");\\n \\n         File tempdir = new File(destinationDir);\\n         if (!tempdir.isDirectory())\\n         {\\n-            log.error(\"\\'\" + ConfigurationManager.getProperty(\"org.dspace.app.itemexport.work.dir\") +\\n-                    \"\\' as defined by the key \\'org.dspace.app.itemexport.work.dir\\' in dspace.cfg \" +\\n+            log.error(\"\\'\" + ConfigurationManager.getProperty(\"org.dspace.app.batchitemimport.work.dir\") +\\n+                    \"\\' as defined by the key \\'org.dspace.app.batchitemimport.work.dir\\' in dspace.cfg \" +\\n                     \"is not a valid directory\");\\n         }\\n \\n         if (!tempdir.exists() && !tempdir.mkdirs())\\n         {\\n             log.error(\"Unable to create temporary directory: \" + tempdir.getAbsolutePath());\\n         }\\n-        String sourcedir = destinationDir + System.getProperty(\"file.separator\") + zipfile.getName();\\n-        String zipDir = destinationDir + System.getProperty(\"file.separator\") + zipfile.getName() + System.getProperty(\"file.separator\");\\n \\n+        if(!destinationDir.endsWith(System.getProperty(\"file.separator\"))) {\\n+            destinationDir += System.getProperty(\"file.separator\");\\n+        }\\n+\\n+        String sourcedir = destinationDir + zipfile.getName();\\n+        String zipDir = destinationDir + zipfile.getName() + System.getProperty(\"file.separator\");\\n+\\n+        log.debug(\"zip directory to use is \" + zipDir);\\n \\n         \/\/ 3\\n         String sourceDirForZip = sourcedir;\\n@@ -2028,11 +2036,26 @@ public static String unzip(File zipfile, String destDir) throws IOException {\\n         while (entries.hasMoreElements())\\n         {\\n             entry = entries.nextElement();\\n+            \/\/ Check that the true path to extract files is never outside allowed temp directories\\n+            \/\/ without creating any actual files on disk\\n+            log.debug(\"Inspecting entry name: \" + entry.getName() + \" for path traversal security\");\\n+            File potentialExtract = new File(zipDir + entry.getName());\\n+            String canonicalPath = potentialExtract.getCanonicalPath();\\n+            log.debug(\"Canonical path to potential File is \" + canonicalPath);\\n+            if(!canonicalPath.startsWith(zipDir)) {\\n+                log.error(\"Rejecting zip file: \" + zipfile.getName() + \" as it contains an entry that would be extracted \" +\\n+                        \"outside the temporary unzip directory: \" + canonicalPath);\\n+                throw new IOException(\"Error extracting \" + zipfile + \": Canonical path of zip entry: \" +\\n+                        entry.getName() + \" (\" + canonicalPath + \") does not start with permissible temp \" +\\n+                        \"unzip directory (\" + destinationDir + \")\");\\n+            }\\n             if (entry.isDirectory())\\n             {\\n-                if (!new File(zipDir + entry.getName()).mkdir())\\n-                {\\n+                \/\/ Log error and throw IOException if a directory entry could not be created\\n+                File newDir = new File(zipDir + entry.getName());\\n+                if (!newDir.mkdirs()) {\\n                     log.error(\"Unable to create contents directory: \" + zipDir + entry.getName());\\n+                    throw new IOException(\"Unable to create contents directory: \" + zipDir + entry.getName());\\n                 }\\n             }\\n             else\\n@@ -2074,6 +2097,7 @@ public static String unzip(File zipfile, String destDir) throws IOException {\\n                 byte[] buffer = new byte[1024];\\n                 int len;\\n                 InputStream in = zf.getInputStream(entry);\\n+                log.debug(\"Reading \" + zipDir + entry.getName() + \" into InputStream\");\\n                 BufferedOutputStream out = new BufferedOutputStream(\\n                         new FileOutputStream(zipDir + entry.getName()));\\n                 while((len = in.read(buffer)) >= 0)'}}",
            "message_norm":"[ds-4131] better path handling in itemimport zips",
            "language":"en",
            "entities":null,
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['dspace-api\/src\/main\/java\/org\/dspace\/app\/itemimport\/ItemImport.java'])",
            "num_files":1.0,
            "patch_content":"From 56e76049185bbd87c994128a9d77735ad7af0199 Mon Sep 17 00:00:00 2001\nFrom: Kim Shepherd <kim@shepherd.nz>\nDate: Fri, 14 Jan 2022 13:37:25 +1300\nSubject: [PATCH] [DS-4131] Better path handling in ItemImport zips\n\n---\n ...\/org\/dspace\/app\/itemimport\/ItemImport.java | 36 +++++++++++++++----\n 1 file changed, 30 insertions(+), 6 deletions(-)\n\ndiff --git a\/dspace-api\/src\/main\/java\/org\/dspace\/app\/itemimport\/ItemImport.java b\/dspace-api\/src\/main\/java\/org\/dspace\/app\/itemimport\/ItemImport.java\nindex 6ddc994a65e5..24ec2af16ba0 100644\n--- a\/dspace-api\/src\/main\/java\/org\/dspace\/app\/itemimport\/ItemImport.java\n+++ b\/dspace-api\/src\/main\/java\/org\/dspace\/app\/itemimport\/ItemImport.java\n@@ -2003,12 +2003,14 @@ public static String unzip(File zipfile, String destDir) throws IOException {\n         if (destinationDir == null){\n         \tdestinationDir = tempWorkDir;\n         }\n+        log.debug(\"Using directory \" + destinationDir + \" for zip extraction. (destDir arg is \" + destDir +\n+                \", tempWorkDir is \" + tempWorkDir + \")\");\n \n         File tempdir = new File(destinationDir);\n         if (!tempdir.isDirectory())\n         {\n-            log.error(\"'\" + ConfigurationManager.getProperty(\"org.dspace.app.itemexport.work.dir\") +\n-                    \"' as defined by the key 'org.dspace.app.itemexport.work.dir' in dspace.cfg \" +\n+            log.error(\"'\" + ConfigurationManager.getProperty(\"org.dspace.app.batchitemimport.work.dir\") +\n+                    \"' as defined by the key 'org.dspace.app.batchitemimport.work.dir' in dspace.cfg \" +\n                     \"is not a valid directory\");\n         }\n \n@@ -2016,9 +2018,15 @@ public static String unzip(File zipfile, String destDir) throws IOException {\n         {\n             log.error(\"Unable to create temporary directory: \" + tempdir.getAbsolutePath());\n         }\n-        String sourcedir = destinationDir + System.getProperty(\"file.separator\") + zipfile.getName();\n-        String zipDir = destinationDir + System.getProperty(\"file.separator\") + zipfile.getName() + System.getProperty(\"file.separator\");\n \n+        if(!destinationDir.endsWith(System.getProperty(\"file.separator\"))) {\n+            destinationDir += System.getProperty(\"file.separator\");\n+        }\n+\n+        String sourcedir = destinationDir + zipfile.getName();\n+        String zipDir = destinationDir + zipfile.getName() + System.getProperty(\"file.separator\");\n+\n+        log.debug(\"zip directory to use is \" + zipDir);\n \n         \/\/ 3\n         String sourceDirForZip = sourcedir;\n@@ -2028,11 +2036,26 @@ public static String unzip(File zipfile, String destDir) throws IOException {\n         while (entries.hasMoreElements())\n         {\n             entry = entries.nextElement();\n+            \/\/ Check that the true path to extract files is never outside allowed temp directories\n+            \/\/ without creating any actual files on disk\n+            log.debug(\"Inspecting entry name: \" + entry.getName() + \" for path traversal security\");\n+            File potentialExtract = new File(zipDir + entry.getName());\n+            String canonicalPath = potentialExtract.getCanonicalPath();\n+            log.debug(\"Canonical path to potential File is \" + canonicalPath);\n+            if(!canonicalPath.startsWith(zipDir)) {\n+                log.error(\"Rejecting zip file: \" + zipfile.getName() + \" as it contains an entry that would be extracted \" +\n+                        \"outside the temporary unzip directory: \" + canonicalPath);\n+                throw new IOException(\"Error extracting \" + zipfile + \": Canonical path of zip entry: \" +\n+                        entry.getName() + \" (\" + canonicalPath + \") does not start with permissible temp \" +\n+                        \"unzip directory (\" + destinationDir + \")\");\n+            }\n             if (entry.isDirectory())\n             {\n-                if (!new File(zipDir + entry.getName()).mkdir())\n-                {\n+                \/\/ Log error and throw IOException if a directory entry could not be created\n+                File newDir = new File(zipDir + entry.getName());\n+                if (!newDir.mkdirs()) {\n                     log.error(\"Unable to create contents directory: \" + zipDir + entry.getName());\n+                    throw new IOException(\"Unable to create contents directory: \" + zipDir + entry.getName());\n                 }\n             }\n             else\n@@ -2074,6 +2097,7 @@ public static String unzip(File zipfile, String destDir) throws IOException {\n                 byte[] buffer = new byte[1024];\n                 int len;\n                 InputStream in = zf.getInputStream(entry);\n+                log.debug(\"Reading \" + zipDir + entry.getName() + \" into InputStream\");\n                 BufferedOutputStream out = new BufferedOutputStream(\n                         new FileOutputStream(zipDir + entry.getName()));\n                 while((len = in.read(buffer)) >= 0)"
        }
    ]
}