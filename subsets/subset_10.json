{
    "schema":{
        "fields":[
            {
                "name":"index",
                "type":"integer"
            },
            {
                "name":"vuln_id",
                "type":"string"
            },
            {
                "name":"cwe_id",
                "type":"string"
            },
            {
                "name":"score",
                "type":"number"
            },
            {
                "name":"chain",
                "type":"string"
            },
            {
                "name":"dataset",
                "type":"string"
            },
            {
                "name":"summary",
                "type":"string"
            },
            {
                "name":"published_date",
                "type":"string"
            },
            {
                "name":"chain_len",
                "type":"integer"
            },
            {
                "name":"project",
                "type":"string"
            },
            {
                "name":"commit_href",
                "type":"string"
            },
            {
                "name":"commit_sha",
                "type":"string"
            },
            {
                "name":"patch",
                "type":"string"
            },
            {
                "name":"chain_ord",
                "type":"string"
            },
            {
                "name":"before_first_fix_commit",
                "type":"string"
            },
            {
                "name":"last_fix_commit",
                "type":"string"
            },
            {
                "name":"chain_ord_pos",
                "type":"number"
            },
            {
                "name":"commit_datetime",
                "type":"string"
            },
            {
                "name":"message",
                "type":"string"
            },
            {
                "name":"author",
                "type":"string"
            },
            {
                "name":"comments",
                "type":"string"
            },
            {
                "name":"stats",
                "type":"string"
            },
            {
                "name":"files",
                "type":"string"
            },
            {
                "name":"message_norm",
                "type":"string"
            },
            {
                "name":"language",
                "type":"string"
            },
            {
                "name":"entities",
                "type":"string"
            },
            {
                "name":"classification_level_1",
                "type":"string"
            },
            {
                "name":"classification_level_2",
                "type":"string"
            },
            {
                "name":"list_files",
                "type":"string"
            },
            {
                "name":"num_files",
                "type":"number"
            }
        ],
        "primaryKey":[
            "index"
        ],
        "pandas_version":"1.4.0"
    },
    "data":[
        {
            "index":1978,
            "vuln_id":"GHSA-h3vq-wv8j-36gw",
            "cwe_id":"{'CWE-79'}",
            "score":0.0,
            "chain":"{'https:\/\/github.com\/LLK\/scratch-svg-renderer\/commit\/7c74ec7de3254143ec3c557677f5355a90a3d07f'}",
            "dataset":"osv",
            "summary":"Cross-site Scripting in Scratch-Svg-Renderer A DOM-based cross-site scripting (XSS) vulnerability in Scratch-Svg-Renderer v0.2.0 allows attackers to execute arbitrary web scripts or HTML via a crafted sb3 file.",
            "published_date":"2022-01-08",
            "chain_len":1,
            "project":"https:\/\/github.com\/LLK\/scratch-svg-renderer",
            "commit_href":"https:\/\/github.com\/LLK\/scratch-svg-renderer\/commit\/7c74ec7de3254143ec3c557677f5355a90a3d07f",
            "commit_sha":"7c74ec7de3254143ec3c557677f5355a90a3d07f",
            "patch":"SINGLE",
            "chain_ord":"['7c74ec7de3254143ec3c557677f5355a90a3d07f']",
            "before_first_fix_commit":"{'d010f2d0edc3ab87ecabb27b9160f91317aa2722'}",
            "last_fix_commit":"7c74ec7de3254143ec3c557677f5355a90a3d07f",
            "chain_ord_pos":1.0,
            "commit_datetime":"10\/09\/2020, 19:50:27",
            "message":"Remove lots of event handlers",
            "author":"Eric Rosenbaum",
            "comments":null,
            "stats":"{'additions': 15, 'deletions': 2, 'total': 17}",
            "files":"{'src\/fixup-svg-string.js': {'additions': 15, 'deletions': 2, 'changes': 17, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/LLK\/scratch-svg-renderer\/raw\/7c74ec7de3254143ec3c557677f5355a90a3d07f\/src%2Ffixup-svg-string.js', 'patch': '@@ -50,8 +50,21 @@ module.exports = function (svgString) {\\n     \/\/ Empty script tags and javascript executing\\n     svgString = svgString.replace(\/<script[\\\\s\\\\S]*>[\\\\s\\\\S]*<\\\\\/script>\/, \\'<script><\/script>\\');\\n     \\n-    \/\/ Remove error handlers\\n-    svgString = svgString.replace(\/onerror=[\\\\s\\\\S]*[\\'\"].*?[\\'\"]\/i, \\'\\');\\n+    \/\/ Remove event handlers\\n+    const eventNames = [\\n+        \\'onbegin\\', \\'onend\\', \\'onrepeat\\', \\'onabort\\', \\'onerror\\', \\'onresize\\', \\'onscroll\\', \\'onunload\\', \\'oncopy\\',\\n+        \\'oncut\\', \\'onpaste\\', \\'oncancel\\', \\'oncanplay\\', \\'oncanplaythrough\\', \\'onchange\\', \\'onclick\\', \\'onclose\\',\\n+        \\'oncuechange\\', \\'ondblclick\\', \\'ondrag\\', \\'ondragend\\', \\'ondragenter\\', \\'ondragexit\\', \\'ondragleave\\',\\n+        \\'ondragover\\', \\'ondragstart\\', \\'ondrop\\', \\'ondurationchange\\', \\'onloadeddata\\', \\'onloadedmetadata\\',\\n+        \\'onloadstart\\', \\'onmousedown\\', \\'onmouseenter\\', \\'onmouseleave\\', \\'onmousemove\\',\\n+        \\'onemptied\\', \\'onended\\', \\'onerror\\', \\'onfocus\\', \\'oninput\\', \\'oninvalid\\', \\'onkeydown\\', \\'onkeypress\\',\\n+        \\'onkeyup\\', \\'onload\\', \\'onmouseout\\', \\'onmouseover\\', \\'onmouseup\\', \\'onmousewheel\\', \\'onpause\\', \\'onplay\\',\\n+        \\'onplaying\\', \\'onprogress\\', \\'onratechange\\', \\'onreset\\', \\'onresize\\', \\'onscroll\\', \\'onseeked\\', \\'onseeking\\',\\n+        \\'onselect\\', \\'onshow\\', \\'onstalled\\', \\'onsubmit\\', \\'onsuspend\\', \\'ontimeupdate\\', \\'ontoggle\\', \\'onvolumechange\\',\\n+        \\'onwaiting\\', \\'onactivate\\', \\'onfocusin\\', \\'onfocusout\\'\\n+    ];\\n+    const eventsRegex = new RegExp(`(${eventNames.join(\\'|\\')})\\\\\\\\s*=\\\\\\\\s*[\\'\"].*[\\'\"]`, \\'i\\');\\n+    svgString = svgString.replace(eventsRegex, \\'\\');\\n \\n     return svgString;\\n };'}}",
            "message_norm":"remove lots of event handlers",
            "language":"en",
            "entities":"[('remove', 'ACTION', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['src\/fixup-svg-string.js'])",
            "num_files":1.0
        },
        {
            "index":1995,
            "vuln_id":"GHSA-h5g4-ppwx-48q2",
            "cwe_id":"{'CWE-20'}",
            "score":5.5,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/cff267650c6a1b266e4b4500f69fbc49cdd773c5'}",
            "dataset":"osv",
            "summary":"Missing validation causes denial of service via `DeleteSessionTensor` ### Impact\nThe implementation of [`tf.raw_ops.DeleteSessionTensor`](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/f3b9bf4c3c0597563b289c0512e98d4ce81f886e\/tensorflow\/core\/kernels\/session_ops.cc#L128-L144) does not fully validate the input arguments. This results in a `CHECK`-failure which can be used to trigger a denial of service attack:\n\n```python\nimport tensorflow as tf\n\nhandle = tf.constant(\"[]\", shape=[0], dtype=tf.string)\ntf.raw_ops.DeleteSessionTensor(handle=handle)\n```\n  \nThe code assumes `handle` is a scalar but there is no validation for this:\n  \n```cc\n    const Tensor& handle = ctx->input(0);\n    const string& name = handle.scalar<tstring>()();\n```\n\n### Patches\nWe have patched the issue in GitHub commit [cff267650c6a1b266e4b4500f69fbc49cdd773c5](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/cff267650c6a1b266e4b4500f69fbc49cdd773c5).\n\nThe fix will be included in TensorFlow 2.9.0. We will also cherrypick this commit on TensorFlow 2.8.1, TensorFlow 2.7.2, and TensorFlow 2.6.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by Neophytos Christou from Secure Systems Lab at Brown University.",
            "published_date":"2022-05-24",
            "chain_len":1,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/cff267650c6a1b266e4b4500f69fbc49cdd773c5",
            "commit_sha":"cff267650c6a1b266e4b4500f69fbc49cdd773c5",
            "patch":"SINGLE",
            "chain_ord":"['cff267650c6a1b266e4b4500f69fbc49cdd773c5']",
            "before_first_fix_commit":"{'339d5de981acaa8580da62c5de8c0da64ae88ad4'}",
            "last_fix_commit":"cff267650c6a1b266e4b4500f69fbc49cdd773c5",
            "chain_ord_pos":1.0,
            "commit_datetime":"04\/28\/2022, 20:08:57",
            "message":"Fix tf.raw_ops.DeleteSessionTensor vulnerability with invalid `handle`.\n\nCheck that `handle` input is actually a scalar before treating it as such.\n\nPiperOrigin-RevId: 445228994",
            "author":"Alan Liu",
            "comments":null,
            "stats":"{'additions': 2, 'deletions': 0, 'total': 2}",
            "files":"{'tensorflow\/core\/kernels\/session_ops.cc': {'additions': 2, 'deletions': 0, 'changes': 2, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/cff267650c6a1b266e4b4500f69fbc49cdd773c5\/tensorflow%2Fcore%2Fkernels%2Fsession_ops.cc', 'patch': '@@ -134,6 +134,8 @@ class DeleteSessionTensorOp : public OpKernel {\\n \\n   void Compute(OpKernelContext* ctx) override {\\n     const Tensor& handle = ctx->input(0);\\n+    OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(handle.shape()),\\n+                errors::InvalidArgument(\"`handle` must be scalar\"));\\n     const string& name = handle.scalar<tstring>()();\\n     auto session_state = ctx->session_state();\\n     OP_REQUIRES(ctx, session_state != nullptr,'}}",
            "message_norm":"fix tf.raw_ops.deletesessiontensor vulnerability with invalid `handle`.\n\ncheck that `handle` input is actually a scalar before treating it as such.\n\npiperorigin-revid: 445228994",
            "language":"en",
            "entities":"[('fix', 'ACTION', ''), ('vulnerability', 'SECWORD', ''), ('445228994', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/core\/kernels\/session_ops.cc'])",
            "num_files":1.0
        },
        {
            "index":75,
            "vuln_id":"GHSA-29vr-79w7-p649",
            "cwe_id":"{'CWE-863'}",
            "score":9.8,
            "chain":"{'https:\/\/github.com\/Gerapy\/Gerapy\/commit\/49bcb19be5e0320e7e1535f34fe00f16a3cf3b28'}",
            "dataset":"osv",
            "summary":"Incorrect Authorization in Gerapy An Access Control vunerabiity exists in Gerapy v 0.9.7 via the spider parameter in project_configure function.",
            "published_date":"2022-03-11",
            "chain_len":1,
            "project":"https:\/\/github.com\/Gerapy\/Gerapy",
            "commit_href":"https:\/\/github.com\/Gerapy\/Gerapy\/commit\/49bcb19be5e0320e7e1535f34fe00f16a3cf3b28",
            "commit_sha":"49bcb19be5e0320e7e1535f34fe00f16a3cf3b28",
            "patch":"SINGLE",
            "chain_ord":"['49bcb19be5e0320e7e1535f34fe00f16a3cf3b28']",
            "before_first_fix_commit":"{'f1cd46d80328497c016fbac12c9239b9dcaef047'}",
            "last_fix_commit":"49bcb19be5e0320e7e1535f34fe00f16a3cf3b28",
            "chain_ord_pos":1.0,
            "commit_datetime":"12\/26\/2021, 09:50:00",
            "message":"fix remote execute",
            "author":"Germey",
            "comments":null,
            "stats":"{'additions': 56, 'deletions': 40, 'total': 96}",
            "files":"{'gerapy\/server\/core\/views.py': {'additions': 56, 'deletions': 40, 'changes': 96, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/Gerapy\/Gerapy\/raw\/49bcb19be5e0320e7e1535f34fe00f16a3cf3b28\/gerapy%2Fserver%2Fcore%2Fviews.py', 'patch': '@@ -2,7 +2,12 @@\\n from pathlib import Path\\n from urllib.parse import unquote\\n import base64\\n-import json, os, requests, time, pytz, pymongo\\n+import json\\n+import os\\n+import requests\\n+import time\\n+import pytz\\n+import pymongo\\n from shutil import rmtree\\n from requests.exceptions import ConnectionError\\n from os.path import join, exists\\n@@ -173,7 +178,8 @@ def spider_list(request, client_id, project_name):\\n         client = Client.objects.get(id=client_id)\\n         scrapyd = get_scrapyd(client)\\n         spiders = scrapyd.list_spiders(project_name)\\n-        spiders = [{\\'name\\': spider, \\'id\\': index + 1} for index, spider in enumerate(spiders)]\\n+        spiders = [{\\'name\\': spider, \\'id\\': index + 1}\\n+                   for index, spider in enumerate(spiders)]\\n         return JsonResponse(spiders)\\n \\n \\n@@ -242,23 +248,25 @@ def project_configure(request, project_name):\\n     if request.method == \\'GET\\':\\n         project = Project.objects.get(name=project_name)\\n         project = model_to_dict(project)\\n-        project[\\'configuration\\'] = json.loads(project[\\'configuration\\']) if project[\\'configuration\\'] else None\\n+        project[\\'configuration\\'] = json.loads(\\n+            project[\\'configuration\\']) if project[\\'configuration\\'] else None\\n         return JsonResponse(project)\\n-    \\n+\\n     # update configuration\\n     elif request.method == \\'POST\\':\\n         project = Project.objects.filter(name=project_name)\\n         data = json.loads(request.body)\\n-        configuration = json.dumps(data.get(\\'configuration\\'), ensure_ascii=False)\\n+        configuration = json.dumps(\\n+            data.get(\\'configuration\\'), ensure_ascii=False)\\n         project.update(**{\\'configuration\\': configuration})\\n-        \\n         # for safe protection\\n-        project_name = re.sub(\\'[\\\\!\\\\@\\\\#\\\\$\\\\;\\\\&\\\\*\\\\~\\\\\"\\\\\\'\\\\{\\\\}\\\\]\\\\[\\\\-\\\\+\\\\%\\\\^]+\\', \\'\\', project_name)\\n+        project_name = re.sub(\\n+            \\'[\\\\s\\\\!\\\\@\\\\#\\\\$\\\\;\\\\&\\\\*\\\\~\\\\\"\\\\\\'\\\\{\\\\}\\\\]\\\\[\\\\-\\\\+\\\\%\\\\^]+\\', \\'\\', project_name)\\n         # execute generate cmd\\n-        cmd = \\' \\'.join([\\'gerapy\\', \\'generate\\', project_name])\\n-        p = Popen(cmd, shell=True, stdin=PIPE, stdout=PIPE, stderr=PIPE)\\n+        cmd = [\\'gerapy\\', \\'generate\\', project_name]\\n+        p = Popen(cmd, shell=False, stdin=PIPE, stdout=PIPE, stderr=PIPE)\\n         stdout, stderr = bytes2str(p.stdout.read()), bytes2str(p.stderr.read())\\n-        \\n+\\n         if not stderr:\\n             return JsonResponse({\\'status\\': \\'1\\'})\\n         else:\\n@@ -294,7 +302,8 @@ def project_create(request):\\n         data[\\'configurable\\'] = 1\\n         project, result = Project.objects.update_or_create(**data)\\n         # generate a single project folder\\n-        path = join(os.path.abspath(join(os.getcwd(), PROJECTS_FOLDER)), data[\\'name\\'])\\n+        path = join(os.path.abspath(\\n+            join(os.getcwd(), PROJECTS_FOLDER)), data[\\'name\\'])\\n         os.mkdir(path)\\n         return JsonResponse(model_to_dict(project))\\n \\n@@ -334,12 +343,13 @@ def project_clone(request):\\n         if not address.startswith(\\'http\\'):\\n             return JsonResponse({\\'status\\': False})\\n         address = address + \\'.git\\' if not address.endswith(\\'.git\\') else address\\n-        cmd = \\'git clone {address} {target}\\'.format(address=address, target=join(PROJECTS_FOLDER, Path(address).stem))\\n+        cmd = [\\'git\\', \\'clone\\', \\'address\\', join(PROJECTS_FOLDER, Path(address).stem)]\\n         logger.debug(\\'clone cmd %s\\', cmd)\\n-        p = Popen(cmd, shell=True, stdin=PIPE, stdout=PIPE, stderr=PIPE)\\n+        p = Popen(cmd, shell=False, stdin=PIPE, stdout=PIPE, stderr=PIPE)\\n         stdout, stderr = bytes2str(p.stdout.read()), bytes2str(p.stderr.read())\\n         logger.debug(\\'clone run result %s\\', stdout)\\n-        if stderr: logger.error(stderr)\\n+        if stderr:\\n+            logger.error(stderr)\\n         return JsonResponse({\\'status\\': True}) if not stderr else JsonResponse({\\'status\\': False})\\n \\n \\n@@ -393,10 +403,12 @@ def project_version(request, client_id, project_name):\\n                 return JsonResponse({\\'message\\': \\'Connect Error\\'}, status=500)\\n             if len(versions) > 0:\\n                 version = versions[-1]\\n-                deployed_at = timezone.datetime.fromtimestamp(int(version), tz=pytz.timezone(TIME_ZONE))\\n+                deployed_at = timezone.datetime.fromtimestamp(\\n+                    int(version), tz=pytz.timezone(TIME_ZONE))\\n             else:\\n                 deployed_at = None\\n-            deploy, result = Deploy.objects.update_or_create(client=client, project=project, deployed_at=deployed_at)\\n+            deploy, result = Deploy.objects.update_or_create(\\n+                client=client, project=project, deployed_at=deployed_at)\\n         # return deploy json info\\n         return JsonResponse(model_to_dict(deploy))\\n \\n@@ -446,7 +458,7 @@ def project_build(request, project_name):\\n     # get project folder\\n     path = os.path.abspath(join(os.getcwd(), PROJECTS_FOLDER))\\n     project_path = join(path, project_name)\\n-    \\n+\\n     # get build version\\n     if request.method == \\'GET\\':\\n         egg = find_egg(project_path)\\n@@ -470,7 +482,7 @@ def project_build(request, project_name):\\n         # transfer model to dict then dumps it to json\\n         data = model_to_dict(model)\\n         return JsonResponse(data)\\n-    \\n+\\n     # build operation manually by clicking button\\n     elif request.method == \\'POST\\':\\n         data = json.loads(request.body)\\n@@ -483,7 +495,8 @@ def project_build(request, project_name):\\n         built_at = timezone.now()\\n         # if project does not exists in db, create it\\n         if not Project.objects.filter(name=project_name):\\n-            Project(name=project_name, description=description, built_at=built_at, egg=egg).save()\\n+            Project(name=project_name, description=description,\\n+                    built_at=built_at, egg=egg).save()\\n             model = Project.objects.get(name=project_name)\\n         # if project exists, update egg, description, built_at info\\n         else:\\n@@ -526,17 +539,16 @@ def project_parse(request, project_name):\\n         body = data.get(\\'body\\', \\'\\')\\n         if args.get(\\'method\\').lower() != \\'get\\':\\n             args[\\'body\\'] = \"\\'\" + json.dumps(body, ensure_ascii=False) + \"\\'\"\\n-        \\n-        args_cmd = \\' \\'.join(\\n-            [\\'--{arg} {value}\\'.format(arg=arg, value=value) for arg, value in args.items()])\\n-        logger.debug(\\'args cmd %s\\', args_cmd)\\n-        cmd = \\'gerapy parse {args_cmd} {project_path} {spider_name}\\'.format(\\n-            args_cmd=args_cmd,\\n-            project_path=project_path,\\n-            spider_name=spider_name\\n-        )\\n+\\n+        args_array = []\\n+        for arg, value in args.items():\\n+            args_array.append(f\\'--{arg}\\')\\n+            args_array.append(f\\'{value}\\')\\n+        cmd = [\\'gerapy\\', \\'parse\\'] + args_array + [project_path] + [spider_name]\\n+        print(\\'cmd\\', cmd)\\n         logger.debug(\\'parse cmd %s\\', cmd)\\n-        p = Popen(cmd, shell=True, stdin=PIPE, stdout=PIPE, stderr=PIPE, close_fds=True)\\n+        p = Popen(cmd, shell=False, stdin=PIPE,\\n+                         stdout=PIPE, stderr=PIPE, close_fds=True)\\n         stdout, stderr = bytes2str(p.stdout.read()), bytes2str(p.stderr.read())\\n         logger.debug(\\'stdout %s, stderr %s\\', stdout, stderr)\\n         if not stderr:\\n@@ -645,7 +657,6 @@ def job_list(request, client_id, project_name):\\n                 job[\\'status\\'] = status\\n                 jobs.append(job)\\n         return JsonResponse(jobs)\\n-    \\n \\n \\n @api_view([\\'GET\\'])\\n@@ -663,7 +674,8 @@ def job_log(request, client_id, project_name, spider_name, job_id):\\n     if request.method == \\'GET\\':\\n         client = Client.objects.get(id=client_id)\\n         # get log url\\n-        url = log_url(client.ip, client.port, project_name, spider_name, job_id)\\n+        url = log_url(client.ip, client.port,\\n+                      project_name, spider_name, job_id)\\n         # get last 1000 bytes of log\\n         response = requests.get(url, timeout=5, headers={\\n             \\'Range\\': \\'bytes=-1000\\'\\n@@ -765,7 +777,8 @@ def monitor_create(request):\\n     if request.method == \\'POST\\':\\n         data = json.loads(request.body)\\n         data = data[\\'form\\']\\n-        data[\\'configuration\\'] = json.dumps(data[\\'configuration\\'], ensure_ascii=False)\\n+        data[\\'configuration\\'] = json.dumps(\\n+            data[\\'configuration\\'], ensure_ascii=False)\\n         monitor = Monitor.objects.create(**data)\\n         return JsonResponse(model_to_dict(monitor))\\n \\n@@ -785,7 +798,8 @@ def task_create(request):\\n                                    name=data.get(\\'name\\'),\\n                                    spider=data.get(\\'spider\\'),\\n                                    trigger=data.get(\\'trigger\\'),\\n-                                   configuration=json.dumps(data.get(\\'configuration\\'), ensure_ascii=False),\\n+                                   configuration=json.dumps(\\n+                                       data.get(\\'configuration\\'), ensure_ascii=False),\\n                                    modified=1)\\n         return JsonResponse({\\'result\\': \\'1\\', \\'data\\': model_to_dict(task)})\\n \\n@@ -803,7 +817,8 @@ def task_update(request, task_id):\\n         task = Task.objects.filter(id=task_id)\\n         data = json.loads(request.body)\\n         data[\\'clients\\'] = json.dumps(data.get(\\'clients\\'), ensure_ascii=False)\\n-        data[\\'configuration\\'] = json.dumps(data.get(\\'configuration\\'), ensure_ascii=False)\\n+        data[\\'configuration\\'] = json.dumps(\\n+            data.get(\\'configuration\\'), ensure_ascii=False)\\n         data[\\'modified\\'] = 1\\n         task.update(**data)\\n         return JsonResponse(model_to_dict(Task.objects.get(id=task_id)))\\n@@ -823,11 +838,10 @@ def task_remove(request, task_id):\\n         clients = clients_of_task(task)\\n         for client in clients:\\n             job_id = get_job_id(client, task)\\n-            DjangoJob.objects.filter(name=job_id).delete()\\n+            DjangoJob.objects.filter(id=job_id).delete()\\n         # delete task\\n         Task.objects.filter(id=task_id).delete()\\n         return JsonResponse({\\'result\\': \\'1\\'})\\n-    \\n \\n \\n @api_view([\\'GET\\'])\\n@@ -875,12 +889,14 @@ def task_status(request, task_id):\\n         clients = clients_of_task(task)\\n         for client in clients:\\n             job_id = get_job_id(client, task)\\n-            jobs = DjangoJob.objects.filter(name=job_id)\\n+            jobs = DjangoJob.objects.filter(id=job_id)\\n             logger.debug(\\'jobs from djangojob %s\\', jobs)\\n             # if job does not exist, for date mode exceed time\\n-            if not jobs: continue\\n-            job = DjangoJob.objects.get(name=job_id)\\n-            executions = serialize(\\'json\\', DjangoJobExecution.objects.filter(job=job))\\n+            if not jobs:\\n+                continue\\n+            job = DjangoJob.objects.get(id=job_id)\\n+            executions = serialize(\\n+                \\'json\\', DjangoJobExecution.objects.filter(job=job))\\n             result.append({\\n                 \\'client\\': model_to_dict(client),\\n                 \\'next\\': job.next_run_time,'}}",
            "message_norm":"fix remote execute",
            "language":"ro",
            "entities":"[('fix', 'ACTION', ''), ('remote execute', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['gerapy\/server\/core\/views.py'])",
            "num_files":1.0
        },
        {
            "index":2237,
            "vuln_id":"GHSA-jf9v-q8vh-3fmc",
            "cwe_id":"{'CWE-79'}",
            "score":5.4,
            "chain":"{'https:\/\/github.com\/icecoder\/ICEcoder\/commit\/21d6ae0f2a3fce7d076ae430d48f5df56bd0f256'}",
            "dataset":"osv",
            "summary":"Cross-site scripting in ICEcoder In ICEcoder 8.0 allows, a reflected XSS vulnerability was identified in the multipe-results.php page due to insufficient sanitization of the _GET['replace'] variable. As a result, arbitrary Javascript code can get executed.",
            "published_date":"2021-09-09",
            "chain_len":1,
            "project":"https:\/\/github.com\/icecoder\/ICEcoder",
            "commit_href":"https:\/\/github.com\/icecoder\/ICEcoder\/commit\/21d6ae0f2a3fce7d076ae430d48f5df56bd0f256",
            "commit_sha":"21d6ae0f2a3fce7d076ae430d48f5df56bd0f256",
            "patch":"SINGLE",
            "chain_ord":"['21d6ae0f2a3fce7d076ae430d48f5df56bd0f256']",
            "before_first_fix_commit":"{'54e4aff163d29edb13fe885219f82fca258c7e99'}",
            "last_fix_commit":"21d6ae0f2a3fce7d076ae430d48f5df56bd0f256",
            "chain_ord_pos":1.0,
            "commit_datetime":"06\/25\/2021, 20:22:25",
            "message":"XSS and usage fixes on multiple-results.php",
            "author":"mattpass",
            "comments":null,
            "stats":"{'additions': 15, 'deletions': 12, 'total': 27}",
            "files":"{'lib\/multiple-results.php': {'additions': 15, 'deletions': 12, 'changes': 27, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/icecoder\/ICEcoder\/raw\/21d6ae0f2a3fce7d076ae430d48f5df56bd0f256\/lib%2Fmultiple-results.php', 'patch': '@@ -1,4 +1,5 @@\\n <?php\\n+\/\/ TODO: The whole file needs a refactor and comments!\\n include \"headers.php\";\\n include \"settings.php\";\\n $t = $text[\\'multiple-results\\'];\\n@@ -101,16 +102,18 @@\\n             if (\\n                 \/\/ TODO: Find in filenames not working with regex, see all instances of findText and $findText below\\n                 true === haveMatch && -1 < targetURL.indexOf(\\'_perms\\')) {\\n-                if (-1 < userTarget.indexOf(\"selected\")) {\\n-                    for (let j = 0; j < parent.ICEcoder.selectedFiles.length; j++) {\\n-                        \/\/ TODO: This whole file needs comments - what does the below do?!\\n+                    if (-1 < userTarget.indexOf(\"selected\")) {\\n+                        for (let j = 0; j < parent.ICEcoder.selectedFiles.length; j++) {\\n                         if (\\n-                            0 === targetURL.replace(\/\\\\\/\/g, \"|\").indexOf(parent.ICEcoder.selectedFiles[j].replace(\/\\\\\/\/g, \"|\").replace(\/_perms\/g, \"\"))\\n+                            \/\/ If the pipe delimited targetURL starts with this pipe delimited, non _perms elem selectedFile\\n+                            0 === targetURL.replace(\/\\\\\/\/g, \"|\").indexOf(parent.ICEcoder.selectedFiles[j].replace(\/\\\\\/\/g, \"|\").replace(\/_perms\/g, \"\").toLowerCase())\\n                             && (\\n-                            targetURL.replace(\/\\\\|\/g, \"\/\").replace(\/_perms\/g, \"\") === parent.ICEcoder.selectedFiles[j].replace(\/\\\\|\/g, \"\/\").replace(\/_perms\/g, \"\")\\n+                            \/\/ If the slash delimited, non _perms elem matches the slasj delimited, non _perms elem\\n+                            targetURL.replace(\/\\\\|\/g, \"\/\").replace(\/_perms\/g, \"\") === parent.ICEcoder.selectedFiles[j].replace(\/\\\\|\/g, \"\/\").replace(\/_perms\/g, \"\").toLowerCase()\\n                             ||\\n+                            \/\/ Path length for targetURL is greater than path length for this selectedFile and targetURL char at selectedFiles length ends with a slash\\n                             (targetURL.replace(\/\\\\|\/g, \"\/\").split(\"\/\").length > parent.ICEcoder.selectedFiles[j].replace(\/\\\\|\/g, \"\/\").split(\"\/\").length && \"\/\" === targetURL.charAt(parent.ICEcoder.selectedFiles[j].length)))) {\\n-                            foundInSelected = true;\\n+                                foundInSelected = true;\\n                         }\\n                     }\\n                 }\\n@@ -124,8 +127,8 @@\\n                     \/\/ TODO: get this line working\\n                     resultsDisplay +=\\n                         targetURL.replace(\/\\\\|\/g, \"\/\").replace(\/_perms\/g, \"\").replace(\/<?php\\n-                            echo str_replace(\"\/\", \"\\\\\/\",strtolower($findText)); ?>\/g, \"<b>\" +\\n-                            findText.toLowerCase() + \"<\/b>\");\\n+                            echo str_replace(\"\/\", \"\\\\\/\",strtolower(preg_quote($findText))); ?>\/g, \"<b>\" +\\n+                            parent.ICEcoder.xssClean(findText).toLowerCase() + \"<\/b>\");\\n                         resultsDisplay += \\'<\/a><br>\\';\\n                     <?php if (false === isset($_GET[\\'replace\\'])) { ?>\\n                     resultsDisplay += \\'<div id=\"foundCount\\' + i +\\'\">\\' + spansArray[i].innerHTML + \\'<\/div>\\';\\n@@ -134,8 +137,8 @@\\n                     resultsDisplay +=\\n                         \\'<div id=\"foundCount\\' + i + \\'\">\\' + spansArray[i].innerHTML +\\n                         \\', <?php echo $t[\\'rename to\\'];?> \\' +\\n-                        targetURL.replace(\/\\\\|\/g, \"\/\").replace(\/_perms\/g, \"\").replace(\/<?php echo str_replace(\"\/\", \"\\\\\/\",strtolower($findText)); ?>\/g,\"<b><?php\\n-                            if (isset($_GET[\\'replace\\'])) {echo $_GET[\\'replace\\'];};\\n+                        targetURL.replace(\/\\\\|\/g, \"\/\").replace(\/_perms\/g, \"\").replace(\/<?php echo str_replace(\"\/\", \"\\\\\/\",strtolower(preg_quote($findText))); ?>\/g,\"<b><?php\\n+                            if (isset($_GET[\\'replace\\'])) {echo str_replace(\"&amp;\", \"&\", xssClean($_GET[\\'replace\\'], \\'script\\'));};\\n                         ?><\/b>\")+\\'<\/div>\\';\\n                         <?php\\n                         ;};\\n@@ -253,7 +256,7 @@ function phpGrep($q, $path, $base) {\\n \\n     const replaceInFileSingle = function(fileRef) {\\n         \/\/ TODO: findText in this line\\n-        parent.ICEcoder.replaceInFile(fileRef, true === parent.ICEcoder.findRegex ? findText : parent.ICEcoder.escapeRegex(findText), \\'<?php if (isset($_GET[\\'replace\\'])) {echo $_GET[\\'replace\\'];}; ?>\\');\\n+        parent.ICEcoder.replaceInFile(fileRef, true === parent.ICEcoder.findRegex ? findText : parent.ICEcoder.escapeRegex(findText), \\'<?php if (isset($_GET[\\'replace\\'])) {echo xssClean($_GET[\\'replace\\'], \\'script\\');}; ?>\\');\\n     };\\n \\n     const replaceInFilesAll = function() {\\n@@ -267,7 +270,7 @@ function phpGrep($q, $path, $base) {\\n         fileRef = spansArray[arrayRef].id.replace(\/\\\\|\/g, \"\/\").replace(\/_perms\/g, \"\");\\n         const rExp = new RegExp(true === parent.ICEcoder.findRegex ? findText : parent.ICEcoder.escapeRegex(findText), \"gi\");\\n         \/\/ TODO: get this working\\n-        newName = spansArray[arrayRef].id.replace(\/\\\\|\/g, \"\/\").replace(\/_perms\/g, \"\").replace(rExp, \"<?php if (isset($_GET[\\'replace\\'])) {echo $_GET[\\'replace\\'];}; ?>\");\\n+        newName = spansArray[arrayRef].id.replace(\/\\\\|\/g, \"\/\").replace(\/_perms\/g, \"\").replace(rExp, \"<?php if (isset($_GET[\\'replace\\'])) {echo xssClean($_GET[\\'replace\\'], \\'script\\');}; ?>\");\\n         parent.ICEcoder.renameFile(fileRef,newName);\\n     };'}}",
            "message_norm":"xss and usage fixes on multiple-results.php",
            "language":"en",
            "entities":"[('xss', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['lib\/multiple-results.php'])",
            "num_files":1.0
        },
        {
            "index":493,
            "vuln_id":"GHSA-4v5p-v5h9-6xjx",
            "cwe_id":"{'CWE-617'}",
            "score":6.5,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/c2b31ff2d3151acb230edc3f5b1832d2c713a9e0'}",
            "dataset":"osv",
            "summary":"`CHECK`-failures in Tensorflow ### Impact\nAn attacker can trigger denial of service via assertion failure by altering a `SavedModel` on disk such that `AttrDef`s of some operation are duplicated.\n\n### Patches\nWe have patched the issue in GitHub commit [c2b31ff2d3151acb230edc3f5b1832d2c713a9e0](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/c2b31ff2d3151acb230edc3f5b1832d2c713a9e0).\n\nThe fix will be included in TensorFlow 2.8.0. We will also cherrypick this commit on TensorFlow 2.7.1, TensorFlow 2.6.3, and TensorFlow 2.5.3, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.",
            "published_date":"2022-02-09",
            "chain_len":1,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/c2b31ff2d3151acb230edc3f5b1832d2c713a9e0",
            "commit_sha":"c2b31ff2d3151acb230edc3f5b1832d2c713a9e0",
            "patch":"SINGLE",
            "chain_ord":"['c2b31ff2d3151acb230edc3f5b1832d2c713a9e0']",
            "before_first_fix_commit":"{'41424fd983e23b11ed13bbd5a2b2be0e25ab4244'}",
            "last_fix_commit":"c2b31ff2d3151acb230edc3f5b1832d2c713a9e0",
            "chain_ord_pos":1.0,
            "commit_datetime":"11\/08\/2021, 18:14:10",
            "message":"Remove a `DCHECK`-fail, log an error instead.\n\n`DCHECK` in debug mode results in crashes. TensorFlow has had multiple vulnerabilities due to this.\n\nOutside of debug mode, `DCHECK` is a no-op.\n\nA better alternative is to report an error to the log buffer and continue. This should happen both in debug mode and in prod mode.\n\nPiperOrigin-RevId: 408375925\nChange-Id: Id5b3e19c73f3fbe0cc4bba26ca44ff9607bb6356",
            "author":"Mihai Maruseac",
            "comments":null,
            "stats":"{'additions': 4, 'deletions': 3, 'total': 7}",
            "files":"{'tensorflow\/core\/framework\/op_def_util.cc': {'additions': 4, 'deletions': 3, 'changes': 7, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/c2b31ff2d3151acb230edc3f5b1832d2c713a9e0\/tensorflow%2Fcore%2Fframework%2Fop_def_util.cc', 'patch': '@@ -821,9 +821,10 @@ bool RepeatedAttrDefEqual(\\n     const protobuf::RepeatedPtrField<OpDef::AttrDef>& a2) {\\n   std::unordered_map<string, const OpDef::AttrDef*> a1_set;\\n   for (const OpDef::AttrDef& def : a1) {\\n-    DCHECK(a1_set.find(def.name()) == a1_set.end())\\n-        << \"AttrDef names must be unique, but \\'\" << def.name()\\n-        << \"\\' appears more than once\";\\n+    if (a1_set.find(def.name()) != a1_set.end()) {\\n+      LOG(ERROR) << \"AttrDef names must be unique, but \\'\" << def.name()\\n+                 << \"\\' appears more than once\";\\n+    }\\n     a1_set[def.name()] = &def;\\n   }\\n   for (const OpDef::AttrDef& def : a2) {'}}",
            "message_norm":"remove a `dcheck`-fail, log an error instead.\n\n`dcheck` in debug mode results in crashes. tensorflow has had multiple vulnerabilities due to this.\n\noutside of debug mode, `dcheck` is a no-op.\n\na better alternative is to report an error to the log buffer and continue. this should happen both in debug mode and in prod mode.\n\npiperorigin-revid: 408375925\nchange-id: id5b3e19c73f3fbe0cc4bba26ca44ff9607bb6356",
            "language":"en",
            "entities":"[('remove', 'ACTION', ''), ('error', 'FLAW', ''), ('vulnerabilities', 'SECWORD', ''), ('error', 'FLAW', ''), ('408375925', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/core\/framework\/op_def_util.cc'])",
            "num_files":1.0
        },
        {
            "index":983,
            "vuln_id":"GHSA-772j-h9xw-ffp5",
            "cwe_id":"{'CWE-843'}",
            "score":2.5,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/b1cc5e5a50e7cee09f2c6eb48eb40ee9c4125025'}",
            "dataset":"osv",
            "summary":"CHECK-fail in SparseCross due to type confusion ### Impact\nThe API of `tf.raw_ops.SparseCross` allows combinations which would result in a `CHECK`-failure and denial of service:\n\n```python\nimport tensorflow as tf\n\nhashed_output = False\nnum_buckets = 1949315406\nhash_key = 1869835877\nout_type = tf.string \ninternal_type = tf.string\n\nindices_1 = tf.constant([0, 6], shape=[1, 2], dtype=tf.int64)\nindices_2 = tf.constant([0, 0], shape=[1, 2], dtype=tf.int64)\nindices = [indices_1, indices_2]\n\nvalues_1 = tf.constant([0], dtype=tf.int64)\nvalues_2 = tf.constant([72], dtype=tf.int64)\nvalues = [values_1, values_2]\n\nbatch_size = 4\nshape_1 = tf.constant([4, 122], dtype=tf.int64)\nshape_2 = tf.constant([4, 188], dtype=tf.int64)\nshapes = [shape_1, shape_2]\n\ndense_1 = tf.constant([188, 127, 336, 0], shape=[4, 1], dtype=tf.int64)\ndense_2 = tf.constant([341, 470, 470, 470], shape=[4, 1], dtype=tf.int64)\ndense_3 = tf.constant([188, 188, 341, 922], shape=[4, 1], dtype=tf.int64)\ndenses = [dense_1, dense_2, dense_3]\n\ntf.raw_ops.SparseCross(indices=indices, values=values, shapes=shapes, dense_inputs=denses, hashed_output=hashed_output,\n                       num_buckets=num_buckets, hash_key=hash_key, out_type=out_type, internal_type=internal_type)\n```\n\nThe above code will result in a `CHECK` fail in [`tensor.cc`](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/3d782b7d47b1bf2ed32bd4a246d6d6cadc4c903d\/tensorflow\/core\/framework\/tensor.cc#L670-L675):\n\n```cc\nvoid Tensor::CheckTypeAndIsAligned(DataType expected_dtype) const {\n  CHECK_EQ(dtype(), expected_dtype)\n      << \" \" << DataTypeString(expected_dtype) << \" expected, got \"\n      << DataTypeString(dtype());\n  ...\n}\n```\n\nThis is because the [implementation](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/3d782b7d47b1bf2ed32bd4a246d6d6cadc4c903d\/tensorflow\/core\/kernels\/sparse_cross_op.cc#L114-L116) is tricked to consider a tensor of type `tstring` which in fact contains integral elements:\n\n```cc\n  if (DT_STRING == values_.dtype())\n      return Fingerprint64(values_.vec<tstring>().data()[start + n]);\n  return values_.vec<int64>().data()[start + n];\n```\n\nFixing the type confusion by preventing mixing `DT_STRING` and `DT_INT64` types solves this issue.\n\n### Patches\nWe have patched the issue in GitHub commit [b1cc5e5a50e7cee09f2c6eb48eb40ee9c4125025](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/b1cc5e5a50e7cee09f2c6eb48eb40ee9c4125025).\n\nThe fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by Yakun Zhang and Ying Wang of Baidu X-Team.",
            "published_date":"2021-05-21",
            "chain_len":1,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/b1cc5e5a50e7cee09f2c6eb48eb40ee9c4125025",
            "commit_sha":"b1cc5e5a50e7cee09f2c6eb48eb40ee9c4125025",
            "patch":"SINGLE",
            "chain_ord":"['b1cc5e5a50e7cee09f2c6eb48eb40ee9c4125025']",
            "before_first_fix_commit":"{'3d782b7d47b1bf2ed32bd4a246d6d6cadc4c903d'}",
            "last_fix_commit":"b1cc5e5a50e7cee09f2c6eb48eb40ee9c4125025",
            "chain_ord_pos":1.0,
            "commit_datetime":"04\/15\/2021, 20:03:19",
            "message":"Fix `tf.raw_ops.SparseCross` failing CHECK.\n\nPiperOrigin-RevId: 368701671\nChange-Id: Id805729dd9ba0bda36e4bb309408129b55fb649d",
            "author":"Amit Patankar",
            "comments":null,
            "stats":"{'additions': 48, 'deletions': 7, 'total': 55}",
            "files":"{'tensorflow\/core\/kernels\/sparse_cross_op.cc': {'additions': 48, 'deletions': 7, 'changes': 55, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/b1cc5e5a50e7cee09f2c6eb48eb40ee9c4125025\/tensorflow%2Fcore%2Fkernels%2Fsparse_cross_op.cc', 'patch': '@@ -27,6 +27,7 @@ limitations under the License.\\n #include \"tensorflow\/core\/framework\/tensor.h\"\\n #include \"tensorflow\/core\/framework\/tensor_shape.h\"\\n #include \"tensorflow\/core\/framework\/types.h\"\\n+#include \"tensorflow\/core\/framework\/types.pb.h\"\\n #include \"tensorflow\/core\/lib\/core\/stringpiece.h\"\\n #include \"tensorflow\/core\/lib\/strings\/str_util.h\"\\n #include \"tensorflow\/core\/platform\/fingerprint.h\"\\n@@ -460,10 +461,19 @@ int64 CalculateBatchSize(const OpInputList& shapes_list_in,\\n Status ValidateInput(const OpInputList& indices_list_in,\\n                      const OpInputList& values_list_in,\\n                      const OpInputList& shapes_list_in,\\n-                     const OpInputList& dense_list_in) {\\n+                     const OpInputList& dense_list_in,\\n+                     const DataType& internal_type) {\\n   const auto size = indices_list_in.size();\\n+  \/\/ Only perform internal_type check for SparseCrossOp.\\n+  \/\/ Check if the internal_type is not invalid before doing so.\\n+  bool check_type = internal_type != DT_INVALID;\\n   \/\/ Validates indices_list_in OpInputList.\\n   for (int i = 0; i < size; i++) {\\n+    if (check_type && indices_list_in[i].dtype() != DT_INT64) {\\n+      return errors::InvalidArgument(\"Input indices should be of type \",\\n+                                     DT_INT64, \" but received \",\\n+                                     indices_list_in[i].dtype());\\n+    }\\n     if (!TensorShapeUtils::IsMatrix(indices_list_in[i].shape())) {\\n       return errors::InvalidArgument(\\n           \"Input indices should be a matrix but received shape \",\\n@@ -482,6 +492,14 @@ Status ValidateInput(const OpInputList& indices_list_in,\\n                                    values_list_in.size());\\n   }\\n   for (int i = 0; i < size; i++) {\\n+    \/\/ Make sure to avoid the expected type to be string, but input values to be\\n+    \/\/ int64.\\n+    if (check_type && internal_type == DT_STRING &&\\n+        values_list_in[i].dtype() == DT_INT64) {\\n+      return errors::InvalidArgument(\"Input values should be of internal type \",\\n+                                     internal_type, \" but received \",\\n+                                     values_list_in[i].dtype());\\n+    }\\n     if (!TensorShapeUtils::IsVector(values_list_in[i].shape())) {\\n       return errors::InvalidArgument(\\n           \"Input values should be a vector but received shape \",\\n@@ -502,6 +520,11 @@ Status ValidateInput(const OpInputList& indices_list_in,\\n                                    shapes_list_in.size());\\n   }\\n   for (int i = 0; i < size; i++) {\\n+    if (check_type && shapes_list_in[i].dtype() != DT_INT64) {\\n+      return errors::InvalidArgument(\"Input shape should be of type \", DT_INT64,\\n+                                     \" but received \",\\n+                                     shapes_list_in[i].dtype());\\n+    }\\n     if (!TensorShapeUtils::IsVector(shapes_list_in[i].shape())) {\\n       return errors::InvalidArgument(\\n           \"Input shapes should be a vector but received shape \",\\n@@ -517,6 +540,14 @@ Status ValidateInput(const OpInputList& indices_list_in,\\n \\n   \/\/ Validates dense_list_in OpInputList\\n   for (int i = 0; i < dense_list_in.size(); ++i) {\\n+    \/\/ Make sure to avoid the expected type to be string, but input values to be\\n+    \/\/ int64.\\n+    if (check_type && internal_type == DT_STRING &&\\n+        dense_list_in[i].dtype() == DT_INT64) {\\n+      return errors::InvalidArgument(\"Dense inputs should be of internal type \",\\n+                                     internal_type, \" but received \",\\n+                                     dense_list_in[i].dtype());\\n+    }\\n     if (!TensorShapeUtils::IsMatrix(dense_list_in[i].shape())) {\\n       return errors::InvalidArgument(\\n           \"Dense inputs should be a matrix but received shape \",\\n@@ -698,6 +729,7 @@ class SparseCrossOp : public OpKernel {\\n     int64 signed_hash_key_;\\n     OP_REQUIRES_OK(context, context->GetAttr(\"hash_key\", &signed_hash_key_));\\n     hash_key_ = static_cast<uint64>(signed_hash_key_);\\n+    OP_REQUIRES_OK(context, context->GetAttr(\"internal_type\", &internal_type_));\\n   }\\n \\n   void Compute(OpKernelContext* context) override {\\n@@ -711,8 +743,10 @@ class SparseCrossOp : public OpKernel {\\n     OP_REQUIRES_OK(context,\\n                    context->input_list(\"dense_inputs\", &dense_list_in));\\n \\n-    OP_REQUIRES_OK(context, ValidateInput(indices_list_in, values_list_in,\\n-                                          shapes_list_in, dense_list_in));\\n+    DataType internal_type = internal_type_;\\n+    OP_REQUIRES_OK(\\n+        context, ValidateInput(indices_list_in, values_list_in, shapes_list_in,\\n+                               dense_list_in, internal_type));\\n \\n     std::vector<std::unique_ptr<ColumnInterface<InternalType>>> columns =\\n         GenerateColumnsFromInput<InternalType>(indices_list_in, values_list_in,\\n@@ -756,6 +790,7 @@ class SparseCrossOp : public OpKernel {\\n  private:\\n   int64 num_buckets_;\\n   uint64 hash_key_;\\n+  DataType internal_type_;\\n };\\n \\n class SparseCrossV2Op : public OpKernel {\\n@@ -773,8 +808,11 @@ class SparseCrossV2Op : public OpKernel {\\n     OP_REQUIRES_OK(context,\\n                    context->input_list(\"dense_inputs\", &dense_list_in));\\n \\n-    OP_REQUIRES_OK(context, ValidateInput(indices_list_in, values_list_in,\\n-                                          shapes_list_in, dense_list_in));\\n+    \/\/ Set internal_type to invalid_type so that the check will be ignored.\\n+    DataType internal_type = DT_INVALID;\\n+    OP_REQUIRES_OK(\\n+        context, ValidateInput(indices_list_in, values_list_in, shapes_list_in,\\n+                               dense_list_in, internal_type));\\n \\n     const Tensor* sep_t;\\n     OP_REQUIRES_OK(context, context->input(\"sep\", &sep_t));\\n@@ -832,8 +870,11 @@ class SparseCrossHashedOp : public OpKernel {\\n     OP_REQUIRES_OK(context,\\n                    context->input_list(\"dense_inputs\", &dense_list_in));\\n \\n-    OP_REQUIRES_OK(context, ValidateInput(indices_list_in, values_list_in,\\n-                                          shapes_list_in, dense_list_in));\\n+    \/\/ Set internal_type to invalid_type so that the check will be ignored.\\n+    DataType internal_type = DT_INVALID;\\n+    OP_REQUIRES_OK(\\n+        context, ValidateInput(indices_list_in, values_list_in, shapes_list_in,\\n+                               dense_list_in, internal_type));\\n \\n     const Tensor* num_buckets_t;\\n     OP_REQUIRES_OK(context, context->input(\"num_buckets\", &num_buckets_t));'}}",
            "message_norm":"fix `tf.raw_ops.sparsecross` failing check.\n\npiperorigin-revid: 368701671\nchange-id: id805729dd9ba0bda36e4bb309408129b55fb649d",
            "language":"en",
            "entities":"[('fix', 'ACTION', ''), ('368701671', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/core\/kernels\/sparse_cross_op.cc'])",
            "num_files":1.0
        },
        {
            "index":2461,
            "vuln_id":"GHSA-mr7p-25v2-35wr",
            "cwe_id":"{'CWE-22'}",
            "score":7.5,
            "chain":"{'https:\/\/github.com\/nltk\/nltk\/commit\/f59d7ed8df2e0e957f7f247fe218032abdbe9a10'}",
            "dataset":"osv",
            "summary":"Path Traversal in nltk NLTK Downloader before 3.4.5 is vulnerable to a directory traversal, allowing attackers to write arbitrary files via a ..\/ (dot dot slash) in an NLTK package (ZIP archive) that is mishandled during extraction.",
            "published_date":"2019-08-23",
            "chain_len":1,
            "project":"https:\/\/github.com\/nltk\/nltk",
            "commit_href":"https:\/\/github.com\/nltk\/nltk\/commit\/f59d7ed8df2e0e957f7f247fe218032abdbe9a10",
            "commit_sha":"f59d7ed8df2e0e957f7f247fe218032abdbe9a10",
            "patch":"SINGLE",
            "chain_ord":"['f59d7ed8df2e0e957f7f247fe218032abdbe9a10']",
            "before_first_fix_commit":"{'2554ff48feed878ba7e830ada9825196f3eaa86a'}",
            "last_fix_commit":"f59d7ed8df2e0e957f7f247fe218032abdbe9a10",
            "chain_ord_pos":1.0,
            "commit_datetime":"08\/20\/2019, 10:35:00",
            "message":"CVE-2019-14751:\nFixed security bug in downloader\n(https:\/\/cve.mitre.org\/cgi-bin\/cvename.cgi?name=CVE-2019-14751)",
            "author":"Steven Bird",
            "comments":"{'com_1': {'author': 'greysteil', 'datetime': '08\/26\/2019, 11:01:35', 'body': \"Thanks for this @stevenbird, and for all your work on `nltk`.\\r\\n\\r\\nHave you got 5 minutes to talk me through the process you went through fixing this, and any way GitHub can help? I'm on GitHub's security team and am working to make it easier for maintainers to alert users of security vulnerabilities.\\r\\n\\r\\nCurrently we have the security alert emails (which we're working to improve) and Security Advisories (the security tab on this repo). In future we're planning to make it easy for maintainers to apply for CVEs through GitHub (via creating Security Advisories).\\r\\n\\r\\nWas there any part of the flow of finding, fixing, and alerting users of this vulnerability that GitHub could have helped with? Or anything we're doing now that you'd like us to do differently?\\r\\n\\r\\nAny feedback very much appreciated. I'm on greysteil@github.com if you'd rather email it privately.\\r\\n\\r\\nThanks for all your do, and please don't hesitate to reach out if there's ever any way GitHub can help.\"}}",
            "stats":"{'additions': 1, 'deletions': 35, 'total': 36}",
            "files":"{'nltk\/downloader.py': {'additions': 1, 'deletions': 35, 'changes': 36, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/nltk\/nltk\/raw\/f59d7ed8df2e0e957f7f247fe218032abdbe9a10\/nltk%2Fdownloader.py', 'patch': \"@@ -2260,42 +2260,8 @@ def _unzip_iter(filename, root, verbose=True):\\n         yield ErrorMessage(filename, e)\\n         return\\n \\n-    # Get lists of directories & files\\n-    namelist = zf.namelist()\\n-    dirlist = set()\\n-    for x in namelist:\\n-        if x.endswith('\/'):\\n-            dirlist.add(x)\\n-        else:\\n-            dirlist.add(x.rsplit('\/', 1)[0] + '\/')\\n-    filelist = [x for x in namelist if not x.endswith('\/')]\\n-\\n-    # Create the target directory if it doesn't exist\\n-    if not os.path.exists(root):\\n-        os.mkdir(root)\\n-\\n-    # Create the directory structure\\n-    for dirname in sorted(dirlist):\\n-        pieces = dirname[:-1].split('\/')\\n-        for i in range(len(pieces)):\\n-            dirpath = os.path.join(root, *pieces[: i + 1])\\n-            if not os.path.exists(dirpath):\\n-                os.mkdir(dirpath)\\n-\\n-    # Extract files.\\n-    for i, filename in enumerate(filelist):\\n-        filepath = os.path.join(root, *filename.split('\/'))\\n-\\n-        try:\\n-            with open(filepath, 'wb') as dstfile, zf.open(filename) as srcfile:\\n-                shutil.copyfileobj(srcfile, dstfile)\\n-        except Exception as e:\\n-            yield ErrorMessage(filename, e)\\n-            return\\n+    zf.extractall(root)\\n \\n-        if verbose and (i * 10 \/ len(filelist) > (i - 1) * 10 \/ len(filelist)):\\n-            sys.stdout.write('.')\\n-            sys.stdout.flush()\\n     if verbose:\\n         print()\"}}",
            "message_norm":"cve-2019-14751:\nfixed security bug in downloader\n(https:\/\/cve.mitre.org\/cgi-bin\/cvename.cgi?name=cve-2019-14751)",
            "language":"en",
            "entities":"[('cve-2019-14751', 'VULNID', 'CVE'), ('fixed', 'ACTION', ''), ('security', 'SECWORD', ''), ('bug', 'FLAW', ''), ('https:\/\/cve.mitre.org\/cgi-bin\/cvename.cgi?name=cve-2019-14751', 'VULNID', 'CVE')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['nltk\/downloader.py'])",
            "num_files":1.0
        },
        {
            "index":806,
            "vuln_id":"GHSA-69q2-p9xp-739v",
            "cwe_id":"{'CWE-91'}",
            "score":9.8,
            "chain":"{'https:\/\/github.com\/petl-developers\/petl\/pull\/527\/commits\/1b0a09f08c3cdfe2e69647bd02f97c1367a5b5f8'}",
            "dataset":"osv",
            "summary":"XML Injection in petl petl before 1.68, in some configurations, allows resolution of entities in an XML document.",
            "published_date":"2021-04-20",
            "chain_len":1,
            "project":"https:\/\/github.com\/petl-developers\/petl",
            "commit_href":"https:\/\/github.com\/petl-developers\/petl\/pull\/527\/commits\/1b0a09f08c3cdfe2e69647bd02f97c1367a5b5f8",
            "commit_sha":"1b0a09f08c3cdfe2e69647bd02f97c1367a5b5f8",
            "patch":"SINGLE",
            "chain_ord":"['1b0a09f08c3cdfe2e69647bd02f97c1367a5b5f8']",
            "before_first_fix_commit":"{'364c3e5d0263a99dffebcd9df70b17bce57b3b06'}",
            "last_fix_commit":"1b0a09f08c3cdfe2e69647bd02f97c1367a5b5f8",
            "chain_ord_pos":1.0,
            "commit_datetime":"10\/05\/2020, 22:42:56",
            "message":"allow using a custom\/restricted xml parser",
            "author":"Juarez Rudsatz",
            "comments":null,
            "stats":"{'additions': 20, 'deletions': 2, 'total': 22}",
            "files":"{'petl\/io\/xml.py': {'additions': 20, 'deletions': 2, 'changes': 22, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/petl-developers\/petl\/raw\/1b0a09f08c3cdfe2e69647bd02f97c1367a5b5f8\/petl%2Fio%2Fxml.py', 'patch': '@@ -133,6 +133,9 @@ def fromxml(source, *args, **kwargs):\\n     or list of paths can be provided, e.g.,\\n     ``fromxml(\\'example.html\\', \\'.\/\/tr\\', (\\'th\\', \\'td\\'))``.\\n \\n+    Optionally a custom parser can be provided, e.g.,\\n+    ``etl.fromxml(\\'example1.xml\\', \\'tr\\', \\'td\\', parser=my_parser)``.\\n+\\n     \"\"\"\\n \\n     source = read_source_from_arg(source)\\n@@ -162,14 +165,15 @@ def __init__(self, source, *args, **kwargs):\\n         else:\\n             assert False, \\'bad parameters\\'\\n         self.missing = kwargs.get(\\'missing\\', None)\\n+        self.user_parser = kwargs.get(\\'parser\\', None)\\n \\n     def __iter__(self):\\n         vmatch = self.vmatch\\n         vdict = self.vdict\\n \\n         with self.source.open(\\'rb\\') as xmlf:\\n-\\n-            tree = etree.parse(xmlf)\\n+            parser2 = _create_xml_parser(self.user_parser)\\n+            tree = etree.parse(xmlf, parser=parser2)\\n             if not hasattr(tree, \\'iterfind\\'):\\n                 # Python 2.6 compatibility\\n                 tree.iterfind = tree.findall\\n@@ -219,6 +223,20 @@ def __iter__(self):\\n                                 for f in flds)\\n \\n \\n+def _create_xml_parser(user_parser):\\n+    if user_parser is not None:\\n+        return user_parser\\n+    try:\\n+        # Default lxml parser.\\n+        # This will throw an error if parser is not set and lxml could not be imported\\n+        # because Python\\'s built XML parser doesn\\'t like the `resolve_entities` kwarg.\\n+        # return etree.XMLParser(resolve_entities=False)\\n+        return etree.XMLParser(resolve_entities=False)\\n+    except TypeError:\\n+        # lxml not available\\n+        return None\\n+\\n+\\n def element_text_getter(missing):\\n     def _get(v):\\n         if len(v) > 1:'}}",
            "message_norm":"allow using a custom\/restricted xml parser",
            "language":"en",
            "entities":null,
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['petl\/io\/xml.py'])",
            "num_files":1.0
        },
        {
            "index":3457,
            "vuln_id":"GHSA-xm9f-vxmx-4m58",
            "cwe_id":"{'CWE-20'}",
            "score":0.0,
            "chain":"{'https:\/\/github.com\/OpenMage\/magento-lts\/commit\/34709ac642d554aa1824892059186dd329db744b'}",
            "dataset":"osv",
            "summary":"Data Flow Sanitation Issue Fix  ### Impact\nDue to missing sanitation in data flow it was possible for admin users to upload arbitrary executable files to the server.",
            "published_date":"2021-08-30",
            "chain_len":1,
            "project":"https:\/\/github.com\/OpenMage\/magento-lts",
            "commit_href":"https:\/\/github.com\/OpenMage\/magento-lts\/commit\/34709ac642d554aa1824892059186dd329db744b",
            "commit_sha":"34709ac642d554aa1824892059186dd329db744b",
            "patch":"SINGLE",
            "chain_ord":"['34709ac642d554aa1824892059186dd329db744b']",
            "before_first_fix_commit":"{'b99307d00b59c4a226a1e3e4083f02cf2fc8fce7'}",
            "last_fix_commit":"34709ac642d554aa1824892059186dd329db744b",
            "chain_ord_pos":1.0,
            "commit_datetime":"08\/26\/2021, 01:13:20",
            "message":"Merge pull request from GHSA-xm9f-vxmx-4m58\n\nCo-authored-by: Mark Lewis <markwlewis@Marks-MacBook-Pro.local>",
            "author":"Mark Lewis",
            "comments":null,
            "stats":"{'additions': 1, 'deletions': 1, 'total': 2}",
            "files":"{'app\/code\/core\/Mage\/Dataflow\/Model\/Convert\/Adapter\/Io.php': {'additions': 1, 'deletions': 1, 'changes': 2, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/OpenMage\/magento-lts\/raw\/34709ac642d554aa1824892059186dd329db744b\/app%2Fcode%2Fcore%2FMage%2FDataflow%2FModel%2FConvert%2FAdapter%2FIo.php', 'patch': \"@@ -49,7 +49,7 @@ public function getResource($forWrite = false)\\n             $isError = false;\\n \\n             $ioConfig = $this->getVars();\\n-            switch ($this->getVar('type', 'file')) {\\n+            switch (strtolower($this->getVar('type', 'file'))) {\\n                 case 'file':\\n                     \/\/validate export\/import path\\n                     $path = rtrim($ioConfig['path'], '\\\\\\\\\/')\"}}",
            "message_norm":"merge pull request from ghsa-xm9f-vxmx-4m58\n\nco-authored-by: mark lewis <markwlewis@marks-macbook-pro.local>",
            "language":"en",
            "entities":"[('ghsa-xm9f-vxmx-4m58', 'VULNID', 'GHSA')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['app\/code\/core\/Mage\/Dataflow\/Model\/Convert\/Adapter\/Io.php'])",
            "num_files":1.0
        },
        {
            "index":2039,
            "vuln_id":"GHSA-hc6c-75p4-hmq4",
            "cwe_id":"{'CWE-476'}",
            "score":2.5,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/a7116dd3913c4a4afd2a3a938573aa7c785fdfc6'}",
            "dataset":"osv",
            "summary":"Reference binding to null pointer in `MatrixDiag*` ops ### Impact\nThe implementation of [`MatrixDiag*` operations](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/4c4f420e68f1cfaf8f4b6e8e3eb857e9e4c3ff33\/tensorflow\/core\/kernels\/linalg\/matrix_diag_op.cc#L195-L197) does not validate that the tensor arguments are non-empty:\n\n```cc\n      num_rows = context->input(2).flat<int32>()(0);\n      num_cols = context->input(3).flat<int32>()(0);\n      padding_value = context->input(4).flat<T>()(0); \n``` \n\nThus, users can trigger null pointer dereferences if any of the above tensors are null:\n\n```python\nimport tensorflow as tf\n\nd = tf.convert_to_tensor([],dtype=tf.float32)\np = tf.convert_to_tensor([],dtype=tf.float32)\ntf.raw_ops.MatrixDiagV2(diagonal=d, k=0, num_rows=0, num_cols=0, padding_value=p)\n```\n\nChanging from `tf.raw_ops.MatrixDiagV2` to `tf.raw_ops.MatrixDiagV3` still reproduces the issue.\n\n### Patches\nWe have patched the issue in GitHub commit [a7116dd3913c4a4afd2a3a938573aa7c785fdfc6](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/a7116dd3913c4a4afd2a3a938573aa7c785fdfc6).\n\nThe fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by Ye Zhang and Yakun Zhang of Baidu X-Team.",
            "published_date":"2021-05-21",
            "chain_len":1,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/a7116dd3913c4a4afd2a3a938573aa7c785fdfc6",
            "commit_sha":"a7116dd3913c4a4afd2a3a938573aa7c785fdfc6",
            "patch":"SINGLE",
            "chain_ord":"['a7116dd3913c4a4afd2a3a938573aa7c785fdfc6']",
            "before_first_fix_commit":"{'4c4f420e68f1cfaf8f4b6e8e3eb857e9e4c3ff33'}",
            "last_fix_commit":"a7116dd3913c4a4afd2a3a938573aa7c785fdfc6",
            "chain_ord_pos":1.0,
            "commit_datetime":"04\/18\/2021, 03:55:53",
            "message":"Validate `MatrixDiagV{2,3}` arguments to prevent breakage.\n\nPiperOrigin-RevId: 369056033\nChange-Id: Ic2018c297d3dd6f252dc1dd3667f1ed5cb1eaa42",
            "author":"Mihai Maruseac",
            "comments":null,
            "stats":"{'additions': 16, 'deletions': 3, 'total': 19}",
            "files":"{'tensorflow\/core\/kernels\/linalg\/matrix_diag_op.cc': {'additions': 16, 'deletions': 3, 'changes': 19, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/a7116dd3913c4a4afd2a3a938573aa7c785fdfc6\/tensorflow%2Fcore%2Fkernels%2Flinalg%2Fmatrix_diag_op.cc', 'patch': '@@ -192,9 +192,22 @@ class MatrixDiagOp : public OpKernel {\\n           upper_diag_index = diag_index.flat<int32>()(1);\\n         }\\n       }\\n-      num_rows = context->input(2).flat<int32>()(0);\\n-      num_cols = context->input(3).flat<int32>()(0);\\n-      padding_value = context->input(4).flat<T>()(0);\\n+\\n+      auto& num_rows_tensor = context->input(2);\\n+      OP_REQUIRES(context, TensorShapeUtils::IsScalar(num_rows_tensor.shape()),\\n+                  errors::InvalidArgument(\"num_rows must be a scalar\"));\\n+      num_rows = num_rows_tensor.flat<int32>()(0);\\n+\\n+      auto& num_cols_tensor = context->input(3);\\n+      OP_REQUIRES(context, TensorShapeUtils::IsScalar(num_cols_tensor.shape()),\\n+                  errors::InvalidArgument(\"num_cols must be a scalar\"));\\n+      num_cols = num_cols_tensor.flat<int32>()(0);\\n+\\n+      auto& padding_value_tensor = context->input(4);\\n+      OP_REQUIRES(context,\\n+                  TensorShapeUtils::IsScalar(padding_value_tensor.shape()),\\n+                  errors::InvalidArgument(\"padding_value must be a scalar\"));\\n+      padding_value = padding_value_tensor.flat<T>()(0);\\n     }\\n \\n     \/\/ Size validations.'}}",
            "message_norm":"validate `matrixdiagv{2,3}` arguments to prevent breakage.\n\npiperorigin-revid: 369056033\nchange-id: ic2018c297d3dd6f252dc1dd3667f1ed5cb1eaa42",
            "language":"en",
            "entities":"[('validate', 'ACTION', ''), ('prevent', 'ACTION', ''), ('369056033', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/core\/kernels\/linalg\/matrix_diag_op.cc'])",
            "num_files":1.0
        },
        {
            "index":3138,
            "vuln_id":"GHSA-vm37-j55j-8655",
            "cwe_id":"{'CWE-78'}",
            "score":7.8,
            "chain":"{'https:\/\/github.com\/microweber\/microweber\/commit\/0a7e5f1d81de884861ca677ee1aaac31f188d632'}",
            "dataset":"osv",
            "summary":"OS Command Injection in Microweber Microweber is a content management system with drag and drop. Prior to version 1.2.11, Microweber is vulnerable to OS Command Injection.",
            "published_date":"2022-02-12",
            "chain_len":1,
            "project":"https:\/\/github.com\/microweber\/microweber",
            "commit_href":"https:\/\/github.com\/microweber\/microweber\/commit\/0a7e5f1d81de884861ca677ee1aaac31f188d632",
            "commit_sha":"0a7e5f1d81de884861ca677ee1aaac31f188d632",
            "patch":"SINGLE",
            "chain_ord":"['0a7e5f1d81de884861ca677ee1aaac31f188d632']",
            "before_first_fix_commit":"{'b66537fbd7792d10f07fa7870ead7aae293f1120'}",
            "last_fix_commit":"0a7e5f1d81de884861ca677ee1aaac31f188d632",
            "chain_ord_pos":1.0,
            "commit_datetime":"02\/10\/2022, 08:27:09",
            "message":"Update plupload.php",
            "author":"Bozhidar Slaveykov",
            "comments":null,
            "stats":"{'additions': 1, 'deletions': 2, 'total': 3}",
            "files":"{'src\/MicroweberPackages\/App\/functions\/plupload.php': {'additions': 1, 'deletions': 2, 'changes': 3, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/microweber\/microweber\/raw\/0a7e5f1d81de884861ca677ee1aaac31f188d632\/src%2FMicroweberPackages%2FApp%2Ffunctions%2Fplupload.php', 'patch': \"@@ -149,14 +149,13 @@\\n                             $is_ext = strtolower($is_ext);\\n \\n                             switch ($is_ext) {\\n-                                case 'php':\\n+                                case 'php': \\n                                 case 'php12':\\n                                 case 'php11':\\n                                 case 'php10':\\n                                 case 'php9':\\n                                 case 'php8':\\n                                 case 'php7':\\n-                                case 'php6':\\n                                 case 'php5':\\n                                 case 'php4':\\n                                 case 'php3':\"}}",
            "message_norm":"update plupload.php",
            "language":"ro",
            "entities":"[('update', 'ACTION', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['src\/MicroweberPackages\/App\/functions\/plupload.php'])",
            "num_files":1.0
        },
        {
            "index":1578,
            "vuln_id":"GHSA-cpgw-2wxr-pww3",
            "cwe_id":"{'CWE-601'}",
            "score":6.1,
            "chain":"{'https:\/\/github.com\/gogs\/gogs\/commit\/1f247cf8139cb483276cd8dd06385a800ce9d4b2'}",
            "dataset":"osv",
            "summary":"Open Redirect Open redirect vulnerability in Gogs before 0.12 allows remote attackers to redirect users to arbitrary websites and conduct phishing attacks via an initial \/\\ substring in the user\/login redirect_to parameter, related to the function isValidRedirect in routes\/user\/auth.go.",
            "published_date":"2021-06-29",
            "chain_len":1,
            "project":"https:\/\/github.com\/gogs\/gogs",
            "commit_href":"https:\/\/github.com\/gogs\/gogs\/commit\/1f247cf8139cb483276cd8dd06385a800ce9d4b2",
            "commit_sha":"1f247cf8139cb483276cd8dd06385a800ce9d4b2",
            "patch":"SINGLE",
            "chain_ord":"['1f247cf8139cb483276cd8dd06385a800ce9d4b2']",
            "before_first_fix_commit":"{'c9bb33afc3ae35db21b26fd914bd80ca277a4e0d'}",
            "last_fix_commit":"1f247cf8139cb483276cd8dd06385a800ce9d4b2",
            "chain_ord_pos":1.0,
            "commit_datetime":"08\/06\/2018, 09:10:16",
            "message":"routes: fix open redirect vulnerability #5364 (#5365)",
            "author":"chromium1337",
            "comments":null,
            "stats":"{'additions': 2, 'deletions': 2, 'total': 4}",
            "files":"{'routes\/user\/auth.go': {'additions': 2, 'deletions': 2, 'changes': 4, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/gogs\/gogs\/raw\/1f247cf8139cb483276cd8dd06385a800ce9d4b2\/routes%2Fuser%2Fauth.go', 'patch': \"@@ -73,10 +73,10 @@ func AutoLogin(c *context.Context) (bool, error) {\\n }\\n \\n \/\/ isValidRedirect returns false if the URL does not redirect to same site.\\n-\/\/ False: \/\/url, http:\/\/url\\n+\/\/ False: \/\/url, http:\/\/url, \/\\\\url\\n \/\/ True: \/url\\n func isValidRedirect(url string) bool {\\n-\\treturn len(url) >= 2 && url[0] == '\/' && url[1] != '\/'\\n+\\treturn len(url) >= 2 && url[0] == '\/' && url[1] != '\/' && url[1] != '\\\\\\\\'\\n }\\n \\n func Login(c *context.Context) {\"}}",
            "message_norm":"routes: fix open redirect vulnerability #5364 (#5365)",
            "language":"en",
            "entities":"[('fix', 'ACTION', ''), ('open redirect', 'SECWORD', ''), ('vulnerability', 'SECWORD', ''), ('#5364', 'ISSUE', ''), ('#5365', 'ISSUE', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['routes\/user\/auth.go'])",
            "num_files":1.0
        },
        {
            "index":2909,
            "vuln_id":"GHSA-r9vm-rhmf-7hxx",
            "cwe_id":"{'CWE-78'}",
            "score":9.8,
            "chain":"{'https:\/\/github.com\/Turistforeningen\/node-im-resize\/commit\/de624dacf6a50e39fe3472af1414d44937ce1f03'}",
            "dataset":"osv",
            "summary":"OS Command Injection in im-resize im-resize through 2.3.2 allows remote attackers to execute arbitrary commands via the \"exec\" argument. The cmd argument used within index.js, can be controlled by user without any sanitization.",
            "published_date":"2021-04-13",
            "chain_len":1,
            "project":"https:\/\/github.com\/Turistforeningen\/node-im-resize",
            "commit_href":"https:\/\/github.com\/Turistforeningen\/node-im-resize\/commit\/de624dacf6a50e39fe3472af1414d44937ce1f03",
            "commit_sha":"de624dacf6a50e39fe3472af1414d44937ce1f03",
            "patch":"SINGLE",
            "chain_ord":"['de624dacf6a50e39fe3472af1414d44937ce1f03']",
            "before_first_fix_commit":"{'499fe82028337ae55cb61c24696c1ec16f0f9c9a'}",
            "last_fix_commit":"de624dacf6a50e39fe3472af1414d44937ce1f03",
            "chain_ord_pos":1.0,
            "commit_datetime":"02\/03\/2020, 21:25:54",
            "message":"fix: check image arguments before processing (#19)\n\nRegex hotfix to check for command injection",
            "author":"Sam Sanoop",
            "comments":null,
            "stats":"{'additions': 4, 'deletions': 0, 'total': 4}",
            "files":"{'index.js': {'additions': 4, 'deletions': 0, 'changes': 4, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/Turistforeningen\/node-im-resize\/raw\/de624dacf6a50e39fe3472af1414d44937ce1f03\/index.js', 'patch': \"@@ -7,13 +7,17 @@ var join = require('path').join;\\n var sprintf = require('util').format;\\n \\n module.exports = function(image, output, cb) {\\n+  if(\/;|&|`|\\\\$|\\\\(|\\\\)|\\\\|\\\\||\\\\||!|>|<|\\\\?|\\\\${\/g.test(JSON.stringify(image))) {\\n+    console.log('Input Validation failed, Suspicious Characters found');\\n+  } else {\\n   var cmd = module.exports.cmd(image, output);\\n   exec(cmd, {timeout: 30000}, function(e, stdout, stderr) {\\n     if (e) { return cb(e); }\\n     if (stderr) { return cb(new Error(stderr)); }\\n \\n     return cb(null, output.versions);\\n   });\\n+}\\n };\\n \\n \/**\"}}",
            "message_norm":"fix: check image arguments before processing (#19)\n\nregex hotfix to check for command injection",
            "language":"en",
            "entities":"[('#19', 'ISSUE', ''), ('hotfix', 'ACTION', ''), ('command injection', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['index.js'])",
            "num_files":1.0
        },
        {
            "index":2704,
            "vuln_id":"GHSA-q5wr-fvpq-p67g",
            "cwe_id":"{'CWE-787', 'CWE-190'}",
            "score":8.8,
            "chain":"{'https:\/\/github.com\/gemini-testing\/png-img\/commit\/14ac462a32ca4b3b78f56502ac976d5b0222ce3d'}",
            "dataset":"osv",
            "summary":"Integer Overflow in png-img An integer overflow in the PngImg::InitStorage_() function of png-img before 3.1.0 leads to an under-allocation of heap memory and subsequently an exploitable heap-based buffer overflow when loading a crafted PNG file.",
            "published_date":"2021-12-10",
            "chain_len":1,
            "project":"https:\/\/github.com\/gemini-testing\/png-img",
            "commit_href":"https:\/\/github.com\/gemini-testing\/png-img\/commit\/14ac462a32ca4b3b78f56502ac976d5b0222ce3d",
            "commit_sha":"14ac462a32ca4b3b78f56502ac976d5b0222ce3d",
            "patch":"SINGLE",
            "chain_ord":"['14ac462a32ca4b3b78f56502ac976d5b0222ce3d']",
            "before_first_fix_commit":"{'9fedfccb9ab2d1ccee4d7d544f3e03d505317352'}",
            "last_fix_commit":"14ac462a32ca4b3b78f56502ac976d5b0222ce3d",
            "chain_ord_pos":1.0,
            "commit_datetime":"08\/06\/2020, 00:45:40",
            "message":"Handle image size overflow",
            "author":"Mikhail Cheshkov",
            "comments":null,
            "stats":"{'additions': 12, 'deletions': 2, 'total': 14}",
            "files":"{'src\/PngImg.cc': {'additions': 12, 'deletions': 2, 'changes': 14, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/gemini-testing\/png-img\/raw\/14ac462a32ca4b3b78f56502ac976d5b0222ce3d\/src%2FPngImg.cc', 'patch': '@@ -60,10 +60,20 @@ void PngImg::ReadInfo_(PngReadStruct& rs) {\\n \/\/\/\\n void PngImg::InitStorage_() {\\n     rowPtrs_.resize(info_.height, nullptr);\\n-    data_ = new png_byte[info_.height * info_.rowbytes];\\n+    \/\/ Extend height and rowbytes from uint32_t to size_t to avoid multiplication overflow when size_t is larger\\n+    size_t h = info_.height;\\n+    size_t rb = info_.rowbytes;\\n+    \/\/ We need to make sure that info_.height * info_.rowbytes will not overflow size_t\\n+    \/\/ Unfotunately, there\\'s no simple and portable way to do this in C++\\n+    \/\/ For integer division of positive numbers a * b > c <==> a > c \/ b holds\\n+    if (h > std::numeric_limits<size_t>::max() \/ rb) {\\n+        \/\/ TODO Propagate this exception to JS, and test it\\n+        throw std::runtime_error(\"Image is too large to allocate single buffer\");\\n+    }\\n+    data_ = new png_byte[h * rb];\\n \\n     for(size_t i = 0; i < info_.height; ++i) {\\n-        rowPtrs_[i] = data_ + i * info_.rowbytes;\\n+        rowPtrs_[i] = data_ + i * rb;\\n     }\\n }'}}",
            "message_norm":"handle image size overflow",
            "language":"en",
            "entities":"[('overflow', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['src\/PngImg.cc'])",
            "num_files":1.0
        },
        {
            "index":80,
            "vuln_id":"GHSA-2cfc-865j-gm4w",
            "cwe_id":"{'CWE-611'}",
            "score":7.3,
            "chain":"{'https:\/\/github.com\/detekt\/detekt\/commit\/c965a8d2a6bbdb9bcfc6acfa7bbffd3da81f5395'}",
            "dataset":"osv",
            "summary":"XML External Entity Reference in detekt Improper Restriction of XML External Entity Reference in GitHub repository detekt\/detekt prior to 1.20.0.",
            "published_date":"2022-04-22",
            "chain_len":1,
            "project":"https:\/\/github.com\/detekt\/detekt",
            "commit_href":"https:\/\/github.com\/detekt\/detekt\/commit\/c965a8d2a6bbdb9bcfc6acfa7bbffd3da81f5395",
            "commit_sha":"c965a8d2a6bbdb9bcfc6acfa7bbffd3da81f5395",
            "patch":"SINGLE",
            "chain_ord":"['c965a8d2a6bbdb9bcfc6acfa7bbffd3da81f5395']",
            "before_first_fix_commit":"{'08eac68caa24ced140cc017d4de3b258a470232b'}",
            "last_fix_commit":"c965a8d2a6bbdb9bcfc6acfa7bbffd3da81f5395",
            "chain_ord_pos":1.0,
            "commit_datetime":"01\/18\/2022, 17:21:06",
            "message":"Parse Baseline in a secure way (#4499)",
            "author":"Brais Gab\u00edn",
            "comments":null,
            "stats":"{'additions': 6, 'deletions': 1, 'total': 7}",
            "files":"{'detekt-core\/src\/main\/kotlin\/io\/gitlab\/arturbosch\/detekt\/core\/baseline\/BaselineFormat.kt': {'additions': 6, 'deletions': 1, 'changes': 7, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/detekt\/detekt\/raw\/c965a8d2a6bbdb9bcfc6acfa7bbffd3da81f5395\/detekt-core%2Fsrc%2Fmain%2Fkotlin%2Fio%2Fgitlab%2Farturbosch%2Fdetekt%2Fcore%2Fbaseline%2FBaselineFormat.kt', 'patch': '@@ -3,6 +3,7 @@ package io.gitlab.arturbosch.detekt.core.baseline\\n import org.xml.sax.SAXParseException\\n import java.nio.file.Files\\n import java.nio.file.Path\\n+import javax.xml.XMLConstants\\n import javax.xml.parsers.SAXParserFactory\\n import javax.xml.stream.XMLStreamException\\n import javax.xml.stream.XMLStreamWriter\\n@@ -17,7 +18,11 @@ internal class BaselineFormat {\\n     fun read(path: Path): Baseline {\\n         try {\\n             Files.newInputStream(path).use {\\n-                val reader = SAXParserFactory.newInstance().newSAXParser()\\n+                val reader = SAXParserFactory.newInstance()\\n+                    .apply {\\n+                        setFeature(XMLConstants.FEATURE_SECURE_PROCESSING, true)\\n+                    }\\n+                    .newSAXParser()\\n                 val handler = BaselineHandler()\\n                 reader.parse(it, handler)\\n                 return handler.createBaseline()'}}",
            "message_norm":"parse baseline in a secure way (#4499)",
            "language":"en",
            "entities":"[('secure', 'SECWORD', ''), ('#4499', 'ISSUE', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['detekt-core\/src\/main\/kotlin\/io\/gitlab\/arturbosch\/detekt\/core\/baseline\/BaselineFormat.kt'])",
            "num_files":1.0
        },
        {
            "index":1223,
            "vuln_id":"GHSA-8jj7-5vxc-pg2q",
            "cwe_id":"{'CWE-190'}",
            "score":8.8,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/0aaaae6eca5a7175a193696383f582f53adab23f'}",
            "dataset":"osv",
            "summary":"Integer overflow in TensorFlow ### Impact\nUnder certain scenarios, Grappler component of TensorFlow is vulnerable to an integer overflow during [cost estimation for crop and resize](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/a1320ec1eac186da1d03f033109191f715b2b130\/tensorflow\/core\/grappler\/costs\/op_level_cost_estimator.cc#L2621-L2689). Since the cropping parameters are user controlled, a malicious person can trigger undefined behavior.\n\n### Patches\nWe have patched the issue in GitHub commit [0aaaae6eca5a7175a193696383f582f53adab23f](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/0aaaae6eca5a7175a193696383f582f53adab23f).\n\nThe fix will be included in TensorFlow 2.8.0. We will also cherrypick this commit on TensorFlow 2.7.1, TensorFlow 2.6.3, and TensorFlow 2.5.3, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.",
            "published_date":"2022-02-09",
            "chain_len":1,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/0aaaae6eca5a7175a193696383f582f53adab23f",
            "commit_sha":"0aaaae6eca5a7175a193696383f582f53adab23f",
            "patch":"SINGLE",
            "chain_ord":"['0aaaae6eca5a7175a193696383f582f53adab23f']",
            "before_first_fix_commit":"{'6b5adc0877de832b2a7c189532dbbbc64622eeb6'}",
            "last_fix_commit":"0aaaae6eca5a7175a193696383f582f53adab23f",
            "chain_ord_pos":1.0,
            "commit_datetime":"11\/13\/2021, 16:19:05",
            "message":"Prevent overflow in grappler cost estimation of crop&resize op.\n\nThe crop parameters are user controlled, so we should make sure a user can not trigger an overflow maliciously.\n\nPiperOrigin-RevId: 409670234\nChange-Id: I7994734a98b037c5642e051240329d16f959aae4",
            "author":"Mihai Maruseac",
            "comments":null,
            "stats":"{'additions': 22, 'deletions': 7, 'total': 29}",
            "files":"{'tensorflow\/core\/grappler\/costs\/op_level_cost_estimator.cc': {'additions': 22, 'deletions': 7, 'changes': 29, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/0aaaae6eca5a7175a193696383f582f53adab23f\/tensorflow%2Fcore%2Fgrappler%2Fcosts%2Fop_level_cost_estimator.cc', 'patch': '@@ -2681,27 +2681,42 @@ Status OpLevelCostEstimator::PredictCropAndResize(const OpContext& op_context,\\n   \/\/ calculation differs from rough estimate in implementation, as it separates\\n   \/\/ out cost per box from cost per pixel and cost per element.\\n \\n+  \/\/ Since crop arguments are user controlled, check for overflow.\\n+  int64_t crop_area = MultiplyWithoutOverflow(crop_height, crop_width);\\n+  if (crop_area < 0)\\n+    return errors::InvalidArgument(\"Cannot estimate cost, multiplying \",\\n+                                   crop_height, \" with \", crop_width,\\n+                                   \" would overflow\");\\n+  int64_t crop_volume = MultiplyWithoutOverflow(crop_area, num_boxes);\\n+  if (crop_volume < 0)\\n+    return errors::InvalidArgument(\"Cannot estimate cost, multiplying \",\\n+                                   crop_area, \" with \", num_boxes,\\n+                                   \" would overflow\");\\n+  int64_t crop_depth = MultiplyWithoutOverflow(crop_height, num_boxes);\\n+  if (crop_depth < 0)\\n+    return errors::InvalidArgument(\"Cannot estimate cost, multiplying \",\\n+                                   crop_height, \" with \", num_boxes,\\n+                                   \" would overflow\");\\n+\\n   \/\/ Ops for variables height_scale and width_scale.\\n   int64_t ops = (sub_cost * 6 + mul_cost * 2 + div_cost * 2) * num_boxes;\\n   \/\/ Ops for variable in_y.\\n-  ops += (mul_cost * 2 + sub_cost + add_cost) * crop_height * num_boxes;\\n+  ops += (mul_cost * 2 + sub_cost + add_cost) * crop_depth;\\n   \/\/ Ops for variable in_x (same computation across both branches).\\n-  ops += (mul_cost * 2 + sub_cost + add_cost) * crop_height * crop_width *\\n-         num_boxes;\\n+  ops += (mul_cost * 2 + sub_cost + add_cost) * crop_volume;\\n   \/\/ Specify op_cost based on the method.\\n   if (use_bilinear_interp) {\\n     \/\/ Ops for variables top_y_index, bottom_y_index, y_lerp.\\n-    ops += (floor_cost + ceil_cost + sub_cost) * crop_height * num_boxes;\\n+    ops += (floor_cost + ceil_cost + sub_cost) * crop_depth;\\n     \/\/ Ops for variables left_x, right_x, x_lerp;\\n-    ops += (floor_cost + ceil_cost + sub_cost) * crop_height * crop_width *\\n-           num_boxes;\\n+    ops += (floor_cost + ceil_cost + sub_cost) * crop_volume;\\n     \/\/ Ops for innermost loop across depth.\\n     ops +=\\n         (cast_to_float_cost * 4 + add_cost * 3 + sub_cost * 3 + mul_cost * 3) *\\n         output_elements;\\n   } else \/* method == \"nearest\" *\/ {\\n     \/\/ Ops for variables closest_x_index and closest_y_index.\\n-    ops += round_cost * 2 * crop_height * crop_width * num_boxes;\\n+    ops += round_cost * 2 * crop_volume;\\n     \/\/ Ops for innermost loop across depth.\\n     ops += cast_to_float_cost * output_elements;\\n   }'}}",
            "message_norm":"prevent overflow in grappler cost estimation of crop&resize op.\n\nthe crop parameters are user controlled, so we should make sure a user can not trigger an overflow maliciously.\n\npiperorigin-revid: 409670234\nchange-id: i7994734a98b037c5642e051240329d16f959aae4",
            "language":"en",
            "entities":"[('prevent', 'ACTION', ''), ('overflow', 'SECWORD', ''), ('overflow', 'SECWORD', ''), ('maliciously', 'SECWORD', ''), ('409670234', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/core\/grappler\/costs\/op_level_cost_estimator.cc'])",
            "num_files":1.0
        },
        {
            "index":757,
            "vuln_id":"GHSA-662x-fhqg-9p8v",
            "cwe_id":"{'CWE-400'}",
            "score":7.5,
            "chain":"{'https:\/\/github.com\/faisalman\/ua-parser-js\/commit\/233d3bae22a795153a7e6638887ce159c63e557d'}",
            "dataset":"osv",
            "summary":"Regular Expression Denial of Service in ua-parser-js The package ua-parser-js before 0.7.22 are vulnerable to Regular Expression Denial of Service (ReDoS) via the regex for Redmi Phones and Mi Pad Tablets UA.",
            "published_date":"2021-05-07",
            "chain_len":1,
            "project":"https:\/\/github.com\/faisalman\/ua-parser-js",
            "commit_href":"https:\/\/github.com\/faisalman\/ua-parser-js\/commit\/233d3bae22a795153a7e6638887ce159c63e557d",
            "commit_sha":"233d3bae22a795153a7e6638887ce159c63e557d",
            "patch":"SINGLE",
            "chain_ord":"['233d3bae22a795153a7e6638887ce159c63e557d']",
            "before_first_fix_commit":"{'5230745280ba8aee775b0f5d2c8a2332f8ef2c4e'}",
            "last_fix_commit":"233d3bae22a795153a7e6638887ce159c63e557d",
            "chain_ord_pos":1.0,
            "commit_datetime":"09\/12\/2020, 08:47:15",
            "message":"Fix potential ReDoS vulnerability",
            "author":"Faisal Salman",
            "comments":null,
            "stats":"{'additions': 2, 'deletions': 2, 'total': 4}",
            "files":"{'src\/ua-parser.js': {'additions': 2, 'deletions': 2, 'changes': 4, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/faisalman\/ua-parser-js\/raw\/233d3bae22a795153a7e6638887ce159c63e557d\/src%2Fua-parser.js', 'patch': \"@@ -585,9 +585,9 @@\\n             \/android.+(hm[\\\\s\\\\-_]*note?[\\\\s_]*(?:\\\\d\\\\w)?)\\\\s+build\/i,               \/\/ Xiaomi Hongmi\\n             \/android.+(mi[\\\\s\\\\-_]*(?:a\\\\d|one|one[\\\\s_]plus|note lte)?[\\\\s_]*(?:\\\\d?\\\\w?)[\\\\s_]*(?:plus)?)\\\\s+build\/i,    \\n                                                                                 \/\/ Xiaomi Mi\\n-            \/android.+(redmi[\\\\s\\\\-_]*(?:note)?(?:[\\\\s_]*[\\\\w\\\\s]+))\\\\s+build\/i       \/\/ Redmi Phones\\n+            \/android.+(redmi[\\\\s\\\\-_]*(?:note)?(?:[\\\\s_]?[\\\\w\\\\s]+))\\\\s+build\/i       \/\/ Redmi Phones\\n             ], [[MODEL, \/_\/g, ' '], [VENDOR, 'Xiaomi'], [TYPE, MOBILE]], [\\n-            \/android.+(mi[\\\\s\\\\-_]*(?:pad)(?:[\\\\s_]*[\\\\w\\\\s]+))\\\\s+build\/i            \/\/ Mi Pad tablets\\n+            \/android.+(mi[\\\\s\\\\-_]*(?:pad)(?:[\\\\s_]?[\\\\w\\\\s]+))\\\\s+build\/i            \/\/ Mi Pad tablets\\n             ],[[MODEL, \/_\/g, ' '], [VENDOR, 'Xiaomi'], [TYPE, TABLET]], [\\n             \/android.+;\\\\s(m[1-5]\\\\snote)\\\\sbuild\/i                                \/\/ Meizu\\n             ], [MODEL, [VENDOR, 'Meizu'], [TYPE, MOBILE]], [\"}}",
            "message_norm":"fix potential redos vulnerability",
            "language":"ca",
            "entities":"[('fix', 'ACTION', ''), ('redos', 'SECWORD', ''), ('vulnerability', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['src\/ua-parser.js'])",
            "num_files":1.0
        },
        {
            "index":1687,
            "vuln_id":"GHSA-f7r3-p866-q9qr",
            "cwe_id":"{'CWE-400'}",
            "score":3.7,
            "chain":"{'https:\/\/github.com\/Twipped\/ircdkit\/pull\/2\/commits\/595ed02cde517fad57854d2ac2855a09a626e665', 'https:\/\/github.com\/Twipped\/ircdkit\/commit\/f0cc6dc913ec17b499fa33a676bb72c624456f2c'}",
            "dataset":"osv",
            "summary":"ircdkit vulnerable to Denial of Service due to unhandled connection end event Versions of `ircdkit` 1.0.3 and prior are vulnerable to a remote denial of service.\n\n\n## Recommendation\n\nUpgrade to version 1.0.4.",
            "published_date":"2019-06-03",
            "chain_len":2,
            "project":"https:\/\/github.com\/Twipped\/ircdkit",
            "commit_href":"https:\/\/github.com\/Twipped\/ircdkit\/pull\/2\/commits\/595ed02cde517fad57854d2ac2855a09a626e665",
            "commit_sha":"595ed02cde517fad57854d2ac2855a09a626e665",
            "patch":"MULTI",
            "chain_ord":"['f0cc6dc913ec17b499fa33a676bb72c624456f2c', '595ed02cde517fad57854d2ac2855a09a626e665']",
            "before_first_fix_commit":"{'74aa751e75a90af34ef63377fcbd41285d155380'}",
            "last_fix_commit":"595ed02cde517fad57854d2ac2855a09a626e665",
            "chain_ord_pos":2.0,
            "commit_datetime":"05\/30\/2019, 03:10:50",
            "message":"Update index.js\n\ncorrected unhandled connection 'end' event, fixes issue #1",
            "author":"Trinity Fox",
            "comments":null,
            "stats":"{'additions': 1, 'deletions': 1, 'total': 2}",
            "files":"{'lib\/index.js': {'additions': 1, 'deletions': 1, 'changes': 2, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/Twipped\/ircdkit\/raw\/595ed02cde517fad57854d2ac2855a09a626e665\/lib%2Findex.js', 'patch': \"@@ -47,7 +47,7 @@ function create (options) {\\n \\n \\t\\tclient.on('end', function () {\\n \\t\\t\\tdebug('connection ended');\\n-\\t\\t\\tremoveClient(client);\\n+\\t\\t\\tclient.close();\\n \\t\\t\\tapp.emit('connection:end', client);\\n \\t\\t});\"}}",
            "message_norm":"update index.js\n\ncorrected unhandled connection 'end' event, fixes issue #1",
            "language":"en",
            "entities":"[('update', 'ACTION', ''), ('#1', 'ISSUE', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['lib\/index.js'])",
            "num_files":1.0
        },
        {
            "index":2959,
            "vuln_id":"GHSA-rj44-gpjc-29r7",
            "cwe_id":"{'CWE-78'}",
            "score":6.4,
            "chain":"{'https:\/\/github.com\/thi-ng\/umbrella\/commit\/88f61656e5f5cfba960013b8133186389efaf243'}",
            "dataset":"osv",
            "summary":"[thi.ng\/egf] Potential arbitrary code execution of `#gpg`-tagged property values ### Impact\n\nPotential for arbitrary code execution in `#gpg`-tagged property values (only if `decrypt: true` option is enabled)\n\n### Patches\n\n[A fix](https:\/\/github.com\/thi-ng\/umbrella\/commit\/3e14765d6bfd8006742c9e7860bc7d58ae94dfa5) has already been released as v0.4.0\n\n### Workarounds\n\nBy default, EGF parse functions do NOT attempt to decrypt values (since GPG is only available in non-browser env).\n\nHowever, if GPG encrypted values are used\/required:\n\n1. Perform a regex search for `#gpg`-tagged values in the EGF source file\/string and check for backtick (\\`) chars in the encrypted value string\n2. Replace\/remove them or skip parsing if present...\n\n### References\n\nhttps:\/\/github.com\/thi-ng\/umbrella\/security\/advisories\/GHSA-rj44-gpjc-29r7#advisory-comment-65261\n\n### For more information\n\nIf you have any questions or comments about this advisory, please open an issue in the [thi.ng\/umbrella repo](https:\/\/github.com\/thi-ng\/umbrella\/issues), of which this package is part of.",
            "published_date":"2021-04-06",
            "chain_len":1,
            "project":"https:\/\/github.com\/thi-ng\/umbrella",
            "commit_href":"https:\/\/github.com\/thi-ng\/umbrella\/commit\/88f61656e5f5cfba960013b8133186389efaf243",
            "commit_sha":"88f61656e5f5cfba960013b8133186389efaf243",
            "patch":"SINGLE",
            "chain_ord":"['88f61656e5f5cfba960013b8133186389efaf243']",
            "before_first_fix_commit":"{'c3f5ec12f324a4e627b26dc45d480c0e761602ea', '3e14765d6bfd8006742c9e7860bc7d58ae94dfa5'}",
            "last_fix_commit":"88f61656e5f5cfba960013b8133186389efaf243",
            "chain_ord_pos":1.0,
            "commit_datetime":"03\/27\/2021, 08:52:42",
            "message":"Merge pull request from GHSA-rj44-gpjc-29r7\n\nfix(egf): update GPG invocation to avoid arb code exec",
            "author":"Karsten Schmidt",
            "comments":null,
            "stats":"{'additions': 4, 'deletions': 2, 'total': 6}",
            "files":"{'packages\/egf\/src\/tags.ts': {'additions': 4, 'deletions': 2, 'changes': 6, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/thi-ng\/umbrella\/raw\/88f61656e5f5cfba960013b8133186389efaf243\/packages%2Fegf%2Fsrc%2Ftags.ts', 'patch': '@@ -1,7 +1,7 @@\\n import type { IObjectOf } from \"@thi.ng\/api\";\\n import { maybeParseFloat, maybeParseInt, unescape } from \"@thi.ng\/strings\";\\n import { base64Decode } from \"@thi.ng\/transducers-binary\";\\n-import { execSync } from \"child_process\";\\n+import { execFileSync } from \"child_process\";\\n import { readFileSync } from \"fs\";\\n import { resolve as resolvePath } from \"path\";\\n import { IS_NODE, NODE_ONLY, TagParser } from \".\/api\";\\n@@ -24,7 +24,9 @@ export const BUILTINS: IObjectOf<TagParser> = {\\n     gpg: IS_NODE\\n         ? (_, body, ctx) =>\\n               (ctx.opts.decrypt\\n-                  ? execSync(`echo \"${body}\" | gpg --decrypt`).toString()\\n+                  ? execFileSync(\"gpg\", [\"--decrypt\"], {\\n+                        input: body,\\n+                    }).toString()\\n                   : body\\n               ).trim()\\n         : NODE_ONLY,'}}",
            "message_norm":"merge pull request from ghsa-rj44-gpjc-29r7\n\nfix(egf): update gpg invocation to avoid arb code exec",
            "language":"ca",
            "entities":"[('ghsa-rj44-gpjc-29r7', 'VULNID', 'GHSA'), ('fix(egf', 'ACTION', ''), ('code exec', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['packages\/egf\/src\/tags.ts'])",
            "num_files":1.0
        },
        {
            "index":3330,
            "vuln_id":"GHSA-wwgf-3xp7-cxj4",
            "cwe_id":"{'CWE-200'}",
            "score":5.9,
            "chain":"{'https:\/\/github.com\/FriendsOfPHP\/security-advisories\/commit\/942fd37245cb724ba8cc8d6f11f075a1bd53b338'}",
            "dataset":"osv",
            "summary":"Potentially sensitive data exposure in Symfony Web Socket Bundle ### Impact\nInside `Gos\\Bundle\\WebSocketBundle\\Server\\App\\Dispatcher\\TopicDispatcher::onPublish()`, messages are arbitrarily broadcasted to the related Topic if `Gos\\Bundle\\WebSocketBundle\\Server\\App\\Dispatcher\\TopicDispatcher::dispatch()` does not succeed.  The `dispatch()` method can be considered to not succeed if (depending on the version of the bundle) the callback defined on a topic route is misconfigured, a `Gos\\Bundle\\WebSocketBundle\\Topic\\TopicInterface` implementation is not found for the callback, a topic which also implements `Gos\\Bundle\\WebSocketBundle\\Topic\\SecuredTopicInterface` rejects the connection, or an Exception is unhandled.  This can result in an unintended broadcast to the websocket server potentially with data that should be considered sensitive.\n\n### Patches\nIn 1.10.4, 2.6.1, and 3.3.0, `Gos\\Bundle\\WebSocketBundle\\Server\\App\\Dispatcher\\TopicDispatcher::onPublish()` has been changed to no longer broadcast an event's data if `Gos\\Bundle\\WebSocketBundle\\Server\\App\\Dispatcher\\TopicDispatcher::dispatch()` fails.\n\n### Workarounds\nUpgrade to 1.10.4, 2.6.1, and 3.3.0\n\nNote, the 1.x branch is considered end of support as of July 1, 2020.\n\n### For more information\nIf you have any questions or comments about this advisory:\n* Open an issue in [this repository](https:\/\/github.com\/GeniusesOfSymfony\/WebSocketBundle)",
            "published_date":"2020-07-07",
            "chain_len":1,
            "project":"https:\/\/github.com\/FriendsOfPHP\/security-advisories",
            "commit_href":"https:\/\/github.com\/FriendsOfPHP\/security-advisories\/commit\/942fd37245cb724ba8cc8d6f11f075a1bd53b338",
            "commit_sha":"942fd37245cb724ba8cc8d6f11f075a1bd53b338",
            "patch":"SINGLE",
            "chain_ord":"['942fd37245cb724ba8cc8d6f11f075a1bd53b338']",
            "before_first_fix_commit":"{'c6fc7221e152c3a73fed562ad9c815ccf258a476'}",
            "last_fix_commit":"942fd37245cb724ba8cc8d6f11f075a1bd53b338",
            "chain_ord_pos":1.0,
            "commit_datetime":"07\/06\/2020, 14:36:56",
            "message":"Add gos\/web-socket-bundle advisory",
            "author":"Michael Babker",
            "comments":null,
            "stats":"{'additions': 13, 'deletions': 0, 'total': 13}",
            "files":"{'gos\/web-socket-bundle\/2020-07-06-1.yaml': {'additions': 13, 'deletions': 0, 'changes': 13, 'status': 'added', 'raw_url': 'https:\/\/github.com\/FriendsOfPHP\/security-advisories\/raw\/942fd37245cb724ba8cc8d6f11f075a1bd53b338\/gos%2Fweb-socket-bundle%2F2020-07-06-1.yaml', 'patch': \"@@ -0,0 +1,13 @@\\n+title:     Potentially sensitive data exposure\\n+link:      https:\/\/github.com\/GeniusesOfSymfony\/WebSocketBundle\/security\/advisories\/GHSA-wwgf-3xp7-cxj4\\n+branches:\\n+    1.x:\\n+        time:       2020-07-06 14:08:35\\n+        versions:   ['<1.10.4']\\n+    3.x:\\n+        time:       2020-07-06 14:08:35\\n+        versions:   ['>=2.0.0', '<2.6.1']\\n+    3.x:\\n+        time:       2020-07-06 14:08:35\\n+        versions:   ['>=3.0.0', '<3.3.0']\\n+reference: composer:\/\/gos\/web-socket-bundle\"}}",
            "message_norm":"add gos\/web-socket-bundle advisory",
            "language":"en",
            "entities":"[('add', 'ACTION', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['gos\/web-socket-bundle\/2020-07-06-1.yaml'])",
            "num_files":1.0
        },
        {
            "index":339,
            "vuln_id":"GHSA-434h-p4gx-jm89",
            "cwe_id":"{'CWE-203'}",
            "score":5.3,
            "chain":"{'https:\/\/github.com\/dpgaspar\/Flask-AppBuilder\/commit\/780bd0e8fbf2d36ada52edb769477e0a4edae580'}",
            "dataset":"osv",
            "summary":"Observable Response Discrepancy in Flask-AppBuilder ### Impact\nUser enumeration in database authentication in Flask-AppBuilder <= 3.2.3. Allows for a non authenticated user to enumerate existing accounts by timing the response time from the server when you are logging in.\n\n### Patches\nUpgrade to 3.3.0\n\n### For more information\nIf you have any questions or comments about this advisory:\n* Open an issue in [Flask-AppBuilder](https:\/\/github.com\/dpgaspar\/Flask-AppBuilder)",
            "published_date":"2021-05-27",
            "chain_len":1,
            "project":"https:\/\/github.com\/dpgaspar\/Flask-AppBuilder",
            "commit_href":"https:\/\/github.com\/dpgaspar\/Flask-AppBuilder\/commit\/780bd0e8fbf2d36ada52edb769477e0a4edae580",
            "commit_sha":"780bd0e8fbf2d36ada52edb769477e0a4edae580",
            "patch":"SINGLE",
            "chain_ord":"['780bd0e8fbf2d36ada52edb769477e0a4edae580']",
            "before_first_fix_commit":"{'b60dea9cedf98b56c926ba41020c73f287d5826e'}",
            "last_fix_commit":"780bd0e8fbf2d36ada52edb769477e0a4edae580",
            "chain_ord_pos":1.0,
            "commit_datetime":"05\/10\/2021, 08:37:55",
            "message":"fix: auth balance (#1634)",
            "author":"Daniel Vaz Gaspar",
            "comments":null,
            "stats":"{'additions': 6, 'deletions': 0, 'total': 6}",
            "files":"{'flask_appbuilder\/security\/manager.py': {'additions': 6, 'deletions': 0, 'changes': 6, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/dpgaspar\/Flask-AppBuilder\/raw\/780bd0e8fbf2d36ada52edb769477e0a4edae580\/flask_appbuilder%2Fsecurity%2Fmanager.py', 'patch': '@@ -833,6 +833,12 @@ def auth_user_db(self, username, password):\\n         if user is None:\\n             user = self.find_user(email=username)\\n         if user is None or (not user.is_active):\\n+            # Balance failure and success\\n+            check_password_hash(\\n+                \"pbkdf2:sha256:150000$Z3t6fmj2$22da622d94a1f8118\"\\n+                \"c0976a03d2f18f680bfff877c9a965db9eedc51bc0be87c\",\\n+                \"password\",\\n+            )\\n             log.info(LOGMSG_WAR_SEC_LOGIN_FAILED.format(username))\\n             return None\\n         elif check_password_hash(user.password, password):'}}",
            "message_norm":"fix: auth balance (#1634)",
            "language":"en",
            "entities":"[('auth', 'SECWORD', ''), ('#1634', 'ISSUE', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['flask_appbuilder\/security\/manager.py'])",
            "num_files":1.0
        },
        {
            "index":1037,
            "vuln_id":"GHSA-7hmh-8gwv-mfvq",
            "cwe_id":"{'CWE-89'}",
            "score":6.5,
            "chain":"{'https:\/\/github.com\/apache\/kylin\/commit\/e373c64c96a54a7abfe4bccb82e8feb60db04749'}",
            "dataset":"osv",
            "summary":"SQL Injection in Kylin Kylin has some restful apis which will concatenate SQLs with the user input string, a user is likely to be able to run malicious database queries.",
            "published_date":"2020-07-27",
            "chain_len":1,
            "project":"https:\/\/github.com\/apache\/kylin",
            "commit_href":"https:\/\/github.com\/apache\/kylin\/commit\/e373c64c96a54a7abfe4bccb82e8feb60db04749",
            "commit_sha":"e373c64c96a54a7abfe4bccb82e8feb60db04749",
            "patch":"SINGLE",
            "chain_ord":"['e373c64c96a54a7abfe4bccb82e8feb60db04749']",
            "before_first_fix_commit":"{'ebfc745dd681d7e0c129ded50bd50ff509d2a393'}",
            "last_fix_commit":"e373c64c96a54a7abfe4bccb82e8feb60db04749",
            "chain_ord_pos":1.0,
            "commit_datetime":"02\/07\/2020, 12:22:59",
            "message":"Fix sql injection issue",
            "author":"nichunen",
            "comments":null,
            "stats":"{'additions': 51, 'deletions': 30, 'total': 81}",
            "files":"{'server-base\/src\/main\/java\/org\/apache\/kylin\/rest\/service\/CubeService.java': {'additions': 51, 'deletions': 30, 'changes': 81, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/apache\/kylin\/raw\/e373c64c96a54a7abfe4bccb82e8feb60db04749\/server-base%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkylin%2Frest%2Fservice%2FCubeService.java', 'patch': '@@ -71,6 +71,7 @@\\n import org.apache.kylin.metadata.project.RealizationEntry;\\n import org.apache.kylin.metadata.realization.RealizationStatusEnum;\\n import org.apache.kylin.metadata.realization.RealizationType;\\n+import org.apache.kylin.metrics.MetricsManager;\\n import org.apache.kylin.metrics.property.QueryCubePropertyEnum;\\n import org.apache.kylin.rest.constant.Constant;\\n import org.apache.kylin.rest.exception.BadRequestException;\\n@@ -79,6 +80,7 @@\\n import org.apache.kylin.rest.msg.Message;\\n import org.apache.kylin.rest.msg.MsgPicker;\\n import org.apache.kylin.rest.request.MetricsRequest;\\n+import org.apache.kylin.rest.request.PrepareSqlRequest;\\n import org.apache.kylin.rest.response.CubeInstanceResponse;\\n import org.apache.kylin.rest.response.CuboidTreeResponse;\\n import org.apache.kylin.rest.response.CuboidTreeResponse.NodeInfo;\\n@@ -544,7 +546,8 @@ public HBaseResponse getHTableInfo(String cubeName, String tableName) throws IOE\\n \\n         hr = new HBaseResponse();\\n         CubeInstance cube = CubeManager.getInstance(getConfig()).getCube(cubeName);\\n-        if (cube.getStorageType() == IStorageAware.ID_HBASE || cube.getStorageType() == IStorageAware.ID_SHARDED_HBASE || cube.getStorageType() == IStorageAware.ID_REALTIME_AND_HBASE) {\\n+        if (cube.getStorageType() == IStorageAware.ID_HBASE || cube.getStorageType() == IStorageAware.ID_SHARDED_HBASE\\n+                || cube.getStorageType() == IStorageAware.ID_REALTIME_AND_HBASE) {\\n             try {\\n                 logger.debug(\"Loading HTable info \" + cubeName + \", \" + tableName);\\n \\n@@ -633,7 +636,8 @@ private void cleanSegmentStorage(List<CubeSegment> toRemoveSegs) throws IOExcept\\n             List<String> toDelHDFSPaths = Lists.newArrayListWithCapacity(toRemoveSegs.size());\\n             for (CubeSegment seg : toRemoveSegs) {\\n                 toDropHTables.add(seg.getStorageLocationIdentifier());\\n-                toDelHDFSPaths.add(JobBuilderSupport.getJobWorkingDir(seg.getConfig().getHdfsWorkingDirectory(), seg.getLastBuildJobID()));\\n+                toDelHDFSPaths.add(JobBuilderSupport.getJobWorkingDir(seg.getConfig().getHdfsWorkingDirectory(),\\n+                        seg.getLastBuildJobID()));\\n             }\\n \\n             StorageCleanUtil.dropHTables(new HBaseAdmin(HBaseConnection.getCurrentHBaseConfiguration()), toDropHTables);\\n@@ -763,10 +767,12 @@ public String mergeCubeSegment(String cubeName) {\\n     }\\n \\n     \/\/Don\\'t merge the job that has been discarded manually before\\n-    private boolean isMergingJobBeenDiscarded(CubeInstance cubeInstance, String cubeName, String projectName, SegmentRange offsets) {\\n+    private boolean isMergingJobBeenDiscarded(CubeInstance cubeInstance, String cubeName, String projectName,\\n+            SegmentRange offsets) {\\n         SegmentRange.TSRange tsRange = new SegmentRange.TSRange((Long) offsets.start.v, (Long) offsets.end.v);\\n         String segmentName = CubeSegment.makeSegmentName(tsRange, null, cubeInstance.getModel());\\n-        final List<CubingJob> jobInstanceList = jobService.listJobsByRealizationName(cubeName, projectName, EnumSet.of(ExecutableState.DISCARDED));\\n+        final List<CubingJob> jobInstanceList = jobService.listJobsByRealizationName(cubeName, projectName,\\n+                EnumSet.of(ExecutableState.DISCARDED));\\n         for (CubingJob cubingJob : jobInstanceList) {\\n             if (cubingJob.getSegmentName().equals(segmentName)) {\\n                 logger.debug(\"Merge job {} has been discarded before, will not merge.\", segmentName);\\n@@ -777,7 +783,6 @@ private boolean isMergingJobBeenDiscarded(CubeInstance cubeInstance, String cube\\n         return false;\\n     }\\n \\n-\\n     public void validateCubeDesc(CubeDesc desc, boolean isDraft) {\\n         Message msg = MsgPicker.getMsg();\\n \\n@@ -931,24 +936,6 @@ public void afterPropertiesSet() throws Exception {\\n         Broadcaster.getInstance(getConfig()).registerStaticListener(new HTableInfoSyncListener(), \"cube\");\\n     }\\n \\n-    private class HTableInfoSyncListener extends Broadcaster.Listener {\\n-        @Override\\n-        public void onClearAll(Broadcaster broadcaster) throws IOException {\\n-            htableInfoCache.invalidateAll();\\n-        }\\n-\\n-        @Override\\n-        public void onEntityChange(Broadcaster broadcaster, String entity, Broadcaster.Event event, String cacheKey)\\n-                throws IOException {\\n-            String cubeName = cacheKey;\\n-            String keyPrefix = cubeName + \"\/\";\\n-            for (String k : htableInfoCache.asMap().keySet()) {\\n-                if (k.startsWith(keyPrefix))\\n-                    htableInfoCache.invalidate(k);\\n-            }\\n-        }\\n-    }\\n-\\n     public CubeInstanceResponse createCubeInstanceResponse(CubeInstance cube) {\\n         return new CubeInstanceResponse(cube, projectService.getProjectOfCube(cube.getName()));\\n     }\\n@@ -995,7 +982,7 @@ private NodeInfo generateNodeInfo(long cuboidId, int dimensionCount, long cubeQu\\n         long queryExactlyMatchCount = queryMatchMap == null || queryMatchMap.get(cuboidId) == null ? 0L\\n                 : queryMatchMap.get(cuboidId);\\n         boolean ifExist = currentCuboidSet.contains(cuboidId);\\n-        long rowCount = rowCountMap == null ? 0L : rowCountMap.get(cuboidId);\\n+        long rowCount = (rowCountMap == null || rowCountMap.size() == 0) ? 0L : rowCountMap.get(cuboidId);\\n \\n         NodeInfo node = new NodeInfo();\\n         node.setId(cuboidId);\\n@@ -1044,9 +1031,10 @@ public Map<Long, Long> getCuboidHitFrequency(String cubeName, boolean isCuboidSo\\n         String table = getMetricsManager().getSystemTableFromSubject(getConfig().getKylinMetricsSubjectQueryCube());\\n         String sql = \"select \" + cuboidColumn + \", sum(\" + hitMeasure + \")\" \/\/\\n                 + \" from \" + table\/\/\\n-                + \" where \" + QueryCubePropertyEnum.CUBE.toString() + \" = \\'\" + cubeName + \"\\'\" \/\/\\n+                + \" where \" + QueryCubePropertyEnum.CUBE.toString() + \" = ?\" \/\/\\n                 + \" group by \" + cuboidColumn;\\n-        List<List<String>> orgHitFrequency = queryService.querySystemCube(sql).getResults();\\n+\\n+        List<List<String>> orgHitFrequency = getPrepareQueryResult(cubeName, sql);\\n         return formatQueryCount(orgHitFrequency);\\n     }\\n \\n@@ -1058,9 +1046,10 @@ public Map<Long, Map<Long, Pair<Long, Long>>> getCuboidRollingUpStats(String cub\\n         String table = getMetricsManager().getSystemTableFromSubject(getConfig().getKylinMetricsSubjectQueryCube());\\n         String sql = \"select \" + cuboidSource + \", \" + cuboidTgt + \", avg(\" + aggCount + \"), avg(\" + returnCount + \")\"\/\/\\n                 + \" from \" + table \/\/\\n-                + \" where \" + QueryCubePropertyEnum.CUBE.toString() + \" = \\'\" + cubeName + \"\\' \" \/\/\\n+                + \" where \" + QueryCubePropertyEnum.CUBE.toString() + \" = ?\" \/\/\\n                 + \" group by \" + cuboidSource + \", \" + cuboidTgt;\\n-        List<List<String>> orgRollingUpCount = queryService.querySystemCube(sql).getResults();\\n+\\n+        List<List<String>> orgRollingUpCount = getPrepareQueryResult(cubeName, sql);\\n         return formatRollingUpStats(orgRollingUpCount);\\n     }\\n \\n@@ -1070,13 +1059,27 @@ public Map<Long, Long> getCuboidQueryMatchCount(String cubeName) {\\n         String table = getMetricsManager().getSystemTableFromSubject(getConfig().getKylinMetricsSubjectQueryCube());\\n         String sql = \"select \" + cuboidSource + \", sum(\" + hitMeasure + \")\" \/\/\\n                 + \" from \" + table \/\/\\n-                + \" where \" + QueryCubePropertyEnum.CUBE.toString() + \" = \\'\" + cubeName + \"\\'\" \/\/\\n+                + \" where \" + QueryCubePropertyEnum.CUBE.toString() + \" = ?\" \/\/\\n                 + \" and \" + QueryCubePropertyEnum.IF_MATCH.toString() + \" = true\" \/\/\\n                 + \" group by \" + cuboidSource;\\n-        List<List<String>> orgMatchHitFrequency = queryService.querySystemCube(sql).getResults();\\n+\\n+        List<List<String>> orgMatchHitFrequency = getPrepareQueryResult(cubeName, sql);\\n         return formatQueryCount(orgMatchHitFrequency);\\n     }\\n \\n+    private List<List<String>> getPrepareQueryResult(String cubeName, String sql) {\\n+        PrepareSqlRequest sqlRequest = new PrepareSqlRequest();\\n+        sqlRequest.setProject(MetricsManager.SYSTEM_PROJECT);\\n+        PrepareSqlRequest.StateParam[] params = new PrepareSqlRequest.StateParam[1];\\n+        params[0] = new PrepareSqlRequest.StateParam();\\n+        params[0].setClassName(\"java.lang.String\");\\n+        params[0].setValue(cubeName);\\n+        sqlRequest.setParams(params);\\n+        sqlRequest.setSql(sql);\\n+\\n+        return queryService.doQueryWithCache(sqlRequest, false).getResults();\\n+    }\\n+\\n     @PreAuthorize(Constant.ACCESS_HAS_ROLE_ADMIN\\n             + \" or hasPermission(#cube, \\'ADMINISTRATION\\') or hasPermission(#cube, \\'MANAGEMENT\\')\")\\n     public void migrateCube(CubeInstance cube, String projectName) {\\n@@ -1114,4 +1117,22 @@ public void migrateCube(CubeInstance cube, String projectName) {\\n             throw new InternalErrorException(\"Failed to perform one-click migrating\", e);\\n         }\\n     }\\n+\\n+    private class HTableInfoSyncListener extends Broadcaster.Listener {\\n+        @Override\\n+        public void onClearAll(Broadcaster broadcaster) throws IOException {\\n+            htableInfoCache.invalidateAll();\\n+        }\\n+\\n+        @Override\\n+        public void onEntityChange(Broadcaster broadcaster, String entity, Broadcaster.Event event, String cacheKey)\\n+                throws IOException {\\n+            String cubeName = cacheKey;\\n+            String keyPrefix = cubeName + \"\/\";\\n+            for (String k : htableInfoCache.asMap().keySet()) {\\n+                if (k.startsWith(keyPrefix))\\n+                    htableInfoCache.invalidate(k);\\n+            }\\n+        }\\n+    }\\n }'}}",
            "message_norm":"fix sql injection issue",
            "language":"fr",
            "entities":"[('fix', 'ACTION', ''), ('sql injection', 'SECWORD', ''), ('issue', 'FLAW', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['server-base\/src\/main\/java\/org\/apache\/kylin\/rest\/service\/CubeService.java'])",
            "num_files":1.0
        },
        {
            "index":2205,
            "vuln_id":"GHSA-j7c4-2xj8-wm7r",
            "cwe_id":"{'CWE-20'}",
            "score":7.5,
            "chain":"{'https:\/\/github.com\/latchset\/kdcproxy\/commit\/f274aa6787cb8b3ec1cc12c440a56665b7231882'}",
            "dataset":"osv",
            "summary":"Moderate severity vulnerability that affects kdcproxy python-kdcproxy before 0.3.2 allows remote attackers to cause a denial of service via a large POST request.",
            "published_date":"2018-11-01",
            "chain_len":1,
            "project":"https:\/\/github.com\/latchset\/kdcproxy",
            "commit_href":"https:\/\/github.com\/latchset\/kdcproxy\/commit\/f274aa6787cb8b3ec1cc12c440a56665b7231882",
            "commit_sha":"f274aa6787cb8b3ec1cc12c440a56665b7231882",
            "patch":"SINGLE",
            "chain_ord":"['f274aa6787cb8b3ec1cc12c440a56665b7231882']",
            "before_first_fix_commit":"{'e4a71193099cd395578bcf32f4eb8beaa7da3e43'}",
            "last_fix_commit":"f274aa6787cb8b3ec1cc12c440a56665b7231882",
            "chain_ord_pos":1.0,
            "commit_datetime":"08\/03\/2015, 18:38:49",
            "message":"Enforce a maximum packet length\n\nPermanently fixes CVE-2015-5159 for all applications.",
            "author":"Nathaniel McCallum",
            "comments":null,
            "stats":"{'additions': 6, 'deletions': 1, 'total': 7}",
            "files":"{'kdcproxy\/__init__.py': {'additions': 6, 'deletions': 1, 'changes': 7, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/latchset\/kdcproxy\/raw\/f274aa6787cb8b3ec1cc12c440a56665b7231882\/kdcproxy%2F__init__.py', 'patch': '@@ -61,6 +61,7 @@ def __str__(self):\\n \\n \\n class Application:\\n+    MAX_LENGTH = 128 * 1024\\n     SOCKTYPES = {\\n         \"tcp\": socket.SOCK_STREAM,\\n         \"udp\": socket.SOCK_DGRAM,\\n@@ -180,7 +181,11 @@ def __call__(self, env, start_response):\\n             try:\\n                 length = int(env[\"CONTENT_LENGTH\"])\\n             except AttributeError:\\n-                length = -1\\n+                raise HTTPException(411, \"Length required.\")\\n+            if length < 0:\\n+                raise HTTPException(411, \"Length required.\")\\n+            if length > self.MAX_LENGTH:\\n+                raise HTTPException(413, \"Request entity too large.\")\\n             try:\\n                 pr = codec.decode(env[\"wsgi.input\"].read(length))\\n             except codec.ParsingError as e:'}}",
            "message_norm":"enforce a maximum packet length\n\npermanently fixes cve-2015-5159 for all applications.",
            "language":"en",
            "entities":"[('fixes', 'ACTION', ''), ('cve-2015-5159', 'VULNID', 'CVE')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['kdcproxy\/__init__.py'])",
            "num_files":1.0
        },
        {
            "index":2219,
            "vuln_id":"GHSA-j8cx-j9j2-f29w",
            "cwe_id":"{'CWE-922'}",
            "score":0.0,
            "chain":"{'https:\/\/github.com\/microweber\/microweber\/commit\/b592c86d2b927c0cae5b73b87fb541f25e777aa3'}",
            "dataset":"osv",
            "summary":"Insecure Storage of Sensitive Information in Microweber Microweber prior to version 1.3 does not strip images of EXIF data, exposing information about users' locations, device hardware, and device software.",
            "published_date":"2022-02-24",
            "chain_len":1,
            "project":"https:\/\/github.com\/microweber\/microweber",
            "commit_href":"https:\/\/github.com\/microweber\/microweber\/commit\/b592c86d2b927c0cae5b73b87fb541f25e777aa3",
            "commit_sha":"b592c86d2b927c0cae5b73b87fb541f25e777aa3",
            "patch":"SINGLE",
            "chain_ord":"['b592c86d2b927c0cae5b73b87fb541f25e777aa3']",
            "before_first_fix_commit":"{'bfb86241bbb8cffe8291822091c6411498ac2a3e'}",
            "last_fix_commit":"b592c86d2b927c0cae5b73b87fb541f25e777aa3",
            "chain_ord_pos":1.0,
            "commit_datetime":"02\/22\/2022, 11:07:12",
            "message":"Update plupload.php",
            "author":"Bozhidar Slaveykov",
            "comments":null,
            "stats":"{'additions': 3, 'deletions': 3, 'total': 6}",
            "files":"{'src\/MicroweberPackages\/App\/functions\/plupload.php': {'additions': 3, 'deletions': 3, 'changes': 6, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/microweber\/microweber\/raw\/b592c86d2b927c0cae5b73b87fb541f25e777aa3\/src%2FMicroweberPackages%2FApp%2Ffunctions%2Fplupload.php', 'patch': \"@@ -522,22 +522,22 @@\\n                 \/\/ This will clear exif data - security issue\\n                 $imgCreatedFromJpeg = @imagecreatefromjpeg($filePath);\\n                 if ($imgCreatedFromJpeg) {\\n-                    imagejpeg($imgCreatedFromJpeg, $filePath,100);\\n+                    imagejpeg($imgCreatedFromJpeg, $filePath,100);  \/\/ this will create fresh new image without exif sensitive data\\n                     $valid = true;\\n                 }\\n             } else if ($ext === 'png') {\\n \\n                 $imgCreatedFromPng = @imagecreatefrompng($filePath);\\n                 if ($imgCreatedFromPng) {\\n-                    imagepng($imgCreatedFromPng, $filePath,100);\\n+                    imagepng($imgCreatedFromPng, $filePath,100);  \/\/ this will create fresh new image without exif sensitive data\\n                     $valid = true;\\n                 }\\n \\n             } else if ($ext === 'gif') {\\n \\n                 $imgCreatedFromGif = @imagecreatefromgif($filePath);\\n                 if ($imgCreatedFromGif) {\\n-                    imagegif($imgCreatedFromGif, $filePath,100);\\n+                    imagegif($imgCreatedFromGif, $filePath,100); \/\/ this will create fresh new image without exif sensitive data\\n                     $valid = true;\\n                 }\"}}",
            "message_norm":"update plupload.php",
            "language":"ro",
            "entities":"[('update', 'ACTION', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['src\/MicroweberPackages\/App\/functions\/plupload.php'])",
            "num_files":1.0
        },
        {
            "index":616,
            "vuln_id":"GHSA-5f2r-qp73-37mr",
            "cwe_id":"{'CWE-617'}",
            "score":6.5,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/92dba16749fae36c246bec3f9ba474d9ddeb7662'}",
            "dataset":"osv",
            "summary":"`CHECK`-failures during Grappler's `SafeToRemoveIdentity` in Tensorflow ### Impact\nThe Grappler optimizer in TensorFlow can be used to cause a denial of service by altering a `SavedModel` such that [`SafeToRemoveIdentity`](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/a1320ec1eac186da1d03f033109191f715b2b130\/tensorflow\/core\/grappler\/optimizers\/dependency_optimizer.cc#L59-L98) would trigger `CHECK` failures.\n\n### Patches\nWe have patched the issue in GitHub commit [92dba16749fae36c246bec3f9ba474d9ddeb7662](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/92dba16749fae36c246bec3f9ba474d9ddeb7662).\nThe fix will be included in TensorFlow 2.8.0. We will also cherrypick this commit on TensorFlow 2.7.1, TensorFlow 2.6.3, and TensorFlow 2.5.3, as these are also affected and still in supported range.\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.",
            "published_date":"2022-02-10",
            "chain_len":1,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/92dba16749fae36c246bec3f9ba474d9ddeb7662",
            "commit_sha":"92dba16749fae36c246bec3f9ba474d9ddeb7662",
            "patch":"SINGLE",
            "chain_ord":"['92dba16749fae36c246bec3f9ba474d9ddeb7662']",
            "before_first_fix_commit":"{'1cda4d4a26acea3814d06e7d9525772ab357fc1c'}",
            "last_fix_commit":"92dba16749fae36c246bec3f9ba474d9ddeb7662",
            "chain_ord_pos":1.0,
            "commit_datetime":"11\/11\/2021, 18:43:29",
            "message":"Prevent a null-pointer dereference \/ `CHECK`-fail in grappler.\n\nPiperOrigin-RevId: 409187354\nChange-Id: I369c249cca32e6c56ec193f0ebbf2f2768fc7d43",
            "author":"Mihai Maruseac",
            "comments":null,
            "stats":"{'additions': 4, 'deletions': 2, 'total': 6}",
            "files":"{'tensorflow\/core\/grappler\/optimizers\/dependency_optimizer.cc': {'additions': 4, 'deletions': 2, 'changes': 6, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/92dba16749fae36c246bec3f9ba474d9ddeb7662\/tensorflow%2Fcore%2Fgrappler%2Foptimizers%2Fdependency_optimizer.cc', 'patch': '@@ -75,8 +75,10 @@ bool DependencyOptimizer::SafeToRemoveIdentity(const NodeDef& node) const {\\n   }\\n \\n   const NodeDef* input = node_map_->GetNode(NodeName(node.input(0)));\\n-  CHECK(input != nullptr) << \"node = \" << node.name()\\n-                          << \" input = \" << node.input(0);\\n+  if (input == nullptr) {\\n+    VLOG(1) << \"node = \" << node.name() << \" input = \" << node.input(0);\\n+    return false;\\n+  }\\n   \/\/ Don\\'t remove Identity nodes corresponding to Variable reads or following\\n   \/\/ Recv.\\n   if (IsVariable(*input) || IsRecv(*input)) {'}}",
            "message_norm":"prevent a null-pointer dereference \/ `check`-fail in grappler.\n\npiperorigin-revid: 409187354\nchange-id: i369c249cca32e6c56ec193f0ebbf2f2768fc7d43",
            "language":"en",
            "entities":"[('prevent', 'ACTION', ''), ('null-pointer dereference', 'SECWORD', ''), ('409187354', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/core\/grappler\/optimizers\/dependency_optimizer.cc'])",
            "num_files":1.0
        },
        {
            "index":2348,
            "vuln_id":"GHSA-m4hj-wg2r-qpcr",
            "cwe_id":"{'CWE-352'}",
            "score":4.3,
            "chain":"{'https:\/\/github.com\/star7th\/showdoc\/commit\/67093c879a6563aa6ee08003177777d1975e2351'}",
            "dataset":"osv",
            "summary":"showdoc is vulnerable to Cross-Site Request Forgery (CSRF) showdoc is vulnerable to Cross-Site Request Forgery (CSRF).",
            "published_date":"2021-11-15",
            "chain_len":1,
            "project":"https:\/\/github.com\/star7th\/showdoc",
            "commit_href":"https:\/\/github.com\/star7th\/showdoc\/commit\/67093c879a6563aa6ee08003177777d1975e2351",
            "commit_sha":"67093c879a6563aa6ee08003177777d1975e2351",
            "patch":"SINGLE",
            "chain_ord":"['67093c879a6563aa6ee08003177777d1975e2351']",
            "before_first_fix_commit":"{'7e6b547ac23be296f0a6066382eb1fd389af0439'}",
            "last_fix_commit":"67093c879a6563aa6ee08003177777d1975e2351",
            "chain_ord_pos":1.0,
            "commit_datetime":"10\/19\/2021, 14:19:10",
            "message":"Strict cookie",
            "author":"star7th",
            "comments":null,
            "stats":"{'additions': 16, 'deletions': 5, 'total': 21}",
            "files":"{'server\/Application\/Api\/Controller\/UserController.class.php': {'additions': 16, 'deletions': 5, 'changes': 21, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/star7th\/showdoc\/raw\/67093c879a6563aa6ee08003177777d1975e2351\/server%2FApplication%2FApi%2FController%2FUserController.class.php', 'patch': '@@ -42,8 +42,12 @@ public function register(){\\n                     unset($ret[\\'password\\']);\\n                     session(\"login_user\" , $ret );\\n                     $token = D(\"UserToken\")->createToken($ret[\\'uid\\']);\\n-                    cookie(\\'cookie_token\\',$token,array(\\'expire\\'=>60*60*24*90,\\'httponly\\'=>\\'httponly\\'));\/\/\u6b64\u5904\u7531\u670d\u52a1\u7aef\u63a7\u5236token\u662f\u5426\u8fc7\u671f\uff0c\u6240\u4ee5cookies\u8fc7\u671f\u65f6\u95f4\u8bbe\u7f6e\u591a\u4e45\u90fd\u65e0\u6240\u8c13\\n-                  $this->sendResult(array(\\n+                    if(version_compare(PHP_VERSION,\\'7.3.0\\',\\'>\\')){\\n+                        setcookie(\\'cookie_token\\',$token,array(\\'expires\\'=>time()+60*60*24*180,\\'httponly\\'=>\\'httponly\\',\\'samesite\\' => \\'Strict\\',\\'path\\'=>\\'\/\\'));\\n+                      }else{\\n+                        cookie(\\'cookie_token\\',$token,array(\\'expire\\'=>60*60*24*180,\\'httponly\\'=>\\'httponly\\'));\\n+                    }\\n+                    $this->sendResult(array(\\n                     \"uid\" => $ret[\\'uid\\'] ,\\n                     \"username\" => $ret[\\'username\\'] ,\\n                     \"name\" => $ret[\\'name\\'] ,\\n@@ -134,7 +138,11 @@ public function login(){\\n           session(\"login_user\" , $ret );\\n           D(\"User\")->setLastTime($ret[\\'uid\\']);\\n           $token = D(\"UserToken\")->createToken($ret[\\'uid\\'],60*60*24*180);\\n-          cookie(\\'cookie_token\\',$token,array(\\'expire\\'=>60*60*24*180,\\'httponly\\'=>\\'httponly\\'));\/\/\u6b64\u5904\u7531\u670d\u52a1\u7aef\u63a7\u5236token\u662f\u5426\u8fc7\u671f\uff0c\u6240\u4ee5cookies\u8fc7\u671f\u65f6\u95f4\u8bbe\u7f6e\u591a\u4e45\u90fd\u65e0\u6240\u8c13\\n+          if(version_compare(PHP_VERSION,\\'7.3.0\\',\\'>\\')){\\n+            setcookie(\\'cookie_token\\',$token,array(\\'expires\\'=>time()+60*60*24*180,\\'httponly\\'=>\\'httponly\\',\\'samesite\\' => \\'Strict\\',\\'path\\'=>\\'\/\\'));\\n+          }else{\\n+            cookie(\\'cookie_token\\',$token,array(\\'expire\\'=>60*60*24*180,\\'httponly\\'=>\\'httponly\\'));\\n+          }\\n           $this->sendResult(array(\\n             \"uid\" => $ret[\\'uid\\'] ,\\n             \"username\" => $ret[\\'username\\'] ,\\n@@ -247,8 +255,11 @@ public function registerByVerify(){\\n                     unset($ret[\\'password\\']);\\n                     session(\"login_user\" , $ret );\\n                     $token = D(\"UserToken\")->createToken($ret[\\'uid\\']);\\n-                    cookie(\\'cookie_token\\',$token,array(\\'expire\\'=>60*60*24*90,\\'httponly\\'=>\\'httponly\\'));\/\/\u6b64\u5904\u7531\u670d\u52a1\u7aef\u63a7\u5236token\u662f\u5426\u8fc7\u671f\uff0c\u6240\u4ee5cookies\u8fc7\u671f\u65f6\u95f4\u8bbe\u7f6e\u591a\u4e45\u90fd\u65e0\u6240\u8c13\\n-                    \\n+                    if(version_compare(PHP_VERSION,\\'7.3.0\\',\\'>\\')){\\n+                        setcookie(\\'cookie_token\\',$token,array(\\'expires\\'=>time()+60*60*24*180,\\'httponly\\'=>\\'httponly\\',\\'samesite\\' => \\'Strict\\',\\'path\\'=>\\'\/\\'));\\n+                      }else{\\n+                        cookie(\\'cookie_token\\',$token,array(\\'expire\\'=>60*60*24*180,\\'httponly\\'=>\\'httponly\\'));\\n+                    }                    \\n                     $this->sendResult(array(\\n                         \"uid\" => $ret[\\'uid\\'] ,\\n                         \"username\" => $ret[\\'username\\'] ,'}}",
            "message_norm":"strict cookie",
            "language":"en",
            "entities":"[('cookie', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['server\/Application\/Api\/Controller\/UserController.class.php'])",
            "num_files":1.0
        },
        {
            "index":716,
            "vuln_id":"GHSA-5x33-h32w-6vr2",
            "cwe_id":"{'CWE-79'}",
            "score":6.1,
            "chain":"{'https:\/\/github.com\/moodle\/moodle\/commit\/e8632a4ad0b4da3763cbbe5949594aa449b483bb'}",
            "dataset":"osv",
            "summary":"Cross site-scripting (XSS) moodle The filter in the tag manager required extra sanitizing to prevent a reflected XSS risk. This affects 3.9 to 3.9.1, 3.8 to 3.8.4, 3.7 to 3.7.7, 3.5 to 3.5.13 and earlier unsupported versions. Fixed in 3.9.2, 3.8.5, 3.7.8 and 3.5.14.",
            "published_date":"2021-03-29",
            "chain_len":1,
            "project":"https:\/\/github.com\/moodle\/moodle",
            "commit_href":"https:\/\/github.com\/moodle\/moodle\/commit\/e8632a4ad0b4da3763cbbe5949594aa449b483bb",
            "commit_sha":"e8632a4ad0b4da3763cbbe5949594aa449b483bb",
            "patch":"SINGLE",
            "chain_ord":"['e8632a4ad0b4da3763cbbe5949594aa449b483bb']",
            "before_first_fix_commit":"{'630078eb4a189a17378ea6cf19be989da2114c1c'}",
            "last_fix_commit":"e8632a4ad0b4da3763cbbe5949594aa449b483bb",
            "chain_ord_pos":1.0,
            "commit_datetime":"08\/04\/2020, 10:04:27",
            "message":"MDL-69340 tag: Correct the filter input HTML in the tag manager",
            "author":"Michael Hawkins",
            "comments":null,
            "stats":"{'additions': 1, 'deletions': 1, 'total': 2}",
            "files":"{'tag\/manage.php': {'additions': 1, 'deletions': 1, 'changes': 2, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/moodle\/moodle\/raw\/e8632a4ad0b4da3763cbbe5949594aa449b483bb\/tag%2Fmanage.php', 'patch': '@@ -211,7 +211,7 @@\\n print(\\'<div class=\"tag-management-form generalbox\"><label class=\"accesshide\" for=\"id_tagfilter\">\\'. get_string(\\'search\\') .\\'<\/label>\\'.\\n     \\'<input type=\"hidden\" name=\"tc\" value=\"\\'.$tagcollid.\\'\" \/>\\'.\\n     \\'<input type=\"hidden\" name=\"perpage\" value=\"\\'.$perpage.\\'\" \/>\\'.\\n-    \\'<input id=\"id_tagfilter\" name=\"filter\" type=\"text\" value=\\' . s($filter) . \\'>\\'.\\n+    \\'<input id=\"id_tagfilter\" name=\"filter\" type=\"text\" value=\"\\' . s($filter) . \\'\">\\'.\\n     \\'<input value=\"\\'. s(get_string(\\'search\\')) .\\'\" type=\"submit\" class=\"btn btn-secondary\"> \\'.\\n     ($filter !== \\'\\' ? html_writer::link(new moodle_url($PAGE->url, array(\\'filter\\' => null)),\\n         get_string(\\'resetfilter\\', \\'tag\\'), array(\\'class\\' => \\'resetfilterlink\\')) : \\'\\').'}}",
            "message_norm":"mdl-69340 tag: correct the filter input html in the tag manager",
            "language":"en",
            "entities":null,
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tag\/manage.php'])",
            "num_files":1.0
        },
        {
            "index":3147,
            "vuln_id":"GHSA-vmjw-c2vp-p33c",
            "cwe_id":"{'CWE-681'}",
            "score":5.5,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/b5cdbf12ffcaaffecf98f22a6be5a64bb96e4f58', 'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/3a7362750d5c372420aa8f0caf7bf5b5c3d0f52d'}",
            "dataset":"osv",
            "summary":"Crash in NMS ops caused by integer conversion to unsigned ### Impact\nAn attacker can cause denial of service in applications serving models using `tf.raw_ops.NonMaxSuppressionV5` by triggering a division by 0:\n\n```python\nimport tensorflow as tf\n\ntf.raw_ops.NonMaxSuppressionV5(\n  boxes=[[0.1,0.1,0.1,0.1],[0.2,0.2,0.2,0.2],[0.3,0.3,0.3,0.3]],\n  scores=[1.0,2.0,3.0],\n  max_output_size=-1,\n  iou_threshold=0.5,\n  score_threshold=0.5,\n  soft_nms_sigma=1.0,\n  pad_to_max_output_size=True)\n```\n  \nThe [implementation](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/460e000de3a83278fb00b61a16d161b1964f15f4\/tensorflow\/core\/kernels\/image\/non_max_suppression_op.cc#L170-L271) uses a user controlled argument to resize a `std::vector`:\n\n```cc\n  const int output_size = max_output_size.scalar<int>()();\n  \/\/ ...\n  std::vector<int> selected;\n  \/\/ ...\n  if (pad_to_max_output_size) {\n    selected.resize(output_size, 0);\n    \/\/ ...\n  }\n```\n    \nHowever, as `std::vector::resize` takes the size argument as a `size_t` and `output_size` is an `int`, there is an implicit conversion to usigned. If the attacker supplies a negative value, this conversion results in a crash.\n\nA similar issue occurs in `CombinedNonMaxSuppression`:\n\n```python\nimport tensorflow as tf\n\ntf.raw_ops.NonMaxSuppressionV5(\n  boxes=[[[[0.1,0.1,0.1,0.1],[0.2,0.2,0.2,0.2],[0.3,0.3,0.3,0.3]],[[0.1,0.1,0.1,0.1],[0.2,0.2,0.2,0.2],[0.3,0.3,0.3,0.3]],[[0.1,0.1,0.1,0.1],[0.2,0.2,0.2,0.2],[0.3,0.3,0.3,0.3]]]],\n  scores=[[[1.0,2.0,3.0],[1.0,2.0,3.0],[1.0,2.0,3.0]]],\n  max_output_size_per_class=-1,\n  max_total_size=10,\n  iou_threshold=score_threshold=0.5,\n  pad_per_class=True,\n  clip_boxes=True)\n```\n  \n### Patches\nWe have patched the issue in GitHub commit [3a7362750d5c372420aa8f0caf7bf5b5c3d0f52d](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/3a7362750d5c372420aa8f0caf7bf5b5c3d0f52d) and commit [b5cdbf12ffcaaffecf98f22a6be5a64bb96e4f58](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/b5cdbf12ffcaaffecf98f22a6be5a64bb96e4f58).\n\nThe fix will be included in TensorFlow 2.6.0. We will also cherrypick this commit on TensorFlow 2.5.1, TensorFlow 2.4.3, and TensorFlow 2.3.4, as these are also affected and still in supported range.\n\n### For more information \nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by members of the Aivul Team from Qihoo 360.",
            "published_date":"2021-08-25",
            "chain_len":2,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/3a7362750d5c372420aa8f0caf7bf5b5c3d0f52d",
            "commit_sha":"3a7362750d5c372420aa8f0caf7bf5b5c3d0f52d",
            "patch":"MULTI",
            "chain_ord":"['b5cdbf12ffcaaffecf98f22a6be5a64bb96e4f58', '3a7362750d5c372420aa8f0caf7bf5b5c3d0f52d']",
            "before_first_fix_commit":"{'a87fa31dc3becc97c7e945b9b8c8711acb92fc12'}",
            "last_fix_commit":"3a7362750d5c372420aa8f0caf7bf5b5c3d0f52d",
            "chain_ord_pos":2.0,
            "commit_datetime":"07\/31\/2021, 05:02:22",
            "message":"Prevent crash\/heap OOB due to integer conversion to unsigned in NMS kernels\n\nPiperOrigin-RevId: 387938262\nChange-Id: Id361a715307e7179977cf5c64391c199a966f2ad",
            "author":"Mihai Maruseac",
            "comments":null,
            "stats":"{'additions': 8, 'deletions': 0, 'total': 8}",
            "files":"{'tensorflow\/core\/kernels\/image\/non_max_suppression_op.cc': {'additions': 8, 'deletions': 0, 'changes': 8, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/3a7362750d5c372420aa8f0caf7bf5b5c3d0f52d\/tensorflow%2Fcore%2Fkernels%2Fimage%2Fnon_max_suppression_op.cc', 'patch': '@@ -169,6 +169,8 @@ void DoNonMaxSuppressionOp(OpKernelContext* context, const Tensor& scores,\\n                            bool pad_to_max_output_size = false,\\n                            int* ptr_num_valid_outputs = nullptr) {\\n   const int output_size = max_output_size.scalar<int>()();\\n+  OP_REQUIRES(context, output_size >= 0,\\n+              errors::InvalidArgument(\"output size must be non-negative\"));\\n \\n   std::vector<T> scores_data(num_boxes);\\n   std::copy_n(scores.flat<T>().data(), num_boxes, scores_data.begin());\\n@@ -768,6 +770,9 @@ class NonMaxSuppressionV4Op : public OpKernel {\\n         context, scores, num_boxes, max_output_size, iou_threshold_val,\\n         score_threshold_val, dummy_soft_nms_sigma, similarity_fn,\\n         return_scores_tensor_, pad_to_max_output_size_, &num_valid_outputs);\\n+    if (!context->status().ok()) {\\n+      return;\\n+    }\\n \\n     \/\/ Allocate scalar output tensor for number of indices computed.\\n     Tensor* num_outputs_t = nullptr;\\n@@ -845,6 +850,9 @@ class NonMaxSuppressionV5Op : public OpKernel {\\n         context, scores, num_boxes, max_output_size, iou_threshold_val,\\n         score_threshold_val, soft_nms_sigma_val, similarity_fn,\\n         return_scores_tensor_, pad_to_max_output_size_, &num_valid_outputs);\\n+    if (!context->status().ok()) {\\n+      return;\\n+    }\\n \\n     \/\/ Allocate scalar output tensor for number of indices computed.\\n     Tensor* num_outputs_t = nullptr;'}}",
            "message_norm":"prevent crash\/heap oob due to integer conversion to unsigned in nms kernels\n\npiperorigin-revid: 387938262\nchange-id: id361a715307e7179977cf5c64391c199a966f2ad",
            "language":"en",
            "entities":"[('prevent', 'ACTION', ''), ('heap oob', 'SECWORD', ''), ('387938262', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/core\/kernels\/image\/non_max_suppression_op.cc'])",
            "num_files":1.0
        },
        {
            "index":3426,
            "vuln_id":"GHSA-xg72-6c83-ghh4",
            "cwe_id":"{'CWE-79'}",
            "score":4.8,
            "chain":"{'https:\/\/github.com\/microweber\/microweber\/commit\/d35e691e72d358430abc8e99f5ba9eb374423b9f'}",
            "dataset":"osv",
            "summary":"Microweber Stored Cross-site Scripting before v1.2.20 Microwerber prior to version 1.2.20 is vulnerable to stored Cross-site Scripting (XSS).",
            "published_date":"2022-07-23",
            "chain_len":1,
            "project":"https:\/\/github.com\/microweber\/microweber",
            "commit_href":"https:\/\/github.com\/microweber\/microweber\/commit\/d35e691e72d358430abc8e99f5ba9eb374423b9f",
            "commit_sha":"d35e691e72d358430abc8e99f5ba9eb374423b9f",
            "patch":"SINGLE",
            "chain_ord":"['d35e691e72d358430abc8e99f5ba9eb374423b9f']",
            "before_first_fix_commit":"{'b39736f1191589e89eb4e54f5f6f05b6349626e3'}",
            "last_fix_commit":"d35e691e72d358430abc8e99f5ba9eb374423b9f",
            "chain_ord_pos":1.0,
            "commit_datetime":"07\/08\/2022, 13:41:01",
            "message":"update",
            "author":"Peter Ivanov",
            "comments":null,
            "stats":"{'additions': 12, 'deletions': 4, 'total': 16}",
            "files":"{'src\/MicroweberPackages\/App\/functions\/plupload.php': {'additions': 12, 'deletions': 4, 'changes': 16, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/microweber\/microweber\/raw\/d35e691e72d358430abc8e99f5ba9eb374423b9f\/src%2FMicroweberPackages%2FApp%2Ffunctions%2Fplupload.php', 'patch': \"@@ -563,17 +563,25 @@\\n                 }\\n \\n             } else if ($ext === 'svg') {\\n-\\n+                $valid = false;\\n                 if (is_file($filePath)) {\\n                     $sanitizer = new \\\\enshrined\\\\svgSanitize\\\\Sanitizer();\\n                     \/\/ Load the dirty svg\\n                     $dirtySVG = file_get_contents($filePath);\\n                      \/\/ Pass it to the sanitizer and get it back clean\\n-                    $cleanSVG = $sanitizer->sanitize($dirtySVG);\\n-                    file_put_contents($filePath, $cleanSVG);\\n+                    try {\\n+                        $cleanSVG = $sanitizer->sanitize($dirtySVG);\\n+                        $valid = true;\\n+                    } catch (\\\\Exception $e) {\\n+                        $valid = false;\\n+                    }\\n+\\n+                    if ($valid) {\\n+                        file_put_contents($filePath, $cleanSVG);\\n+                    }\\n \\n                 }\\n-               $valid = true;\\n+\\n \\n             } else {\\n                 $valid = false;\"}}",
            "message_norm":"update",
            "language":"ro",
            "entities":"[('update', 'ACTION', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['src\/MicroweberPackages\/App\/functions\/plupload.php'])",
            "num_files":1.0
        },
        {
            "index":1491,
            "vuln_id":"GHSA-c5hf-mc85-2hx4",
            "cwe_id":"{'CWE-863'}",
            "score":4.3,
            "chain":"{'https:\/\/github.com\/moodle\/moodle\/commit\/cdc78a16a5da95a17fb10bf1c66689237f5a3f7d'}",
            "dataset":"osv",
            "summary":"Missing authorization in Moodle Users with the capability to configure badge criteria (teachers and managers by default) were able to configure course badges with profile field criteria, which should only be available for site badges.",
            "published_date":"2022-04-30",
            "chain_len":1,
            "project":"https:\/\/github.com\/moodle\/moodle",
            "commit_href":"https:\/\/github.com\/moodle\/moodle\/commit\/cdc78a16a5da95a17fb10bf1c66689237f5a3f7d",
            "commit_sha":"cdc78a16a5da95a17fb10bf1c66689237f5a3f7d",
            "patch":"SINGLE",
            "chain_ord":"['cdc78a16a5da95a17fb10bf1c66689237f5a3f7d']",
            "before_first_fix_commit":"{'4c00836de97bea26a0c5ba6068a55a8c1b16f260'}",
            "last_fix_commit":"cdc78a16a5da95a17fb10bf1c66689237f5a3f7d",
            "chain_ord_pos":1.0,
            "commit_datetime":"03\/07\/2022, 12:39:16",
            "message":"MDL-74075 core_badges: Check accepted criterias",
            "author":"Amaia Anabitarte",
            "comments":null,
            "stats":"{'additions': 6, 'deletions': 0, 'total': 6}",
            "files":"{'badges\/criteria_settings.php': {'additions': 6, 'deletions': 0, 'changes': 6, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/moodle\/moodle\/raw\/cdc78a16a5da95a17fb10bf1c66689237f5a3f7d\/badges%2Fcriteria_settings.php', 'patch': \"@@ -55,6 +55,12 @@\\n     redirect($return);\\n }\\n \\n+\/\/ Make sure the criteria type is accepted.\\n+$accepted = $badge->get_accepted_criteria();\\n+if (!in_array($type, $accepted)) {\\n+    redirect($return);\\n+}\\n+\\n if ($badge->type == BADGE_TYPE_COURSE) {\\n     require_login($badge->courseid);\\n     $navurl = new moodle_url('\/badges\/index.php', array('type' => $badge->type, 'id' => $badge->courseid));\"}}",
            "message_norm":"mdl-74075 core_badges: check accepted criterias",
            "language":"en",
            "entities":null,
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['badges\/criteria_settings.php'])",
            "num_files":1.0
        },
        {
            "index":82,
            "vuln_id":"GHSA-2cpx-427x-q2c6",
            "cwe_id":"{'CWE-190'}",
            "score":2.5,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/69c68ecbb24dff3fa0e46da0d16c821a2dd22d7c'}",
            "dataset":"osv",
            "summary":"CHECK-fail in AddManySparseToTensorsMap ### Impact\nAn attacker can trigger a denial of service via a `CHECK`-fail in  `tf.raw_ops.AddManySparseToTensorsMap`:\n\n```python\nimport tensorflow as tf\nimport numpy as np\n\nsparse_indices = tf.constant(530, shape=[1, 1], dtype=tf.int64)\nsparse_values = tf.ones([1], dtype=tf.int64)\n\nshape = tf.Variable(tf.ones([55], dtype=tf.int64))\nshape[:8].assign(np.array([855, 901, 429, 892, 892, 852, 93, 96], dtype=np.int64))\n\ntf.raw_ops.AddManySparseToTensorsMap(sparse_indices=sparse_indices,\n                    sparse_values=sparse_values,\n                    sparse_shape=shape)\n```\n\nThis is because the [implementation](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/6f9896890c4c703ae0a0845394086e2e1e523299\/tensorflow\/core\/kernels\/sparse_tensors_map_ops.cc#L257) takes the values specified in `sparse_shape` as dimensions for the output shape: \n\n```cc\n    TensorShape tensor_input_shape(input_shape->vec<int64>());\n```\n\nThe [`TensorShape` constructor](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/6f9896890c4c703ae0a0845394086e2e1e523299\/tensorflow\/core\/framework\/tensor_shape.cc#L183-L188) uses a `CHECK` operation which triggers when [`InitDims`](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/6f9896890c4c703ae0a0845394086e2e1e523299\/tensorflow\/core\/framework\/tensor_shape.cc#L212-L296) returns a non-OK status.\n  \n```cc\ntemplate <class Shape>\nTensorShapeBase<Shape>::TensorShapeBase(gtl::ArraySlice<int64> dim_sizes) {\n  set_tag(REP16);\n  set_data_type(DT_INVALID);\n  TF_CHECK_OK(InitDims(dim_sizes));\n}\n```\n\nIn our scenario, this occurs when adding a dimension from the argument results in overflow:\n\n```cc\ntemplate <class Shape>\nStatus TensorShapeBase<Shape>::InitDims(gtl::ArraySlice<int64> dim_sizes) {\n  ...\n  Status status = Status::OK();\n  for (int64 s : dim_sizes) {\n    status.Update(AddDimWithStatus(internal::SubtleMustCopy(s)));\n    if (!status.ok()) {\n      return status;\n    }\n  }\n}\n\ntemplate <class Shape>\nStatus TensorShapeBase<Shape>::AddDimWithStatus(int64 size) {\n  ...\n  int64 new_num_elements;\n  if (kIsPartial && (num_elements() < 0 || size < 0)) {\n    new_num_elements = -1;\n  } else {\n    new_num_elements = MultiplyWithoutOverflow(num_elements(), size);\n    if (TF_PREDICT_FALSE(new_num_elements < 0)) {\n        return errors::Internal(\"Encountered overflow when multiplying \",\n                                num_elements(), \" with \", size,\n                                \", result: \", new_num_elements);\n      }\n  }\n  ...\n}\n```\n\nThis is a legacy implementation of the constructor and operations should use `BuildTensorShapeBase` or `AddDimWithStatus` to prevent `CHECK`-failures in the presence of overflows.\n\n### Patches\nWe have patched the issue in GitHub commit [69c68ecbb24dff3fa0e46da0d16c821a2dd22d7c](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/69c68ecbb24dff3fa0e46da0d16c821a2dd22d7c).\n\nThe fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by Yakun Zhang and Ying Wang of Baidu X-Team.",
            "published_date":"2021-05-21",
            "chain_len":1,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/69c68ecbb24dff3fa0e46da0d16c821a2dd22d7c",
            "commit_sha":"69c68ecbb24dff3fa0e46da0d16c821a2dd22d7c",
            "patch":"SINGLE",
            "chain_ord":"['69c68ecbb24dff3fa0e46da0d16c821a2dd22d7c']",
            "before_first_fix_commit":"{'6f9896890c4c703ae0a0845394086e2e1e523299'}",
            "last_fix_commit":"69c68ecbb24dff3fa0e46da0d16c821a2dd22d7c",
            "chain_ord_pos":1.0,
            "commit_datetime":"04\/20\/2021, 19:14:41",
            "message":"Fix overflow CHECK issue with `tf.raw_ops.AddManySparseToTensorsMap`.\n\nPiperOrigin-RevId: 369492969\nChange-Id: I1d70d6c0c92e3d7a25bc3b3aa2a0c0ac9688bf81",
            "author":"Amit Patankar",
            "comments":null,
            "stats":"{'additions': 19, 'deletions': 7, 'total': 26}",
            "files":"{'tensorflow\/core\/kernels\/sparse_tensors_map_ops.cc': {'additions': 19, 'deletions': 7, 'changes': 26, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/69c68ecbb24dff3fa0e46da0d16c821a2dd22d7c\/tensorflow%2Fcore%2Fkernels%2Fsparse_tensors_map_ops.cc', 'patch': '@@ -21,16 +21,14 @@ limitations under the License.\\n #include <utility>\\n #include <vector>\\n \\n-#include \"tensorflow\/core\/framework\/op_kernel.h\"\\n-#include \"tensorflow\/core\/framework\/register_types.h\"\\n-\\n #include \"tensorflow\/core\/framework\/op_kernel.h\"\\n #include \"tensorflow\/core\/framework\/register_types.h\"\\n #include \"tensorflow\/core\/framework\/resource_mgr.h\"\\n #include \"tensorflow\/core\/framework\/tensor.h\"\\n #include \"tensorflow\/core\/framework\/tensor_util.h\"\\n #include \"tensorflow\/core\/framework\/types.h\"\\n #include \"tensorflow\/core\/lib\/gtl\/inlined_vector.h\"\\n+#include \"tensorflow\/core\/util\/overflow.h\"\\n #include \"tensorflow\/core\/util\/sparse\/sparse_tensor.h\"\\n \\n namespace tensorflow {\\n@@ -254,16 +252,30 @@ class AddManySparseToTensorsMapOp : public SparseTensorAccessingOp {\\n         errors::InvalidArgument(\\n             \"Rank of input SparseTensor should be > 1, but saw rank: \", rank));\\n \\n-    TensorShape tensor_input_shape(input_shape->vec<int64>());\\n+    auto input_shape_vec = input_shape->vec<int64>();\\n+    int new_num_elements = 1;\\n+    bool overflow_ocurred = false;\\n+    for (int i = 0; i < input_shape_vec.size(); i++) {\\n+      new_num_elements =\\n+          MultiplyWithoutOverflow(new_num_elements, input_shape_vec(i));\\n+      if (new_num_elements < 0) {\\n+        overflow_ocurred = true;\\n+      }\\n+    }\\n+\\n+    OP_REQUIRES(\\n+        context, !overflow_ocurred,\\n+        errors::Internal(\"Encountered overflow from large input shape.\"));\\n+\\n+    TensorShape tensor_input_shape(input_shape_vec);\\n     gtl::InlinedVector<int64, 8> std_order(rank);\\n     std::iota(std_order.begin(), std_order.end(), 0);\\n     SparseTensor input_st;\\n     OP_REQUIRES_OK(context, SparseTensor::Create(*input_indices, *input_values,\\n                                                  tensor_input_shape, std_order,\\n                                                  &input_st));\\n \\n-    auto input_shape_t = input_shape->vec<int64>();\\n-    const int64 N = input_shape_t(0);\\n+    const int64 N = input_shape_vec(0);\\n \\n     Tensor sparse_handles(DT_INT64, TensorShape({N}));\\n     auto sparse_handles_t = sparse_handles.vec<int64>();\\n@@ -274,7 +286,7 @@ class AddManySparseToTensorsMapOp : public SparseTensorAccessingOp {\\n     \/\/ minibatch entries.\\n     TensorShape output_shape;\\n     OP_REQUIRES_OK(context, TensorShapeUtils::MakeShape(\\n-                                input_shape_t.data() + 1,\\n+                                input_shape_vec.data() + 1,\\n                                 input_shape->NumElements() - 1, &output_shape));\\n \\n     \/\/ Get groups by minibatch dimension'}}",
            "message_norm":"fix overflow check issue with `tf.raw_ops.addmanysparsetotensorsmap`.\n\npiperorigin-revid: 369492969\nchange-id: i1d70d6c0c92e3d7a25bc3b3aa2a0c0ac9688bf81",
            "language":"en",
            "entities":"[('fix', 'ACTION', ''), ('overflow', 'SECWORD', ''), ('issue', 'FLAW', ''), ('369492969', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/core\/kernels\/sparse_tensors_map_ops.cc'])",
            "num_files":1.0
        },
        {
            "index":1954,
            "vuln_id":"GHSA-gxg6-rc6c-v673",
            "cwe_id":"{'CWE-20'}",
            "score":8.1,
            "chain":"{'https:\/\/github.com\/beanshell\/beanshell\/commit\/7c68fde2d6fc65e362f20863d868c112a90a9b49', 'https:\/\/github.com\/beanshell\/beanshell\/commit\/1ccc66bb693d4e46a34a904db8eeff07808d2ced'}",
            "dataset":"osv",
            "summary":"Improper Input Validation in BeanShell BeanShell (bsh) before 2.0b6, when included on the classpath by an application that uses Java serialization or XStream, allows remote attackers to execute arbitrary code via crafted serialized data, related to XThis.Handler.",
            "published_date":"2022-05-13",
            "chain_len":2,
            "project":"https:\/\/github.com\/beanshell\/beanshell",
            "commit_href":"https:\/\/github.com\/beanshell\/beanshell\/commit\/7c68fde2d6fc65e362f20863d868c112a90a9b49",
            "commit_sha":"7c68fde2d6fc65e362f20863d868c112a90a9b49",
            "patch":"MULTI",
            "chain_ord":"['1ccc66bb693d4e46a34a904db8eeff07808d2ced', '7c68fde2d6fc65e362f20863d868c112a90a9b49']",
            "before_first_fix_commit":"{'1ccc66bb693d4e46a34a904db8eeff07808d2ced'}",
            "last_fix_commit":"7c68fde2d6fc65e362f20863d868c112a90a9b49",
            "chain_ord_pos":2.0,
            "commit_datetime":"02\/03\/2016, 01:03:20",
            "message":"Prevent deserialization of Handler",
            "author":"Stian Soiland-Reyes",
            "comments":null,
            "stats":"{'additions': 4, 'deletions': 0, 'total': 4}",
            "files":"{'src\/bsh\/XThis.java': {'additions': 4, 'deletions': 0, 'changes': 4, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/beanshell\/beanshell\/raw\/7c68fde2d6fc65e362f20863d868c112a90a9b49\/src%2Fbsh%2FXThis.java', 'patch': '@@ -118,6 +118,10 @@ interface from JDK1.2 VM...\\n \\t*\/\\n \\tclass Handler implements InvocationHandler\\n \\t{\\n+\\t\\tprivate Object readResolve() throws ObjectStreamException {\\n+\\t\\t\\tthrow new NotSerializableException();\\n+\\t\\t}\\n+\\n \\t\\tpublic Object invoke( Object proxy, Method method, Object[] args )\\n \\t\\t\\tthrows Throwable\\n \\t\\t{'}}",
            "message_norm":"prevent deserialization of handler",
            "language":"en",
            "entities":"[('prevent', 'ACTION', ''), ('deserialization', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['src\/bsh\/XThis.java'])",
            "num_files":1.0
        },
        {
            "index":2452,
            "vuln_id":"GHSA-mq5c-prh3-3f3h",
            "cwe_id":"{'CWE-665'}",
            "score":3.6,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/c5b0d5f8ac19888e46ca14b0e27562e7fbbee9a9'}",
            "dataset":"osv",
            "summary":"Invalid validation in `QuantizeAndDequantizeV2` ### Impact\nThe validation in `tf.raw_ops.QuantizeAndDequantizeV2` allows invalid values for `axis` argument:\n\n```python\nimport tensorflow as tf\n\ninput_tensor = tf.constant([0.0], shape=[1], dtype=float)\ninput_min = tf.constant(-10.0)\ninput_max = tf.constant(-10.0)\n\ntf.raw_ops.QuantizeAndDequantizeV2(\n  input=input_tensor, input_min=input_min, input_max=input_max,\n  signed_input=False, num_bits=1, range_given=False, round_mode='HALF_TO_EVEN',\n  narrow_range=False, axis=-2)\n``` \n\nThe [validation](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/eccb7ec454e6617738554a255d77f08e60ee0808\/tensorflow\/core\/kernels\/quantize_and_dequantize_op.cc#L74-L77) uses `||` to mix two different conditions:\n\n```cc\nOP_REQUIRES(ctx,\n  (axis_ == -1 || axis_ < input.shape().dims()),\n  errors::InvalidArgument(...));\n```\n\nIf `axis_ < -1` the condition in `OP_REQUIRES` will still be true, but this value of `axis_` results in heap underflow. This allows attackers to read\/write to other data on the heap.\n\n### Patches\nWe have patched the issue in GitHub commit [c5b0d5f8ac19888e46ca14b0e27562e7fbbee9a9](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/c5b0d5f8ac19888e46ca14b0e27562e7fbbee9a9).\n\nThe fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by Yakun Zhang and Ying Wang of Baidu X-Team.",
            "published_date":"2021-05-21",
            "chain_len":1,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/c5b0d5f8ac19888e46ca14b0e27562e7fbbee9a9",
            "commit_sha":"c5b0d5f8ac19888e46ca14b0e27562e7fbbee9a9",
            "patch":"SINGLE",
            "chain_ord":"['c5b0d5f8ac19888e46ca14b0e27562e7fbbee9a9']",
            "before_first_fix_commit":"{'ab6fafc1e32fb20855b7f3a642e36cb08aedbbbf'}",
            "last_fix_commit":"c5b0d5f8ac19888e46ca14b0e27562e7fbbee9a9",
            "chain_ord_pos":1.0,
            "commit_datetime":"04\/30\/2021, 17:39:05",
            "message":"Fix the CHECK failure in tf.raw_ops.QuantizeAndDequantizeV2.\n\nPiperOrigin-RevId: 371361603\nChange-Id: Ia70e34d41adaadddf928e95e5e5c5c97d5bc60d0",
            "author":"Amit Patankar",
            "comments":null,
            "stats":"{'additions': 3, 'deletions': 0, 'total': 3}",
            "files":"{'tensorflow\/core\/kernels\/quantize_and_dequantize_op.cc': {'additions': 3, 'deletions': 0, 'changes': 3, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/c5b0d5f8ac19888e46ca14b0e27562e7fbbee9a9\/tensorflow%2Fcore%2Fkernels%2Fquantize_and_dequantize_op.cc', 'patch': '@@ -72,6 +72,9 @@ class QuantizeAndDequantizeV2Op : public OpKernel {\\n \\n   void Compute(OpKernelContext* ctx) override {\\n     const Tensor& input = ctx->input(0);\\n+    OP_REQUIRES(\\n+        ctx, axis_ >= -1,\\n+        errors::InvalidArgument(\"Axis must be at least -1. Found \", axis_));\\n     OP_REQUIRES(\\n         ctx, (axis_ == -1 || axis_ < input.shape().dims()),\\n         errors::InvalidArgument(\"Shape must be at least rank \", axis_ + 1,'}}",
            "message_norm":"fix the check failure in tf.raw_ops.quantizeanddequantizev2.\n\npiperorigin-revid: 371361603\nchange-id: ia70e34d41adaadddf928e95e5e5c5c97d5bc60d0",
            "language":"en",
            "entities":"[('fix', 'ACTION', ''), ('371361603', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/core\/kernels\/quantize_and_dequantize_op.cc'])",
            "num_files":1.0
        },
        {
            "index":263,
            "vuln_id":"GHSA-3j58-p785-f27x",
            "cwe_id":"{'CWE-79'}",
            "score":5.4,
            "chain":"{'https:\/\/github.com\/microweber\/microweber\/commit\/fc7e1a026735b93f0e0047700d08c44954fce9ce'}",
            "dataset":"osv",
            "summary":"Cross-site Scripting in microweber There is a reflected cross sitem scripting attack in microweber via url parameters.",
            "published_date":"2022-01-28",
            "chain_len":1,
            "project":"https:\/\/github.com\/microweber\/microweber",
            "commit_href":"https:\/\/github.com\/microweber\/microweber\/commit\/fc7e1a026735b93f0e0047700d08c44954fce9ce",
            "commit_sha":"fc7e1a026735b93f0e0047700d08c44954fce9ce",
            "patch":"SINGLE",
            "chain_ord":"['fc7e1a026735b93f0e0047700d08c44954fce9ce']",
            "before_first_fix_commit":"{'6e9fcaa043b4211ef21a494f9892dd19ba8a572c'}",
            "last_fix_commit":"fc7e1a026735b93f0e0047700d08c44954fce9ce",
            "chain_ord_pos":1.0,
            "commit_datetime":"01\/19\/2022, 10:33:18",
            "message":"fix xss on module api call in value parameters",
            "author":"Bozhidar Slaveykov",
            "comments":null,
            "stats":"{'additions': 4, 'deletions': 4, 'total': 8}",
            "files":"{'src\/MicroweberPackages\/App\/Http\/Controllers\/ApiController.php': {'additions': 4, 'deletions': 4, 'changes': 8, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/microweber\/microweber\/raw\/fc7e1a026735b93f0e0047700d08c44954fce9ce\/src%2FMicroweberPackages%2FApp%2FHttp%2FControllers%2FApiController.php', 'patch': \"@@ -17,9 +17,6 @@\\n class ApiController  extends FrontendController\\n {\\n \\n-\\n-\\n-\\n     public function api_html()\\n     {\\n         if (!defined('MW_API_HTML_OUTPUT')) {\\n@@ -609,12 +606,14 @@ public function module()\\n \\n         $request_data = array_merge($_GET, $_POST);\\n \\n-\\n         \/\/ sanitize attributes\\n         if($request_data){\\n             $request_data_new = [];\\n             $antixss = new AntiXSS();\\n             foreach ($request_data as $k=>$v){\\n+\\n+                $v = $antixss->xss_clean($v);\\n+\\n                 if(is_string($k)){\\n                     $k = $antixss->xss_clean($k);\\n                     if($k){\\n@@ -623,6 +622,7 @@ public function module()\\n                 } else {\\n                     $request_data_new[$k] = $v;\\n                 }\\n+                \\n             }\\n             $request_data = $request_data_new;\\n         }\"}}",
            "message_norm":"fix xss on module api call in value parameters",
            "language":"ca",
            "entities":"[('fix', 'ACTION', ''), ('xss', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['src\/MicroweberPackages\/App\/Http\/Controllers\/ApiController.php'])",
            "num_files":1.0
        },
        {
            "index":1790,
            "vuln_id":"GHSA-fx5c-h9f6-rv7c",
            "cwe_id":"{'CWE-617'}",
            "score":6.5,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/6b5adc0877de832b2a7c189532dbbbc64622eeb6'}",
            "dataset":"osv",
            "summary":"`CHECK`-fails due to attempting to build a reference tensor ### Impact\nA malicious user can cause a denial of service by altering a `SavedModel` such that [Grappler optimizer would attempt to build a tensor using a reference `dtype`](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/a1320ec1eac186da1d03f033109191f715b2b130\/tensorflow\/core\/grappler\/optimizers\/constant_folding.cc#L1328-L1402). This would result in a crash due to a `CHECK`-fail [in the `Tensor` constructor](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/a1320ec1eac186da1d03f033109191f715b2b130\/tensorflow\/core\/framework\/tensor.cc#L733-L781) as reference types are not allowed.\n### Patches\nWe have patched the issue in GitHub commit [6b5adc0877de832b2a7c189532dbbbc64622eeb6](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/6b5adc0877de832b2a7c189532dbbbc64622eeb6).\nThe fix will be included in TensorFlow 2.8.0. We will also cherrypick this commit on TensorFlow 2.7.1, TensorFlow 2.6.3, and TensorFlow 2.5.3, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.",
            "published_date":"2022-02-09",
            "chain_len":1,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/6b5adc0877de832b2a7c189532dbbbc64622eeb6",
            "commit_sha":"6b5adc0877de832b2a7c189532dbbbc64622eeb6",
            "patch":"SINGLE",
            "chain_ord":"['6b5adc0877de832b2a7c189532dbbbc64622eeb6']",
            "before_first_fix_commit":"{'af2cab9355e8d5bf48c2c7042b3faaf31262ea8c'}",
            "last_fix_commit":"6b5adc0877de832b2a7c189532dbbbc64622eeb6",
            "chain_ord_pos":1.0,
            "commit_datetime":"11\/13\/2021, 15:28:58",
            "message":"Prevent `CHECK`-fail when building reference tensor.\n\nThe tensor constructor does not allow reference dtypes, as these should not show up explicitly. However, when passed these invalid types instead of building an invalid object the constructor crashes via a `CHECK`-fail. We have a static builder that properly handles this case but is not applicable given current usage.\n\nInstead, before calling the constructor, we can check that the dtype is not a reference type and return an error otherwise, given that the dtype is user controlled so malicious users can trigger denial of service.\n\nPiperOrigin-RevId: 409662503\nChange-Id: I5892f831fde7f276cd7ab34519cf6b8061c71a59",
            "author":"Mihai Maruseac",
            "comments":null,
            "stats":"{'additions': 5, 'deletions': 0, 'total': 5}",
            "files":"{'tensorflow\/core\/grappler\/optimizers\/constant_folding.cc': {'additions': 5, 'deletions': 0, 'changes': 5, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/6b5adc0877de832b2a7c189532dbbbc64622eeb6\/tensorflow%2Fcore%2Fgrappler%2Foptimizers%2Fconstant_folding.cc', 'patch': '@@ -1363,6 +1363,11 @@ Status ConstantFolding::EvaluateOneFoldable(const NodeDef& node,\\n                           input_tensor.ToString(),\\n                           \" has a dtype of DT_INVALID.\"));\\n     }\\n+    if (IsRefType(raw_val.dtype())) {\\n+      return errors::InvalidArgument(\\n+          \"Not allowed to construct a tensor with reference dtype, got \",\\n+          DataTypeString(raw_val.dtype()));\\n+    }\\n     Tensor* value = new Tensor(raw_val.dtype(), raw_val.tensor_shape());\\n     if (!value->FromProto(raw_val)) {\\n       delete (value);'}}",
            "message_norm":"prevent `check`-fail when building reference tensor.\n\nthe tensor constructor does not allow reference dtypes, as these should not show up explicitly. however, when passed these invalid types instead of building an invalid object the constructor crashes via a `check`-fail. we have a static builder that properly handles this case but is not applicable given current usage.\n\ninstead, before calling the constructor, we can check that the dtype is not a reference type and return an error otherwise, given that the dtype is user controlled so malicious users can trigger denial of service.\n\npiperorigin-revid: 409662503\nchange-id: i5892f831fde7f276cd7ab34519cf6b8061c71a59",
            "language":"en",
            "entities":"[('prevent', 'ACTION', ''), ('error', 'FLAW', ''), ('malicious', 'SECWORD', ''), ('denial of service', 'SECWORD', ''), ('409662503', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/core\/grappler\/optimizers\/constant_folding.cc'])",
            "num_files":1.0
        },
        {
            "index":2255,
            "vuln_id":"GHSA-jhj6-5mh6-4pvf",
            "cwe_id":"{'CWE-476'}",
            "score":7.5,
            "chain":"{'https:\/\/github.com\/kitabisa\/teler\/commit\/ec6082049dba9e44a21f35fb7b123d42ce1a1a7e'}",
            "dataset":"osv",
            "summary":"Denial-of-Service within Docker container ### Impact\nIf you run teler inside a Docker container and encounter `errors.Exit` function, it will cause denial-of-service (`SIGSEGV`) because it doesn't get process ID and process group ID of teler properly to kills.\n\n### Patches\nUpgrade to the >= 0.0.1 version.\n\n### Workarounds\nN\/A\n\n### References\n- https:\/\/github.com\/kitabisa\/teler\/commit\/ec6082049dba9e44a21f35fb7b123d42ce1a1a7e\n\n### For more information\nIf you have any questions or comments about this advisory:\n* Open an issue in [Issues Section](https:\/\/github.com\/kitabisa\/teler\/issues)\n* Email us at [infosec@kitabisa.com](mailto:infosec@kitabisa.com)",
            "published_date":"2021-05-24",
            "chain_len":1,
            "project":"https:\/\/github.com\/kitabisa\/teler",
            "commit_href":"https:\/\/github.com\/kitabisa\/teler\/commit\/ec6082049dba9e44a21f35fb7b123d42ce1a1a7e",
            "commit_sha":"ec6082049dba9e44a21f35fb7b123d42ce1a1a7e",
            "patch":"SINGLE",
            "chain_ord":"['ec6082049dba9e44a21f35fb7b123d42ce1a1a7e']",
            "before_first_fix_commit":"{'7be1cb4511a8236d5203a924bc0aa4db008aafb8'}",
            "last_fix_commit":"ec6082049dba9e44a21f35fb7b123d42ce1a1a7e",
            "chain_ord_pos":1.0,
            "commit_datetime":"11\/02\/2020, 14:09:38",
            "message":":hammer: Fix segmentation fault of syscall\n\nIf teler is run via the docker image, the syscall function cannot allocate and return a nil pointer to address.",
            "author":"Dwi Siswanto",
            "comments":null,
            "stats":"{'additions': 8, 'deletions': 1, 'total': 9}",
            "files":"{'pkg\/errors\/abort.go': {'additions': 8, 'deletions': 1, 'changes': 9, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/kitabisa\/teler\/raw\/ec6082049dba9e44a21f35fb7b123d42ce1a1a7e\/pkg%2Ferrors%2Fabort.go', 'patch': '@@ -2,10 +2,17 @@\\n \\n package errors\\n \\n-import \"syscall\"\\n+import (\\n+\\t\"os\"\\n+\\t\"syscall\"\\n+)\\n \\n \/\/ Abort will terminate & sends SIGTERM to process\\n func Abort(i ...int) {\\n+\\tif _, err := os.Stat(\"\/.dockerenv\"); err == nil {\\n+\\t\\tos.Exit(i[0])\\n+\\t}\\n+\\n \\tpgid, err := syscall.Getpgid(syscall.Getpid())\\n \\tif err != nil {\\n \\t\\tExit(err.Error())'}}",
            "message_norm":":hammer: fix segmentation fault of syscall\n\nif teler is run via the docker image, the syscall function cannot allocate and return a nil pointer to address.",
            "language":"en",
            "entities":"[('segmentation fault', 'SECWORD', ''), ('address', 'ACTION', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['pkg\/errors\/abort.go'])",
            "num_files":1.0
        },
        {
            "index":1058,
            "vuln_id":"GHSA-7q4h-pj78-j7vg",
            "cwe_id":"{'CWE-400', 'CWE-918'}",
            "score":7.5,
            "chain":"{'https:\/\/github.com\/apache\/cxf\/commit\/aa789c5c4686597a7bdef2443909ab491fc2bc04', 'https:\/\/github.com\/apache\/cxf\/commit\/40503a53914758759894f704bbf139ae89ace286'}",
            "dataset":"osv",
            "summary":"Authorization service vulnerable to DDos attacks in Apache CFX CXF supports (via JwtRequestCodeFilter) passing OAuth 2 parameters via a JWT token as opposed to query parameters (see: The OAuth 2.0 Authorization Framework: JWT Secured Authorization Request (JAR)). Instead of sending a JWT token as a \"request\" parameter, the spec also supports specifying a URI from which to retrieve a JWT token from via the \"request_uri\" parameter. CXF was not validating the \"request_uri\" parameter (apart from ensuring it uses \"https) and was making a REST request to the parameter in the request to retrieve a token. This means that CXF was vulnerable to DDos attacks on the authorization server, as specified in section 10.4.1 of the spec. This issue affects Apache CXF versions prior to 3.4.3; Apache CXF versions prior to 3.3.10.",
            "published_date":"2021-05-13",
            "chain_len":2,
            "project":"https:\/\/github.com\/apache\/cxf",
            "commit_href":"https:\/\/github.com\/apache\/cxf\/commit\/aa789c5c4686597a7bdef2443909ab491fc2bc04",
            "commit_sha":"aa789c5c4686597a7bdef2443909ab491fc2bc04",
            "patch":"MULTI",
            "chain_ord":"['40503a53914758759894f704bbf139ae89ace286', 'aa789c5c4686597a7bdef2443909ab491fc2bc04']",
            "before_first_fix_commit":"{'40503a53914758759894f704bbf139ae89ace286'}",
            "last_fix_commit":"aa789c5c4686597a7bdef2443909ab491fc2bc04",
            "chain_ord_pos":2.0,
            "commit_datetime":"01\/06\/2021, 10:38:21",
            "message":"Make sure both a request + request_uri can't be specified",
            "author":"Colm O hEigeartaigh",
            "comments":null,
            "stats":"{'additions': 11, 'deletions': 1, 'total': 12}",
            "files":"{'rt\/rs\/security\/oauth-parent\/oauth2\/src\/main\/java\/org\/apache\/cxf\/rs\/security\/oauth2\/grants\/code\/JwtRequestCodeFilter.java': {'additions': 11, 'deletions': 1, 'changes': 12, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/apache\/cxf\/raw\/aa789c5c4686597a7bdef2443909ab491fc2bc04\/rt%2Frs%2Fsecurity%2Foauth-parent%2Foauth2%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fcxf%2Frs%2Fsecurity%2Foauth2%2Fgrants%2Fcode%2FJwtRequestCodeFilter.java', 'patch': '@@ -21,9 +21,11 @@\\n import java.security.cert.X509Certificate;\\n import java.util.List;\\n import java.util.Map;\\n+import java.util.logging.Logger;\\n \\n import javax.ws.rs.core.MultivaluedMap;\\n \\n+import org.apache.cxf.common.logging.LogUtils;\\n import org.apache.cxf.helpers.CastUtils;\\n import org.apache.cxf.jaxrs.client.WebClient;\\n import org.apache.cxf.jaxrs.impl.MetadataMap;\\n@@ -42,23 +44,31 @@\\n import org.apache.cxf.rt.security.crypto.CryptoUtils;\\n \\n public class JwtRequestCodeFilter extends OAuthJoseJwtConsumer implements AuthorizationRequestFilter {\\n+    protected static final Logger LOG = LogUtils.getL7dLogger(JwtRequestCodeFilter.class);\\n     private static final String REQUEST_URI_CONTENT_TYPE = \"application\/oauth-authz-req+jwt\";\\n     private static final String REQUEST_PARAM = \"request\";\\n     private static final String REQUEST_URI_PARAM = \"request_uri\";\\n+\\n     private boolean verifyWithClientCertificates;\\n     private String issuer;\\n     private JsonMapObjectReaderWriter jsonHandler = new JsonMapObjectReaderWriter();\\n+\\n     @Override\\n     public MultivaluedMap<String, String> process(MultivaluedMap<String, String> params,\\n                                                   UserSubject endUser,\\n                                                   Client client) {\\n         String requestToken = params.getFirst(REQUEST_PARAM);\\n+        String requestUri = params.getFirst(REQUEST_URI_PARAM);\\n+\\n         if (requestToken == null) {\\n-            String requestUri = params.getFirst(REQUEST_URI_PARAM);\\n             if (isRequestUriValid(client, requestUri)) {\\n                 requestToken = WebClient.create(requestUri).accept(REQUEST_URI_CONTENT_TYPE).get(String.class);\\n             }\\n+        } else if (requestUri != null) {\\n+            LOG.warning(\"It is not valid to specify both a request and request_uri value\");\\n+            throw new SecurityException();\\n         }\\n+\\n         if (requestToken != null) {\\n             JweDecryptionProvider theDecryptor = super.getInitializedDecryptionProvider(client.getClientSecret());\\n             JwsSignatureVerifier theSigVerifier = getInitializedSigVerifier(client);'}}",
            "message_norm":"make sure both a request + request_uri can't be specified",
            "language":"en",
            "entities":null,
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['rt\/rs\/security\/oauth-parent\/oauth2\/src\/main\/java\/org\/apache\/cxf\/rs\/security\/oauth2\/grants\/code\/JwtRequestCodeFilter.java'])",
            "num_files":1.0
        },
        {
            "index":2279,
            "vuln_id":"GHSA-jmqm-f2gx-4fjv",
            "cwe_id":"{'CWE-352'}",
            "score":5.3,
            "chain":"{'https:\/\/github.com\/npm\/npm-registry-fetch\/commit\/18bf9b97fb1deecdba01ffb05580370846255c88'}",
            "dataset":"osv",
            "summary":"Sensitive information exposure through logs in npm-registry-fetch Affected versions of `npm-registry-fetch` are vulnerable to an information exposure vulnerability through log files. The cli supports URLs like `<protocol>:\/\/[<user>[:<password>]@]<hostname>[:<port>][:][\/]<path>`. The password value is not redacted and is printed to stdout and also to any generated log files.",
            "published_date":"2020-07-07",
            "chain_len":1,
            "project":"https:\/\/github.com\/npm\/npm-registry-fetch",
            "commit_href":"https:\/\/github.com\/npm\/npm-registry-fetch\/commit\/18bf9b97fb1deecdba01ffb05580370846255c88",
            "commit_sha":"18bf9b97fb1deecdba01ffb05580370846255c88",
            "patch":"SINGLE",
            "chain_ord":"['18bf9b97fb1deecdba01ffb05580370846255c88']",
            "before_first_fix_commit":"{'09e540b09a951ded299ee028e7f1bd21cef5a6da'}",
            "last_fix_commit":"18bf9b97fb1deecdba01ffb05580370846255c88",
            "chain_ord_pos":1.0,
            "commit_datetime":"06\/30\/2020, 16:32:16",
            "message":"chore: remove auth data from logs (#29)",
            "author":"Claudia Hern\u00e1ndez",
            "comments":null,
            "stats":"{'additions': 10, 'deletions': 1, 'total': 11}",
            "files":"{'check-response.js': {'additions': 10, 'deletions': 1, 'changes': 11, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/npm\/npm-registry-fetch\/raw\/18bf9b97fb1deecdba01ffb05580370846255c88\/check-response.js', 'patch': \"@@ -30,9 +30,18 @@ function logRequest (method, res, startTime, opts) {\\n   const attempt = res.headers.get('x-fetch-attempts')\\n   const attemptStr = attempt && attempt > 1 ? ` attempt #${attempt}` : ''\\n   const cacheStr = res.headers.get('x-local-cache') ? ' (from cache)' : ''\\n+\\n+  let urlStr\\n+  try {\\n+    const url = new URL(res.url)\\n+    urlStr = res.url.replace(url.password, '***')\\n+  } catch {\\n+    urlStr = res.url\\n+  }\\n+\\n   opts.log.http(\\n     'fetch',\\n-    `${method.toUpperCase()} ${res.status} ${res.url} ${elapsedTime}ms${attemptStr}${cacheStr}`\\n+    `${method.toUpperCase()} ${res.status} ${urlStr} ${elapsedTime}ms${attemptStr}${cacheStr}`\\n   )\\n }\"}}",
            "message_norm":"chore: remove auth data from logs (#29)",
            "language":"en",
            "entities":"[('remove', 'ACTION', ''), ('auth', 'SECWORD', ''), ('#29', 'ISSUE', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['check-response.js'])",
            "num_files":1.0
        },
        {
            "index":383,
            "vuln_id":"GHSA-4873-36h9-wv49",
            "cwe_id":"{'CWE-787', 'CWE-125', 'CWE-590'}",
            "score":6.3,
            "chain":"{'https:\/\/github.com\/bytecodealliance\/wasmtime\/commit\/398a73f0dd862dbe703212ebae8e34036a18c11c'}",
            "dataset":"osv",
            "summary":"Out-of-bounds read\/write and invalid free with `externref`s and GC safepoints in Wasmtime  ### Impact\n\nThere was an invalid free and out-of-bounds read and write bug when running Wasm that uses `externref`s in Wasmtime.\n\nTo trigger this bug, Wasmtime needs to be running Wasm that uses `externref`s, the host creates non-null `externrefs`, Wasmtime performs a garbage collection (GC), and there has to be a Wasm frame on the stack that is at a GC safepoint where\n\n* there are no live references at this safepoint, and\n* there is a safepoint with live references earlier in this frame's function.\n\nUnder this scenario, Wasmtime would incorrectly use the GC stack map for the safepoint from earlier in the function instead of the empty safepoint. This would result in Wasmtime treating arbitrary stack slots as `externref`s that needed to be rooted for GC. At the *next* GC, it would be determined that nothing was referencing these bogus `externref`s (because nothing could ever reference them, because they are not really `externref`s) and then Wasmtime would deallocate them and run `<ExternRef as Drop>::drop` on them. This results in a free of memory that is not necessarily on the heap (and shouldn't be freed at this moment even if it was), as well as potential out-of-bounds reads and writes.\n\nEven though support for `externref`s (via the reference types proposal) is enabled by default, unless you are creating non-null `externref`s in your host code or explicitly triggering GCs, you cannot be affected by this bug.\n\nWe have reason to believe that the effective impact of this bug is relatively small because usage of `externref` is currently quite rare.\n\n### Patches\n\nThis bug has been patched and users should upgrade to Wasmtime version 0.30.0.\n\nAdditionally, we have updated [our primary `externref` fuzz target](https:\/\/github.com\/bytecodealliance\/wasmtime\/blob\/37c094faf53f1b356aab3c79d451395e4f7edb34\/fuzz\/fuzz_targets\/table_ops.rs) such that it better exercises these code paths and we can have greater confidence in their correctness going forward.\n\n### Workarounds\n\nIf you cannot upgrade Wasmtime at this time, you can avoid this bug by disabling the reference types proposal by passing `false` to [`wasmtime::Config::wasm_reference_types`](https:\/\/docs.rs\/wasmtime\/0.29.0\/wasmtime\/struct.Config.html#method.wasm_reference_types)\n\n### References\n\n* [The Wasm reference types proposal, which introduces `externref`](https:\/\/github.com\/WebAssembly\/reference-types\/)\n\n### For more information\n\nIf you have any questions or comments about this advisory:\n\n* Reach out to us on [the Bytecode Alliance Zulip chat](https:\/\/bytecodealliance.zulipchat.com\/#narrow\/stream\/217126-wasmtime)\n* Open an issue in [the `bytecodealliance\/wasmtime` repository](https:\/\/github.com\/bytecodealliance\/wasmtime\/)",
            "published_date":"2021-09-20",
            "chain_len":1,
            "project":"https:\/\/github.com\/bytecodealliance\/wasmtime",
            "commit_href":"https:\/\/github.com\/bytecodealliance\/wasmtime\/commit\/398a73f0dd862dbe703212ebae8e34036a18c11c",
            "commit_sha":"398a73f0dd862dbe703212ebae8e34036a18c11c",
            "patch":"SINGLE",
            "chain_ord":"['398a73f0dd862dbe703212ebae8e34036a18c11c']",
            "before_first_fix_commit":"{'ec4e48d4cbc28bcfd99e25842a90704e765b800f', '101998733b74624cbd348a2366d05760b40181f3'}",
            "last_fix_commit":"398a73f0dd862dbe703212ebae8e34036a18c11c",
            "chain_ord_pos":1.0,
            "commit_datetime":"09\/17\/2021, 17:28:50",
            "message":"Merge pull request from GHSA-4873-36h9-wv49\n\nStop doing fuzzy search for stack maps",
            "author":"Nick Fitzgerald",
            "comments":null,
            "stats":"{'additions': 52, 'deletions': 48, 'total': 100}",
            "files":"{'crates\/wasmtime\/src\/module\/registry.rs': {'additions': 52, 'deletions': 48, 'changes': 100, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/bytecodealliance\/wasmtime\/raw\/398a73f0dd862dbe703212ebae8e34036a18c11c\/crates%2Fwasmtime%2Fsrc%2Fmodule%2Fregistry.rs', 'patch': '@@ -122,61 +122,65 @@ impl ModuleInfo for RegisteredModule {\\n         let info = self.module.func_info(index);\\n \\n         \/\/ Do a binary search to find the stack map for the given offset.\\n-        \/\/\\n-        \/\/ Because GC safepoints are technically only associated with a single\\n-        \/\/ PC, we should ideally only care about `Ok(index)` values returned\\n-        \/\/ from the binary search. However, safepoints are inserted right before\\n-        \/\/ calls, and there are two things that can disturb the PC\/offset\\n-        \/\/ associated with the safepoint versus the PC we actually use to query\\n-        \/\/ for the stack map:\\n-        \/\/\\n-        \/\/ 1. The `backtrace` crate gives us the PC in a frame that will be\\n-        \/\/    *returned to*, and where execution will continue from, rather than\\n-        \/\/    the PC of the call we are currently at. So we would need to\\n-        \/\/    disassemble one instruction backwards to query the actual PC for\\n-        \/\/    the stack map.\\n-        \/\/\\n-        \/\/    TODO: One thing we *could* do to make this a little less error\\n-        \/\/    prone, would be to assert\/check that the nearest GC safepoint\\n-        \/\/    found is within `max_encoded_size(any kind of call instruction)`\\n-        \/\/    our queried PC for the target architecture.\\n-        \/\/\\n-        \/\/ 2. Cranelift\\'s stack maps only handle the stack, not\\n-        \/\/    registers. However, some references that are arguments to a call\\n-        \/\/    may need to be in registers. In these cases, what Cranelift will\\n-        \/\/    do is:\\n-        \/\/\\n-        \/\/      a. spill all the live references,\\n-        \/\/      b. insert a GC safepoint for those references,\\n-        \/\/      c. reload the references into registers, and finally\\n-        \/\/      d. make the call.\\n-        \/\/\\n-        \/\/    Step (c) adds drift between the GC safepoint and the location of\\n-        \/\/    the call, which is where we actually walk the stack frame and\\n-        \/\/    collect its live references.\\n-        \/\/\\n-        \/\/    Luckily, the spill stack slots for the live references are still\\n-        \/\/    up to date, so we can still find all the on-stack roots.\\n-        \/\/    Furthermore, we do not have a moving GC, so we don\\'t need to worry\\n-        \/\/    whether the following code will reuse the references in registers\\n-        \/\/    (which would not have been updated to point to the moved objects)\\n-        \/\/    or reload from the stack slots (which would have been updated to\\n-        \/\/    point to the moved objects).\\n-\\n         let index = match info\\n             .stack_maps\\n             .binary_search_by_key(&func_offset, |i| i.code_offset)\\n         {\\n-            \/\/ Exact hit.\\n+            \/\/ Found it.\\n             Ok(i) => i,\\n \\n-            \/\/ `Err(0)` means that the associated stack map would have been the\\n-            \/\/ first element in the array if this pc had an associated stack\\n-            \/\/ map, but this pc does not have an associated stack map. This can\\n-            \/\/ only happen inside a Wasm frame if there are no live refs at this\\n-            \/\/ pc.\\n+            \/\/ No stack map associated with this PC.\\n+            \/\/\\n+            \/\/ Because we know we are in Wasm code, and we must be at some kind\\n+            \/\/ of call\/safepoint, then the Cranelift backend must have avoided\\n+            \/\/ emitting a stack map for this location because no refs were live.\\n+            #[cfg(not(feature = \"old-x86-backend\"))]\\n+            Err(_) => return None,\\n+\\n+            \/\/ ### Old x86_64 backend specific code.\\n+            \/\/\\n+            \/\/ Because GC safepoints are technically only associated with a\\n+            \/\/ single PC, we should ideally only care about `Ok(index)` values\\n+            \/\/ returned from the binary search. However, safepoints are inserted\\n+            \/\/ right before calls, and there are two things that can disturb the\\n+            \/\/ PC\/offset associated with the safepoint versus the PC we actually\\n+            \/\/ use to query for the stack map:\\n+            \/\/\\n+            \/\/ 1. The `backtrace` crate gives us the PC in a frame that will be\\n+            \/\/    *returned to*, and where execution will continue from, rather than\\n+            \/\/    the PC of the call we are currently at. So we would need to\\n+            \/\/    disassemble one instruction backwards to query the actual PC for\\n+            \/\/    the stack map.\\n+            \/\/\\n+            \/\/    TODO: One thing we *could* do to make this a little less error\\n+            \/\/    prone, would be to assert\/check that the nearest GC safepoint\\n+            \/\/    found is within `max_encoded_size(any kind of call instruction)`\\n+            \/\/    our queried PC for the target architecture.\\n+            \/\/\\n+            \/\/ 2. Cranelift\\'s stack maps only handle the stack, not\\n+            \/\/    registers. However, some references that are arguments to a call\\n+            \/\/    may need to be in registers. In these cases, what Cranelift will\\n+            \/\/    do is:\\n+            \/\/\\n+            \/\/      a. spill all the live references,\\n+            \/\/      b. insert a GC safepoint for those references,\\n+            \/\/      c. reload the references into registers, and finally\\n+            \/\/      d. make the call.\\n+            \/\/\\n+            \/\/    Step (c) adds drift between the GC safepoint and the location of\\n+            \/\/    the call, which is where we actually walk the stack frame and\\n+            \/\/    collect its live references.\\n+            \/\/\\n+            \/\/    Luckily, the spill stack slots for the live references are still\\n+            \/\/    up to date, so we can still find all the on-stack roots.\\n+            \/\/    Furthermore, we do not have a moving GC, so we don\\'t need to worry\\n+            \/\/    whether the following code will reuse the references in registers\\n+            \/\/    (which would not have been updated to point to the moved objects)\\n+            \/\/    or reload from the stack slots (which would have been updated to\\n+            \/\/    point to the moved objects).\\n+            #[cfg(feature = \"old-x86-backend\")]\\n             Err(0) => return None,\\n-\\n+            #[cfg(feature = \"old-x86-backend\")]\\n             Err(i) => i - 1,\\n         };'}}",
            "message_norm":"merge pull request from ghsa-4873-36h9-wv49\n\nstop doing fuzzy search for stack maps",
            "language":"en",
            "entities":"[('ghsa-4873-36h9-wv49', 'VULNID', 'GHSA'), ('fuzzy', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['crates\/wasmtime\/src\/module\/registry.rs'])",
            "num_files":1.0
        },
        {
            "index":742,
            "vuln_id":"GHSA-6445-fm66-fvq2",
            "cwe_id":"{'CWE-190'}",
            "score":6.5,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/b51b82fe65ebace4475e3c54eb089c18a4403f1c', 'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/a68f68061e263a88321c104a6c911fe5598050a8'}",
            "dataset":"osv",
            "summary":"Integer overflows in Tensorflow ### Impact \nThe [implementation of `AddManySparseToTensorsMap`](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/5100e359aef5c8021f2e71c7b986420b85ce7b3d\/tensorflow\/core\/kernels\/sparse_tensors_map_ops.cc) is vulnerable to an integer overflow which results in a `CHECK`-fail when building new `TensorShape` objects (so, an assert failure based denial of service):\n\n```python\nimport tensorflow as tf\nimport numpy as np\n\ntf.raw_ops.AddManySparseToTensorsMap(\n    sparse_indices=[(0,0),(0,1),(0,2),(4,3),(5,0),(5,1)],\n    sparse_values=[1,1,1,1,1,1],\n    sparse_shape=[2**32,2**32],\n    container='',\n    shared_name='',\n    name=None)\n```\n\nWe are missing some validation on the shapes of the input tensors as well as directly constructing a large `TensorShape` with user-provided dimensions. The latter is an instance of [TFSA-2021-198](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/tensorflow\/security\/advisory\/tfsa-2021-198.md) (CVE-2021-41197) and is easily fixed by replacing a call to `TensorShape` constructor with a call to `BuildTensorShape` static helper factory.\n### Patches\nWe have patched the issue in GitHub commits [b51b82fe65ebace4475e3c54eb089c18a4403f1c](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/b51b82fe65ebace4475e3c54eb089c18a4403f1c) and [a68f68061e263a88321c104a6c911fe5598050a8](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/a68f68061e263a88321c104a6c911fe5598050a8).\n\nThe fix will be included in TensorFlow 2.8.0. We will also cherrypick this commit on TensorFlow 2.7.1, TensorFlow 2.6.3, and TensorFlow 2.5.3, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by Faysal Hossain Shezan from University of Virginia.",
            "published_date":"2022-02-09",
            "chain_len":2,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/b51b82fe65ebace4475e3c54eb089c18a4403f1c",
            "commit_sha":"b51b82fe65ebace4475e3c54eb089c18a4403f1c",
            "patch":"MULTI",
            "chain_ord":"['b51b82fe65ebace4475e3c54eb089c18a4403f1c', 'a68f68061e263a88321c104a6c911fe5598050a8']",
            "before_first_fix_commit":"{'e8f4be7958736823b9f56090611ec2fb09824d51'}",
            "last_fix_commit":"a68f68061e263a88321c104a6c911fe5598050a8",
            "chain_ord_pos":1.0,
            "commit_datetime":"12\/09\/2021, 22:32:48",
            "message":"Add missing validation to `AddManySparseToTensorsMap`.\n\nSparse tensors have a set of requirements for the 3 components and not all of them were checked.\n\nPiperOrigin-RevId: 415358027\nChange-Id: I96cbb672999cd1da772c22fabbd15507e32e12dc",
            "author":"Mihai Maruseac",
            "comments":null,
            "stats":"{'additions': 15, 'deletions': 2, 'total': 17}",
            "files":"{'tensorflow\/core\/kernels\/sparse_tensors_map_ops.cc': {'additions': 15, 'deletions': 2, 'changes': 17, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/b51b82fe65ebace4475e3c54eb089c18a4403f1c\/tensorflow%2Fcore%2Fkernels%2Fsparse_tensors_map_ops.cc', 'patch': '@@ -231,16 +231,29 @@ class AddManySparseToTensorsMapOp : public SparseTensorAccessingOp {\\n                 errors::InvalidArgument(\\n                     \"Input indices should be a matrix but received shape \",\\n                     input_indices->shape().DebugString()));\\n-\\n     OP_REQUIRES(context, TensorShapeUtils::IsVector(input_values->shape()),\\n                 errors::InvalidArgument(\\n                     \"Input values should be a vector but received shape \",\\n                     input_values->shape().DebugString()));\\n-\\n     OP_REQUIRES(context, TensorShapeUtils::IsVector(input_shape->shape()),\\n                 errors::InvalidArgument(\\n                     \"Input shape should be a vector but received shape \",\\n                     input_shape->shape().DebugString()));\\n+    OP_REQUIRES(\\n+        context,\\n+        input_values->shape().dim_size(0) == input_indices->shape().dim_size(0),\\n+        errors::InvalidArgument(\\n+            \"Number of values must match first dimension of indices. \", \"Got \",\\n+            input_values->shape().dim_size(0),\\n+            \" values, indices shape: \", input_indices->shape().DebugString()));\\n+    OP_REQUIRES(\\n+        context,\\n+        input_shape->shape().dim_size(0) == input_indices->shape().dim_size(1),\\n+        errors::InvalidArgument(\\n+            \"Number of dimensions must match second dimension of indices. \",\\n+            \"Got \", input_shape->shape().dim_size(0),\\n+            \" dimensions, indices shape: \",\\n+            input_indices->shape().DebugString()));\\n \\n     int rank = input_shape->NumElements();'}}",
            "message_norm":"add missing validation to `addmanysparsetotensorsmap`.\n\nsparse tensors have a set of requirements for the 3 components and not all of them were checked.\n\npiperorigin-revid: 415358027\nchange-id: i96cbb672999cd1da772c22fabbd15507e32e12dc",
            "language":"en",
            "entities":"[('add', 'ACTION', ''), ('missing validation', 'SECWORD', ''), ('415358027', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/core\/kernels\/sparse_tensors_map_ops.cc'])",
            "num_files":1.0
        },
        {
            "index":1127,
            "vuln_id":"GHSA-84cm-v6jp-gjmr",
            "cwe_id":"{'CWE-78'}",
            "score":9.8,
            "chain":"{'https:\/\/github.com\/kellyselden\/git-diff-apply\/commit\/106d61d3ae723b4257c2a13e67b95eb40a27e0b5'}",
            "dataset":"osv",
            "summary":"OS command injection in git-diff-apply In \"index.js\" file line 240, the run command executes the git command with a user controlled variable called remoteUrl. This affects git-diff-apply all versions prior to 0.22.2.",
            "published_date":"2020-02-14",
            "chain_len":1,
            "project":"https:\/\/github.com\/kellyselden\/git-diff-apply",
            "commit_href":"https:\/\/github.com\/kellyselden\/git-diff-apply\/commit\/106d61d3ae723b4257c2a13e67b95eb40a27e0b5",
            "commit_sha":"106d61d3ae723b4257c2a13e67b95eb40a27e0b5",
            "patch":"SINGLE",
            "chain_ord":"['106d61d3ae723b4257c2a13e67b95eb40a27e0b5']",
            "before_first_fix_commit":"{'bfcc903a961d9f17bde1889cf49745a2dffefd73'}",
            "last_fix_commit":"106d61d3ae723b4257c2a13e67b95eb40a27e0b5",
            "chain_ord_pos":1.0,
            "commit_datetime":"01\/06\/2020, 12:05:14",
            "message":"spawn git clone\n\nto prevent injecting a command",
            "author":"Kelly Selden",
            "comments":null,
            "stats":"{'additions': 2, 'deletions': 1, 'total': 3}",
            "files":"{'src\/index.js': {'additions': 2, 'deletions': 1, 'changes': 3, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/kellyselden\/git-diff-apply\/raw\/106d61d3ae723b4257c2a13e67b95eb40a27e0b5\/src%2Findex.js', 'patch': \"@@ -18,6 +18,7 @@ const resolveConflicts = require('.\/resolve-conflicts');\\n const commitAndTag = require('.\/commit-and-tag');\\n const gitRemoveAll = require('.\/git-remove-all');\\n const createCustomRemote = require('.\/create-custom-remote');\\n+const { runWithSpawn } = require('.\/run');\\n \\n const { isGitClean } = gitStatus;\\n const { gitConfigInit } = gitInit;\\n@@ -222,7 +223,7 @@ module.exports = async function gitDiffApply({\\n     _tmpDir = await tmpDir();\\n     tmpWorkingDir = _tmpDir;\\n \\n-    await utils.run(`git clone ${remoteUrl} ${_tmpDir}`);\\n+    await runWithSpawn('git', ['clone', remoteUrl, _tmpDir]);\\n \\n     \/\/ needed because we are going to be committing in here\\n     await gitConfigInit({ cwd: _tmpDir });\"}}",
            "message_norm":"spawn git clone\n\nto prevent injecting a command",
            "language":"en",
            "entities":"[('prevent', 'ACTION', ''), ('injecting a command', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['src\/index.js'])",
            "num_files":1.0
        },
        {
            "index":2090,
            "vuln_id":"GHSA-hm37-9xh2-q499",
            "cwe_id":"{'CWE-209'}",
            "score":7.7,
            "chain":"{'https:\/\/github.com\/scottcwang\/openssh_key_parser\/commit\/d5b53b4b7e76c5b666fc657019dbf864fb04076c', 'https:\/\/github.com\/scottcwang\/openssh_key_parser\/commit\/26e0a471e9fdb23e635bc3014cf4cbd2323a08d3', 'https:\/\/github.com\/scottcwang\/openssh_key_parser\/commit\/274447f91b4037b7050ae634879b657554523b39'}",
            "dataset":"osv",
            "summary":"Possible leak of key's raw field if declared length is incorrect ### Impact\nIf a field of a key is shorter than it is declared to be, the parser raises an error with a message containing the raw field value. An attacker able to modify the declared length of a key's sensitive field can thus expose the raw value of that field.\n\n### Patches\nUpgrade to version 0.0.6, which no longer includes the raw field value in the error message.\n\n### Workarounds\nN\/A\n\n### References\nN\/A\n\n### For more information\nIf you have any questions or comments about this advisory:\n* Open an issue in [openssh_key_parser](https:\/\/github.com\/scottcwang\/openssh_key_parser)",
            "published_date":"2022-07-06",
            "chain_len":3,
            "project":"https:\/\/github.com\/scottcwang\/openssh_key_parser",
            "commit_href":"https:\/\/github.com\/scottcwang\/openssh_key_parser\/commit\/26e0a471e9fdb23e635bc3014cf4cbd2323a08d3",
            "commit_sha":"26e0a471e9fdb23e635bc3014cf4cbd2323a08d3",
            "patch":"MULTI",
            "chain_ord":"['26e0a471e9fdb23e635bc3014cf4cbd2323a08d3', 'd5b53b4b7e76c5b666fc657019dbf864fb04076c', '274447f91b4037b7050ae634879b657554523b39']",
            "before_first_fix_commit":"{'ae4d131d1cd8fe06325bfd6b749305aca60873bf', '69fe5b7addc21d3f39626ae93c6961811aea9d4c'}",
            "last_fix_commit":"274447f91b4037b7050ae634879b657554523b39",
            "chain_ord_pos":1.0,
            "commit_datetime":"06\/22\/2022, 14:49:37",
            "message":"Changed an exception message to prevent possible disclosures of keying material.",
            "author":"Michael Doyle",
            "comments":null,
            "stats":"{'additions': 2, 'deletions': 1, 'total': 3}",
            "files":"{'openssh_key\/pascal_style_byte_stream.py': {'additions': 2, 'deletions': 1, 'changes': 3, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/scottcwang\/openssh_key_parser\/raw\/26e0a471e9fdb23e635bc3014cf4cbd2323a08d3\/openssh_key%2Fpascal_style_byte_stream.py', 'patch': '@@ -236,7 +236,8 @@ def read_fixed_bytes(self, num_bytes: int) -> bytes:\\n         \"\"\"\\n         read_bytes = self.read(num_bytes)\\n         if len(read_bytes) < num_bytes:\\n-            raise EOFError(read_bytes)\\n+            raise EOFError(\"Fewer than \\'num_bytes\\' bytes remaining in the \"\\n+                    \"underlying bytestream\")\\n         return read_bytes\\n \\n     def read_pascal_bytes(self, string_length_size: int) -> bytes:'}}",
            "message_norm":"changed an exception message to prevent possible disclosures of keying material.",
            "language":"en",
            "entities":"[('changed', 'ACTION', ''), ('prevent', 'ACTION', ''), ('keying', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['openssh_key\/pascal_style_byte_stream.py'])",
            "num_files":1.0
        },
        {
            "index":1662,
            "vuln_id":"GHSA-f522-ffg8-j8r6",
            "cwe_id":"{'CWE-20'}",
            "score":7.5,
            "chain":"{'https:\/\/github.com\/mafintosh\/is-my-json-valid\/commit\/eca4beb21e61877d76fdf6bea771f72f39544d9b', 'https:\/\/github.com\/mafintosh\/is-my-json-valid\/commit\/b3051b277f7caa08cd2edc6f74f50aeda65d2976'}",
            "dataset":"osv",
            "summary":"Regular Expression Denial of Service in is-my-json-valid Version of `is-my-json-valid` before 1.4.1 or 2.17.2 are vulnerable to regular expression denial of service (ReDoS) via the email validation function.\n\n\n## Recommendation\n\nUpdate to version 1.4.1, 2.17.2 or later.",
            "published_date":"2017-10-24",
            "chain_len":2,
            "project":"https:\/\/github.com\/mafintosh\/is-my-json-valid",
            "commit_href":"https:\/\/github.com\/mafintosh\/is-my-json-valid\/commit\/eca4beb21e61877d76fdf6bea771f72f39544d9b",
            "commit_sha":"eca4beb21e61877d76fdf6bea771f72f39544d9b",
            "patch":"MULTI",
            "chain_ord":"['eca4beb21e61877d76fdf6bea771f72f39544d9b', 'b3051b277f7caa08cd2edc6f74f50aeda65d2976']",
            "before_first_fix_commit":"{'5c6a43dedebf975508465b375af98e9afde2f49c', '767c6c0ee5e55c506ccd49141e7f31cb1910c47f'}",
            "last_fix_commit":"b3051b277f7caa08cd2edc6f74f50aeda65d2976",
            "chain_ord_pos":1.0,
            "commit_datetime":"01\/17\/2016, 21:55:32",
            "message":"fix utc-millisec regex to avoid a ddos attack",
            "author":"Mathias Buus",
            "comments":null,
            "stats":"{'additions': 1, 'deletions': 1, 'total': 2}",
            "files":"{'formats.js': {'additions': 1, 'deletions': 1, 'changes': 2, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/mafintosh\/is-my-json-valid\/raw\/eca4beb21e61877d76fdf6bea771f72f39544d9b\/formats.js', 'patch': \"@@ -11,4 +11,4 @@ exports['alpha'] = \/^[a-zA-Z]+$\/\\n exports['alphanumeric'] = \/^[a-zA-Z0-9]+$\/\\n exports['style'] = \/\\\\s*(.+?):\\\\s*([^;]+);?\/g\\n exports['phone'] = \/^\\\\+(?:[0-9] ?){6,14}[0-9]$\/\\n-exports['utc-millisec'] = \/^[0-9]+(\\\\.?[0-9]+)?$\/\\n+exports['utc-millisec'] = \/^[0-9]{1,15}\\\\.?[0-9]{0,15}$\/\"}}",
            "message_norm":"fix utc-millisec regex to avoid a ddos attack",
            "language":"en",
            "entities":"[('fix', 'ACTION', ''), ('ddos', 'SECWORD', ''), ('attack', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['formats.js'])",
            "num_files":1.0
        },
        {
            "index":455,
            "vuln_id":"GHSA-4p92-fv6v-fhfj",
            "cwe_id":"{'CWE-79'}",
            "score":5.4,
            "chain":"{'https:\/\/github.com\/microweber\/microweber\/commit\/15e519a86e4b24526abaf9e6dc81cb1af86843a5'}",
            "dataset":"osv",
            "summary":"Cross-site Scripting in microweber Microweber prior to 1.2.11 is vulnerable to reflected cross-site scripting.",
            "published_date":"2022-02-27",
            "chain_len":1,
            "project":"https:\/\/github.com\/microweber\/microweber",
            "commit_href":"https:\/\/github.com\/microweber\/microweber\/commit\/15e519a86e4b24526abaf9e6dc81cb1af86843a5",
            "commit_sha":"15e519a86e4b24526abaf9e6dc81cb1af86843a5",
            "patch":"SINGLE",
            "chain_ord":"['15e519a86e4b24526abaf9e6dc81cb1af86843a5']",
            "before_first_fix_commit":"{'c897d0dc159849763a813184d9b75b966c6360bf'}",
            "last_fix_commit":"15e519a86e4b24526abaf9e6dc81cb1af86843a5",
            "chain_ord_pos":1.0,
            "commit_datetime":"02\/25\/2022, 10:57:48",
            "message":"update",
            "author":"Peter Ivanov",
            "comments":null,
            "stats":"{'additions': 0, 'deletions': 0, 'total': 0}",
            "files":"{'.github\/workflows\/templates.yml': {'additions': 0, 'deletions': 0, 'changes': 0, 'status': 'renamed', 'raw_url': 'https:\/\/github.com\/microweber\/microweber\/raw\/15e519a86e4b24526abaf9e6dc81cb1af86843a5\/.github%2Fworkflows%2Ftemplates.yml', 'patch': None}}",
            "message_norm":"update",
            "language":"ro",
            "entities":"[('update', 'ACTION', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['.github\/workflows\/templates.yml'])",
            "num_files":1.0
        },
        {
            "index":2028,
            "vuln_id":"GHSA-h8v5-p258-pqf4",
            "cwe_id":"{'CWE-327'}",
            "score":5.4,
            "chain":"{'https:\/\/github.com\/xwiki\/xwiki-platform\/commit\/26728f3f23658288683667a5182a916c7ecefc52'}",
            "dataset":"osv",
            "summary":"Use of a Broken or Risky Cryptographic Algorithm in XWiki Crypto API ### Impact\nXWiki Crypto API will generate X509 certificates signed by default using SHA1 with RSA, which is not considered safe anymore for use in certificate signatures, due to the risk of collisions with SHA1.\nNote that this API is never used in XWiki Standard but it might be used in some extensions of XWiki.\n\n### Patches\nThe problem has been patched in XWiki version 13.10.6, 14.3.1 and 14.4-rc-1. Since then, the Crypto API will generate X509 certificates signed by default using SHA256 with RSA.\n\n### Workarounds\nAdministrators are advised to upgrade their XWiki installation to one of the patched versions.\nIf the upgrade is not possible, it is possible to patch the module xwiki-platform-crypto in a local installation by applying the change exposed in https:\/\/github.com\/xwiki\/xwiki-platform\/commit\/26728f3f23658288683667a5182a916c7ecefc52 and re-compiling the module.\n\n### References\nhttps:\/\/jira.xwiki.org\/browse\/XWIKI-19676\nhttps:\/\/github.com\/openssl\/openssl\/blob\/master\/CHANGES.md?plain=1#L938\nhttps:\/\/github.com\/openssl\/openssl\/issues\/16650\n\n### For more information\nIf you have any questions or comments about this advisory:\n* Open an issue in [Jira XWiki](https:\/\/jira.xwiki.org)\n* Email us at [security ML](mailto:security@xwiki.org)",
            "published_date":"2022-05-24",
            "chain_len":1,
            "project":"https:\/\/github.com\/xwiki\/xwiki-platform",
            "commit_href":"https:\/\/github.com\/xwiki\/xwiki-platform\/commit\/26728f3f23658288683667a5182a916c7ecefc52",
            "commit_sha":"26728f3f23658288683667a5182a916c7ecefc52",
            "patch":"SINGLE",
            "chain_ord":"['26728f3f23658288683667a5182a916c7ecefc52']",
            "before_first_fix_commit":"{'3b871d906e664fa1875fbeb088404cf31e9f0094'}",
            "last_fix_commit":"26728f3f23658288683667a5182a916c7ecefc52",
            "chain_ord_pos":1.0,
            "commit_datetime":"04\/30\/2022, 10:13:25",
            "message":"XWIKI-19676: Update the RSA Crypto script service to use SHA256 instead of SHA1 for certificate signature",
            "author":"Cl\u00e9ment Aubin",
            "comments":"{'com_1': {'author': 'surli', 'datetime': '05\/02\/2022, 06:58:35', 'body': \"I don't know much this class, but is that ok in term of backward compatibility? If you have some signed stuff in the wiki with that script service, will it be still be able to verify the signature?\"}, 'com_2': {'author': 'aubincleme', 'datetime': '05\/02\/2022, 07:27:55', 'body': 'Yes to me it is fine : the SignerFactory is only used to sign certificates, not verify them. For this the CMSSignedDataVerifier is used instead, which is able to verify signatures based on the different algorithms supported by the crypto API.'}, 'com_3': {'author': 'tmortagne', 'datetime': '05\/02\/2022, 09:10:05', 'body': \"In that case, I'm wondering if this should be cherry-picked in 13.10.x. WDYT @aubincleme ?\"}, 'com_4': {'author': 'aubincleme', 'datetime': '05\/02\/2022, 09:46:37', 'body': 'Yes why not ; doing it now'}, 'com_5': {'author': 'aubincleme', 'datetime': '05\/02\/2022, 09:48:21', 'body': 'done as part of a7c3628609f63b04de80935efa2e1f82e1356846 ;\\xa0updating issue + release notes'}}",
            "stats":"{'additions': 1, 'deletions': 1, 'total': 2}",
            "files":"{'xwiki-platform-core\/xwiki-platform-crypto\/xwiki-platform-crypto-script\/src\/main\/java\/org\/xwiki\/crypto\/script\/RSACryptoScriptService.java': {'additions': 1, 'deletions': 1, 'changes': 2, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/xwiki\/xwiki-platform\/raw\/26728f3f23658288683667a5182a916c7ecefc52\/xwiki-platform-core%2Fxwiki-platform-crypto%2Fxwiki-platform-crypto-script%2Fsrc%2Fmain%2Fjava%2Forg%2Fxwiki%2Fcrypto%2Fscript%2FRSACryptoScriptService.java', 'patch': '@@ -86,7 +86,7 @@ public class RSACryptoScriptService implements ScriptService\\n     private KeyPairGenerator keyPairGenerator;\\n \\n     @Inject\\n-    @Named(\"SHA1withRSAEncryption\")\\n+    @Named(\"SHA256withRSAEncryption\")\\n     private SignerFactory signerFactory;\\n \\n     @Inject'}}",
            "message_norm":"xwiki-19676: update the rsa crypto script service to use sha256 instead of sha1 for certificate signature",
            "language":"en",
            "entities":"[('update', 'ACTION', ''), ('rsa', 'SECWORD', ''), ('crypto', 'SECWORD', ''), ('certificate', 'SECWORD', ''), ('signature', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['xwiki-platform-core\/xwiki-platform-crypto\/xwiki-platform-crypto-script\/src\/main\/java\/org\/xwiki\/crypto\/script\/RSACryptoScriptService.java'])",
            "num_files":1.0
        },
        {
            "index":402,
            "vuln_id":"GHSA-4c4g-crqm-xrxw",
            "cwe_id":"{'CWE-908'}",
            "score":4.4,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/4a91f2069f7145aab6ba2d8cfe41be8a110c18a5', 'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/8933b8a21280696ab119b63263babdb54c298538', 'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/537bc7c723439b9194a358f64d871dd326c18887'}",
            "dataset":"osv",
            "summary":"Use of unitialized value in TFLite ### Impact\nAll TFLite operations that use quantization can be made to use unitialized values. [For example](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/460e000de3a83278fb00b61a16d161b1964f15f4\/tensorflow\/lite\/kernels\/depthwise_conv.cc#L198-L200):\n\n```cc\n    const auto* affine_quantization =\n        reinterpret_cast<TfLiteAffineQuantization*>(\n            filter->quantization.params);\n```\n\nThe issue stems from the fact that `quantization.params` is only valid if `quantization.type` is different that `kTfLiteNoQuantization`. However, these checks are missing in large parts of the code.\n\n### Patches\nWe have patched the issue in GitHub commits [537bc7c723439b9194a358f64d871dd326c18887](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/537bc7c723439b9194a358f64d871dd326c18887),\n[4a91f2069f7145aab6ba2d8cfe41be8a110c18a5](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/4a91f2069f7145aab6ba2d8cfe41be8a110c18a5) and [8933b8a21280696ab119b63263babdb54c298538](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/8933b8a21280696ab119b63263babdb54c298538).\n\nThe fix will be included in TensorFlow 2.6.0. We will also cherrypick this commit on TensorFlow 2.5.1, TensorFlow 2.4.3, and TensorFlow 2.3.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution \nThis vulnerability has been reported by members of the Aivul Team from Qihoo 360.",
            "published_date":"2021-08-25",
            "chain_len":3,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/537bc7c723439b9194a358f64d871dd326c18887",
            "commit_sha":"537bc7c723439b9194a358f64d871dd326c18887",
            "patch":"MULTI",
            "chain_ord":"['537bc7c723439b9194a358f64d871dd326c18887', '4a91f2069f7145aab6ba2d8cfe41be8a110c18a5', '8933b8a21280696ab119b63263babdb54c298538']",
            "before_first_fix_commit":"{'e35be978351a8578549d30b6f483825d36dc0f8b'}",
            "last_fix_commit":"8933b8a21280696ab119b63263babdb54c298538",
            "chain_ord_pos":1.0,
            "commit_datetime":"07\/16\/2021, 16:35:48",
            "message":"Fix a null pointer exception caused by branching on uninitialized data.\n\nThis is due to not checking that the params for the quantization exists. If there is no quantization, we should not access the `.params` field.\n\nPiperOrigin-RevId: 385163909\nChange-Id: I2beb8d50649b6542db224c163033fbcbaa49314f",
            "author":"Mihai Maruseac",
            "comments":null,
            "stats":"{'additions': 7, 'deletions': 0, 'total': 7}",
            "files":"{'tensorflow\/lite\/kernels\/svdf.cc': {'additions': 7, 'deletions': 0, 'changes': 7, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/537bc7c723439b9194a358f64d871dd326c18887\/tensorflow%2Flite%2Fkernels%2Fsvdf.cc', 'patch': '@@ -256,14 +256,21 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\\n                                                      output_temp_size_array));\\n \\n     \/\/ Calculate effective scales.\\n+    TF_LITE_ENSURE(context, input->quantization.type != kTfLiteNoQuantization);\\n     auto* input_params =\\n         reinterpret_cast<TfLiteAffineQuantization*>(input->quantization.params);\\n+    TF_LITE_ENSURE(context,\\n+                   weights_feature->quantization.type != kTfLiteNoQuantization);\\n     auto* weights_feature_params = reinterpret_cast<TfLiteAffineQuantization*>(\\n         weights_feature->quantization.params);\\n+    TF_LITE_ENSURE(context, state->quantization.type != kTfLiteNoQuantization);\\n     auto* state_params =\\n         reinterpret_cast<TfLiteAffineQuantization*>(state->quantization.params);\\n+    TF_LITE_ENSURE(context,\\n+                   weights_time->quantization.type != kTfLiteNoQuantization);\\n     auto* weight_time_params = reinterpret_cast<TfLiteAffineQuantization*>(\\n         weights_time->quantization.params);\\n+    TF_LITE_ENSURE(context, output->quantization.type != kTfLiteNoQuantization);\\n     auto* output_params = reinterpret_cast<TfLiteAffineQuantization*>(\\n         output->quantization.params);\\n     const double effective_scale_1 = input_params->scale->data[0] *'}}",
            "message_norm":"fix a null pointer exception caused by branching on uninitialized data.\n\nthis is due to not checking that the params for the quantization exists. if there is no quantization, we should not access the `.params` field.\n\npiperorigin-revid: 385163909\nchange-id: i2beb8d50649b6542db224c163033fbcbaa49314f",
            "language":"en",
            "entities":"[('fix', 'ACTION', ''), ('null pointer exception', 'SECWORD', ''), ('uninitialized', 'SECWORD', ''), ('385163909', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/lite\/kernels\/svdf.cc'])",
            "num_files":1.0
        },
        {
            "index":3365,
            "vuln_id":"GHSA-x4qx-4fjv-hmw6",
            "cwe_id":"{'CWE-190'}",
            "score":6.5,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/6f4d3e8139ec724dbbcb40505891c81dd1052c4a'}",
            "dataset":"osv",
            "summary":"Integer overflow leading to crash in Tensorflow ### Impact \nThe [implementation of `SparseCountSparseOutput`](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/5100e359aef5c8021f2e71c7b986420b85ce7b3d\/tensorflow\/core\/kernels\/count_ops.cc#L168-L273) can be made to crash a TensorFlow process by an integer overflow whose result is then used in a memory allocation:\n\n```python\nimport tensorflow as tf\nimport numpy as np\n    \ntf.raw_ops.SparseCountSparseOutput(\n  indices=[[1,1]],\n  values=[2],\n  dense_shape=[2 ** 31, 2 ** 32],\n  weights=[1],\n  binary_output=True,\n  minlength=-1,\n  maxlength=-1,\n  name=None)\n```\n\n### Patches\nWe have patched the issue in GitHub commit [6f4d3e8139ec724dbbcb40505891c81dd1052c4a](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/6f4d3e8139ec724dbbcb40505891c81dd1052c4a).\n\nThe fix will be included in TensorFlow 2.8.0. We will also cherrypick this commit on TensorFlow 2.7.1, TensorFlow 2.6.3, and TensorFlow 2.5.3, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by Faysal Hossain Shezan from University of Virginia.",
            "published_date":"2022-02-09",
            "chain_len":1,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/6f4d3e8139ec724dbbcb40505891c81dd1052c4a",
            "commit_sha":"6f4d3e8139ec724dbbcb40505891c81dd1052c4a",
            "patch":"SINGLE",
            "chain_ord":"['6f4d3e8139ec724dbbcb40505891c81dd1052c4a']",
            "before_first_fix_commit":"{'adbbabdb0d3abb3cdeac69e38a96de1d678b24b3'}",
            "last_fix_commit":"6f4d3e8139ec724dbbcb40505891c81dd1052c4a",
            "chain_ord_pos":1.0,
            "commit_datetime":"12\/08\/2021, 04:04:02",
            "message":"Prevent crash due to integer overflow followed by allocating negative sized array.\n\nPiperOrigin-RevId: 414891322\nChange-Id: I5df390e0dc1d9f115209293708950cdf9306931c",
            "author":"Mihai Maruseac",
            "comments":null,
            "stats":"{'additions': 9, 'deletions': 0, 'total': 9}",
            "files":"{'tensorflow\/core\/kernels\/count_ops.cc': {'additions': 9, 'deletions': 0, 'changes': 9, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/6f4d3e8139ec724dbbcb40505891c81dd1052c4a\/tensorflow%2Fcore%2Fkernels%2Fcount_ops.cc', 'patch': '@@ -13,6 +13,8 @@ See the License for the specific language governing permissions and\\n limitations under the License.\\n ==============================================================================*\/\\n \\n+#include <limits>\\n+\\n #include \"absl\/container\/flat_hash_map.h\"\\n #include \"tensorflow\/core\/framework\/op_kernel.h\"\\n #include \"tensorflow\/core\/framework\/op_requires.h\"\\n@@ -23,6 +25,9 @@ limitations under the License.\\n \\n namespace tensorflow {\\n \\n+\/\/ Don\\'t allocate too large `BatchedMap<T>` objects\\n+static int kMaxBatches = std::numeric_limits<int>::max();\\n+\\n template <class T>\\n using BatchedMap = std::vector<absl::flat_hash_map<int64_t, T>>;\\n \\n@@ -235,6 +240,10 @@ class SparseCount : public OpKernel {\\n \\n     bool is_1d = shape.NumElements() == 1;\\n     int num_batches = is_1d ? 1 : shape_vector(0);\\n+    OP_REQUIRES(\\n+        context, 0 < num_batches && num_batches < kMaxBatches,\\n+        errors::InvalidArgument(\"Cannot allocate \", num_batches,\\n+                                \" batches, is the dense shape too wide?\"));\\n \\n     const auto values_values = values.flat<T>();\\n     const auto weight_values = weights.flat<W>();'}}",
            "message_norm":"prevent crash due to integer overflow followed by allocating negative sized array.\n\npiperorigin-revid: 414891322\nchange-id: i5df390e0dc1d9f115209293708950cdf9306931c",
            "language":"en",
            "entities":"[('prevent', 'ACTION', ''), ('integer overflow', 'SECWORD', ''), ('414891322', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/core\/kernels\/count_ops.cc'])",
            "num_files":1.0
        },
        {
            "index":1492,
            "vuln_id":"GHSA-c5r5-7pfh-6qg6",
            "cwe_id":"{'CWE-78'}",
            "score":9.8,
            "chain":"{'https:\/\/github.com\/inukshuk\/bibtex-ruby\/commit\/14406f4460f4e1ecabd25ca94f809b3ea7c5fb11'}",
            "dataset":"osv",
            "summary":"OS command injection in BibTeX-Ruby BibTeX-ruby before 5.1.0 allows shell command injection due to unsanitized user input being passed directly to the built-in Ruby Kernel.open method through BibTeX.open.",
            "published_date":"2020-02-14",
            "chain_len":1,
            "project":"https:\/\/github.com\/inukshuk\/bibtex-ruby",
            "commit_href":"https:\/\/github.com\/inukshuk\/bibtex-ruby\/commit\/14406f4460f4e1ecabd25ca94f809b3ea7c5fb11",
            "commit_sha":"14406f4460f4e1ecabd25ca94f809b3ea7c5fb11",
            "patch":"SINGLE",
            "chain_ord":"['14406f4460f4e1ecabd25ca94f809b3ea7c5fb11']",
            "before_first_fix_commit":"{'707b9303e4ed9a7e136dd1268e21d73d5faab817'}",
            "last_fix_commit":"14406f4460f4e1ecabd25ca94f809b3ea7c5fb11",
            "chain_ord_pos":1.0,
            "commit_datetime":"01\/17\/2020, 13:34:37",
            "message":"Use File.read instead of Kernel.open\n\nTo avoid command injection with | strings",
            "author":"Sylvester Keil",
            "comments":null,
            "stats":"{'additions': 1, 'deletions': 1, 'total': 2}",
            "files":"{'lib\/bibtex\/bibliography.rb': {'additions': 1, 'deletions': 1, 'changes': 2, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/inukshuk\/bibtex-ruby\/raw\/14406f4460f4e1ecabd25ca94f809b3ea7c5fb11\/lib%2Fbibtex%2Fbibliography.rb', 'patch': \"@@ -47,7 +47,7 @@ class << self\\n       # -:filter: convert all entries using the sepcified filter (not set by default)\\n       #\\n       def open(path, options = {})\\n-        b = parse(Kernel.open(path, 'r:UTF-8').read, options)\\n+        b = parse(File.read(path), options)\\n         b.path = path\\n         return b unless block_given?\"}}",
            "message_norm":"use file.read instead of kernel.open\n\nto avoid command injection with | strings",
            "language":"en",
            "entities":"[('command injection', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['lib\/bibtex\/bibliography.rb'])",
            "num_files":1.0
        },
        {
            "index":1355,
            "vuln_id":"GHSA-9c84-4hx6-xmm4",
            "cwe_id":"{'CWE-190'}",
            "score":6.3,
            "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/4253f96a58486ffe84b61c0415bb234a4632ee73'}",
            "dataset":"osv",
            "summary":"Integer overflow in TFLite concatentation ### Impact\nThe TFLite implementation of concatenation is [vulnerable to an integer overflow issue](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/7b7352a724b690b11bfaae2cd54bc3907daf6285\/tensorflow\/lite\/kernels\/concatenation.cc#L70-L76):\n\n```cc\nfor (int d = 0; d < t0->dims->size; ++d) {\n  if (d == axis) { \n    sum_axis += t->dims->data[axis]; \n  } else {\n    TF_LITE_ENSURE_EQ(context, t->dims->data[d], t0->dims->data[d]);\n  }\n}\n```\n\nAn attacker can craft a model such that the dimensions of one of the concatenation input overflow the values of `int`. TFLite uses `int` to represent tensor dimensions, whereas TF uses `int64`. Hence, valid TF models can trigger an integer overflow when converted to TFLite format.\n\n### Patches\nWe have patched the issue in GitHub commit [4253f96a58486ffe84b61c0415bb234a4632ee73](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/4253f96a58486ffe84b61c0415bb234a4632ee73).\n\nThe fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by members of the Aivul Team from Qihoo 360.",
            "published_date":"2021-05-21",
            "chain_len":1,
            "project":"https:\/\/github.com\/tensorflow\/tensorflow",
            "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/4253f96a58486ffe84b61c0415bb234a4632ee73",
            "commit_sha":"4253f96a58486ffe84b61c0415bb234a4632ee73",
            "patch":"SINGLE",
            "chain_ord":"['4253f96a58486ffe84b61c0415bb234a4632ee73']",
            "before_first_fix_commit":"{'7b7352a724b690b11bfaae2cd54bc3907daf6285'}",
            "last_fix_commit":"4253f96a58486ffe84b61c0415bb234a4632ee73",
            "chain_ord_pos":1.0,
            "commit_datetime":"04\/28\/2021, 23:50:55",
            "message":"Fix integer overflow in TFLite concat\n\nPiperOrigin-RevId: 371013841\nChange-Id: I6a4782ce7ca753e23ff31e7fb6aeb7f9d412cd29",
            "author":"Mihai Maruseac",
            "comments":null,
            "stats":"{'additions': 6, 'deletions': 0, 'total': 6}",
            "files":"{'tensorflow\/lite\/kernels\/concatenation.cc': {'additions': 6, 'deletions': 0, 'changes': 6, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/4253f96a58486ffe84b61c0415bb234a4632ee73\/tensorflow%2Flite%2Fkernels%2Fconcatenation.cc', 'patch': '@@ -16,6 +16,8 @@ limitations under the License.\\n \\n #include <stdint.h>\\n \\n+#include <limits>\\n+\\n #include \"tensorflow\/lite\/c\/builtin_op_data.h\"\\n #include \"tensorflow\/lite\/c\/common.h\"\\n #include \"tensorflow\/lite\/kernels\/internal\/compatibility.h\"\\n@@ -69,6 +71,10 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\\n     TF_LITE_ENSURE_EQ(context, t->type, input_type);\\n     for (int d = 0; d < t0->dims->size; ++d) {\\n       if (d == axis) {\\n+        \/\/ Avoid integer overflow in sum_axis below\\n+        TF_LITE_ENSURE(context, t->dims->data[axis] >= 0);\\n+        TF_LITE_ENSURE(context, t->dims->data[axis] <=\\n+                                    std::numeric_limits<int>::max() - sum_axis);\\n         sum_axis += t->dims->data[axis];\\n       } else {\\n         TF_LITE_ENSURE_EQ(context, t->dims->data[d], t0->dims->data[d]);'}}",
            "message_norm":"fix integer overflow in tflite concat\n\npiperorigin-revid: 371013841\nchange-id: i6a4782ce7ca753e23ff31e7fb6aeb7f9d412cd29",
            "language":"en",
            "entities":"[('fix', 'ACTION', ''), ('integer overflow', 'SECWORD', ''), ('371013841', 'SHA', 'generic_sha')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['tensorflow\/lite\/kernels\/concatenation.cc'])",
            "num_files":1.0
        },
        {
            "index":296,
            "vuln_id":"GHSA-3pxp-6963-46r9",
            "cwe_id":"{'CWE-77'}",
            "score":0.0,
            "chain":"{'https:\/\/github.com\/fagbokforlaget\/pdfinfojs\/commit\/5cc59cd8aa13ca8d16bb41da8affdfef370ad4fd'}",
            "dataset":"osv",
            "summary":"Command Injection in pdfinfojs Versions of `pdfinfojs` before 0.4.1 are vulnerable to command injection. This is exploitable if an attacker can control the filename parameter that is passed into the `pdfinfojs` constructor.\n\n\n## Recommendation\n\nUpdate to version 0.4.1 or later.",
            "published_date":"2018-06-07",
            "chain_len":1,
            "project":"https:\/\/github.com\/fagbokforlaget\/pdfinfojs",
            "commit_href":"https:\/\/github.com\/fagbokforlaget\/pdfinfojs\/commit\/5cc59cd8aa13ca8d16bb41da8affdfef370ad4fd",
            "commit_sha":"5cc59cd8aa13ca8d16bb41da8affdfef370ad4fd",
            "patch":"SINGLE",
            "chain_ord":"['5cc59cd8aa13ca8d16bb41da8affdfef370ad4fd']",
            "before_first_fix_commit":"{'379c0e14db8298b465269653b9ac308d83eb6d4e'}",
            "last_fix_commit":"5cc59cd8aa13ca8d16bb41da8affdfef370ad4fd",
            "chain_ord_pos":1.0,
            "commit_datetime":"04\/13\/2018, 11:36:11",
            "message":"fix: command injection vulnerability",
            "author":"Deepak Thukral",
            "comments":null,
            "stats":"{'additions': 4, 'deletions': 4, 'total': 8}",
            "files":"{'lib\/pdfinfo.js': {'additions': 4, 'deletions': 4, 'changes': 8, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/fagbokforlaget\/pdfinfojs\/raw\/5cc59cd8aa13ca8d16bb41da8affdfef370ad4fd\/lib%2Fpdfinfo.js', 'patch': '@@ -1,9 +1,9 @@\\n-const { exec, execSync } = require(\\'child_process\\');\\n+const { execFile, execFileSync } = require(\\'child_process\\');\\n const utils = require(\\'.\/utils\\');\\n \\n function pdfinfo (filename, options) {\\n   this.options = options || {};\\n-  this.options.additional = [\\'\"\\' + filename + \\'\"\\'];\\n+  this.options.additional = [filename];\\n \\n   pdfinfo.prototype.add_options = function(optionArray) {\\n     if (typeof optionArray.length !== undefined) {\\n@@ -23,7 +23,7 @@ function pdfinfo (filename, options) {\\n   pdfinfo.prototype.getInfoSync = function() {\\n     const self = this;\\n     try {\\n-    \\tlet data = execSync(\\'pdfinfo \\' + self.options.additional.join(\\' \\')).toString(\\'utf8\\');\\n+    \\tlet data = execFileSync(\\'pdfinfo\\', self.options.additional).toString(\\'utf8\\');\\n         return utils.parse(data);\\n     } catch(err) {\\n         throw new Error(\"pdfinfo error: \"+ err.msg);\\n@@ -33,7 +33,7 @@ function pdfinfo (filename, options) {\\n \\n   pdfinfo.prototype.getInfo = function(cb) {\\n     let self = this;\\n-    let child = exec(\\'pdfinfo \\' + self.options.additional.join(\\' \\'), function(error, stdout, stderr) {\\n+    let child = execFile(\\'pdfinfo\\', self.options.additional, (error, stdout, stderr) => {\\n       if (!error) {\\n         let data = utils.parse(stdout);\\n         if (cb && typeof cb === \"function\") {'}}",
            "message_norm":"fix: command injection vulnerability",
            "language":"en",
            "entities":"[('command injection', 'SECWORD', ''), ('vulnerability', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['lib\/pdfinfo.js'])",
            "num_files":1.0
        },
        {
            "index":29,
            "vuln_id":"GHSA-257v-vj4p-3w2h",
            "cwe_id":"{'CWE-770'}",
            "score":5.3,
            "chain":"{'https:\/\/github.com\/Qix-\/color-string\/commit\/0789e21284c33d89ebc4ab4ca6f759b9375ac9d3'}",
            "dataset":"osv",
            "summary":"Regular Expression Denial of Service (ReDOS) In the npm package `color-string`, there is a  ReDos (Regular Expression Denial of Service) vulnerability regarding an exponential time complexity for\nlinearly increasing input lengths for `hwb()` color strings.\n\nStrings reaching more than 5000 characters would see several\nmilliseconds of processing time; strings reaching more than\n50,000 characters began seeing 1500ms (1.5s) of processing time.\n\nThe cause was due to a the regular expression that parses\nhwb() strings - specifically, the hue value - where\nthe integer portion of the hue value used a 0-or-more quantifier\nshortly thereafter followed by a 1-or-more quantifier.\n\nThis caused excessive backtracking and a cartesian scan,\nresulting in exponential time complexity given a linear\nincrease in input length.",
            "published_date":"2021-06-22",
            "chain_len":1,
            "project":"https:\/\/github.com\/Qix-\/color-string",
            "commit_href":"https:\/\/github.com\/Qix-\/color-string\/commit\/0789e21284c33d89ebc4ab4ca6f759b9375ac9d3",
            "commit_sha":"0789e21284c33d89ebc4ab4ca6f759b9375ac9d3",
            "patch":"SINGLE",
            "chain_ord":"['0789e21284c33d89ebc4ab4ca6f759b9375ac9d3']",
            "before_first_fix_commit":"{'60f3f66477a298589288e3df6e895f88e6cd8e8e'}",
            "last_fix_commit":"0789e21284c33d89ebc4ab4ca6f759b9375ac9d3",
            "chain_ord_pos":1.0,
            "commit_datetime":"03\/05\/2021, 17:48:41",
            "message":"fix ReDos in hwb() parser (low-severity)\n\nDiscovered by Yeting Li, c\/o Colin Ife via Snyk.io.\n\nA ReDos (Regular Expression Denial of Service) vulnerability\nwas responsibly disclosed to me via email by Colin on\nMar 5 2021 regarding an exponential time complexity for\nlinearly increasing input lengths for `hwb()` color strings.\n\nStrings reaching more than 5000 characters would see several\nmilliseconds of processing time; strings reaching more than\n50,000 characters began seeing 1500ms (1.5s) of processing time.\n\nThe cause was due to a the regular expression that parses\nhwb() strings - specifically, the hue value - where\nthe integer portion of the hue value used a 0-or-more quantifier\nshortly thereafter followed by a 1-or-more quantifier.\n\nThis caused excessive backtracking and a cartesian scan,\nresulting in exponential time complexity given a linear\nincrease in input length.\n\nThank you Yeting Li and Colin Ife for bringing this to my\nattention in a secure, responsible and professional manner.\n\nA CVE will not be assigned for this vulnerability.",
            "author":"Josh Junon",
            "comments":null,
            "stats":"{'additions': 2, 'deletions': 2, 'total': 4}",
            "files":"{'index.js': {'additions': 2, 'deletions': 2, 'changes': 4, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/Qix-\/color-string\/raw\/0789e21284c33d89ebc4ab4ca6f759b9375ac9d3\/index.js', 'patch': '@@ -129,7 +129,7 @@ cs.get.hsl = function (string) {\\n \\t\\treturn null;\\n \\t}\\n \\n-\\tvar hsl = \/^hsla?\\\\(\\\\s*([+-]?(?:\\\\d*\\\\.)?\\\\d+)(?:deg)?\\\\s*,\\\\s*([+-]?[\\\\d\\\\.]+)%\\\\s*,\\\\s*([+-]?[\\\\d\\\\.]+)%\\\\s*(?:,\\\\s*([+-]?[\\\\d\\\\.]+)\\\\s*)?\\\\)$\/;\\n+\\tvar hsl = \/^hsla?\\\\(\\\\s*([+-]?(?:\\\\d{0,3}\\\\.)?\\\\d+)(?:deg)?\\\\s*,\\\\s*([+-]?[\\\\d\\\\.]+)%\\\\s*,\\\\s*([+-]?[\\\\d\\\\.]+)%\\\\s*(?:,\\\\s*([+-]?[\\\\d\\\\.]+)\\\\s*)?\\\\)$\/;\\n \\tvar match = string.match(hsl);\\n \\n \\tif (match) {\\n@@ -150,7 +150,7 @@ cs.get.hwb = function (string) {\\n \\t\\treturn null;\\n \\t}\\n \\n-\\tvar hwb = \/^hwb\\\\(\\\\s*([+-]?\\\\d*[\\\\.]?\\\\d+)(?:deg)?\\\\s*,\\\\s*([+-]?[\\\\d\\\\.]+)%\\\\s*,\\\\s*([+-]?[\\\\d\\\\.]+)%\\\\s*(?:,\\\\s*([+-]?[\\\\d\\\\.]+)\\\\s*)?\\\\)$\/;\\n+\\tvar hwb = \/^hwb\\\\(\\\\s*([+-]?\\\\d{0,3}(?:\\\\.\\\\d+)?)(?:deg)?\\\\s*,\\\\s*([+-]?[\\\\d\\\\.]+)%\\\\s*,\\\\s*([+-]?[\\\\d\\\\.]+)%\\\\s*(?:,\\\\s*([+-]?[\\\\d\\\\.]+)\\\\s*)?\\\\)$\/;\\n \\tvar match = string.match(hwb);\\n \\n \\tif (match) {'}}",
            "message_norm":"fix redos in hwb() parser (low-severity)\n\ndiscovered by yeting li, c\/o colin ife via snyk.io.\n\na redos (regular expression denial of service) vulnerability\nwas responsibly disclosed to me via email by colin on\nmar 5 2021 regarding an exponential time complexity for\nlinearly increasing input lengths for `hwb()` color strings.\n\nstrings reaching more than 5000 characters would see several\nmilliseconds of processing time; strings reaching more than\n50,000 characters began seeing 1500ms (1.5s) of processing time.\n\nthe cause was due to a the regular expression that parses\nhwb() strings - specifically, the hue value - where\nthe integer portion of the hue value used a 0-or-more quantifier\nshortly thereafter followed by a 1-or-more quantifier.\n\nthis caused excessive backtracking and a cartesian scan,\nresulting in exponential time complexity given a linear\nincrease in input length.\n\nthank you yeting li and colin ife for bringing this to my\nattention in a secure, responsible and professional manner.\n\na cve will not be assigned for this vulnerability.",
            "language":"en",
            "entities":"[('fix', 'ACTION', ''), ('redos', 'SECWORD', ''), ('low', 'SEVERITY', ''), ('redos', 'SECWORD', ''), ('denial of service', 'SECWORD', ''), ('vulnerability', 'SECWORD', ''), ('secure', 'SECWORD', ''), ('cve', 'SECWORD', ''), ('vulnerability', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['index.js'])",
            "num_files":1.0
        },
        {
            "index":3252,
            "vuln_id":"GHSA-w8rc-pgxq-x2cj",
            "cwe_id":"{'CWE-20'}",
            "score":6.5,
            "chain":"{'https:\/\/github.com\/shopizer-ecommerce\/shopizer\/commit\/929ca0839a80c6f4dad087e0259089908787ad2a'}",
            "dataset":"osv",
            "summary":"Negative charge in shopping cart in Shopizer ### Impact\nUsing API or Controller based versions negative quantity is not adequately validated hence creating incorrect shopping cart and order total. \n\n### Patches\nAdding a back-end verification to check that quantity parameter isn't negative. If so, it is set to 1. Patched in 2.11.0\n\n### Workarounds\nWithout uprading, it's possible to just apply the fixes in the same files it's done for the patch. Or you use javax constraint validation on the quantity parameter.\n\n### References\n[Input Validation](https:\/\/cheatsheetseries.owasp.org\/cheatsheets\/Input_Validation_Cheat_Sheet.html)\n[Using bean validation constraint](https:\/\/javaee.github.io\/tutorial\/bean-validation002.html)\n[Commits with fixes](https:\/\/github.com\/shopizer-ecommerce\/shopizer\/commit\/929ca0839a80c6f4dad087e0259089908787ad2a)\nCVE Details below : \n[Mitre](https:\/\/cve.mitre.org\/cgi-bin\/cvename.cgi?name=CVE-2020-11007)\n[NVD](https:\/\/nvd.nist.gov\/vuln\/detail\/CVE-2020-11007)\n\n### Credits\nFound and solved by Yannick Gosset from Aix-Marseille University cybersecurity\nmaster program supervised by Yassine Ilmi",
            "published_date":"2020-04-22",
            "chain_len":1,
            "project":"https:\/\/github.com\/shopizer-ecommerce\/shopizer",
            "commit_href":"https:\/\/github.com\/shopizer-ecommerce\/shopizer\/commit\/929ca0839a80c6f4dad087e0259089908787ad2a",
            "commit_sha":"929ca0839a80c6f4dad087e0259089908787ad2a",
            "patch":"SINGLE",
            "chain_ord":"['929ca0839a80c6f4dad087e0259089908787ad2a']",
            "before_first_fix_commit":"{'de8a8e3183f8c5fed4695f889e309a6fff70adae', '6858049b39bdc51b71e6419b7c4bba1347737cb7'}",
            "last_fix_commit":"929ca0839a80c6f4dad087e0259089908787ad2a",
            "chain_ord_pos":1.0,
            "commit_datetime":"04\/10\/2020, 13:35:12",
            "message":"Merge pull request from GHSA-w8rc-pgxq-x2cj\n\nFixing negative charge vulnerability",
            "author":"Shopizer",
            "comments":null,
            "stats":"{'additions': 5, 'deletions': 7, 'total': 12}",
            "files":"{'sm-shop\/src\/main\/java\/com\/salesmanager\/shop\/store\/controller\/shoppingCart\/facade\/ShoppingCartFacadeImpl.java': {'additions': 5, 'deletions': 7, 'changes': 12, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/shopizer-ecommerce\/shopizer\/raw\/929ca0839a80c6f4dad087e0259089908787ad2a\/sm-shop%2Fsrc%2Fmain%2Fjava%2Fcom%2Fsalesmanager%2Fshop%2Fstore%2Fcontroller%2FshoppingCart%2Ffacade%2FShoppingCartFacadeImpl.java', 'patch': '@@ -108,7 +108,7 @@ public ShoppingCartData addItemsToShoppingCart( final ShoppingCartData shoppingC\\n     {\\n \\n         ShoppingCart cartModel = null;\\n-        \\n+        if(item.getQuantity() < 1) item.setQuantity(1);\\n         \/**\\n          * Sometimes a user logs in and a shopping cart is present in db (shoppingCartData\\n          * but ui has no cookie with shopping cart code so the cart code will have\\n@@ -216,7 +216,7 @@ private com.salesmanager.core.model.shoppingcart.ShoppingCartItem createCartItem\\n         }\\n         \\t\\n         for(ProductAvailability availability : availabilities) {\\n-        \\tif(availability.getProductQuantity() == null || availability.getProductQuantity().intValue() ==0) {\\n+        \\tif(availability.getProductQuantity() == null || availability.getProductQuantity().intValue() <= 0) {\\n                 throw new Exception( \"Item with id \" + product.getId() + \" is not available\");\\n         \\t}\\n         }\\n@@ -288,7 +288,7 @@ private com.salesmanager.core.model.shoppingcart.ShoppingCartItem createCartItem\\n         }\\n         \\t\\n         for(ProductAvailability availability : availabilities) {\\n-        \\tif(availability.getProductQuantity() == null || availability.getProductQuantity().intValue() ==0) {\\n+        \\tif(availability.getProductQuantity() == null || availability.getProductQuantity().intValue() <= 0) {\\n                 throw new Exception( \"Item with id \" + product.getId() + \" is not available\");\\n         \\t}\\n         }\\n@@ -554,8 +554,7 @@ public ShoppingCartData updateCartItem( final Long itemID, final String cartId,\\n         return null;\\n     }\\n     \\n-    @SuppressWarnings(\"unchecked\")\\n-\\t@Override\\n+    @Override\\n     public ShoppingCartData updateCartItems( final List<ShoppingCartItem> shoppingCartItems, final MerchantStore store, final Language language )\\n             throws Exception\\n         {\\n@@ -720,7 +719,6 @@ public ReadableShoppingCart addToCart(PersistableShoppingCartItem item, Merchant\\n \\t}\\n \\t\\n \\n-\\t@SuppressWarnings(\"unchecked\")\\n \\t@Override\\n \\tpublic void removeShoppingCartItem(String cartCode, Long productId,\\n \\t      MerchantStore merchant, Language language) throws Exception {\\n@@ -914,7 +912,7 @@ public ReadableShoppingCart addToCart(Customer customer, PersistableShoppingCart\\n \\t\\t\\n \\t\\tValidate.notNull(customer,\"Customer cannot be null\");\\n \\t\\tValidate.notNull(customer.getId(),\"Customer.id cannot be null or empty\");\\n-\\t\\t\\n+\\t\\tif(item.getQuantity() < 1) item.setQuantity(1);\\n \\t\\t\/\/Check if customer has an existing shopping cart\\n \\t\\tShoppingCart cartModel = shoppingCartService.getByCustomer(customer);'}}",
            "message_norm":"merge pull request from ghsa-w8rc-pgxq-x2cj\n\nfixing negative charge vulnerability",
            "language":"en",
            "entities":"[('ghsa-w8rc-pgxq-x2cj', 'VULNID', 'GHSA'), ('fixing', 'ACTION', ''), ('vulnerability', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['sm-shop\/src\/main\/java\/com\/salesmanager\/shop\/store\/controller\/shoppingCart\/facade\/ShoppingCartFacadeImpl.java'])",
            "num_files":1.0
        },
        {
            "index":3484,
            "vuln_id":"GHSA-xr38-w74q-r8jv",
            "cwe_id":"{'CWE-862', 'CWE-863'}",
            "score":6.4,
            "chain":"{'https:\/\/github.com\/inveniosoftware\/invenio-drafts-resources\/commit\/039b0cff1ad4b952000f4d8c3a93f347108b6626'}",
            "dataset":"osv",
            "summary":"Permissions not properly checked in Invenio-Drafts-Resources ### Impact\n\nInvenio-Drafts-Resources does not properly check permissions when a record is published. The vulnerability is exploitable in a default installation of InvenioRDM. An authenticated user is able via REST API calls to publish draft records of other users if they know the record identifier and the draft validates (e.g. all require fields filled out). An attacker is not able to modify the data in the record, and thus e.g. *cannot* change a record from restricted to public.\n\n### Details\n\nThe service's ``publish()`` method contains the following permission check:\n\n```python\ndef publish(..):\n    self.require_permission(identity, \"publish\")\n```\nHowever, the record should have been passed into the permission check so that the need generators have access to e.g. the record owner.\n\n```python\ndef publish(..):\n    self.require_permission(identity, \"publish\", record=record)\n```\nThe bug is activated in Invenio-RDM-Records which has a need generator called ``RecordOwners()``, which when no record is passed in defaults to allow any authenticated user:\n\n```python\nclass RecordOwners(Generator):\n    def needs(self, record=None, **kwargs):\n        if record is None:\n            return [authenticated_user]\n    # ...\n```\n\n### Patches\n\nThe problem is patched in Invenio-Drafts-Resources v0.13.7 and 0.14.6+, which is part of InvenioRDM v6.0.1 and InvenioRDM v7.0 respectively.\n\nYou can verify the version installed of Invenio-Drafts-Resources via PIP:\n\n```console\ncd ~\/src\/my-site\npipenv run pip freeze | grep invenio-drafts-resources\n```\n\n### References\n\n- [Security policy](https:\/\/invenio.readthedocs.io\/en\/latest\/community\/security-policy.html)\n\n### For more information\n\nIf you have any questions or comments about this advisory:\n* Chat with us on Discord: https:\/\/discord.gg\/8qatqBC",
            "published_date":"2021-12-06",
            "chain_len":1,
            "project":"https:\/\/github.com\/inveniosoftware\/invenio-drafts-resources",
            "commit_href":"https:\/\/github.com\/inveniosoftware\/invenio-drafts-resources\/commit\/039b0cff1ad4b952000f4d8c3a93f347108b6626",
            "commit_sha":"039b0cff1ad4b952000f4d8c3a93f347108b6626",
            "patch":"SINGLE",
            "chain_ord":"['039b0cff1ad4b952000f4d8c3a93f347108b6626']",
            "before_first_fix_commit":"{'998ede99c377c84f11fe22c07c20f90c88c463dc'}",
            "last_fix_commit":"039b0cff1ad4b952000f4d8c3a93f347108b6626",
            "chain_ord_pos":1.0,
            "commit_datetime":"11\/24\/2021, 14:32:41",
            "message":"security: fix missing permission check of publish\n\n* Invenio-Drafts-Resources does not properly check permissions when a\n  record is published. The vulnerability is exploitable in a default\n  installation of InvenioRDM. An authenticated a user is able via REST\n  API calls to publish draft records of other users if they know the\n  record identifier and the draft validates (e.g. all require fields\n  filled out). An attacker is not able to modify the data in the record,\n  and thus e.g. cannot change a record from restricted to public.",
            "author":"Lars Holm Nielsen",
            "comments":null,
            "stats":"{'additions': 1, 'deletions': 2, 'total': 3}",
            "files":"{'invenio_drafts_resources\/services\/records\/service.py': {'additions': 1, 'deletions': 2, 'changes': 3, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/inveniosoftware\/invenio-drafts-resources\/raw\/039b0cff1ad4b952000f4d8c3a93f347108b6626\/invenio_drafts_resources%2Fservices%2Frecords%2Fservice.py', 'patch': '@@ -267,10 +267,9 @@ def publish(self, id_, identity, uow=None):\\n               into records)\\n             - Create or update associated (published) record with data\\n         \"\"\"\\n-        self.require_permission(identity, \"publish\")\\n-\\n         # Get the draft\\n         draft = self.draft_cls.pid.resolve(id_, registered_only=False)\\n+        self.require_permission(identity, \"publish\", record=draft)\\n \\n         # Validate the draft strictly - since a draft can be saved with errors\\n         # we do a strict validation here to make sure only valid drafts can be'}}",
            "message_norm":"security: fix missing permission check of publish\n\n* invenio-drafts-resources does not properly check permissions when a\n  record is published. the vulnerability is exploitable in a default\n  installation of inveniordm. an authenticated a user is able via rest\n  api calls to publish draft records of other users if they know the\n  record identifier and the draft validates (e.g. all require fields\n  filled out). an attacker is not able to modify the data in the record,\n  and thus e.g. cannot change a record from restricted to public.",
            "language":"en",
            "entities":"[('security', 'SECWORD', ''), ('fix', 'ACTION', ''), ('permission', 'SECWORD', ''), ('permissions', 'SECWORD', ''), ('vulnerability', 'SECWORD', ''), ('exploitable', 'SECWORD', ''), ('validates', 'ACTION', ''), ('attacker', 'FLAW', ''), ('change', 'ACTION', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['invenio_drafts_resources\/services\/records\/service.py'])",
            "num_files":1.0
        },
        {
            "index":1870,
            "vuln_id":"GHSA-gfh2-7jg5-653p",
            "cwe_id":"{'CWE-835'}",
            "score":4.0,
            "chain":"{'https:\/\/github.com\/appc\/docker2aci\/pull\/204\/commits\/54331ec7020e102935c31096f336d31f6400064f'}",
            "dataset":"osv",
            "summary":"Denial of Service in docker2aci docker2aci <= 0.12.3 has an infinite loop when handling local images with cyclic dependency chain.",
            "published_date":"2022-02-15",
            "chain_len":1,
            "project":"https:\/\/github.com\/appc\/docker2aci",
            "commit_href":"https:\/\/github.com\/appc\/docker2aci\/pull\/204\/commits\/54331ec7020e102935c31096f336d31f6400064f",
            "commit_sha":"54331ec7020e102935c31096f336d31f6400064f",
            "patch":"SINGLE",
            "chain_ord":"['54331ec7020e102935c31096f336d31f6400064f']",
            "before_first_fix_commit":"{'8a4173c3067a557fba64a03c6efac613dfbba6ac'}",
            "last_fix_commit":"54331ec7020e102935c31096f336d31f6400064f",
            "chain_ord_pos":1.0,
            "commit_datetime":"10\/10\/2016, 13:23:55",
            "message":"backend\/file: fix an infinite loop in deps walking (CVE-2016-8579)\n\nThis commit fixes a possible infinite loop while traversing\nthe dependency ancestry of a malformed local image file.\n\nThis has been assigned CVE-2016-8579:\nhttps:\/\/github.com\/appc\/docker2aci\/issues\/203#issuecomment-253494006",
            "author":"Luca Bruno",
            "comments":null,
            "stats":"{'additions': 11, 'deletions': 0, 'total': 11}",
            "files":"{'lib\/internal\/backend\/file\/file.go': {'additions': 11, 'deletions': 0, 'changes': 11, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/appc\/docker2aci\/raw\/54331ec7020e102935c31096f336d31f6400064f\/lib%2Finternal%2Fbackend%2Ffile%2Ffile.go', 'patch': '@@ -279,14 +279,24 @@ func extractEmbeddedLayer(file *os.File, layerID string, outputPath string) (*os\\n \\treturn layerFile, nil\\n }\\n \\n+\/\/ getAncestry computes an image ancestry, returning an ordered list\\n+\/\/ of dependencies starting from the topmost image to the base.\\n+\/\/ It checks for dependency loops via duplicate detection in the image\\n+\/\/ chain and errors out in such cases.\\n func getAncestry(file *os.File, imgID string) ([]string, error) {\\n \\tvar ancestry []string\\n+\\tdeps := make(map[string]bool)\\n \\n \\tcurImgID := imgID\\n \\n \\tvar err error\\n \\tfor curImgID != \"\" {\\n+\\t\\tif deps[curImgID] {\\n+\\t\\t\\treturn nil, fmt.Errorf(\"dependency loop detected at image %q\", curImgID)\\n+\\t\\t}\\n+\\t\\tdeps[curImgID] = true\\n \\t\\tancestry = append(ancestry, curImgID)\\n+\\t\\tlog.Debug(fmt.Sprintf(\"Getting ancestry for layer %q\", curImgID))\\n \\t\\tcurImgID, err = getParent(file, curImgID)\\n \\t\\tif err != nil {\\n \\t\\t\\treturn nil, err\\n@@ -328,5 +338,6 @@ func getParent(file *os.File, imgID string) (string, error) {\\n \\t\\treturn \"\", err\\n \\t}\\n \\n+\\tlog.Debug(fmt.Sprintf(\"Layer %q depends on layer %q\", imgID, parent))\\n \\treturn parent, nil\\n }'}}",
            "message_norm":"backend\/file: fix an infinite loop in deps walking (cve-2016-8579)\n\nthis commit fixes a possible infinite loop while traversing\nthe dependency ancestry of a malformed local image file.\n\nthis has been assigned cve-2016-8579:\nhttps:\/\/github.com\/appc\/docker2aci\/issues\/203#issuecomment-253494006",
            "language":"en",
            "entities":"[('fix', 'ACTION', ''), ('infinite loop', 'SECWORD', ''), ('cve-2016-8579', 'VULNID', 'CVE'), ('fixes', 'ACTION', ''), ('infinite loop', 'SECWORD', ''), ('cve-2016-8579', 'VULNID', 'CVE'), ('https:\/\/github.com\/appc\/docker2aci\/issues\/203#issuecomment-253494006', 'URL', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['lib\/internal\/backend\/file\/file.go'])",
            "num_files":1.0
        },
        {
            "index":1834,
            "vuln_id":"GHSA-g6ww-v8xp-vmwg",
            "cwe_id":"{'CWE-1321', 'CWE-20'}",
            "score":7.2,
            "chain":"{'https:\/\/github.com\/chaijs\/pathval\/pull\/58\/commits\/21a9046cfa0c2697cb41990f3b4316db410e6c8a'}",
            "dataset":"osv",
            "summary":"Prototype pollution in pathval A prototype pollution vulnerability affects all versions of package pathval under 1.1.1.",
            "published_date":"2022-02-10",
            "chain_len":1,
            "project":"https:\/\/github.com\/chaijs\/pathval",
            "commit_href":"https:\/\/github.com\/chaijs\/pathval\/pull\/58\/commits\/21a9046cfa0c2697cb41990f3b4316db410e6c8a",
            "commit_sha":"21a9046cfa0c2697cb41990f3b4316db410e6c8a",
            "patch":"SINGLE",
            "chain_ord":"['21a9046cfa0c2697cb41990f3b4316db410e6c8a']",
            "before_first_fix_commit":"{'a1230184a33a18f4eb3a92817e9b7492e8082903'}",
            "last_fix_commit":"21a9046cfa0c2697cb41990f3b4316db410e6c8a",
            "chain_ord_pos":1.0,
            "commit_datetime":"08\/25\/2020, 12:37:44",
            "message":"fix: \ud83d\udc1b fix prototype pollution",
            "author":"Adam Gold",
            "comments":null,
            "stats":"{'additions': 3, 'deletions': 0, 'total': 3}",
            "files":"{'index.js': {'additions': 3, 'deletions': 0, 'changes': 3, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/chaijs\/pathval\/raw\/21a9046cfa0c2697cb41990f3b4316db410e6c8a\/index.js', 'patch': '@@ -76,6 +76,9 @@ function parsePath(path) {\\n   var str = path.replace(\/([^\\\\\\\\])\\\\[\/g, \\'$1.[\\');\\n   var parts = str.match(\/(\\\\\\\\\\\\.|[^.]+?)+\/g);\\n   return parts.map(function mapMatches(value) {\\n+    if (value === \"constructor\" || value === \"__proto__\" || value === \"prototype\") {\\n+      return {}\\n+    }\\n     var regexp = \/^\\\\[(\\\\d+)\\\\]$\/;\\n     var mArr = regexp.exec(value);\\n     var parsed = null;'}}",
            "message_norm":"fix: \ud83d\udc1b fix prototype pollution",
            "language":"fr",
            "entities":"[('fix', 'ACTION', ''), ('prototype pollution', 'SECWORD', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['index.js'])",
            "num_files":1.0
        },
        {
            "index":2111,
            "vuln_id":"GHSA-hpx4-xjp7-m4vr",
            "cwe_id":"{'CWE-79'}",
            "score":5.4,
            "chain":"{'https:\/\/github.com\/snipe\/snipe-it\/commit\/f623d05d0c3487ae24c4f13907e4709484e5bf41'}",
            "dataset":"osv",
            "summary":"Stored cross-site scripting in Snipe-IT Snipe-IT prior to version 5.4.3 is vulnerable to stored cross-site scripting because the input to the `checked_out_to` parameter is not escaped. The vulnerability is capable of stealing a user's cookie.",
            "published_date":"2022-04-25",
            "chain_len":1,
            "project":"https:\/\/github.com\/snipe\/snipe-it",
            "commit_href":"https:\/\/github.com\/snipe\/snipe-it\/commit\/f623d05d0c3487ae24c4f13907e4709484e5bf41",
            "commit_sha":"f623d05d0c3487ae24c4f13907e4709484e5bf41",
            "patch":"SINGLE",
            "chain_ord":"['f623d05d0c3487ae24c4f13907e4709484e5bf41']",
            "before_first_fix_commit":"{'ef7f21e3ba01f13da2e656358343ba1236a122de'}",
            "last_fix_commit":"f623d05d0c3487ae24c4f13907e4709484e5bf41",
            "chain_ord_pos":1.0,
            "commit_datetime":"04\/24\/2022, 14:27:11",
            "message":"Escape checkout target name\n\nSigned-off-by: snipe <snipe@snipe.net>",
            "author":"snipe",
            "comments":null,
            "stats":"{'additions': 1, 'deletions': 1, 'total': 2}",
            "files":"{'app\/Http\/Transformers\/DepreciationReportTransformer.php': {'additions': 1, 'deletions': 1, 'changes': 2, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/snipe\/snipe-it\/raw\/f623d05d0c3487ae24c4f13907e4709484e5bf41\/app%2FHttp%2FTransformers%2FDepreciationReportTransformer.php', 'patch': \"@@ -98,7 +98,7 @@ public function transformAsset(Asset $asset)\\n             'purchase_cost' => Helper::formatCurrencyOutput($asset->purchase_cost),\\n             'book_value' => Helper::formatCurrencyOutput($depreciated_value),\\n             'monthly_depreciation' => $monthly_depreciation,\\n-            'checked_out_to' => $checkout_target,\\n+            'checked_out_to' => ($checkout_target) ? e($checkout_target) : null,\\n             'diff' =>  Helper::formatCurrencyOutput($diff),\\n             'number_of_months' =>  ($asset->model && $asset->model->depreciation) ? e($asset->model->depreciation->months) : null,\\n             'depreciation' => (($asset->model) && ($asset->model->depreciation)) ?  e($asset->model->depreciation->name) : null,\"}}",
            "message_norm":"escape checkout target name\n\nsigned-off-by: snipe <snipe@snipe.net>",
            "language":"en",
            "entities":"[('escape', 'SECWORD', ''), ('snipe@snipe.net', 'EMAIL', '')]",
            "classification_level_1":null,
            "classification_level_2":null,
            "list_files":"dict_keys(['app\/Http\/Transformers\/DepreciationReportTransformer.php'])",
            "num_files":1.0
        }
    ]
}