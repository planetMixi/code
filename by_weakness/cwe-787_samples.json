{
  "schema":{
    "fields":[
      {
        "name":"index",
        "type":"integer"
      },
      {
        "name":"vuln_id",
        "type":"string"
      },
      {
        "name":"cwe_id",
        "type":"string"
      },
      {
        "name":"score",
        "type":"number"
      },
      {
        "name":"chain",
        "type":"string"
      },
      {
        "name":"dataset",
        "type":"string"
      },
      {
        "name":"summary",
        "type":"string"
      },
      {
        "name":"published_date",
        "type":"string"
      },
      {
        "name":"chain_len",
        "type":"integer"
      },
      {
        "name":"project",
        "type":"string"
      },
      {
        "name":"commit_href",
        "type":"string"
      },
      {
        "name":"commit_sha",
        "type":"string"
      },
      {
        "name":"patch",
        "type":"string"
      },
      {
        "name":"chain_ord",
        "type":"string"
      },
      {
        "name":"before_first_fix_commit",
        "type":"string"
      },
      {
        "name":"last_fix_commit",
        "type":"string"
      },
      {
        "name":"chain_ord_pos",
        "type":"number"
      },
      {
        "name":"commit_datetime",
        "type":"string"
      },
      {
        "name":"message",
        "type":"string"
      },
      {
        "name":"author",
        "type":"string"
      },
      {
        "name":"comments",
        "type":"string"
      },
      {
        "name":"stats",
        "type":"string"
      },
      {
        "name":"files",
        "type":"string"
      },
      {
        "name":"message_norm",
        "type":"string"
      },
      {
        "name":"language",
        "type":"string"
      },
      {
        "name":"entities",
        "type":"string"
      },
      {
        "name":"classification_level_1",
        "type":"string"
      },
      {
        "name":"classification_level_2",
        "type":"string"
      },
      {
        "name":"list_files",
        "type":"string"
      },
      {
        "name":"num_files",
        "type":"number"
      },
      {
        "name":"patch_content",
        "type":"string"
      },
      {
        "name":"code_diff",
        "type":"string"
      }
    ],
    "primaryKey":[
      "index"
    ],
    "pandas_version":"1.4.0"
  },
  "data":[
    {
      "index":0,
      "vuln_id":"GHSA-crch-j389-5f84",
      "cwe_id":"{'CWE-787'}",
      "score":2.5,
      "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/c59c37e7b2d563967da813fa50fe20b21f4da683'}",
      "dataset":"osv",
      "summary":"Heap OOB write in TFLite ### Impact\nA specially crafted TFLite model could trigger an OOB write on heap in the TFLite implementation of [`ArgMin`\/`ArgMax`](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/102b211d892f3abc14f845a72047809b39cc65ab\/tensorflow\/lite\/kernels\/arg_min_max.cc#L52-L59):\n\n```cc\nTfLiteIntArray* output_dims = TfLiteIntArrayCreate(NumDimensions(input) - 1);\nint j = 0;\nfor (int i = 0; i < NumDimensions(input); ++i) { \n  if (i != axis_value) {\n    output_dims->data[j] = SizeOfDimension(input, i);\n    ++j;\n  }\n}\n```\n\nIf `axis_value` is not a value between 0 and `NumDimensions(input)`, then the condition in the `if` is never true, so code writes past the last valid element of `output_dims->data`.\n  \n### Patches \nWe have patched the issue in GitHub commit [c59c37e7b2d563967da813fa50fe20b21f4da683](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/c59c37e7b2d563967da813fa50fe20b21f4da683).\n\nThe fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by members of the Aivul Team from Qihoo 360.",
      "published_date":"2021-05-21",
      "chain_len":1,
      "project":"https:\/\/github.com\/tensorflow\/tensorflow",
      "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/c59c37e7b2d563967da813fa50fe20b21f4da683",
      "commit_sha":"c59c37e7b2d563967da813fa50fe20b21f4da683",
      "patch":"SINGLE",
      "chain_ord":"['c59c37e7b2d563967da813fa50fe20b21f4da683']",
      "before_first_fix_commit":"{'102b211d892f3abc14f845a72047809b39cc65ab'}",
      "last_fix_commit":"c59c37e7b2d563967da813fa50fe20b21f4da683",
      "chain_ord_pos":1.0,
      "commit_datetime":"04\/29\/2021, 00:50:10",
      "message":"Prevent array write out-of-bounds.\n\nIf user passes an invalid axis, then we copy one too many dimensions to the output in the loop below these checks. Even if we didn't do that, there will be further issues with an invalid axis, so we check for that right now.\n\nPiperOrigin-RevId: 371023299\nChange-Id: I9eca37ffc2b29e8e48710f500701270ef0790224",
      "author":"Mihai Maruseac",
      "comments":null,
      "stats":"{'additions': 3, 'deletions': 0, 'total': 3}",
      "files":"{'tensorflow\/lite\/kernels\/arg_min_max.cc': {'additions': 3, 'deletions': 0, 'changes': 3, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/c59c37e7b2d563967da813fa50fe20b21f4da683\/tensorflow%2Flite%2Fkernels%2Farg_min_max.cc', 'patch': '@@ -48,6 +48,9 @@ TfLiteStatus ResizeOutput(TfLiteContext* context, const TfLiteTensor* input,\\n     axis_value += NumDimensions(input);\\n   }\\n \\n+  TF_LITE_ENSURE(context, axis_value >= 0);\\n+  TF_LITE_ENSURE(context, axis_value < NumDimensions(input));\\n+\\n   \/\/ Copy the input dimensions to output except the axis dimension.\\n   TfLiteIntArray* output_dims = TfLiteIntArrayCreate(NumDimensions(input) - 1);\\n   int j = 0;'}}",
      "message_norm":"prevent array write out-of-bounds.\n\nif user passes an invalid axis, then we copy one too many dimensions to the output in the loop below these checks. even if we didn't do that, there will be further issues with an invalid axis, so we check for that right now.\n\npiperorigin-revid: 371023299\nchange-id: i9eca37ffc2b29e8e48710f500701270ef0790224",
      "language":"en",
      "entities":"[('prevent', 'ACTION', ''), ('out-of-bounds', 'SECWORD', ''), ('issues', 'FLAW', ''), ('371023299', 'SHA', 'generic_sha')]",
      "classification_level_1":null,
      "classification_level_2":null,
      "list_files":"dict_keys(['tensorflow\/lite\/kernels\/arg_min_max.cc'])",
      "num_files":1.0,
      "patch_content":"From c59c37e7b2d563967da813fa50fe20b21f4da683 Mon Sep 17 00:00:00 2001\nFrom: Mihai Maruseac <mihaimaruseac@google.com>\nDate: Wed, 28 Apr 2021 17:50:10 -0700\nSubject: [PATCH] Prevent array write out-of-bounds.\n\nIf user passes an invalid axis, then we copy one too many dimensions to the output in the loop below these checks. Even if we didn't do that, there will be further issues with an invalid axis, so we check for that right now.\n\nPiperOrigin-RevId: 371023299\nChange-Id: I9eca37ffc2b29e8e48710f500701270ef0790224\n---\n tensorflow\/lite\/kernels\/arg_min_max.cc | 3 +++\n 1 file changed, 3 insertions(+)\n\ndiff --git a\/tensorflow\/lite\/kernels\/arg_min_max.cc b\/tensorflow\/lite\/kernels\/arg_min_max.cc\nindex a0ba8cb9f8bbe7..291fd61681f2a8 100644\n--- a\/tensorflow\/lite\/kernels\/arg_min_max.cc\n+++ b\/tensorflow\/lite\/kernels\/arg_min_max.cc\n@@ -48,6 +48,9 @@ TfLiteStatus ResizeOutput(TfLiteContext* context, const TfLiteTensor* input,\n     axis_value += NumDimensions(input);\n   }\n \n+  TF_LITE_ENSURE(context, axis_value >= 0);\n+  TF_LITE_ENSURE(context, axis_value < NumDimensions(input));\n+\n   \/\/ Copy the input dimensions to output except the axis dimension.\n   TfLiteIntArray* output_dims = TfLiteIntArrayCreate(NumDimensions(input) - 1);\n   int j = 0;",
      "code_diff":"@@ -48,6 +48,9 @@ TfLiteStatus ResizeOutput(TfLiteContext* context, const TfLiteTensor* input,\n     axis_value += NumDimensions(input);\n   }\n \n+  TF_LITE_ENSURE(context, axis_value >= 0);\n+  TF_LITE_ENSURE(context, axis_value < NumDimensions(input));\n+\n   \/\/ Copy the input dimensions to output except the axis dimension.\n   TfLiteIntArray* output_dims = TfLiteIntArrayCreate(NumDimensions(input) - 1);\n   int j = 0;"
    },
    {
      "index":1,
      "vuln_id":"GHSA-v6r6-84gr-92rm",
      "cwe_id":"{'CWE-787', 'CWE-119'}",
      "score":2.5,
      "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/6fc9141f42f6a72180ecd24021c3e6b36165fe0d'}",
      "dataset":"osv",
      "summary":"Heap buffer overflow in `AvgPool3DGrad` ### Impact\nThe implementation of `tf.raw_ops.AvgPool3DGrad` is vulnerable to a heap buffer overflow:\n\n```python\nimport tensorflow as tf\n\norig_input_shape = tf.constant([10, 6, 3, 7, 7], shape=[5], dtype=tf.int32)\ngrad = tf.constant([0.01, 0, 0], shape=[3, 1, 1, 1, 1], dtype=tf.float32)\nksize = [1, 1, 1, 1, 1]\nstrides = [1, 1, 1, 1, 1]\npadding = \"SAME\"\n\ntf.raw_ops.AvgPool3DGrad(\n  orig_input_shape=orig_input_shape, grad=grad, ksize=ksize, strides=strides,\n  padding=padding)\n```\n\nThe [implementation](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/d80ffba9702dc19d1fac74fc4b766b3fa1ee976b\/tensorflow\/core\/kernels\/pooling_ops_3d.cc#L376-L450) assumes that the `orig_input_shape` and `grad` tensors have similar first and last dimensions but does not check that this assumption is validated.\n\n### Patches\nWe have patched the issue in GitHub commit [6fc9141f42f6a72180ecd24021c3e6b36165fe0d](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/6fc9141f42f6a72180ecd24021c3e6b36165fe0d).\n\nThe fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by Ying Wang and Yakun Zhang of Baidu X-Team.",
      "published_date":"2021-05-21",
      "chain_len":1,
      "project":"https:\/\/github.com\/tensorflow\/tensorflow",
      "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/6fc9141f42f6a72180ecd24021c3e6b36165fe0d",
      "commit_sha":"6fc9141f42f6a72180ecd24021c3e6b36165fe0d",
      "patch":"SINGLE",
      "chain_ord":"['6fc9141f42f6a72180ecd24021c3e6b36165fe0d']",
      "before_first_fix_commit":"{'d80ffba9702dc19d1fac74fc4b766b3fa1ee976b'}",
      "last_fix_commit":"6fc9141f42f6a72180ecd24021c3e6b36165fe0d",
      "chain_ord_pos":1.0,
      "commit_datetime":"05\/06\/2021, 16:51:26",
      "message":"Fix assertion failure in pooling_ops_3d\n\nPiperOrigin-RevId: 372364504\nChange-Id: Iecde4fe26b47a8fa935d6e2611b5585ed5777781",
      "author":"Mihai Maruseac",
      "comments":null,
      "stats":"{'additions': 13, 'deletions': 0, 'total': 13}",
      "files":"{'tensorflow\/core\/kernels\/pooling_ops_3d.cc': {'additions': 13, 'deletions': 0, 'changes': 13, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/6fc9141f42f6a72180ecd24021c3e6b36165fe0d\/tensorflow%2Fcore%2Fkernels%2Fpooling_ops_3d.cc', 'patch': '@@ -383,6 +383,19 @@ struct LaunchAvgPooling3dGradOp<CPUDevice, T> {\\n                      const std::array<int64, 3>& output_shape,\\n                      const std::array<int64, 3>& padding,\\n                      TensorFormat data_format, Tensor* output) {\\n+    OP_REQUIRES(\\n+        context, tensor_in_shape.dim_size(0) == out_backprop.dim_size(0),\\n+        errors::InvalidArgument(\\n+            \"Expected first dimension of tensor_in_shape and \"\\n+            \"out_backprop to match, got \",\\n+            tensor_in_shape.dim_size(0), \" and \", out_backprop.dim_size(0)));\\n+    OP_REQUIRES(\\n+        context, tensor_in_shape.dim_size(4) == out_backprop.dim_size(4),\\n+        errors::InvalidArgument(\\n+            \"Expected last dimension of tensor_in_shape and \"\\n+            \"out_backprop to match, got \",\\n+            tensor_in_shape.dim_size(4), \" and \", out_backprop.dim_size(4)));\\n+\\n     output->flat<T>().setZero();\\n     std::array<int64, 3> input_size = {{tensor_in_shape.dim_size(3),\\n                                         tensor_in_shape.dim_size(2),'}}",
      "message_norm":"fix assertion failure in pooling_ops_3d\n\npiperorigin-revid: 372364504\nchange-id: iecde4fe26b47a8fa935d6e2611b5585ed5777781",
      "language":"en",
      "entities":"[('fix', 'ACTION', ''), ('372364504', 'SHA', 'generic_sha')]",
      "classification_level_1":null,
      "classification_level_2":null,
      "list_files":"dict_keys(['tensorflow\/core\/kernels\/pooling_ops_3d.cc'])",
      "num_files":1.0,
      "patch_content":"From 6fc9141f42f6a72180ecd24021c3e6b36165fe0d Mon Sep 17 00:00:00 2001\nFrom: Mihai Maruseac <mihaimaruseac@google.com>\nDate: Thu, 6 May 2021 09:51:26 -0700\nSubject: [PATCH] Fix assertion failure in pooling_ops_3d\n\nPiperOrigin-RevId: 372364504\nChange-Id: Iecde4fe26b47a8fa935d6e2611b5585ed5777781\n---\n tensorflow\/core\/kernels\/pooling_ops_3d.cc | 13 +++++++++++++\n 1 file changed, 13 insertions(+)\n\ndiff --git a\/tensorflow\/core\/kernels\/pooling_ops_3d.cc b\/tensorflow\/core\/kernels\/pooling_ops_3d.cc\nindex 9da2d62b0a21d3..56a55bc2ec87bf 100644\n--- a\/tensorflow\/core\/kernels\/pooling_ops_3d.cc\n+++ b\/tensorflow\/core\/kernels\/pooling_ops_3d.cc\n@@ -383,6 +383,19 @@ struct LaunchAvgPooling3dGradOp<CPUDevice, T> {\n                      const std::array<int64, 3>& output_shape,\n                      const std::array<int64, 3>& padding,\n                      TensorFormat data_format, Tensor* output) {\n+    OP_REQUIRES(\n+        context, tensor_in_shape.dim_size(0) == out_backprop.dim_size(0),\n+        errors::InvalidArgument(\n+            \"Expected first dimension of tensor_in_shape and \"\n+            \"out_backprop to match, got \",\n+            tensor_in_shape.dim_size(0), \" and \", out_backprop.dim_size(0)));\n+    OP_REQUIRES(\n+        context, tensor_in_shape.dim_size(4) == out_backprop.dim_size(4),\n+        errors::InvalidArgument(\n+            \"Expected last dimension of tensor_in_shape and \"\n+            \"out_backprop to match, got \",\n+            tensor_in_shape.dim_size(4), \" and \", out_backprop.dim_size(4)));\n+\n     output->flat<T>().setZero();\n     std::array<int64, 3> input_size = {{tensor_in_shape.dim_size(3),\n                                         tensor_in_shape.dim_size(2),",
      "code_diff":"@@ -383,6 +383,19 @@ struct LaunchAvgPooling3dGradOp<CPUDevice, T> {\n                      const std::array<int64, 3>& output_shape,\n                      const std::array<int64, 3>& padding,\n                      TensorFormat data_format, Tensor* output) {\n+    OP_REQUIRES(\n+        context, tensor_in_shape.dim_size(0) == out_backprop.dim_size(0),\n+        errors::InvalidArgument(\n+            \"Expected first dimension of tensor_in_shape and \"\n+            \"out_backprop to match, got \",\n+            tensor_in_shape.dim_size(0), \" and \", out_backprop.dim_size(0)));\n+    OP_REQUIRES(\n+        context, tensor_in_shape.dim_size(4) == out_backprop.dim_size(4),\n+        errors::InvalidArgument(\n+            \"Expected last dimension of tensor_in_shape and \"\n+            \"out_backprop to match, got \",\n+            tensor_in_shape.dim_size(4), \" and \", out_backprop.dim_size(4)));\n+\n     output->flat<T>().setZero();\n     std::array<int64, 3> input_size = {{tensor_in_shape.dim_size(3),\n                                         tensor_in_shape.dim_size(2),"
    },
    {
      "index":2,
      "vuln_id":"GHSA-2wwc-w2gw-4329",
      "cwe_id":"{'CWE-787'}",
      "score":7.5,
      "chain":"{'https:\/\/github.com\/chakra-core\/ChakraCore\/commit\/94181502091b7c22eb86ab1b45ce80bf7ff03aaf', 'https:\/\/github.com\/chakra-core\/ChakraCore\/commit\/cc871514deeaeaedb5b757c2ca8cd4ab9abccb5d'}",
      "dataset":"osv",
      "summary":"Out-of-bounds write A remote code execution vulnerability exists in the way that the Chakra scripting engine handles objects in memory in Microsoft Edge, aka 'Chakra Scripting Engine Memory Corruption Vulnerability'. This CVE ID is unique from CVE-2019-1307, CVE-2019-1308, CVE-2019-1335.",
      "published_date":"2021-03-29",
      "chain_len":2,
      "project":"https:\/\/github.com\/chakra-core\/ChakraCore",
      "commit_href":"https:\/\/github.com\/chakra-core\/ChakraCore\/commit\/94181502091b7c22eb86ab1b45ce80bf7ff03aaf",
      "commit_sha":"94181502091b7c22eb86ab1b45ce80bf7ff03aaf",
      "patch":"MULTI",
      "chain_ord":"['94181502091b7c22eb86ab1b45ce80bf7ff03aaf', 'cc871514deeaeaedb5b757c2ca8cd4ab9abccb5d']",
      "before_first_fix_commit":"{'7e9a2ee60baa95ceb4f48f522f823c812ca90c80', '5989c6e038d80f92dcd8e10d725cdf45396201bb'}",
      "last_fix_commit":"cc871514deeaeaedb5b757c2ca8cd4ab9abccb5d",
      "chain_ord_pos":1.0,
      "commit_datetime":"08\/30\/2019, 22:55:27",
      "message":"CVE-2019-1366",
      "author":"Paul Leathers",
      "comments":null,
      "stats":"{'additions': 1, 'deletions': 1, 'total': 2}",
      "files":"{'lib\/Backend\/GlobOpt.cpp': {'additions': 1, 'deletions': 1, 'changes': 2, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/chakra-core\/ChakraCore\/raw\/94181502091b7c22eb86ab1b45ce80bf7ff03aaf\/lib%2FBackend%2FGlobOpt.cpp', 'patch': '@@ -3603,7 +3603,7 @@ GlobOpt::OptSrc(IR::Opnd *opnd, IR::Instr * *pInstr, Value **indirIndexValRef, I\\n \\n         opnd->SetValueType(valueType);\\n \\n-        if(!IsLoopPrePass() && opnd->IsSymOpnd() && valueType.IsDefinite())\\n+        if(!IsLoopPrePass() && opnd->IsSymOpnd() && (valueType.IsDefinite() || valueType.IsNotTaggedValue()))\\n         {\\n             if (opnd->AsSymOpnd()->m_sym->IsPropertySym())\\n             {'}}",
      "message_norm":"cve-2019-1366",
      "language":"ro",
      "entities":"[('cve-2019-1366', 'VULNID', 'CVE')]",
      "classification_level_1":null,
      "classification_level_2":null,
      "list_files":"dict_keys(['lib\/Backend\/GlobOpt.cpp'])",
      "num_files":1.0,
      "patch_content":"From 94181502091b7c22eb86ab1b45ce80bf7ff03aaf Mon Sep 17 00:00:00 2001\nFrom: Paul Leathers <pleath@microsoft.com>\nDate: Fri, 30 Aug 2019 15:55:27 -0700\nSubject: [PATCH] CVE-2019-1366\n\n---\n lib\/Backend\/GlobOpt.cpp | 2 +-\n 1 file changed, 1 insertion(+), 1 deletion(-)\n\ndiff --git a\/lib\/Backend\/GlobOpt.cpp b\/lib\/Backend\/GlobOpt.cpp\nindex 1a7530499c2..43b52dba7e9 100644\n--- a\/lib\/Backend\/GlobOpt.cpp\n+++ b\/lib\/Backend\/GlobOpt.cpp\n@@ -3603,7 +3603,7 @@ GlobOpt::OptSrc(IR::Opnd *opnd, IR::Instr * *pInstr, Value **indirIndexValRef, I\n \n         opnd->SetValueType(valueType);\n \n-        if(!IsLoopPrePass() && opnd->IsSymOpnd() && valueType.IsDefinite())\n+        if(!IsLoopPrePass() && opnd->IsSymOpnd() && (valueType.IsDefinite() || valueType.IsNotTaggedValue()))\n         {\n             if (opnd->AsSymOpnd()->m_sym->IsPropertySym())\n             {",
      "code_diff":"@@ -3603,7 +3603,7 @@ GlobOpt::OptSrc(IR::Opnd *opnd, IR::Instr * *pInstr, Value **indirIndexValRef, I\n \n         opnd->SetValueType(valueType);\n \n-        if(!IsLoopPrePass() && opnd->IsSymOpnd() && valueType.IsDefinite())\n+        if(!IsLoopPrePass() && opnd->IsSymOpnd() && (valueType.IsDefinite() || valueType.IsNotTaggedValue()))\n         {\n             if (opnd->AsSymOpnd()->m_sym->IsPropertySym())\n             {"
    },
    {
      "index":3,
      "vuln_id":"GHSA-wcv5-qrj6-9pfm",
      "cwe_id":"{'CWE-787', 'CWE-120'}",
      "score":2.5,
      "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/8f37b52e1320d8d72a9529b2468277791a261197'}",
      "dataset":"osv",
      "summary":"Heap buffer overflow in `Conv3DBackprop*` ### Impact\nMissing validation between arguments to `tf.raw_ops.Conv3DBackprop*` operations can result in heap buffer overflows:\n\n```python\nimport tensorflow as tf\n\ninput_sizes = tf.constant([1, 1, 1, 1, 2], shape=[5], dtype=tf.int32)\nfilter_tensor = tf.constant([734.6274508233133, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0,\n                            -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0,\n                            -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0], shape=[4, 1, 6, 1, 1], dtype=tf.float32)\nout_backprop = tf.constant([-10.0], shape=[1, 1, 1, 1, 1], dtype=tf.float32)\n\ntf.raw_ops.Conv3DBackpropInputV2(input_sizes=input_sizes, filter=filter_tensor, out_backprop=out_backprop, strides=[1, 89, 29, 89, 1], padding='SAME', data_format='NDHWC', dilations=[1, 1, 1, 1, 1])\n```\n```python\nimport tensorflow as tf\n\ninput_values = [-10.0] * (7 * 7 * 7 * 7 * 7)\ninput_values[0] = 429.6491056791816\ninput_sizes = tf.constant(input_values, shape=[7, 7, 7, 7, 7], dtype=tf.float32)\nfilter_tensor = tf.constant([7, 7, 7, 1, 1], shape=[5], dtype=tf.int32)\nout_backprop = tf.constant([-10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0], shape=[7, 1, 1, 1, 1], dtype=tf.float32)\n  \ntf.raw_ops.Conv3DBackpropFilterV2(input=input_sizes, filter_sizes=filter_tensor, out_backprop=out_backprop, strides=[1, 37, 65, 93, 1], padding='VALID', data_format='NDHWC', dilations=[1, 1, 1, 1, 1])\n```\n\nThis is because the [implementation](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/4814fafb0ca6b5ab58a09411523b2193fed23fed\/tensorflow\/core\/kernels\/conv_grad_shape_utils.cc#L94-L153) assumes that the `input`, `filter_sizes` and `out_backprop` tensors have the same shape, as they are accessed in parallel.\n\n### Patches\nWe have patched the issue in GitHub commit [8f37b52e1320d8d72a9529b2468277791a261197](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/8f37b52e1320d8d72a9529b2468277791a261197).\n\nThe fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our securityguide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by Yakun Zhang and Ying Wang of Baidu X-Team.",
      "published_date":"2021-05-21",
      "chain_len":1,
      "project":"https:\/\/github.com\/tensorflow\/tensorflow",
      "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/8f37b52e1320d8d72a9529b2468277791a261197",
      "commit_sha":"8f37b52e1320d8d72a9529b2468277791a261197",
      "patch":"SINGLE",
      "chain_ord":"['8f37b52e1320d8d72a9529b2468277791a261197']",
      "before_first_fix_commit":"{'4814fafb0ca6b5ab58a09411523b2193fed23fed'}",
      "last_fix_commit":"8f37b52e1320d8d72a9529b2468277791a261197",
      "chain_ord_pos":1.0,
      "commit_datetime":"04\/19\/2021, 20:46:32",
      "message":"Validate some shape requirements for `Conv3DBackpropFilter*` and `Conv3DBackpropInput*` ops.\n\nOlder versions of Eigen might otherwise crash \/ produce OOB read on specially crafted inputs.\n\nPiperOrigin-RevId: 369293977\nChange-Id: I58f51445a93936d7cf8e616f75de17677df36718",
      "author":"Mihai Maruseac",
      "comments":null,
      "stats":"{'additions': 56, 'deletions': 0, 'total': 56}",
      "files":"{'tensorflow\/core\/kernels\/conv_grad_ops_3d.cc': {'additions': 56, 'deletions': 0, 'changes': 56, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/8f37b52e1320d8d72a9529b2468277791a261197\/tensorflow%2Fcore%2Fkernels%2Fconv_grad_ops_3d.cc', 'patch': '@@ -239,6 +239,20 @@ class Conv3DBackpropInputOp : public OpKernel {\\n       input_shape = context->input(0).shape();\\n     }\\n \\n+    OP_REQUIRES(\\n+        context, input_shape.dim_size(4) == filter_shape.dim_size(3),\\n+        errors::InvalidArgument(\"input and filter_sizes must have the same \"\\n+                                \"number of channels. Got \",\\n+                                input_shape.dim_size(4), \" for input and \",\\n+                                filter_shape.dim_size(3), \" for filter_sizes\"));\\n+    OP_REQUIRES(\\n+        context, out_backprop_shape.dim_size(4) == filter_shape.dim_size(4),\\n+        errors::InvalidArgument(\"out_backprop and filter_sizes must have the \"\\n+                                \"same number of channels. Got \",\\n+                                out_backprop_shape.dim_size(4),\\n+                                \" for out_backprop and \",\\n+                                filter_shape.dim_size(4), \" for filter_sizes\"));\\n+\\n     ConvBackpropDimensions dims;\\n     OP_REQUIRES_OK(context, ConvBackpropComputeDimensions(\\n                                 \"Conv3DBackpropInputOp\", \/*num_spatial_dims=*\/3,\\n@@ -346,6 +360,20 @@ class Conv3DCustomBackpropInputOp : public OpKernel {\\n       input_shape = context->input(0).shape();\\n     }\\n \\n+    OP_REQUIRES(\\n+        context, input_shape.dim_size(4) == filter_shape.dim_size(3),\\n+        errors::InvalidArgument(\"input and filter_sizes must have the same \"\\n+                                \"number of channels. Got \",\\n+                                input_shape.dim_size(4), \" for input and \",\\n+                                filter_shape.dim_size(3), \" for filter_sizes\"));\\n+    OP_REQUIRES(\\n+        context, out_backprop_shape.dim_size(4) == filter_shape.dim_size(4),\\n+        errors::InvalidArgument(\"out_backprop and filter_sizes must have the \"\\n+                                \"same number of channels. Got \",\\n+                                out_backprop_shape.dim_size(4),\\n+                                \" for out_backprop and \",\\n+                                filter_shape.dim_size(4), \" for filter_sizes\"));\\n+\\n     ConvBackpropDimensions dims;\\n     OP_REQUIRES_OK(context, ConvBackpropComputeDimensions(\\n                                 \"Conv3DBackpropInputOp\", \/*num_spatial_dims=*\/3,\\n@@ -696,6 +724,20 @@ class Conv3DBackpropFilterOp : public OpKernel {\\n       filter_shape = context->input(1).shape();\\n     }\\n \\n+    OP_REQUIRES(\\n+        context, input_shape.dim_size(4) == filter_shape.dim_size(3),\\n+        errors::InvalidArgument(\"input and filter_sizes must have the same \"\\n+                                \"number of channels. Got \",\\n+                                input_shape.dim_size(4), \" for input and \",\\n+                                filter_shape.dim_size(3), \" for filter_sizes\"));\\n+    OP_REQUIRES(\\n+        context, out_backprop_shape.dim_size(4) == filter_shape.dim_size(4),\\n+        errors::InvalidArgument(\"out_backprop and filter_sizes must have the \"\\n+                                \"same number of channels. Got \",\\n+                                out_backprop_shape.dim_size(4),\\n+                                \" for out_backprop and \",\\n+                                filter_shape.dim_size(4), \" for filter_sizes\"));\\n+\\n     ConvBackpropDimensions dims;\\n     OP_REQUIRES_OK(context,\\n                    ConvBackpropComputeDimensions(\\n@@ -808,6 +850,20 @@ class Conv3DCustomBackpropFilterOp : public OpKernel {\\n       filter_shape = context->input(1).shape();\\n     }\\n \\n+    OP_REQUIRES(\\n+        context, input_shape.dim_size(4) == filter_shape.dim_size(3),\\n+        errors::InvalidArgument(\"input and filter_sizes must have the same \"\\n+                                \"number of channels. Got \",\\n+                                input_shape.dim_size(4), \" for input and \",\\n+                                filter_shape.dim_size(3), \" for filter_sizes\"));\\n+    OP_REQUIRES(\\n+        context, out_backprop_shape.dim_size(4) == filter_shape.dim_size(4),\\n+        errors::InvalidArgument(\"out_backprop and filter_sizes must have the \"\\n+                                \"same number of channels. Got \",\\n+                                out_backprop_shape.dim_size(4),\\n+                                \" for out_backprop and \",\\n+                                filter_shape.dim_size(4), \" for filter_sizes\"));\\n+\\n     ConvBackpropDimensions dims;\\n     OP_REQUIRES_OK(context,\\n                    ConvBackpropComputeDimensions('}}",
      "message_norm":"validate some shape requirements for `conv3dbackpropfilter*` and `conv3dbackpropinput*` ops.\n\nolder versions of eigen might otherwise crash \/ produce oob read on specially crafted inputs.\n\npiperorigin-revid: 369293977\nchange-id: i58f51445a93936d7cf8e616f75de17677df36718",
      "language":"en",
      "entities":"[('validate', 'ACTION', ''), ('oob', 'SECWORD', ''), ('369293977', 'SHA', 'generic_sha')]",
      "classification_level_1":null,
      "classification_level_2":null,
      "list_files":"dict_keys(['tensorflow\/core\/kernels\/conv_grad_ops_3d.cc'])",
      "num_files":1.0,
      "patch_content":"From 8f37b52e1320d8d72a9529b2468277791a261197 Mon Sep 17 00:00:00 2001\nFrom: Mihai Maruseac <mihaimaruseac@google.com>\nDate: Mon, 19 Apr 2021 13:46:32 -0700\nSubject: [PATCH] Validate some shape requirements for `Conv3DBackpropFilter*`\n and `Conv3DBackpropInput*` ops.\n\nOlder versions of Eigen might otherwise crash \/ produce OOB read on specially crafted inputs.\n\nPiperOrigin-RevId: 369293977\nChange-Id: I58f51445a93936d7cf8e616f75de17677df36718\n---\n tensorflow\/core\/kernels\/conv_grad_ops_3d.cc | 56 +++++++++++++++++++++\n 1 file changed, 56 insertions(+)\n\ndiff --git a\/tensorflow\/core\/kernels\/conv_grad_ops_3d.cc b\/tensorflow\/core\/kernels\/conv_grad_ops_3d.cc\nindex f736a12fb1ca39..8c72d01578d6d1 100644\n--- a\/tensorflow\/core\/kernels\/conv_grad_ops_3d.cc\n+++ b\/tensorflow\/core\/kernels\/conv_grad_ops_3d.cc\n@@ -239,6 +239,20 @@ class Conv3DBackpropInputOp : public OpKernel {\n       input_shape = context->input(0).shape();\n     }\n \n+    OP_REQUIRES(\n+        context, input_shape.dim_size(4) == filter_shape.dim_size(3),\n+        errors::InvalidArgument(\"input and filter_sizes must have the same \"\n+                                \"number of channels. Got \",\n+                                input_shape.dim_size(4), \" for input and \",\n+                                filter_shape.dim_size(3), \" for filter_sizes\"));\n+    OP_REQUIRES(\n+        context, out_backprop_shape.dim_size(4) == filter_shape.dim_size(4),\n+        errors::InvalidArgument(\"out_backprop and filter_sizes must have the \"\n+                                \"same number of channels. Got \",\n+                                out_backprop_shape.dim_size(4),\n+                                \" for out_backprop and \",\n+                                filter_shape.dim_size(4), \" for filter_sizes\"));\n+\n     ConvBackpropDimensions dims;\n     OP_REQUIRES_OK(context, ConvBackpropComputeDimensions(\n                                 \"Conv3DBackpropInputOp\", \/*num_spatial_dims=*\/3,\n@@ -346,6 +360,20 @@ class Conv3DCustomBackpropInputOp : public OpKernel {\n       input_shape = context->input(0).shape();\n     }\n \n+    OP_REQUIRES(\n+        context, input_shape.dim_size(4) == filter_shape.dim_size(3),\n+        errors::InvalidArgument(\"input and filter_sizes must have the same \"\n+                                \"number of channels. Got \",\n+                                input_shape.dim_size(4), \" for input and \",\n+                                filter_shape.dim_size(3), \" for filter_sizes\"));\n+    OP_REQUIRES(\n+        context, out_backprop_shape.dim_size(4) == filter_shape.dim_size(4),\n+        errors::InvalidArgument(\"out_backprop and filter_sizes must have the \"\n+                                \"same number of channels. Got \",\n+                                out_backprop_shape.dim_size(4),\n+                                \" for out_backprop and \",\n+                                filter_shape.dim_size(4), \" for filter_sizes\"));\n+\n     ConvBackpropDimensions dims;\n     OP_REQUIRES_OK(context, ConvBackpropComputeDimensions(\n                                 \"Conv3DBackpropInputOp\", \/*num_spatial_dims=*\/3,\n@@ -696,6 +724,20 @@ class Conv3DBackpropFilterOp : public OpKernel {\n       filter_shape = context->input(1).shape();\n     }\n \n+    OP_REQUIRES(\n+        context, input_shape.dim_size(4) == filter_shape.dim_size(3),\n+        errors::InvalidArgument(\"input and filter_sizes must have the same \"\n+                                \"number of channels. Got \",\n+                                input_shape.dim_size(4), \" for input and \",\n+                                filter_shape.dim_size(3), \" for filter_sizes\"));\n+    OP_REQUIRES(\n+        context, out_backprop_shape.dim_size(4) == filter_shape.dim_size(4),\n+        errors::InvalidArgument(\"out_backprop and filter_sizes must have the \"\n+                                \"same number of channels. Got \",\n+                                out_backprop_shape.dim_size(4),\n+                                \" for out_backprop and \",\n+                                filter_shape.dim_size(4), \" for filter_sizes\"));\n+\n     ConvBackpropDimensions dims;\n     OP_REQUIRES_OK(context,\n                    ConvBackpropComputeDimensions(\n@@ -808,6 +850,20 @@ class Conv3DCustomBackpropFilterOp : public OpKernel {\n       filter_shape = context->input(1).shape();\n     }\n \n+    OP_REQUIRES(\n+        context, input_shape.dim_size(4) == filter_shape.dim_size(3),\n+        errors::InvalidArgument(\"input and filter_sizes must have the same \"\n+                                \"number of channels. Got \",\n+                                input_shape.dim_size(4), \" for input and \",\n+                                filter_shape.dim_size(3), \" for filter_sizes\"));\n+    OP_REQUIRES(\n+        context, out_backprop_shape.dim_size(4) == filter_shape.dim_size(4),\n+        errors::InvalidArgument(\"out_backprop and filter_sizes must have the \"\n+                                \"same number of channels. Got \",\n+                                out_backprop_shape.dim_size(4),\n+                                \" for out_backprop and \",\n+                                filter_shape.dim_size(4), \" for filter_sizes\"));\n+\n     ConvBackpropDimensions dims;\n     OP_REQUIRES_OK(context,\n                    ConvBackpropComputeDimensions(",
      "code_diff":"@@ -239,6 +239,20 @@ class Conv3DBackpropInputOp : public OpKernel {\n       input_shape = context->input(0).shape();\n     }\n \n+    OP_REQUIRES(\n+        context, input_shape.dim_size(4) == filter_shape.dim_size(3),\n+        errors::InvalidArgument(\"input and filter_sizes must have the same \"\n+                                \"number of channels. Got \",\n+                                input_shape.dim_size(4), \" for input and \",\n+                                filter_shape.dim_size(3), \" for filter_sizes\"));\n+    OP_REQUIRES(\n+        context, out_backprop_shape.dim_size(4) == filter_shape.dim_size(4),\n+        errors::InvalidArgument(\"out_backprop and filter_sizes must have the \"\n+                                \"same number of channels. Got \",\n+                                out_backprop_shape.dim_size(4),\n+                                \" for out_backprop and \",\n+                                filter_shape.dim_size(4), \" for filter_sizes\"));\n+\n     ConvBackpropDimensions dims;\n     OP_REQUIRES_OK(context, ConvBackpropComputeDimensions(\n                                 \"Conv3DBackpropInputOp\", \/*num_spatial_dims=*\/3,\n@@ -346,6 +360,20 @@ class Conv3DCustomBackpropInputOp : public OpKernel {\n       input_shape = context->input(0).shape();\n     }\n \n+    OP_REQUIRES(\n+        context, input_shape.dim_size(4) == filter_shape.dim_size(3),\n+        errors::InvalidArgument(\"input and filter_sizes must have the same \"\n+                                \"number of channels. Got \",\n+                                input_shape.dim_size(4), \" for input and \",\n+                                filter_shape.dim_size(3), \" for filter_sizes\"));\n+    OP_REQUIRES(\n+        context, out_backprop_shape.dim_size(4) == filter_shape.dim_size(4),\n+        errors::InvalidArgument(\"out_backprop and filter_sizes must have the \"\n+                                \"same number of channels. Got \",\n+                                out_backprop_shape.dim_size(4),\n+                                \" for out_backprop and \",\n+                                filter_shape.dim_size(4), \" for filter_sizes\"));\n+\n     ConvBackpropDimensions dims;\n     OP_REQUIRES_OK(context, ConvBackpropComputeDimensions(\n                                 \"Conv3DBackpropInputOp\", \/*num_spatial_dims=*\/3,\n@@ -696,6 +724,20 @@ class Conv3DBackpropFilterOp : public OpKernel {\n       filter_shape = context->input(1).shape();\n     }\n \n+    OP_REQUIRES(\n+        context, input_shape.dim_size(4) == filter_shape.dim_size(3),\n+        errors::InvalidArgument(\"input and filter_sizes must have the same \"\n+                                \"number of channels. Got \",\n+                                input_shape.dim_size(4), \" for input and \",\n+                                filter_shape.dim_size(3), \" for filter_sizes\"));\n+    OP_REQUIRES(\n+        context, out_backprop_shape.dim_size(4) == filter_shape.dim_size(4),\n+        errors::InvalidArgument(\"out_backprop and filter_sizes must have the \"\n+                                \"same number of channels. Got \",\n+                                out_backprop_shape.dim_size(4),\n+                                \" for out_backprop and \",\n+                                filter_shape.dim_size(4), \" for filter_sizes\"));\n+\n     ConvBackpropDimensions dims;\n     OP_REQUIRES_OK(context,\n                    ConvBackpropComputeDimensions(\n@@ -808,6 +850,20 @@ class Conv3DCustomBackpropFilterOp : public OpKernel {\n       filter_shape = context->input(1).shape();\n     }\n \n+    OP_REQUIRES(\n+        context, input_shape.dim_size(4) == filter_shape.dim_size(3),\n+        errors::InvalidArgument(\"input and filter_sizes must have the same \"\n+                                \"number of channels. Got \",\n+                                input_shape.dim_size(4), \" for input and \",\n+                                filter_shape.dim_size(3), \" for filter_sizes\"));\n+    OP_REQUIRES(\n+        context, out_backprop_shape.dim_size(4) == filter_shape.dim_size(4),\n+        errors::InvalidArgument(\"out_backprop and filter_sizes must have the \"\n+                                \"same number of channels. Got \",\n+                                out_backprop_shape.dim_size(4),\n+                                \" for out_backprop and \",\n+                                filter_shape.dim_size(4), \" for filter_sizes\"));\n+\n     ConvBackpropDimensions dims;\n     OP_REQUIRES_OK(context,\n                    ConvBackpropComputeDimensions("
    },
    {
      "index":4,
      "vuln_id":"GHSA-4hvf-hxvg-f67v",
      "cwe_id":"{'CWE-787', 'CWE-125'}",
      "score":8.8,
      "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/6364463d6f5b6254cac3d6aedf999b6a96225038'}",
      "dataset":"osv",
      "summary":"Read and Write outside of bounds in TensorFlow ### Impact\nAn attacker can craft a TFLite model that would allow limited reads and writes outside of arrays in TFLite. This exploits missing validation in [the conversion from sparse tensors to dense tensors](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/ca6f96b62ad84207fbec580404eaa7dd7403a550\/tensorflow\/lite\/kernels\/internal\/utils\/sparsity_format_converter.cc#L252-L293).\n\n### Patches\nWe have patched the issue in GitHub commit [6364463d6f5b6254cac3d6aedf999b6a96225038](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/6364463d6f5b6254cac3d6aedf999b6a96225038).\nThe fix will be included in TensorFlow 2.8.0. We will also cherrypick this commit on TensorFlow 2.7.1, TensorFlow 2.6.3, and TensorFlow 2.5.3, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by Wang Xuan of Qihoo 360 AIVul Team.",
      "published_date":"2022-02-09",
      "chain_len":1,
      "project":"https:\/\/github.com\/tensorflow\/tensorflow",
      "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/6364463d6f5b6254cac3d6aedf999b6a96225038",
      "commit_sha":"6364463d6f5b6254cac3d6aedf999b6a96225038",
      "patch":"SINGLE",
      "chain_ord":"['6364463d6f5b6254cac3d6aedf999b6a96225038']",
      "before_first_fix_commit":"{'3e49ff637ad4f05c133d235a568943d19216fa9a'}",
      "last_fix_commit":"6364463d6f5b6254cac3d6aedf999b6a96225038",
      "chain_ord_pos":1.0,
      "commit_datetime":"12\/16\/2021, 23:37:14",
      "message":"[lite] Add some safety checks to avoid out of bound access for sparsity format\n\nPiperOrigin-RevId: 416910386\nChange-Id: Ic0b4dc048dc4b5a6309c572b8c4c9f776e4db60a",
      "author":"Karim Nosir",
      "comments":null,
      "stats":"{'additions': 11, 'deletions': 7, 'total': 18}",
      "files":"{'tensorflow\/lite\/kernels\/internal\/utils\/sparsity_format_converter.cc': {'additions': 11, 'deletions': 7, 'changes': 18, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/6364463d6f5b6254cac3d6aedf999b6a96225038\/tensorflow%2Flite%2Fkernels%2Finternal%2Futils%2Fsparsity_format_converter.cc', 'patch': '@@ -282,10 +282,12 @@ void FormatConverter<T>::InitSparseToDenseConverter(\\n   block_size_.resize(block_map_.size());\\n   for (int i = 0; i < original_rank; i++) {\\n     if (block_dim < block_map_.size() && block_map_[block_dim] == i) {\\n-      int orig_dim = traversal_order_[original_rank + block_dim];\\n-      block_size_[block_dim] = dense_size[orig_dim];\\n-      blocked_shape_[i] = dense_shape_[i] \/ dense_size[orig_dim];\\n-      block_dim++;\\n+      if (original_rank + block_dim < traversal_order_.size()) {\\n+        int orig_dim = traversal_order_[original_rank + block_dim];\\n+        block_size_[block_dim] = dense_size[orig_dim];\\n+        blocked_shape_[i] = dense_shape_[i] \/ dense_size[orig_dim];\\n+        block_dim++;\\n+      }\\n     } else {\\n       blocked_shape_[i] = dense_shape_[i];\\n     }\\n@@ -328,13 +330,15 @@ void FormatConverter<T>::Populate(const T* src_data, std::vector<int> indices,\\n       Populate(src_data, indices, level + 1, prev_idx * shape_of_level + i,\\n                src_data_ptr, dest_data);\\n     }\\n-  } else {\\n+  } else if (prev_idx + 1 < dim_metadata_[metadata_idx].size()) {\\n     const auto& array_segments = dim_metadata_[metadata_idx];\\n     const auto& array_indices = dim_metadata_[metadata_idx + 1];\\n     for (int i = array_segments[prev_idx]; i < array_segments[prev_idx + 1];\\n          i++) {\\n-      indices[level] = array_indices[i];\\n-      Populate(src_data, indices, level + 1, i, src_data_ptr, dest_data);\\n+      if (i < array_indices.size() && level < indices.size()) {\\n+        indices[level] = array_indices[i];\\n+        Populate(src_data, indices, level + 1, i, src_data_ptr, dest_data);\\n+      }\\n     }\\n   }\\n }'}}",
      "message_norm":"[lite] add some safety checks to avoid out of bound access for sparsity format\n\npiperorigin-revid: 416910386\nchange-id: ic0b4dc048dc4b5a6309c572b8c4c9f776e4db60a",
      "language":"en",
      "entities":"[('add', 'ACTION', ''), ('safety checks', 'SECWORD', ''), ('out of bound access', 'SECWORD', ''), ('416910386', 'SHA', 'generic_sha')]",
      "classification_level_1":null,
      "classification_level_2":null,
      "list_files":"dict_keys(['tensorflow\/lite\/kernels\/internal\/utils\/sparsity_format_converter.cc'])",
      "num_files":1.0,
      "patch_content":"From 6364463d6f5b6254cac3d6aedf999b6a96225038 Mon Sep 17 00:00:00 2001\nFrom: Karim Nosir <karimnosseir@google.com>\nDate: Thu, 16 Dec 2021 15:37:14 -0800\nSubject: [PATCH] [lite] Add some safety checks to avoid out of bound access\n for sparsity format\n\nPiperOrigin-RevId: 416910386\nChange-Id: Ic0b4dc048dc4b5a6309c572b8c4c9f776e4db60a\n---\n ...\/utils\/sparsity_format_converter.cc         | 18 +++++++++++-------\n 1 file changed, 11 insertions(+), 7 deletions(-)\n\ndiff --git a\/tensorflow\/lite\/kernels\/internal\/utils\/sparsity_format_converter.cc b\/tensorflow\/lite\/kernels\/internal\/utils\/sparsity_format_converter.cc\nindex 22aa5d019e7395..0595d49365c387 100644\n--- a\/tensorflow\/lite\/kernels\/internal\/utils\/sparsity_format_converter.cc\n+++ b\/tensorflow\/lite\/kernels\/internal\/utils\/sparsity_format_converter.cc\n@@ -282,10 +282,12 @@ void FormatConverter<T>::InitSparseToDenseConverter(\n   block_size_.resize(block_map_.size());\n   for (int i = 0; i < original_rank; i++) {\n     if (block_dim < block_map_.size() && block_map_[block_dim] == i) {\n-      int orig_dim = traversal_order_[original_rank + block_dim];\n-      block_size_[block_dim] = dense_size[orig_dim];\n-      blocked_shape_[i] = dense_shape_[i] \/ dense_size[orig_dim];\n-      block_dim++;\n+      if (original_rank + block_dim < traversal_order_.size()) {\n+        int orig_dim = traversal_order_[original_rank + block_dim];\n+        block_size_[block_dim] = dense_size[orig_dim];\n+        blocked_shape_[i] = dense_shape_[i] \/ dense_size[orig_dim];\n+        block_dim++;\n+      }\n     } else {\n       blocked_shape_[i] = dense_shape_[i];\n     }\n@@ -328,13 +330,15 @@ void FormatConverter<T>::Populate(const T* src_data, std::vector<int> indices,\n       Populate(src_data, indices, level + 1, prev_idx * shape_of_level + i,\n                src_data_ptr, dest_data);\n     }\n-  } else {\n+  } else if (prev_idx + 1 < dim_metadata_[metadata_idx].size()) {\n     const auto& array_segments = dim_metadata_[metadata_idx];\n     const auto& array_indices = dim_metadata_[metadata_idx + 1];\n     for (int i = array_segments[prev_idx]; i < array_segments[prev_idx + 1];\n          i++) {\n-      indices[level] = array_indices[i];\n-      Populate(src_data, indices, level + 1, i, src_data_ptr, dest_data);\n+      if (i < array_indices.size() && level < indices.size()) {\n+        indices[level] = array_indices[i];\n+        Populate(src_data, indices, level + 1, i, src_data_ptr, dest_data);\n+      }\n     }\n   }\n }",
      "code_diff":"@@ -282,10 +282,12 @@ void FormatConverter<T>::InitSparseToDenseConverter(\n   block_size_.resize(block_map_.size());\n   for (int i = 0; i < original_rank; i++) {\n     if (block_dim < block_map_.size() && block_map_[block_dim] == i) {\n-      int orig_dim = traversal_order_[original_rank + block_dim];\n-      block_size_[block_dim] = dense_size[orig_dim];\n-      blocked_shape_[i] = dense_shape_[i] \/ dense_size[orig_dim];\n-      block_dim++;\n+      if (original_rank + block_dim < traversal_order_.size()) {\n+        int orig_dim = traversal_order_[original_rank + block_dim];\n+        block_size_[block_dim] = dense_size[orig_dim];\n+        blocked_shape_[i] = dense_shape_[i] \/ dense_size[orig_dim];\n+        block_dim++;\n+      }\n     } else {\n       blocked_shape_[i] = dense_shape_[i];\n     }\n@@ -328,13 +330,15 @@ void FormatConverter<T>::Populate(const T* src_data, std::vector<int> indices,\n       Populate(src_data, indices, level + 1, prev_idx * shape_of_level + i,\n                src_data_ptr, dest_data);\n     }\n-  } else {\n+  } else if (prev_idx + 1 < dim_metadata_[metadata_idx].size()) {\n     const auto& array_segments = dim_metadata_[metadata_idx];\n     const auto& array_indices = dim_metadata_[metadata_idx + 1];\n     for (int i = array_segments[prev_idx]; i < array_segments[prev_idx + 1];\n          i++) {\n-      indices[level] = array_indices[i];\n-      Populate(src_data, indices, level + 1, i, src_data_ptr, dest_data);\n+      if (i < array_indices.size() && level < indices.size()) {\n+        indices[level] = array_indices[i];\n+        Populate(src_data, indices, level + 1, i, src_data_ptr, dest_data);\n+      }\n     }\n   }\n }"
    },
    {
      "index":5,
      "vuln_id":"GHSA-q5wr-fvpq-p67g",
      "cwe_id":"{'CWE-787', 'CWE-190'}",
      "score":8.8,
      "chain":"{'https:\/\/github.com\/gemini-testing\/png-img\/commit\/14ac462a32ca4b3b78f56502ac976d5b0222ce3d'}",
      "dataset":"osv",
      "summary":"Integer Overflow in png-img An integer overflow in the PngImg::InitStorage_() function of png-img before 3.1.0 leads to an under-allocation of heap memory and subsequently an exploitable heap-based buffer overflow when loading a crafted PNG file.",
      "published_date":"2021-12-10",
      "chain_len":1,
      "project":"https:\/\/github.com\/gemini-testing\/png-img",
      "commit_href":"https:\/\/github.com\/gemini-testing\/png-img\/commit\/14ac462a32ca4b3b78f56502ac976d5b0222ce3d",
      "commit_sha":"14ac462a32ca4b3b78f56502ac976d5b0222ce3d",
      "patch":"SINGLE",
      "chain_ord":"['14ac462a32ca4b3b78f56502ac976d5b0222ce3d']",
      "before_first_fix_commit":"{'9fedfccb9ab2d1ccee4d7d544f3e03d505317352'}",
      "last_fix_commit":"14ac462a32ca4b3b78f56502ac976d5b0222ce3d",
      "chain_ord_pos":1.0,
      "commit_datetime":"08\/06\/2020, 00:45:40",
      "message":"Handle image size overflow",
      "author":"Mikhail Cheshkov",
      "comments":null,
      "stats":"{'additions': 12, 'deletions': 2, 'total': 14}",
      "files":"{'src\/PngImg.cc': {'additions': 12, 'deletions': 2, 'changes': 14, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/gemini-testing\/png-img\/raw\/14ac462a32ca4b3b78f56502ac976d5b0222ce3d\/src%2FPngImg.cc', 'patch': '@@ -60,10 +60,20 @@ void PngImg::ReadInfo_(PngReadStruct& rs) {\\n \/\/\/\\n void PngImg::InitStorage_() {\\n     rowPtrs_.resize(info_.height, nullptr);\\n-    data_ = new png_byte[info_.height * info_.rowbytes];\\n+    \/\/ Extend height and rowbytes from uint32_t to size_t to avoid multiplication overflow when size_t is larger\\n+    size_t h = info_.height;\\n+    size_t rb = info_.rowbytes;\\n+    \/\/ We need to make sure that info_.height * info_.rowbytes will not overflow size_t\\n+    \/\/ Unfotunately, there\\'s no simple and portable way to do this in C++\\n+    \/\/ For integer division of positive numbers a * b > c <==> a > c \/ b holds\\n+    if (h > std::numeric_limits<size_t>::max() \/ rb) {\\n+        \/\/ TODO Propagate this exception to JS, and test it\\n+        throw std::runtime_error(\"Image is too large to allocate single buffer\");\\n+    }\\n+    data_ = new png_byte[h * rb];\\n \\n     for(size_t i = 0; i < info_.height; ++i) {\\n-        rowPtrs_[i] = data_ + i * info_.rowbytes;\\n+        rowPtrs_[i] = data_ + i * rb;\\n     }\\n }'}}",
      "message_norm":"handle image size overflow",
      "language":"en",
      "entities":"[('overflow', 'SECWORD', '')]",
      "classification_level_1":null,
      "classification_level_2":null,
      "list_files":"dict_keys(['src\/PngImg.cc'])",
      "num_files":1.0,
      "patch_content":"From 14ac462a32ca4b3b78f56502ac976d5b0222ce3d Mon Sep 17 00:00:00 2001\nFrom: Mikhail Cheshkov <mcheshkov@gmail.com>\nDate: Thu, 6 Aug 2020 03:45:40 +0300\nSubject: [PATCH] Handle image size overflow\n\n---\n src\/PngImg.cc | 14 ++++++++++++--\n 1 file changed, 12 insertions(+), 2 deletions(-)\n\ndiff --git a\/src\/PngImg.cc b\/src\/PngImg.cc\nindex 17968ae..5a554a7 100644\n--- a\/src\/PngImg.cc\n+++ b\/src\/PngImg.cc\n@@ -60,10 +60,20 @@ void PngImg::ReadInfo_(PngReadStruct& rs) {\n \/\/\/\n void PngImg::InitStorage_() {\n     rowPtrs_.resize(info_.height, nullptr);\n-    data_ = new png_byte[info_.height * info_.rowbytes];\n+    \/\/ Extend height and rowbytes from uint32_t to size_t to avoid multiplication overflow when size_t is larger\n+    size_t h = info_.height;\n+    size_t rb = info_.rowbytes;\n+    \/\/ We need to make sure that info_.height * info_.rowbytes will not overflow size_t\n+    \/\/ Unfotunately, there's no simple and portable way to do this in C++\n+    \/\/ For integer division of positive numbers a * b > c <==> a > c \/ b holds\n+    if (h > std::numeric_limits<size_t>::max() \/ rb) {\n+        \/\/ TODO Propagate this exception to JS, and test it\n+        throw std::runtime_error(\"Image is too large to allocate single buffer\");\n+    }\n+    data_ = new png_byte[h * rb];\n \n     for(size_t i = 0; i < info_.height; ++i) {\n-        rowPtrs_[i] = data_ + i * info_.rowbytes;\n+        rowPtrs_[i] = data_ + i * rb;\n     }\n }",
      "code_diff":"@@ -60,10 +60,20 @@ void PngImg::ReadInfo_(PngReadStruct& rs) {\n \/\/\/\n void PngImg::InitStorage_() {\n     rowPtrs_.resize(info_.height, nullptr);\n-    data_ = new png_byte[info_.height * info_.rowbytes];\n+    \/\/ Extend height and rowbytes from uint32_t to size_t to avoid multiplication overflow when size_t is larger\n+    size_t h = info_.height;\n+    size_t rb = info_.rowbytes;\n+    \/\/ We need to make sure that info_.height * info_.rowbytes will not overflow size_t\n+    \/\/ Unfotunately, there's no simple and portable way to do this in C++\n+    \/\/ For integer division of positive numbers a * b > c <==> a > c \/ b holds\n+    if (h > std::numeric_limits<size_t>::max() \/ rb) {\n+        \/\/ TODO Propagate this exception to JS, and test it\n+        throw std::runtime_error(\"Image is too large to allocate single buffer\");\n+    }\n+    data_ = new png_byte[h * rb];\n \n     for(size_t i = 0; i < info_.height; ++i) {\n-        rowPtrs_[i] = data_ + i * info_.rowbytes;\n+        rowPtrs_[i] = data_ + i * rb;\n     }\n }"
    },
    {
      "index":6,
      "vuln_id":"GHSA-9xh4-23q4-v6wr",
      "cwe_id":"{'CWE-476', 'CWE-787', 'CWE-125'}",
      "score":2.5,
      "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/6972f9dfe325636b3db4e0bc517ee22a159365c0'}",
      "dataset":"osv",
      "summary":"Heap buffer overflow and undefined behavior in `FusedBatchNorm` ### Impact\nThe implementation of `tf.raw_ops.FusedBatchNorm` is vulnerable to a heap buffer overflow:\n      \n```python\nimport tensorflow as tf\n\nx = tf.zeros([10, 10, 10, 6], dtype=tf.float32)\nscale = tf.constant([0.0], shape=[1], dtype=tf.float32)\noffset = tf.constant([0.0], shape=[1], dtype=tf.float32)\nmean = tf.constant([0.0], shape=[1], dtype=tf.float32)\nvariance = tf.constant([0.0], shape=[1], dtype=tf.float32)\nepsilon = 0.0\nexponential_avg_factor = 0.0\ndata_format = \"NHWC\"\nis_training = False\n    \ntf.raw_ops.FusedBatchNorm(\n  x=x, scale=scale, offset=offset, mean=mean, variance=variance,\n  epsilon=epsilon, exponential_avg_factor=exponential_avg_factor,\n  data_format=data_format, is_training=is_training)\n```\n  \nIf the tensors are empty, the same implementation can trigger undefined behavior by dereferencing null pointers:\n\n```python \nimport tensorflow as tf\nimport numpy as np\n\nx = tf.zeros([10, 10, 10, 1], dtype=tf.float32)\nscale = tf.constant([], shape=[0], dtype=tf.float32)\noffset = tf.constant([], shape=[0], dtype=tf.float32)\nmean = tf.constant([], shape=[0], dtype=tf.float32)\nvariance = tf.constant([], shape=[0], dtype=tf.float32)\nepsilon = 0.0\nexponential_avg_factor = 0.0\ndata_format = \"NHWC\"\nis_training = False\n\ntf.raw_ops.FusedBatchNorm(\n  x=x, scale=scale, offset=offset, mean=mean, variance=variance, \n  epsilon=epsilon, exponential_avg_factor=exponential_avg_factor,\n  data_format=data_format, is_training=is_training)\n``` \n\nThe  [implementation](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/57d86e0db5d1365f19adcce848dfc1bf89fdd4c7\/tensorflow\/core\/kernels\/fused_batch_norm_op.cc) fails to validate that `scale`, `offset`, `mean` and `variance` (the last two only when required) all have the same number of elements as the number of channels of `x`. This results in heap out of bounds reads when the buffers backing these tensors are indexed past their boundary.\n\nIf the tensors are empty, the validation mentioned in the above paragraph would also trigger and prevent the undefined behavior.\n\n### Patches\nWe have patched the issue in GitHub commit [6972f9dfe325636b3db4e0bc517ee22a159365c0](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/6972f9dfe325636b3db4e0bc517ee22a159365c0).\n\nThe fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by Ying Wang and Yakun Zhang of Baidu X-Team.",
      "published_date":"2021-05-21",
      "chain_len":1,
      "project":"https:\/\/github.com\/tensorflow\/tensorflow",
      "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/6972f9dfe325636b3db4e0bc517ee22a159365c0",
      "commit_sha":"6972f9dfe325636b3db4e0bc517ee22a159365c0",
      "patch":"SINGLE",
      "chain_ord":"['6972f9dfe325636b3db4e0bc517ee22a159365c0']",
      "before_first_fix_commit":"{'57d86e0db5d1365f19adcce848dfc1bf89fdd4c7'}",
      "last_fix_commit":"6972f9dfe325636b3db4e0bc517ee22a159365c0",
      "chain_ord_pos":1.0,
      "commit_datetime":"05\/07\/2021, 00:45:51",
      "message":"Add missing valuidation to FusedBatchNorm.\n\nPiperOrigin-RevId: 372460336\nChange-Id: Ic8c4e4de67c58a741bd87f2e182bed07247d1126",
      "author":"Mihai Maruseac",
      "comments":null,
      "stats":"{'additions': 27, 'deletions': 1, 'total': 28}",
      "files":"{'tensorflow\/core\/kernels\/fused_batch_norm_op.cc': {'additions': 27, 'deletions': 1, 'changes': 28, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/6972f9dfe325636b3db4e0bc517ee22a159365c0\/tensorflow%2Fcore%2Fkernels%2Ffused_batch_norm_op.cc', 'patch': '@@ -1282,6 +1282,32 @@ class FusedBatchNormOpBase : public OpKernel {\\n                   errors::InvalidArgument(\"Error during tensor copy.\"));\\n     }\\n \\n+    const auto num_channels = GetTensorDim(x, tensor_format_, \\'C\\');\\n+    OP_REQUIRES(\\n+        context, scale.NumElements() == num_channels,\\n+        errors::InvalidArgument(\"scale must have the same number of elements \"\\n+                                \"as the channels of x, got \",\\n+                                scale.NumElements(), \" and \", num_channels));\\n+    OP_REQUIRES(\\n+        context, offset.NumElements() == num_channels,\\n+        errors::InvalidArgument(\"offset must have the same number of elements \"\\n+                                \"as the channels of x, got \",\\n+                                offset.NumElements(), \" and \", num_channels));\\n+    if (estimated_mean.NumElements() != 0) {\\n+      OP_REQUIRES(context, estimated_mean.NumElements() == num_channels,\\n+                  errors::InvalidArgument(\\n+                      \"mean must be empty or have the same number of \"\\n+                      \"elements as the channels of x, got \",\\n+                      estimated_mean.NumElements(), \" and \", num_channels));\\n+    }\\n+    if (estimated_variance.NumElements() != 0) {\\n+      OP_REQUIRES(context, estimated_variance.NumElements() == num_channels,\\n+                  errors::InvalidArgument(\\n+                      \"variance must be empty or have the same number of \"\\n+                      \"elements as the channels of x, got \",\\n+                      estimated_variance.NumElements(), \" and \", num_channels));\\n+    }\\n+\\n     if (has_side_input_) {\\n       OP_REQUIRES(context, side_input->shape() == x.shape(),\\n                   errors::InvalidArgument(\\n@@ -1294,7 +1320,7 @@ class FusedBatchNormOpBase : public OpKernel {\\n       \/\/ NOTE(ezhulenev): This requirement is coming from implementation\\n       \/\/ details of cudnnBatchNormalizationForwardTrainingEx.\\n       OP_REQUIRES(\\n-          context, !is_training_ || x.dim_size(3) % 4 == 0,\\n+          context, !is_training_ || num_channels % 4 == 0,\\n           errors::InvalidArgument(\"FusedBatchNorm with activation requires \"\\n                                   \"channel dimension to be a multiple of 4.\"));\\n     }'}}",
      "message_norm":"add missing valuidation to fusedbatchnorm.\n\npiperorigin-revid: 372460336\nchange-id: ic8c4e4de67c58a741bd87f2e182bed07247d1126",
      "language":"en",
      "entities":"[('add', 'ACTION', ''), ('372460336', 'SHA', 'generic_sha')]",
      "classification_level_1":null,
      "classification_level_2":null,
      "list_files":"dict_keys(['tensorflow\/core\/kernels\/fused_batch_norm_op.cc'])",
      "num_files":1.0,
      "patch_content":"From 6972f9dfe325636b3db4e0bc517ee22a159365c0 Mon Sep 17 00:00:00 2001\nFrom: Mihai Maruseac <mihaimaruseac@google.com>\nDate: Thu, 6 May 2021 17:45:51 -0700\nSubject: [PATCH] Add missing valuidation to FusedBatchNorm.\n\nPiperOrigin-RevId: 372460336\nChange-Id: Ic8c4e4de67c58a741bd87f2e182bed07247d1126\n---\n ...\/core\/kernels\/fused_batch_norm_op.cc       | 28 ++++++++++++++++++-\n 1 file changed, 27 insertions(+), 1 deletion(-)\n\ndiff --git a\/tensorflow\/core\/kernels\/fused_batch_norm_op.cc b\/tensorflow\/core\/kernels\/fused_batch_norm_op.cc\nindex e564b19857c383..7b0932d953261c 100644\n--- a\/tensorflow\/core\/kernels\/fused_batch_norm_op.cc\n+++ b\/tensorflow\/core\/kernels\/fused_batch_norm_op.cc\n@@ -1282,6 +1282,32 @@ class FusedBatchNormOpBase : public OpKernel {\n                   errors::InvalidArgument(\"Error during tensor copy.\"));\n     }\n \n+    const auto num_channels = GetTensorDim(x, tensor_format_, 'C');\n+    OP_REQUIRES(\n+        context, scale.NumElements() == num_channels,\n+        errors::InvalidArgument(\"scale must have the same number of elements \"\n+                                \"as the channels of x, got \",\n+                                scale.NumElements(), \" and \", num_channels));\n+    OP_REQUIRES(\n+        context, offset.NumElements() == num_channels,\n+        errors::InvalidArgument(\"offset must have the same number of elements \"\n+                                \"as the channels of x, got \",\n+                                offset.NumElements(), \" and \", num_channels));\n+    if (estimated_mean.NumElements() != 0) {\n+      OP_REQUIRES(context, estimated_mean.NumElements() == num_channels,\n+                  errors::InvalidArgument(\n+                      \"mean must be empty or have the same number of \"\n+                      \"elements as the channels of x, got \",\n+                      estimated_mean.NumElements(), \" and \", num_channels));\n+    }\n+    if (estimated_variance.NumElements() != 0) {\n+      OP_REQUIRES(context, estimated_variance.NumElements() == num_channels,\n+                  errors::InvalidArgument(\n+                      \"variance must be empty or have the same number of \"\n+                      \"elements as the channels of x, got \",\n+                      estimated_variance.NumElements(), \" and \", num_channels));\n+    }\n+\n     if (has_side_input_) {\n       OP_REQUIRES(context, side_input->shape() == x.shape(),\n                   errors::InvalidArgument(\n@@ -1294,7 +1320,7 @@ class FusedBatchNormOpBase : public OpKernel {\n       \/\/ NOTE(ezhulenev): This requirement is coming from implementation\n       \/\/ details of cudnnBatchNormalizationForwardTrainingEx.\n       OP_REQUIRES(\n-          context, !is_training_ || x.dim_size(3) % 4 == 0,\n+          context, !is_training_ || num_channels % 4 == 0,\n           errors::InvalidArgument(\"FusedBatchNorm with activation requires \"\n                                   \"channel dimension to be a multiple of 4.\"));\n     }",
      "code_diff":"@@ -1282,6 +1282,32 @@ class FusedBatchNormOpBase : public OpKernel {\n                   errors::InvalidArgument(\"Error during tensor copy.\"));\n     }\n \n+    const auto num_channels = GetTensorDim(x, tensor_format_, 'C');\n+    OP_REQUIRES(\n+        context, scale.NumElements() == num_channels,\n+        errors::InvalidArgument(\"scale must have the same number of elements \"\n+                                \"as the channels of x, got \",\n+                                scale.NumElements(), \" and \", num_channels));\n+    OP_REQUIRES(\n+        context, offset.NumElements() == num_channels,\n+        errors::InvalidArgument(\"offset must have the same number of elements \"\n+                                \"as the channels of x, got \",\n+                                offset.NumElements(), \" and \", num_channels));\n+    if (estimated_mean.NumElements() != 0) {\n+      OP_REQUIRES(context, estimated_mean.NumElements() == num_channels,\n+                  errors::InvalidArgument(\n+                      \"mean must be empty or have the same number of \"\n+                      \"elements as the channels of x, got \",\n+                      estimated_mean.NumElements(), \" and \", num_channels));\n+    }\n+    if (estimated_variance.NumElements() != 0) {\n+      OP_REQUIRES(context, estimated_variance.NumElements() == num_channels,\n+                  errors::InvalidArgument(\n+                      \"variance must be empty or have the same number of \"\n+                      \"elements as the channels of x, got \",\n+                      estimated_variance.NumElements(), \" and \", num_channels));\n+    }\n+\n     if (has_side_input_) {\n       OP_REQUIRES(context, side_input->shape() == x.shape(),\n                   errors::InvalidArgument(\n@@ -1294,7 +1320,7 @@ class FusedBatchNormOpBase : public OpKernel {\n       \/\/ NOTE(ezhulenev): This requirement is coming from implementation\n       \/\/ details of cudnnBatchNormalizationForwardTrainingEx.\n       OP_REQUIRES(\n-          context, !is_training_ || x.dim_size(3) % 4 == 0,\n+          context, !is_training_ || num_channels % 4 == 0,\n           errors::InvalidArgument(\"FusedBatchNorm with activation requires \"\n                                   \"channel dimension to be a multiple of 4.\"));\n     }"
    },
    {
      "index":7,
      "vuln_id":"GHSA-pvrc-hg3f-58r6",
      "cwe_id":"{'CWE-787'}",
      "score":2.5,
      "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/3f6fe4dfef6f57e768260b48166c27d148f3015f'}",
      "dataset":"osv",
      "summary":"Heap OOB access in `Dilation2DBackpropInput` ### Impact\nAn attacker can write outside the bounds of heap allocated arrays by passing invalid arguments to `tf.raw_ops.Dilation2DBackpropInput`:\n\n```python\nimport tensorflow as tf\n    \ninput_tensor = tf.constant([1.1] * 81, shape=[3, 3, 3, 3], dtype=tf.float32)\nfilter = tf.constant([], shape=[0, 0, 3], dtype=tf.float32)\nout_backprop = tf.constant([1.1] * 1062, shape=[3, 2, 59, 3], dtype=tf.float32)\n\ntf.raw_ops.Dilation2DBackpropInput(\n  input=input_tensor, filter=filter, out_backprop=out_backprop, \n  strides=[1, 40, 1, 1], rates=[1, 56, 56, 1], padding='VALID')\n```\n\nThis is because the [implementation](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/afd954e65f15aea4d438d0a219136fc4a63a573d\/tensorflow\/core\/kernels\/dilation_ops.cc#L321-L322) does not validate before writing to the output array.\n  \n```cc               \nin_backprop(b, h_in_max, w_in_max, d) += out_backprop(b, h_out, w_out, d);\n```                 \n    \nThe values for `h_out` and `w_out` are guaranteed to be in range for `out_backprop` (as they are loop indices bounded by the size of the array). However, there are no similar guarantees relating `h_in_max`\/`w_in_max` and `in_backprop`.\n\n### Patches\nWe have patched the issue in GitHub commit [3f6fe4dfef6f57e768260b48166c27d148f3015f](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/3f6fe4dfef6f57e768260b48166c27d148f3015f).\n\nThe fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by Yakun Zhang and Ying Wang of Baidu X-Team.",
      "published_date":"2021-05-21",
      "chain_len":1,
      "project":"https:\/\/github.com\/tensorflow\/tensorflow",
      "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/3f6fe4dfef6f57e768260b48166c27d148f3015f",
      "commit_sha":"3f6fe4dfef6f57e768260b48166c27d148f3015f",
      "patch":"SINGLE",
      "chain_ord":"['3f6fe4dfef6f57e768260b48166c27d148f3015f']",
      "before_first_fix_commit":"{'afd954e65f15aea4d438d0a219136fc4a63a573d'}",
      "last_fix_commit":"3f6fe4dfef6f57e768260b48166c27d148f3015f",
      "chain_ord_pos":1.0,
      "commit_datetime":"05\/05\/2021, 01:33:28",
      "message":"Add missing validations in dillation ops.\n\nPiperOrigin-RevId: 372037158\nChange-Id: I4ee304c84a02550c030288a6534000b934fc1599",
      "author":"Mihai Maruseac",
      "comments":null,
      "stats":"{'additions': 11, 'deletions': 4, 'total': 15}",
      "files":"{'tensorflow\/core\/kernels\/dilation_ops.cc': {'additions': 11, 'deletions': 4, 'changes': 15, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/3f6fe4dfef6f57e768260b48166c27d148f3015f\/tensorflow%2Fcore%2Fkernels%2Fdilation_ops.cc', 'patch': '@@ -130,6 +130,7 @@ class DilationOp : public OpKernel {\\n     ParseSizes(context, strides_, rates_, padding_, &stride_rows, &stride_cols,\\n                &rate_rows, &rate_cols, &pad_top, &pad_left, &out_rows,\\n                &out_cols);\\n+    if (!context->status().ok()) return;\\n \\n     \/\/ Output tensor is of the following dimensions:\\n     \/\/ [ batch, out_rows, out_cols, depth ]\\n@@ -229,6 +230,7 @@ class DilationBackpropInputOp : public OpKernel {\\n     ParseSizes(context, strides_, rates_, padding_, &stride_rows, &stride_cols,\\n                &rate_rows, &rate_cols, &pad_top, &pad_left, &out_rows,\\n                &out_cols);\\n+    if (!context->status().ok()) return;\\n \\n     \/\/ Verify that the incoming gradient tensor has the expected size\\n     \/\/ [ batch, out_rows, out_cols, depth ]\\n@@ -318,8 +320,10 @@ struct DilationBackpropInput<CPUDevice, T> {\\n                 }\\n               }\\n             }\\n-            in_backprop(b, h_in_max, w_in_max, d) +=\\n-                out_backprop(b, h_out, w_out, d);\\n+            if (h_in_max < input_rows && w_in_max < input_cols) {\\n+              in_backprop(b, h_in_max, w_in_max, d) +=\\n+                  out_backprop(b, h_out, w_out, d);\\n+            }\\n           }\\n         }\\n       }\\n@@ -349,6 +353,7 @@ class DilationBackpropFilterOp : public OpKernel {\\n     ParseSizes(context, strides_, rates_, padding_, &stride_rows, &stride_cols,\\n                &rate_rows, &rate_cols, &pad_top, &pad_left, &out_rows,\\n                &out_cols);\\n+    if (!context->status().ok()) return;\\n \\n     \/\/ Verify that the incoming gradient tensor has the expected size\\n     \/\/ [ batch, out_rows, out_cols, depth ]\\n@@ -438,8 +443,10 @@ struct DilationBackpropFilter<CPUDevice, T> {\\n                 }\\n               }\\n             }\\n-            filter_backprop(h_max, w_max, d) +=\\n-                out_backprop(b, h_out, w_out, d);\\n+            if (h_max < filter_rows && w_max < filter_cols) {\\n+              filter_backprop(h_max, w_max, d) +=\\n+                  out_backprop(b, h_out, w_out, d);\\n+            }\\n           }\\n         }\\n       }'}}",
      "message_norm":"add missing validations in dillation ops.\n\npiperorigin-revid: 372037158\nchange-id: i4ee304c84a02550c030288a6534000b934fc1599",
      "language":"en",
      "entities":"[('add', 'ACTION', ''), ('missing validations', 'SECWORD', ''), ('372037158', 'SHA', 'generic_sha')]",
      "classification_level_1":null,
      "classification_level_2":null,
      "list_files":"dict_keys(['tensorflow\/core\/kernels\/dilation_ops.cc'])",
      "num_files":1.0,
      "patch_content":"From 3f6fe4dfef6f57e768260b48166c27d148f3015f Mon Sep 17 00:00:00 2001\nFrom: Mihai Maruseac <mihaimaruseac@google.com>\nDate: Tue, 4 May 2021 18:33:28 -0700\nSubject: [PATCH] Add missing validations in dillation ops.\n\nPiperOrigin-RevId: 372037158\nChange-Id: I4ee304c84a02550c030288a6534000b934fc1599\n---\n tensorflow\/core\/kernels\/dilation_ops.cc | 15 +++++++++++----\n 1 file changed, 11 insertions(+), 4 deletions(-)\n\ndiff --git a\/tensorflow\/core\/kernels\/dilation_ops.cc b\/tensorflow\/core\/kernels\/dilation_ops.cc\nindex 738ea31d555d5f..996ddb62bfefeb 100644\n--- a\/tensorflow\/core\/kernels\/dilation_ops.cc\n+++ b\/tensorflow\/core\/kernels\/dilation_ops.cc\n@@ -130,6 +130,7 @@ class DilationOp : public OpKernel {\n     ParseSizes(context, strides_, rates_, padding_, &stride_rows, &stride_cols,\n                &rate_rows, &rate_cols, &pad_top, &pad_left, &out_rows,\n                &out_cols);\n+    if (!context->status().ok()) return;\n \n     \/\/ Output tensor is of the following dimensions:\n     \/\/ [ batch, out_rows, out_cols, depth ]\n@@ -229,6 +230,7 @@ class DilationBackpropInputOp : public OpKernel {\n     ParseSizes(context, strides_, rates_, padding_, &stride_rows, &stride_cols,\n                &rate_rows, &rate_cols, &pad_top, &pad_left, &out_rows,\n                &out_cols);\n+    if (!context->status().ok()) return;\n \n     \/\/ Verify that the incoming gradient tensor has the expected size\n     \/\/ [ batch, out_rows, out_cols, depth ]\n@@ -318,8 +320,10 @@ struct DilationBackpropInput<CPUDevice, T> {\n                 }\n               }\n             }\n-            in_backprop(b, h_in_max, w_in_max, d) +=\n-                out_backprop(b, h_out, w_out, d);\n+            if (h_in_max < input_rows && w_in_max < input_cols) {\n+              in_backprop(b, h_in_max, w_in_max, d) +=\n+                  out_backprop(b, h_out, w_out, d);\n+            }\n           }\n         }\n       }\n@@ -349,6 +353,7 @@ class DilationBackpropFilterOp : public OpKernel {\n     ParseSizes(context, strides_, rates_, padding_, &stride_rows, &stride_cols,\n                &rate_rows, &rate_cols, &pad_top, &pad_left, &out_rows,\n                &out_cols);\n+    if (!context->status().ok()) return;\n \n     \/\/ Verify that the incoming gradient tensor has the expected size\n     \/\/ [ batch, out_rows, out_cols, depth ]\n@@ -438,8 +443,10 @@ struct DilationBackpropFilter<CPUDevice, T> {\n                 }\n               }\n             }\n-            filter_backprop(h_max, w_max, d) +=\n-                out_backprop(b, h_out, w_out, d);\n+            if (h_max < filter_rows && w_max < filter_cols) {\n+              filter_backprop(h_max, w_max, d) +=\n+                  out_backprop(b, h_out, w_out, d);\n+            }\n           }\n         }\n       }",
      "code_diff":"@@ -130,6 +130,7 @@ class DilationOp : public OpKernel {\n     ParseSizes(context, strides_, rates_, padding_, &stride_rows, &stride_cols,\n                &rate_rows, &rate_cols, &pad_top, &pad_left, &out_rows,\n                &out_cols);\n+    if (!context->status().ok()) return;\n \n     \/\/ Output tensor is of the following dimensions:\n     \/\/ [ batch, out_rows, out_cols, depth ]\n@@ -229,6 +230,7 @@ class DilationBackpropInputOp : public OpKernel {\n     ParseSizes(context, strides_, rates_, padding_, &stride_rows, &stride_cols,\n                &rate_rows, &rate_cols, &pad_top, &pad_left, &out_rows,\n                &out_cols);\n+    if (!context->status().ok()) return;\n \n     \/\/ Verify that the incoming gradient tensor has the expected size\n     \/\/ [ batch, out_rows, out_cols, depth ]\n@@ -318,8 +320,10 @@ struct DilationBackpropInput<CPUDevice, T> {\n                 }\n               }\n             }\n-            in_backprop(b, h_in_max, w_in_max, d) +=\n-                out_backprop(b, h_out, w_out, d);\n+            if (h_in_max < input_rows && w_in_max < input_cols) {\n+              in_backprop(b, h_in_max, w_in_max, d) +=\n+                  out_backprop(b, h_out, w_out, d);\n+            }\n           }\n         }\n       }\n@@ -349,6 +353,7 @@ class DilationBackpropFilterOp : public OpKernel {\n     ParseSizes(context, strides_, rates_, padding_, &stride_rows, &stride_cols,\n                &rate_rows, &rate_cols, &pad_top, &pad_left, &out_rows,\n                &out_cols);\n+    if (!context->status().ok()) return;\n \n     \/\/ Verify that the incoming gradient tensor has the expected size\n     \/\/ [ batch, out_rows, out_cols, depth ]\n@@ -438,8 +443,10 @@ struct DilationBackpropFilter<CPUDevice, T> {\n                 }\n               }\n             }\n-            filter_backprop(h_max, w_max, d) +=\n-                out_backprop(b, h_out, w_out, d);\n+            if (h_max < filter_rows && w_max < filter_cols) {\n+              filter_backprop(h_max, w_max, d) +=\n+                  out_backprop(b, h_out, w_out, d);\n+            }\n           }\n         }\n       }"
    },
    {
      "index":8,
      "vuln_id":"GHSA-44qp-9wwf-734r",
      "cwe_id":"{'CWE-787', 'CWE-120'}",
      "score":7.6,
      "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/2b7100d6cdff36aa21010a82269bc05a6d1cc74a', 'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/adbbabdb0d3abb3cdeac69e38a96de1d678b24b3'}",
      "dataset":"osv",
      "summary":"Heap overflow in Tensorflow ### Impact \nThe [implementation of `SparseCountSparseOutput`](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/5100e359aef5c8021f2e71c7b986420b85ce7b3d\/tensorflow\/core\/kernels\/count_ops.cc#L168-L273) is vulnerable to a heap overflow:\n\n```python\nimport tensorflow as tf\nimport numpy as np\n\ntf.raw_ops.SparseCountSparseOutput(\n  indices=[[-1,-1]],\n  values=[2],\n  dense_shape=[1, 1],\n  weights=[1],\n  binary_output=True,\n  minlength=-1,\n  maxlength=-1,\n  name=None)\n```\n\n### Patches\nWe have patched the issue in GitHub commits [2b7100d6cdff36aa21010a82269bc05a6d1cc74a](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/2b7100d6cdff36aa21010a82269bc05a6d1cc74a) and [adbbabdb0d3abb3cdeac69e38a96de1d678b24b3](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/adbbabdb0d3abb3cdeac69e38a96de1d678b24b3).\n\nThe fix will be included in TensorFlow 2.8.0. We will also cherrypick this commit on TensorFlow 2.7.1, TensorFlow 2.6.3, and TensorFlow 2.5.3, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by Faysal Hossain Shezan from University of Virginia.",
      "published_date":"2022-02-09",
      "chain_len":2,
      "project":"https:\/\/github.com\/tensorflow\/tensorflow",
      "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/2b7100d6cdff36aa21010a82269bc05a6d1cc74a",
      "commit_sha":"2b7100d6cdff36aa21010a82269bc05a6d1cc74a",
      "patch":"MULTI",
      "chain_ord":"['2b7100d6cdff36aa21010a82269bc05a6d1cc74a', 'adbbabdb0d3abb3cdeac69e38a96de1d678b24b3']",
      "before_first_fix_commit":"{'2b7100d6cdff36aa21010a82269bc05a6d1cc74a'}",
      "last_fix_commit":"adbbabdb0d3abb3cdeac69e38a96de1d678b24b3",
      "chain_ord_pos":1.0,
      "commit_datetime":"12\/08\/2021, 03:36:18",
      "message":"Cleanup and remove duplicate validation in `SparseCount`.\n\nWe have valdiation that is duplicated, checking different conditions, in different formats and failing to capture all cases. This should fix all the previous bugs.\n\nPiperOrigin-RevId: 414886981\nChange-Id: Ibf0bba0beb057b76d505324bb9487565daf95f01",
      "author":"Mihai Maruseac",
      "comments":null,
      "stats":"{'additions': 21, 'deletions': 27, 'total': 48}",
      "files":"{'tensorflow\/core\/kernels\/count_ops.cc': {'additions': 21, 'deletions': 27, 'changes': 48, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/2b7100d6cdff36aa21010a82269bc05a6d1cc74a\/tensorflow%2Fcore%2Fkernels%2Fcount_ops.cc', 'patch': '@@ -185,6 +185,27 @@ class SparseCount : public OpKernel {\\n                 errors::InvalidArgument(\\n                     \"Input indices must be a 2-dimensional tensor. Got: \",\\n                     indices.shape().DebugString()));\\n+    OP_REQUIRES(context, TensorShapeUtils::IsVector(values.shape()),\\n+                errors::InvalidArgument(\"Input values must be a vector. Got: \",\\n+                                        values.shape().DebugString()));\\n+    OP_REQUIRES(context, TensorShapeUtils::IsVector(shape.shape()),\\n+                errors::InvalidArgument(\"Input shape must be a vector. Got: \",\\n+                                        shape.shape().DebugString()));\\n+    OP_REQUIRES(context,\\n+                values.shape().dim_size(0) == indices.shape().dim_size(0),\\n+                errors::InvalidArgument(\\n+                    \"Number of values must match first dimension of indices.\",\\n+                    \"Got \", values.shape().dim_size(0),\\n+                    \" values, indices shape: \", indices.shape().DebugString()));\\n+    OP_REQUIRES(\\n+        context, shape.shape().dim_size(0) == indices.shape().dim_size(1),\\n+        errors::InvalidArgument(\\n+            \"Number of dimensions must match second dimension of indices.\",\\n+            \"Got \", shape.shape().dim_size(0),\\n+            \" dimensions, indices shape: \", indices.shape().DebugString()));\\n+    OP_REQUIRES(context, shape.NumElements() > 0,\\n+                errors::InvalidArgument(\\n+                    \"The shape argument requires at least one element.\"));\\n \\n     if (use_weights) {\\n       OP_REQUIRES(\\n@@ -195,28 +216,11 @@ class SparseCount : public OpKernel {\\n               \"; values shape: \", values.shape().DebugString()));\\n     }\\n \\n-    OP_REQUIRES(context, shape.NumElements() != 0,\\n-                errors::InvalidArgument(\\n-                    \"The shape argument requires at least one element.\"));\\n-\\n     bool is_1d = shape.NumElements() == 1;\\n     auto shape_vector = shape.flat<int64_t>();\\n     int num_batches = is_1d ? 1 : shape_vector(0);\\n     int num_values = values.NumElements();\\n \\n-    for (int b = 0; b < shape_vector.size(); b++) {\\n-      OP_REQUIRES(context, shape_vector(b) >= 0,\\n-                  errors::InvalidArgument(\\n-                      \"Elements in dense_shape must be >= 0. Instead got:\",\\n-                      shape.DebugString()));\\n-    }\\n-\\n-    OP_REQUIRES(context, num_values == indices.shape().dim_size(0),\\n-                errors::InvalidArgument(\\n-                    \"Number of values must match first dimension of indices.\",\\n-                    \"Got \", num_values,\\n-                    \" values, indices shape: \", indices.shape().DebugString()));\\n-\\n     const auto indices_values = indices.matrix<int64_t>();\\n     const auto values_values = values.flat<T>();\\n     const auto weight_values = weights.flat<W>();\\n@@ -225,16 +229,6 @@ class SparseCount : public OpKernel {\\n \\n     T max_value = 0;\\n \\n-    OP_REQUIRES(context, num_values <= indices.shape().dim_size(0),\\n-                errors::InvalidArgument(\\n-                    \"The first dimension of indices must be equal to or \"\\n-                    \"greather than number of values. ( \",\\n-                    indices.shape().dim_size(0), \" vs. \", num_values, \" )\"));\\n-    OP_REQUIRES(context, indices.shape().dim_size(1) > 0,\\n-                errors::InvalidArgument(\"The second dimension of indices must \"\\n-                                        \"be greater than 0. Received: \",\\n-                                        indices.shape().dim_size(1)));\\n-\\n     for (int idx = 0; idx < num_values; ++idx) {\\n       int batch = is_1d ? 0 : indices_values(idx, 0);\\n       if (batch >= num_batches) {'}}",
      "message_norm":"cleanup and remove duplicate validation in `sparsecount`.\n\nwe have valdiation that is duplicated, checking different conditions, in different formats and failing to capture all cases. this should fix all the previous bugs.\n\npiperorigin-revid: 414886981\nchange-id: ibf0bba0beb057b76d505324bb9487565daf95f01",
      "language":"en",
      "entities":"[('remove', 'ACTION', ''), ('duplicate validation', 'SECWORD', ''), ('fix', 'ACTION', ''), ('bugs', 'FLAW', ''), ('414886981', 'SHA', 'generic_sha')]",
      "classification_level_1":null,
      "classification_level_2":null,
      "list_files":"dict_keys(['tensorflow\/core\/kernels\/count_ops.cc'])",
      "num_files":1.0,
      "patch_content":"From 2b7100d6cdff36aa21010a82269bc05a6d1cc74a Mon Sep 17 00:00:00 2001\nFrom: Mihai Maruseac <mihaimaruseac@google.com>\nDate: Tue, 7 Dec 2021 19:36:18 -0800\nSubject: [PATCH] Cleanup and remove duplicate validation in `SparseCount`.\n\nWe have valdiation that is duplicated, checking different conditions, in different formats and failing to capture all cases. This should fix all the previous bugs.\n\nPiperOrigin-RevId: 414886981\nChange-Id: Ibf0bba0beb057b76d505324bb9487565daf95f01\n---\n tensorflow\/core\/kernels\/count_ops.cc | 48 ++++++++++++----------------\n 1 file changed, 21 insertions(+), 27 deletions(-)\n\ndiff --git a\/tensorflow\/core\/kernels\/count_ops.cc b\/tensorflow\/core\/kernels\/count_ops.cc\nindex 5330c36361e5e6..1f99e0783e26f6 100644\n--- a\/tensorflow\/core\/kernels\/count_ops.cc\n+++ b\/tensorflow\/core\/kernels\/count_ops.cc\n@@ -185,6 +185,27 @@ class SparseCount : public OpKernel {\n                 errors::InvalidArgument(\n                     \"Input indices must be a 2-dimensional tensor. Got: \",\n                     indices.shape().DebugString()));\n+    OP_REQUIRES(context, TensorShapeUtils::IsVector(values.shape()),\n+                errors::InvalidArgument(\"Input values must be a vector. Got: \",\n+                                        values.shape().DebugString()));\n+    OP_REQUIRES(context, TensorShapeUtils::IsVector(shape.shape()),\n+                errors::InvalidArgument(\"Input shape must be a vector. Got: \",\n+                                        shape.shape().DebugString()));\n+    OP_REQUIRES(context,\n+                values.shape().dim_size(0) == indices.shape().dim_size(0),\n+                errors::InvalidArgument(\n+                    \"Number of values must match first dimension of indices.\",\n+                    \"Got \", values.shape().dim_size(0),\n+                    \" values, indices shape: \", indices.shape().DebugString()));\n+    OP_REQUIRES(\n+        context, shape.shape().dim_size(0) == indices.shape().dim_size(1),\n+        errors::InvalidArgument(\n+            \"Number of dimensions must match second dimension of indices.\",\n+            \"Got \", shape.shape().dim_size(0),\n+            \" dimensions, indices shape: \", indices.shape().DebugString()));\n+    OP_REQUIRES(context, shape.NumElements() > 0,\n+                errors::InvalidArgument(\n+                    \"The shape argument requires at least one element.\"));\n \n     if (use_weights) {\n       OP_REQUIRES(\n@@ -195,28 +216,11 @@ class SparseCount : public OpKernel {\n               \"; values shape: \", values.shape().DebugString()));\n     }\n \n-    OP_REQUIRES(context, shape.NumElements() != 0,\n-                errors::InvalidArgument(\n-                    \"The shape argument requires at least one element.\"));\n-\n     bool is_1d = shape.NumElements() == 1;\n     auto shape_vector = shape.flat<int64_t>();\n     int num_batches = is_1d ? 1 : shape_vector(0);\n     int num_values = values.NumElements();\n \n-    for (int b = 0; b < shape_vector.size(); b++) {\n-      OP_REQUIRES(context, shape_vector(b) >= 0,\n-                  errors::InvalidArgument(\n-                      \"Elements in dense_shape must be >= 0. Instead got:\",\n-                      shape.DebugString()));\n-    }\n-\n-    OP_REQUIRES(context, num_values == indices.shape().dim_size(0),\n-                errors::InvalidArgument(\n-                    \"Number of values must match first dimension of indices.\",\n-                    \"Got \", num_values,\n-                    \" values, indices shape: \", indices.shape().DebugString()));\n-\n     const auto indices_values = indices.matrix<int64_t>();\n     const auto values_values = values.flat<T>();\n     const auto weight_values = weights.flat<W>();\n@@ -225,16 +229,6 @@ class SparseCount : public OpKernel {\n \n     T max_value = 0;\n \n-    OP_REQUIRES(context, num_values <= indices.shape().dim_size(0),\n-                errors::InvalidArgument(\n-                    \"The first dimension of indices must be equal to or \"\n-                    \"greather than number of values. ( \",\n-                    indices.shape().dim_size(0), \" vs. \", num_values, \" )\"));\n-    OP_REQUIRES(context, indices.shape().dim_size(1) > 0,\n-                errors::InvalidArgument(\"The second dimension of indices must \"\n-                                        \"be greater than 0. Received: \",\n-                                        indices.shape().dim_size(1)));\n-\n     for (int idx = 0; idx < num_values; ++idx) {\n       int batch = is_1d ? 0 : indices_values(idx, 0);\n       if (batch >= num_batches) {",
      "code_diff":"@@ -185,6 +185,27 @@ class SparseCount : public OpKernel {\n                 errors::InvalidArgument(\n                     \"Input indices must be a 2-dimensional tensor. Got: \",\n                     indices.shape().DebugString()));\n+    OP_REQUIRES(context, TensorShapeUtils::IsVector(values.shape()),\n+                errors::InvalidArgument(\"Input values must be a vector. Got: \",\n+                                        values.shape().DebugString()));\n+    OP_REQUIRES(context, TensorShapeUtils::IsVector(shape.shape()),\n+                errors::InvalidArgument(\"Input shape must be a vector. Got: \",\n+                                        shape.shape().DebugString()));\n+    OP_REQUIRES(context,\n+                values.shape().dim_size(0) == indices.shape().dim_size(0),\n+                errors::InvalidArgument(\n+                    \"Number of values must match first dimension of indices.\",\n+                    \"Got \", values.shape().dim_size(0),\n+                    \" values, indices shape: \", indices.shape().DebugString()));\n+    OP_REQUIRES(\n+        context, shape.shape().dim_size(0) == indices.shape().dim_size(1),\n+        errors::InvalidArgument(\n+            \"Number of dimensions must match second dimension of indices.\",\n+            \"Got \", shape.shape().dim_size(0),\n+            \" dimensions, indices shape: \", indices.shape().DebugString()));\n+    OP_REQUIRES(context, shape.NumElements() > 0,\n+                errors::InvalidArgument(\n+                    \"The shape argument requires at least one element.\"));\n \n     if (use_weights) {\n       OP_REQUIRES(\n@@ -195,28 +216,11 @@ class SparseCount : public OpKernel {\n               \"; values shape: \", values.shape().DebugString()));\n     }\n \n-    OP_REQUIRES(context, shape.NumElements() != 0,\n-                errors::InvalidArgument(\n-                    \"The shape argument requires at least one element.\"));\n-\n     bool is_1d = shape.NumElements() == 1;\n     auto shape_vector = shape.flat<int64_t>();\n     int num_batches = is_1d ? 1 : shape_vector(0);\n     int num_values = values.NumElements();\n \n-    for (int b = 0; b < shape_vector.size(); b++) {\n-      OP_REQUIRES(context, shape_vector(b) >= 0,\n-                  errors::InvalidArgument(\n-                      \"Elements in dense_shape must be >= 0. Instead got:\",\n-                      shape.DebugString()));\n-    }\n-\n-    OP_REQUIRES(context, num_values == indices.shape().dim_size(0),\n-                errors::InvalidArgument(\n-                    \"Number of values must match first dimension of indices.\",\n-                    \"Got \", num_values,\n-                    \" values, indices shape: \", indices.shape().DebugString()));\n-\n     const auto indices_values = indices.matrix<int64_t>();\n     const auto values_values = values.flat<T>();\n     const auto weight_values = weights.flat<W>();\n@@ -225,16 +229,6 @@ class SparseCount : public OpKernel {\n \n     T max_value = 0;\n \n-    OP_REQUIRES(context, num_values <= indices.shape().dim_size(0),\n-                errors::InvalidArgument(\n-                    \"The first dimension of indices must be equal to or \"\n-                    \"greather than number of values. ( \",\n-                    indices.shape().dim_size(0), \" vs. \", num_values, \" )\"));\n-    OP_REQUIRES(context, indices.shape().dim_size(1) > 0,\n-                errors::InvalidArgument(\"The second dimension of indices must \"\n-                                        \"be greater than 0. Received: \",\n-                                        indices.shape().dim_size(1)));\n-\n     for (int idx = 0; idx < num_values; ++idx) {\n       int batch = is_1d ? 0 : indices_values(idx, 0);\n       if (batch >= num_batches) {"
    },
    {
      "index":9,
      "vuln_id":"GHSA-p23j-g745-8449",
      "cwe_id":"{'CWE-787'}",
      "score":7.5,
      "chain":"{'https:\/\/github.com\/chakra-core\/ChakraCore\/commit\/a4e56547fb8b7450656bfd26dfc52b8477c8ef27', 'https:\/\/github.com\/chakra-core\/ChakraCore\/commit\/cc871514deeaeaedb5b757c2ca8cd4ab9abccb5d'}",
      "dataset":"osv",
      "summary":"Out-of-bounds write A remote code execution vulnerability exists in the way that the Chakra scripting engine handles objects in memory in Microsoft Edge, aka 'Chakra Scripting Engine Memory Corruption Vulnerability'. This CVE ID is unique from CVE-2019-1307, CVE-2019-1308, CVE-2019-1366.",
      "published_date":"2021-03-29",
      "chain_len":2,
      "project":"https:\/\/github.com\/chakra-core\/ChakraCore",
      "commit_href":"https:\/\/github.com\/chakra-core\/ChakraCore\/commit\/a4e56547fb8b7450656bfd26dfc52b8477c8ef27",
      "commit_sha":"a4e56547fb8b7450656bfd26dfc52b8477c8ef27",
      "patch":"MULTI",
      "chain_ord":"['a4e56547fb8b7450656bfd26dfc52b8477c8ef27', 'cc871514deeaeaedb5b757c2ca8cd4ab9abccb5d']",
      "before_first_fix_commit":"{'7e9a2ee60baa95ceb4f48f522f823c812ca90c80', '5989c6e038d80f92dcd8e10d725cdf45396201bb'}",
      "last_fix_commit":"cc871514deeaeaedb5b757c2ca8cd4ab9abccb5d",
      "chain_ord_pos":1.0,
      "commit_datetime":"09\/03\/2019, 21:52:17",
      "message":"CVE-2019-1335",
      "author":"Wyatt",
      "comments":null,
      "stats":"{'additions': 52, 'deletions': 13, 'total': 65}",
      "files":"{'lib\/Backend\/GlobOpt.cpp': {'additions': 52, 'deletions': 13, 'changes': 65, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/chakra-core\/ChakraCore\/raw\/a4e56547fb8b7450656bfd26dfc52b8477c8ef27\/lib%2FBackend%2FGlobOpt.cpp', 'patch': '@@ -2161,27 +2161,46 @@ GlobOpt::CollectMemOpInfo(IR::Instr *instrBegin, IR::Instr *instr, Value *src1Va\\n             return false;\\n         }\\n         break;\\n-    case Js::OpCode::Decr_A:\\n-        isIncr = false;\\n-    case Js::OpCode::Incr_A:\\n-        isChangedByOne = true;\\n-        goto MemOpCheckInductionVariable;\\n     case Js::OpCode::Sub_I4:\\n-    case Js::OpCode::Sub_A:\\n         isIncr = false;\\n-    case Js::OpCode::Add_A:\\n     case Js::OpCode::Add_I4:\\n     {\\n-MemOpCheckInductionVariable:\\n-        StackSym *sym = instr->GetSrc1()->GetStackSym();\\n-        if (!sym)\\n+        \/\/ The only case in which these OpCodes can contribute to an inductionVariableChangeInfo\\n+        \/\/ is when the induction variable is being modified and overwritten aswell (ex: j = j + 1)\\n+        \/\/ and not when the induction variable is modified but not overwritten (ex: k = j + 1).\\n+        \/\/ This can either be detected in IR as\\n+        \/\/ s1     = Add_I4 s1     1  \/\/ Case #1, can be seen with \"j++\".\\n+        \/\/ or as\\n+        \/\/ s4(s2) = Add_I4 s3(s1) 1  \/\/ Case #2, can be see with \"j = j + 1\".\\n+        \/\/ s1     = Ld_A   s2\\n+        bool isInductionVar = false;\\n+        IR::Instr* nextInstr = instr->m_next;\\n+        if (\\n+            \/\/ Checks for Case #1 and Case #2\\n+            instr->GetDst()->GetStackSym() != nullptr &&\\n+            instr->GetDst()->IsRegOpnd() &&\\n+            (\\n+                \/\/ Checks for Case #1\\n+                (instr->GetDst()->GetStackSym() == instr->GetSrc1()->GetStackSym()) ||\\n+\\n+                \/\/ Checks for Case #2\\n+                (nextInstr&& nextInstr->m_opcode == Js::OpCode::Ld_A &&\\n+                 nextInstr->GetSrc1()->IsRegOpnd() &&\\n+                 nextInstr->GetDst()->IsRegOpnd() &&\\n+                 GetVarSymID(instr->GetDst()->GetStackSym()) == nextInstr->GetSrc1()->GetStackSym()->m_id &&\\n+                 GetVarSymID(instr->GetSrc1()->GetStackSym()) == nextInstr->GetDst()->GetStackSym()->m_id)\\n+            )\\n+        )\\n         {\\n-            sym = instr->GetSrc2()->GetStackSym();\\n+            isInductionVar = true;\\n         }\\n+        \\n+        \/\/ Even if dstIsInductionVar then dst == src1 so it\\'s safe to use src1 as the induction sym always.\\n+        StackSym* sym = instr->GetSrc1()->GetStackSym();\\n \\n         SymID inductionSymID = GetVarSymID(sym);\\n \\n-        if (IsSymIDInductionVariable(inductionSymID, this->currentBlock->loop))\\n+        if (isInductionVar && IsSymIDInductionVariable(inductionSymID, this->currentBlock->loop))\\n         {\\n             if (!isChangedByOne)\\n             {\\n@@ -2246,7 +2265,6 @@ GlobOpt::CollectMemOpInfo(IR::Instr *instrBegin, IR::Instr *instr, Value *src1Va\\n                     {\\n                         inductionVariableChangeInfo.unroll++;\\n                     }\\n-                    \\n                     inductionVariableChangeInfo.isIncremental = isIncr;\\n                     loop->memOpInfo->inductionVariableChangeInfoMap->Item(inductionSymID, inductionVariableChangeInfo);\\n                 }\\n@@ -2284,6 +2302,27 @@ GlobOpt::CollectMemOpInfo(IR::Instr *instrBegin, IR::Instr *instr, Value *src1Va\\n             }\\n         }\\n         NEXT_INSTR_IN_RANGE;\\n+        IR::Instr* prevInstr = instr->m_prev;\\n+\\n+        \/\/ If an instr where the dst is an induction variable (and thus is being written to) is not caught by a case in the above\\n+        \/\/ switch statement (which implies that this instr does not contributes to a inductionVariableChangeInfo) and in the default\\n+        \/\/ case does not set doMemOp to false (which implies that this instr does not invalidate this MemOp), then FailFast as we\\n+        \/\/ should not be performing a MemOp under these conditions. \\n+        AssertOrFailFast(!instr->GetDst() || instr->m_opcode == Js::OpCode::IncrLoopBodyCount || !loop->memOpInfo ||\\n+\\n+            \/\/ Refer to \"Case #2\" described above in this function. For the following IR:\\n+            \/\/ Line #1: s4(s2) = Add_I4 s3(s1) 1\\n+            \/\/ Line #2: s3(s1) = Ld_A   s4(s2)\\n+            \/\/ do not consider line #2 as a violating instr\\n+            (instr->m_opcode == Js::OpCode::Ld_I4 &&\\n+                prevInstr && (prevInstr->m_opcode == Js::OpCode::Add_I4 || prevInstr->m_opcode == Js::OpCode::Sub_I4) &&\\n+                instr->GetSrc1()->IsRegOpnd() &&\\n+                instr->GetDst()->IsRegOpnd() &&\\n+                prevInstr->GetDst()->IsRegOpnd() &&\\n+                instr->GetDst()->GetStackSym() == prevInstr->GetSrc1()->GetStackSym() &&\\n+                instr->GetSrc1()->GetStackSym() == prevInstr->GetDst()->GetStackSym()) ||\\n+\\n+            !loop->memOpInfo->inductionVariableChangeInfoMap->ContainsKey(GetVarSymID(instr->GetDst()->GetStackSym())));\\n     }\\n \\n     return true;'}}",
      "message_norm":"cve-2019-1335",
      "language":"ro",
      "entities":"[('cve-2019-1335', 'VULNID', 'CVE')]",
      "classification_level_1":null,
      "classification_level_2":null,
      "list_files":"dict_keys(['lib\/Backend\/GlobOpt.cpp'])",
      "num_files":1.0,
      "patch_content":"From a4e56547fb8b7450656bfd26dfc52b8477c8ef27 Mon Sep 17 00:00:00 2001\nFrom: Wyatt <wyrichte@microsoft.com>\nDate: Tue, 3 Sep 2019 14:52:17 -0700\nSubject: [PATCH] CVE-2019-1335\n\n---\n lib\/Backend\/GlobOpt.cpp | 65 ++++++++++++++++++++++++++++++++---------\n 1 file changed, 52 insertions(+), 13 deletions(-)\n\ndiff --git a\/lib\/Backend\/GlobOpt.cpp b\/lib\/Backend\/GlobOpt.cpp\nindex e19eed15127..1a7530499c2 100644\n--- a\/lib\/Backend\/GlobOpt.cpp\n+++ b\/lib\/Backend\/GlobOpt.cpp\n@@ -2161,27 +2161,46 @@ GlobOpt::CollectMemOpInfo(IR::Instr *instrBegin, IR::Instr *instr, Value *src1Va\n             return false;\n         }\n         break;\n-    case Js::OpCode::Decr_A:\n-        isIncr = false;\n-    case Js::OpCode::Incr_A:\n-        isChangedByOne = true;\n-        goto MemOpCheckInductionVariable;\n     case Js::OpCode::Sub_I4:\n-    case Js::OpCode::Sub_A:\n         isIncr = false;\n-    case Js::OpCode::Add_A:\n     case Js::OpCode::Add_I4:\n     {\n-MemOpCheckInductionVariable:\n-        StackSym *sym = instr->GetSrc1()->GetStackSym();\n-        if (!sym)\n+        \/\/ The only case in which these OpCodes can contribute to an inductionVariableChangeInfo\n+        \/\/ is when the induction variable is being modified and overwritten aswell (ex: j = j + 1)\n+        \/\/ and not when the induction variable is modified but not overwritten (ex: k = j + 1).\n+        \/\/ This can either be detected in IR as\n+        \/\/ s1     = Add_I4 s1     1  \/\/ Case #1, can be seen with \"j++\".\n+        \/\/ or as\n+        \/\/ s4(s2) = Add_I4 s3(s1) 1  \/\/ Case #2, can be see with \"j = j + 1\".\n+        \/\/ s1     = Ld_A   s2\n+        bool isInductionVar = false;\n+        IR::Instr* nextInstr = instr->m_next;\n+        if (\n+            \/\/ Checks for Case #1 and Case #2\n+            instr->GetDst()->GetStackSym() != nullptr &&\n+            instr->GetDst()->IsRegOpnd() &&\n+            (\n+                \/\/ Checks for Case #1\n+                (instr->GetDst()->GetStackSym() == instr->GetSrc1()->GetStackSym()) ||\n+\n+                \/\/ Checks for Case #2\n+                (nextInstr&& nextInstr->m_opcode == Js::OpCode::Ld_A &&\n+                 nextInstr->GetSrc1()->IsRegOpnd() &&\n+                 nextInstr->GetDst()->IsRegOpnd() &&\n+                 GetVarSymID(instr->GetDst()->GetStackSym()) == nextInstr->GetSrc1()->GetStackSym()->m_id &&\n+                 GetVarSymID(instr->GetSrc1()->GetStackSym()) == nextInstr->GetDst()->GetStackSym()->m_id)\n+            )\n+        )\n         {\n-            sym = instr->GetSrc2()->GetStackSym();\n+            isInductionVar = true;\n         }\n+        \n+        \/\/ Even if dstIsInductionVar then dst == src1 so it's safe to use src1 as the induction sym always.\n+        StackSym* sym = instr->GetSrc1()->GetStackSym();\n \n         SymID inductionSymID = GetVarSymID(sym);\n \n-        if (IsSymIDInductionVariable(inductionSymID, this->currentBlock->loop))\n+        if (isInductionVar && IsSymIDInductionVariable(inductionSymID, this->currentBlock->loop))\n         {\n             if (!isChangedByOne)\n             {\n@@ -2246,7 +2265,6 @@ GlobOpt::CollectMemOpInfo(IR::Instr *instrBegin, IR::Instr *instr, Value *src1Va\n                     {\n                         inductionVariableChangeInfo.unroll++;\n                     }\n-                    \n                     inductionVariableChangeInfo.isIncremental = isIncr;\n                     loop->memOpInfo->inductionVariableChangeInfoMap->Item(inductionSymID, inductionVariableChangeInfo);\n                 }\n@@ -2284,6 +2302,27 @@ GlobOpt::CollectMemOpInfo(IR::Instr *instrBegin, IR::Instr *instr, Value *src1Va\n             }\n         }\n         NEXT_INSTR_IN_RANGE;\n+        IR::Instr* prevInstr = instr->m_prev;\n+\n+        \/\/ If an instr where the dst is an induction variable (and thus is being written to) is not caught by a case in the above\n+        \/\/ switch statement (which implies that this instr does not contributes to a inductionVariableChangeInfo) and in the default\n+        \/\/ case does not set doMemOp to false (which implies that this instr does not invalidate this MemOp), then FailFast as we\n+        \/\/ should not be performing a MemOp under these conditions. \n+        AssertOrFailFast(!instr->GetDst() || instr->m_opcode == Js::OpCode::IncrLoopBodyCount || !loop->memOpInfo ||\n+\n+            \/\/ Refer to \"Case #2\" described above in this function. For the following IR:\n+            \/\/ Line #1: s4(s2) = Add_I4 s3(s1) 1\n+            \/\/ Line #2: s3(s1) = Ld_A   s4(s2)\n+            \/\/ do not consider line #2 as a violating instr\n+            (instr->m_opcode == Js::OpCode::Ld_I4 &&\n+                prevInstr && (prevInstr->m_opcode == Js::OpCode::Add_I4 || prevInstr->m_opcode == Js::OpCode::Sub_I4) &&\n+                instr->GetSrc1()->IsRegOpnd() &&\n+                instr->GetDst()->IsRegOpnd() &&\n+                prevInstr->GetDst()->IsRegOpnd() &&\n+                instr->GetDst()->GetStackSym() == prevInstr->GetSrc1()->GetStackSym() &&\n+                instr->GetSrc1()->GetStackSym() == prevInstr->GetDst()->GetStackSym()) ||\n+\n+            !loop->memOpInfo->inductionVariableChangeInfoMap->ContainsKey(GetVarSymID(instr->GetDst()->GetStackSym())));\n     }\n \n     return true;",
      "code_diff":"@@ -2161,27 +2161,46 @@ GlobOpt::CollectMemOpInfo(IR::Instr *instrBegin, IR::Instr *instr, Value *src1Va\n             return false;\n         }\n         break;\n-    case Js::OpCode::Decr_A:\n-        isIncr = false;\n-    case Js::OpCode::Incr_A:\n-        isChangedByOne = true;\n-        goto MemOpCheckInductionVariable;\n     case Js::OpCode::Sub_I4:\n-    case Js::OpCode::Sub_A:\n         isIncr = false;\n-    case Js::OpCode::Add_A:\n     case Js::OpCode::Add_I4:\n     {\n-MemOpCheckInductionVariable:\n-        StackSym *sym = instr->GetSrc1()->GetStackSym();\n-        if (!sym)\n+        \/\/ The only case in which these OpCodes can contribute to an inductionVariableChangeInfo\n+        \/\/ is when the induction variable is being modified and overwritten aswell (ex: j = j + 1)\n+        \/\/ and not when the induction variable is modified but not overwritten (ex: k = j + 1).\n+        \/\/ This can either be detected in IR as\n+        \/\/ s1     = Add_I4 s1     1  \/\/ Case #1, can be seen with \"j++\".\n+        \/\/ or as\n+        \/\/ s4(s2) = Add_I4 s3(s1) 1  \/\/ Case #2, can be see with \"j = j + 1\".\n+        \/\/ s1     = Ld_A   s2\n+        bool isInductionVar = false;\n+        IR::Instr* nextInstr = instr->m_next;\n+        if (\n+            \/\/ Checks for Case #1 and Case #2\n+            instr->GetDst()->GetStackSym() != nullptr &&\n+            instr->GetDst()->IsRegOpnd() &&\n+            (\n+                \/\/ Checks for Case #1\n+                (instr->GetDst()->GetStackSym() == instr->GetSrc1()->GetStackSym()) ||\n+\n+                \/\/ Checks for Case #2\n+                (nextInstr&& nextInstr->m_opcode == Js::OpCode::Ld_A &&\n+                 nextInstr->GetSrc1()->IsRegOpnd() &&\n+                 nextInstr->GetDst()->IsRegOpnd() &&\n+                 GetVarSymID(instr->GetDst()->GetStackSym()) == nextInstr->GetSrc1()->GetStackSym()->m_id &&\n+                 GetVarSymID(instr->GetSrc1()->GetStackSym()) == nextInstr->GetDst()->GetStackSym()->m_id)\n+            )\n+        )\n         {\n-            sym = instr->GetSrc2()->GetStackSym();\n+            isInductionVar = true;\n         }\n+        \n+        \/\/ Even if dstIsInductionVar then dst == src1 so it's safe to use src1 as the induction sym always.\n+        StackSym* sym = instr->GetSrc1()->GetStackSym();\n \n         SymID inductionSymID = GetVarSymID(sym);\n \n-        if (IsSymIDInductionVariable(inductionSymID, this->currentBlock->loop))\n+        if (isInductionVar && IsSymIDInductionVariable(inductionSymID, this->currentBlock->loop))\n         {\n             if (!isChangedByOne)\n             {\n@@ -2246,7 +2265,6 @@ GlobOpt::CollectMemOpInfo(IR::Instr *instrBegin, IR::Instr *instr, Value *src1Va\n                     {\n                         inductionVariableChangeInfo.unroll++;\n                     }\n-                    \n                     inductionVariableChangeInfo.isIncremental = isIncr;\n                     loop->memOpInfo->inductionVariableChangeInfoMap->Item(inductionSymID, inductionVariableChangeInfo);\n                 }\n@@ -2284,6 +2302,27 @@ GlobOpt::CollectMemOpInfo(IR::Instr *instrBegin, IR::Instr *instr, Value *src1Va\n             }\n         }\n         NEXT_INSTR_IN_RANGE;\n+        IR::Instr* prevInstr = instr->m_prev;\n+\n+        \/\/ If an instr where the dst is an induction variable (and thus is being written to) is not caught by a case in the above\n+        \/\/ switch statement (which implies that this instr does not contributes to a inductionVariableChangeInfo) and in the default\n+        \/\/ case does not set doMemOp to false (which implies that this instr does not invalidate this MemOp), then FailFast as we\n+        \/\/ should not be performing a MemOp under these conditions. \n+        AssertOrFailFast(!instr->GetDst() || instr->m_opcode == Js::OpCode::IncrLoopBodyCount || !loop->memOpInfo ||\n+\n+            \/\/ Refer to \"Case #2\" described above in this function. For the following IR:\n+            \/\/ Line #1: s4(s2) = Add_I4 s3(s1) 1\n+            \/\/ Line #2: s3(s1) = Ld_A   s4(s2)\n+            \/\/ do not consider line #2 as a violating instr\n+            (instr->m_opcode == Js::OpCode::Ld_I4 &&\n+                prevInstr && (prevInstr->m_opcode == Js::OpCode::Add_I4 || prevInstr->m_opcode == Js::OpCode::Sub_I4) &&\n+                instr->GetSrc1()->IsRegOpnd() &&\n+                instr->GetDst()->IsRegOpnd() &&\n+                prevInstr->GetDst()->IsRegOpnd() &&\n+                instr->GetDst()->GetStackSym() == prevInstr->GetSrc1()->GetStackSym() &&\n+                instr->GetSrc1()->GetStackSym() == prevInstr->GetDst()->GetStackSym()) ||\n+\n+            !loop->memOpInfo->inductionVariableChangeInfoMap->ContainsKey(GetVarSymID(instr->GetDst()->GetStackSym())));\n     }\n \n     return true;"
    },
    {
      "index":10,
      "vuln_id":"GHSA-77gp-3h4r-6428",
      "cwe_id":"{'CWE-787', 'CWE-125'}",
      "score":8.8,
      "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/0657c83d08845cc434175934c642299de2c0f042'}",
      "dataset":"osv",
      "summary":"Out of bounds read and write in Tensorflow ### Impact\nThere is a typo in TensorFlow's [`SpecializeType`](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/a1320ec1eac186da1d03f033109191f715b2b130\/tensorflow\/core\/framework\/full_type_util.cc#L81-L102) which results in heap OOB read\/write:\n\n```cc\nfor (int i = 0; i < op_def.output_arg_size(); i++) {\n  \/\/ ...\n  for (int j = 0; j < t->args_size(); j++) {\n    auto* arg = t->mutable_args(i);\n    \/\/ ...\n  }\n} \n```\n\nDue to a typo, `arg` is initialized to the `i`th mutable argument in a loop where the loop index is `j`. Hence it is possible to assign to `arg` from outside the vector of arguments. Since this is a mutable proto value, it allows both read and write to outside of bounds data.\n\n### Patches\nWe have patched the issue in GitHub commit [0657c83d08845cc434175934c642299de2c0f042](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/0657c83d08845cc434175934c642299de2c0f042).\n\nThe fix will be included in TensorFlow 2.8.0. We will also cherrypick this commit on TensorFlow 2.7.1, and TensorFlow 2.6.3, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.",
      "published_date":"2022-02-09",
      "chain_len":1,
      "project":"https:\/\/github.com\/tensorflow\/tensorflow",
      "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/0657c83d08845cc434175934c642299de2c0f042",
      "commit_sha":"0657c83d08845cc434175934c642299de2c0f042",
      "patch":"SINGLE",
      "chain_ord":"['0657c83d08845cc434175934c642299de2c0f042']",
      "before_first_fix_commit":"{'6e65b0b4ad12fdaa223e87b4ae6d8cb762fcae2b'}",
      "last_fix_commit":"0657c83d08845cc434175934c642299de2c0f042",
      "chain_ord_pos":1.0,
      "commit_datetime":"11\/09\/2021, 12:44:43",
      "message":"Fix heap OOB read\/write due to incorrect indexing.\n\nPiperOrigin-RevId: 408578046\nChange-Id: Ifc9ffea49e5890f55fcb2c27568611052c3ddcfa",
      "author":"Mihai Maruseac",
      "comments":null,
      "stats":"{'additions': 1, 'deletions': 1, 'total': 2}",
      "files":"{'tensorflow\/core\/framework\/full_type_util.cc': {'additions': 1, 'deletions': 1, 'changes': 2, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/0657c83d08845cc434175934c642299de2c0f042\/tensorflow%2Fcore%2Fframework%2Ffull_type_util.cc', 'patch': '@@ -100,7 +100,7 @@ StatusOr<FullTypeDef> SpecializeType(const AttrSlice& attrs,\\n     \/\/ verifications are needed, they should be done by separately, and in a\\n     \/\/ way that can be reused for type inference.\\n     for (int j = 0; j < t->args_size(); j++) {\\n-      auto* arg = t->mutable_args(i);\\n+      auto* arg = t->mutable_args(j);\\n       if (arg->type_id() == TFT_VAR) {\\n         const auto* attr = attrs.Find(arg->s());\\n         if (attr == nullptr) {'}}",
      "message_norm":"fix heap oob read\/write due to incorrect indexing.\n\npiperorigin-revid: 408578046\nchange-id: ifc9ffea49e5890f55fcb2c27568611052c3ddcfa",
      "language":"en",
      "entities":"[('fix', 'ACTION', ''), ('heap oob', 'SECWORD', ''), ('408578046', 'SHA', 'generic_sha')]",
      "classification_level_1":null,
      "classification_level_2":null,
      "list_files":"dict_keys(['tensorflow\/core\/framework\/full_type_util.cc'])",
      "num_files":1.0,
      "patch_content":"From 0657c83d08845cc434175934c642299de2c0f042 Mon Sep 17 00:00:00 2001\nFrom: Mihai Maruseac <mihaimaruseac@google.com>\nDate: Tue, 9 Nov 2021 04:44:43 -0800\nSubject: [PATCH] Fix heap OOB read\/write due to incorrect indexing.\n\nPiperOrigin-RevId: 408578046\nChange-Id: Ifc9ffea49e5890f55fcb2c27568611052c3ddcfa\n---\n tensorflow\/core\/framework\/full_type_util.cc | 2 +-\n 1 file changed, 1 insertion(+), 1 deletion(-)\n\ndiff --git a\/tensorflow\/core\/framework\/full_type_util.cc b\/tensorflow\/core\/framework\/full_type_util.cc\nindex e0d8ca0721c850..89617dc97f2496 100644\n--- a\/tensorflow\/core\/framework\/full_type_util.cc\n+++ b\/tensorflow\/core\/framework\/full_type_util.cc\n@@ -100,7 +100,7 @@ StatusOr<FullTypeDef> SpecializeType(const AttrSlice& attrs,\n     \/\/ verifications are needed, they should be done by separately, and in a\n     \/\/ way that can be reused for type inference.\n     for (int j = 0; j < t->args_size(); j++) {\n-      auto* arg = t->mutable_args(i);\n+      auto* arg = t->mutable_args(j);\n       if (arg->type_id() == TFT_VAR) {\n         const auto* attr = attrs.Find(arg->s());\n         if (attr == nullptr) {",
      "code_diff":"@@ -100,7 +100,7 @@ StatusOr<FullTypeDef> SpecializeType(const AttrSlice& attrs,\n     \/\/ verifications are needed, they should be done by separately, and in a\n     \/\/ way that can be reused for type inference.\n     for (int j = 0; j < t->args_size(); j++) {\n-      auto* arg = t->mutable_args(i);\n+      auto* arg = t->mutable_args(j);\n       if (arg->type_id() == TFT_VAR) {\n         const auto* attr = attrs.Find(arg->s());\n         if (attr == nullptr) {"
    },
    {
      "index":11,
      "vuln_id":"GHSA-37pf-w9ff-gqvm",
      "cwe_id":"{'CWE-787'}",
      "score":7.5,
      "chain":"{'https:\/\/github.com\/chakra-core\/ChakraCore\/commit\/d797e3f00e34c12c8c0ae52f56344325439dccd7', 'https:\/\/github.com\/chakra-core\/ChakraCore\/commit\/87ac2b5a751710ee288fdda3fd4d9818e22387a1'}",
      "dataset":"osv",
      "summary":"Out-of-bounds write A remote code execution vulnerability exists in the way that the Chakra scripting engine handles objects in memory in Microsoft Edge, aka 'Chakra Scripting Engine Memory Corruption Vulnerability'. This CVE ID is unique from CVE-2019-0912, CVE-2019-0913, CVE-2019-0914, CVE-2019-0915, CVE-2019-0916, CVE-2019-0917, CVE-2019-0922, CVE-2019-0923, CVE-2019-0924, CVE-2019-0925, CVE-2019-0933, CVE-2019-0937.",
      "published_date":"2021-03-29",
      "chain_len":2,
      "project":"https:\/\/github.com\/chakra-core\/ChakraCore",
      "commit_href":"https:\/\/github.com\/chakra-core\/ChakraCore\/commit\/87ac2b5a751710ee288fdda3fd4d9818e22387a1",
      "commit_sha":"87ac2b5a751710ee288fdda3fd4d9818e22387a1",
      "patch":"MULTI",
      "chain_ord":"['87ac2b5a751710ee288fdda3fd4d9818e22387a1', 'd797e3f00e34c12c8c0ae52f56344325439dccd7']",
      "before_first_fix_commit":"{'ea0491305137183603bf43844b5584d4cc972e28', '4594e340bc9ca9f857010a68e8b562d65b46eed6'}",
      "last_fix_commit":"d797e3f00e34c12c8c0ae52f56344325439dccd7",
      "chain_ord_pos":1.0,
      "commit_datetime":"04\/17\/2019, 17:22:17",
      "message":"[CVE-2019-0927]",
      "author":"Michael Holman",
      "comments":null,
      "stats":"{'additions': 1, 'deletions': 0, 'total': 1}",
      "files":"{'lib\/Backend\/GlobOptFields.cpp': {'additions': 1, 'deletions': 0, 'changes': 1, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/chakra-core\/ChakraCore\/raw\/87ac2b5a751710ee288fdda3fd4d9818e22387a1\/lib%2FBackend%2FGlobOptFields.cpp', 'patch': '@@ -394,6 +394,7 @@ GlobOpt::ProcessFieldKills(IR::Instr *instr, BVSparse<JitArenaAllocator> *bv, bo\\n     case Js::OpCode::StRootFldStrict:\\n     case Js::OpCode::StSlot:\\n     case Js::OpCode::StSlotChkUndecl:\\n+    case Js::OpCode::StSuperFld:\\n         Assert(dstOpnd != nullptr);\\n         sym = dstOpnd->AsSymOpnd()->m_sym;\\n         if (inGlobOpt)'}}",
      "message_norm":"[cve-2019-0927]",
      "language":"ro",
      "entities":"[('cve-2019-0927', 'VULNID', 'CVE')]",
      "classification_level_1":null,
      "classification_level_2":null,
      "list_files":"dict_keys(['lib\/Backend\/GlobOptFields.cpp'])",
      "num_files":1.0,
      "patch_content":"From 87ac2b5a751710ee288fdda3fd4d9818e22387a1 Mon Sep 17 00:00:00 2001\nFrom: Michael Holman <michhol@microsoft.com>\nDate: Wed, 17 Apr 2019 10:22:17 -0700\nSubject: [PATCH] [CVE-2019-0927]\n\n---\n lib\/Backend\/GlobOptFields.cpp | 1 +\n 1 file changed, 1 insertion(+)\n\ndiff --git a\/lib\/Backend\/GlobOptFields.cpp b\/lib\/Backend\/GlobOptFields.cpp\nindex 2eff8b7c61f..05594e71269 100644\n--- a\/lib\/Backend\/GlobOptFields.cpp\n+++ b\/lib\/Backend\/GlobOptFields.cpp\n@@ -394,6 +394,7 @@ GlobOpt::ProcessFieldKills(IR::Instr *instr, BVSparse<JitArenaAllocator> *bv, bo\n     case Js::OpCode::StRootFldStrict:\n     case Js::OpCode::StSlot:\n     case Js::OpCode::StSlotChkUndecl:\n+    case Js::OpCode::StSuperFld:\n         Assert(dstOpnd != nullptr);\n         sym = dstOpnd->AsSymOpnd()->m_sym;\n         if (inGlobOpt)",
      "code_diff":"@@ -394,6 +394,7 @@ GlobOpt::ProcessFieldKills(IR::Instr *instr, BVSparse<JitArenaAllocator> *bv, bo\n     case Js::OpCode::StRootFldStrict:\n     case Js::OpCode::StSlot:\n     case Js::OpCode::StSlotChkUndecl:\n+    case Js::OpCode::StSuperFld:\n         Assert(dstOpnd != nullptr);\n         sym = dstOpnd->AsSymOpnd()->m_sym;\n         if (inGlobOpt)"
    },
    {
      "index":12,
      "vuln_id":"GHSA-3ff2-r28g-w7h9",
      "cwe_id":"{'CWE-787', 'CWE-120'}",
      "score":5.5,
      "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/c79ba87153ee343401dbe9d1954d7f79e521eb14'}",
      "dataset":"osv",
      "summary":"Heap buffer overflow in `Transpose` ### Impact\nThe [shape inference function for `Transpose`](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/8d72537c6abf5a44103b57b9c2e22c14f5f49698\/tensorflow\/core\/ops\/array_ops.cc#L121-L185) is vulnerable to a heap buffer overflow:\n\n```python\nimport tensorflow as tf\n@tf.function\ndef test():\n  y = tf.raw_ops.Transpose(x=[1,2,3,4],perm=[-10])\n  return y\n\ntest()\n```\n\nThis occurs whenever `perm` contains negative elements. The shape inference function does not validate that the indices in `perm` are all valid:\n        \n```cc\nfor (int32_t i = 0; i < rank; ++i) {\n  int64_t in_idx = data[i];\n  if (in_idx >= rank) {\n    return errors::InvalidArgument(\"perm dim \", in_idx,\n                                   \" is out of range of input rank \", rank);\n  }\n  dims[i] = c->Dim(input, in_idx);\n}\n```\n\nwhere `Dim(tensor, index)` accepts either a positive index less than the rank of the tensor or the special value `-1` for unknown dimensions.\n\n### Patches\nWe have patched the issue in GitHub commit [c79ba87153ee343401dbe9d1954d7f79e521eb14](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/c79ba87153ee343401dbe9d1954d7f79e521eb14).\n\nThe fix will be included in TensorFlow 2.7.0. We will also cherrypick this commit on TensorFlow 2.6.1, TensorFlow 2.5.2, and TensorFlow 2.4.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by members of the Aivul Team from Qihoo 360.",
      "published_date":"2021-11-10",
      "chain_len":1,
      "project":"https:\/\/github.com\/tensorflow\/tensorflow",
      "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/c79ba87153ee343401dbe9d1954d7f79e521eb14",
      "commit_sha":"c79ba87153ee343401dbe9d1954d7f79e521eb14",
      "patch":"SINGLE",
      "chain_ord":"['c79ba87153ee343401dbe9d1954d7f79e521eb14']",
      "before_first_fix_commit":"{'042dc3be4c54a51c2608ad53dabaeb34afa3e63c'}",
      "last_fix_commit":"c79ba87153ee343401dbe9d1954d7f79e521eb14",
      "chain_ord_pos":1.0,
      "commit_datetime":"10\/15\/2021, 02:39:00",
      "message":"Make Transpose's shape inference function validate that negative `perm` values are within the tensor's rank.\n\nPiperOrigin-RevId: 403252853\nChange-Id: Ia6b31b45b237312668bb31c2c3b3c7bbce2d2610",
      "author":"Penporn Koanantakool",
      "comments":null,
      "stats":"{'additions': 1, 'deletions': 1, 'total': 2}",
      "files":"{'tensorflow\/core\/ops\/array_ops.cc': {'additions': 1, 'deletions': 1, 'changes': 2, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/c79ba87153ee343401dbe9d1954d7f79e521eb14\/tensorflow%2Fcore%2Fops%2Farray_ops.cc', 'patch': '@@ -168,7 +168,7 @@ Status TransposeShapeFn(InferenceContext* c) {\\n \\n     for (int32_t i = 0; i < rank; ++i) {\\n       int64_t in_idx = data[i];\\n-      if (in_idx >= rank) {\\n+      if (in_idx >= rank || in_idx <= -rank) {\\n         return errors::InvalidArgument(\"perm dim \", in_idx,\\n                                        \" is out of range of input rank \", rank);\\n       }'}}",
      "message_norm":"make transpose's shape inference function validate that negative `perm` values are within the tensor's rank.\n\npiperorigin-revid: 403252853\nchange-id: ia6b31b45b237312668bb31c2c3b3c7bbce2d2610",
      "language":"en",
      "entities":"[('validate', 'ACTION', ''), ('403252853', 'SHA', 'generic_sha')]",
      "classification_level_1":null,
      "classification_level_2":null,
      "list_files":"dict_keys(['tensorflow\/core\/ops\/array_ops.cc'])",
      "num_files":1.0,
      "patch_content":"From c79ba87153ee343401dbe9d1954d7f79e521eb14 Mon Sep 17 00:00:00 2001\nFrom: Penporn Koanantakool <penporn@google.com>\nDate: Thu, 14 Oct 2021 19:39:00 -0700\nSubject: [PATCH] Make Transpose's shape inference function validate that\n negative `perm` values are within the tensor's rank.\n\nPiperOrigin-RevId: 403252853\nChange-Id: Ia6b31b45b237312668bb31c2c3b3c7bbce2d2610\n---\n tensorflow\/core\/ops\/array_ops.cc | 2 +-\n 1 file changed, 1 insertion(+), 1 deletion(-)\n\ndiff --git a\/tensorflow\/core\/ops\/array_ops.cc b\/tensorflow\/core\/ops\/array_ops.cc\nindex 64bd4f38478542..14c9efae1ddd3b 100644\n--- a\/tensorflow\/core\/ops\/array_ops.cc\n+++ b\/tensorflow\/core\/ops\/array_ops.cc\n@@ -168,7 +168,7 @@ Status TransposeShapeFn(InferenceContext* c) {\n \n     for (int32_t i = 0; i < rank; ++i) {\n       int64_t in_idx = data[i];\n-      if (in_idx >= rank) {\n+      if (in_idx >= rank || in_idx <= -rank) {\n         return errors::InvalidArgument(\"perm dim \", in_idx,\n                                        \" is out of range of input rank \", rank);\n       }",
      "code_diff":"@@ -168,7 +168,7 @@ Status TransposeShapeFn(InferenceContext* c) {\n \n     for (int32_t i = 0; i < rank; ++i) {\n       int64_t in_idx = data[i];\n-      if (in_idx >= rank) {\n+      if (in_idx >= rank || in_idx <= -rank) {\n         return errors::InvalidArgument(\"perm dim \", in_idx,\n                                        \" is out of range of input rank \", rank);\n       }"
    },
    {
      "index":13,
      "vuln_id":"GHSA-6f89-8j54-29xf",
      "cwe_id":"{'CWE-787', 'CWE-119'}",
      "score":2.5,
      "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/12c727cee857fa19be717f336943d95fca4ffe4f'}",
      "dataset":"osv",
      "summary":"Heap buffer overflow in `FractionalAvgPoolGrad` ### Impact\nThe implementation of `tf.raw_ops.FractionalAvgPoolGrad` is vulnerable to a heap buffer overflow:\n  \n```python\nimport tensorflow as tf\n\norig_input_tensor_shape = tf.constant([1, 3, 2, 3], shape=[4], dtype=tf.int64)\nout_backprop = tf.constant([2], shape=[1, 1, 1, 1], dtype=tf.int64)\nrow_pooling_sequence = tf.constant([1], shape=[1], dtype=tf.int64)\ncol_pooling_sequence = tf.constant([1], shape=[1], dtype=tf.int64)\n\n\ntf.raw_ops.FractionalAvgPoolGrad(\n  orig_input_tensor_shape=orig_input_tensor_shape, out_backprop=out_backprop,\n  row_pooling_sequence=row_pooling_sequence,\n  col_pooling_sequence=col_pooling_sequence, overlapping=False)\n```\n\nThe [implementation](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/dcba796a28364d6d7f003f6fe733d82726dda713\/tensorflow\/core\/kernels\/fractional_avg_pool_op.cc#L216) fails to validate that the pooling sequence arguments have enough elements as required by the `out_backprop` tensor shape.\n\n### Patches\nWe have patched the issue in GitHub commit [12c727cee857fa19be717f336943d95fca4ffe4f](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/12c727cee857fa19be717f336943d95fca4ffe4f).\n\nThe fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by Ying Wang and Yakun Zhang of Baidu X-Team.",
      "published_date":"2021-05-21",
      "chain_len":1,
      "project":"https:\/\/github.com\/tensorflow\/tensorflow",
      "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/12c727cee857fa19be717f336943d95fca4ffe4f",
      "commit_sha":"12c727cee857fa19be717f336943d95fca4ffe4f",
      "patch":"SINGLE",
      "chain_ord":"['12c727cee857fa19be717f336943d95fca4ffe4f']",
      "before_first_fix_commit":"{'dcba796a28364d6d7f003f6fe733d82726dda713'}",
      "last_fix_commit":"12c727cee857fa19be717f336943d95fca4ffe4f",
      "chain_ord_pos":1.0,
      "commit_datetime":"05\/06\/2021, 21:02:47",
      "message":"Validate inputs of `FractionalAvgPoolGrad`.\n\nPiperOrigin-RevId: 372420640\nChange-Id: Icc583928e6cdc3062e12498e4d2337a8fe3da016",
      "author":"Mihai Maruseac",
      "comments":null,
      "stats":"{'additions': 13, 'deletions': 0, 'total': 13}",
      "files":"{'tensorflow\/core\/kernels\/fractional_avg_pool_op.cc': {'additions': 13, 'deletions': 0, 'changes': 13, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/12c727cee857fa19be717f336943d95fca4ffe4f\/tensorflow%2Fcore%2Fkernels%2Ffractional_avg_pool_op.cc', 'patch': '@@ -250,6 +250,19 @@ class FractionalAvgPoolGradOp : public OpKernel {\\n     const int64 out_cols = out_backprop.dim_size(2);\\n     const int64 out_depth = out_backprop.dim_size(3);\\n \\n+    OP_REQUIRES(context, row_seq_tensor.NumElements() > out_rows,\\n+                errors::InvalidArgument(\"Given out_backprop shape \",\\n+                                        out_backprop.shape().DebugString(),\\n+                                        \", row_seq_tensor must have at least \",\\n+                                        out_rows + 1, \" elements, but got \",\\n+                                        row_seq_tensor.NumElements()));\\n+    OP_REQUIRES(context, col_seq_tensor.NumElements() > out_cols,\\n+                errors::InvalidArgument(\"Given out_backprop shape \",\\n+                                        out_backprop.shape().DebugString(),\\n+                                        \", col_seq_tensor must have at least \",\\n+                                        out_cols + 1, \" elements, but got \",\\n+                                        col_seq_tensor.NumElements()));\\n+\\n     auto row_seq_tensor_flat = row_seq_tensor.flat<int64>();\\n     auto col_seq_tensor_flat = col_seq_tensor.flat<int64>();\\n     auto orig_input_tensor_shape_flat = orig_input_tensor_shape.flat<int64>();'}}",
      "message_norm":"validate inputs of `fractionalavgpoolgrad`.\n\npiperorigin-revid: 372420640\nchange-id: icc583928e6cdc3062e12498e4d2337a8fe3da016",
      "language":"it",
      "entities":"[('validate', 'ACTION', ''), ('372420640', 'SHA', 'generic_sha')]",
      "classification_level_1":null,
      "classification_level_2":null,
      "list_files":"dict_keys(['tensorflow\/core\/kernels\/fractional_avg_pool_op.cc'])",
      "num_files":1.0,
      "patch_content":"From 12c727cee857fa19be717f336943d95fca4ffe4f Mon Sep 17 00:00:00 2001\nFrom: Mihai Maruseac <mihaimaruseac@google.com>\nDate: Thu, 6 May 2021 14:02:47 -0700\nSubject: [PATCH] Validate inputs of `FractionalAvgPoolGrad`.\n\nPiperOrigin-RevId: 372420640\nChange-Id: Icc583928e6cdc3062e12498e4d2337a8fe3da016\n---\n tensorflow\/core\/kernels\/fractional_avg_pool_op.cc | 13 +++++++++++++\n 1 file changed, 13 insertions(+)\n\ndiff --git a\/tensorflow\/core\/kernels\/fractional_avg_pool_op.cc b\/tensorflow\/core\/kernels\/fractional_avg_pool_op.cc\nindex b8a5083e5340f1..0452638a066795 100644\n--- a\/tensorflow\/core\/kernels\/fractional_avg_pool_op.cc\n+++ b\/tensorflow\/core\/kernels\/fractional_avg_pool_op.cc\n@@ -250,6 +250,19 @@ class FractionalAvgPoolGradOp : public OpKernel {\n     const int64 out_cols = out_backprop.dim_size(2);\n     const int64 out_depth = out_backprop.dim_size(3);\n \n+    OP_REQUIRES(context, row_seq_tensor.NumElements() > out_rows,\n+                errors::InvalidArgument(\"Given out_backprop shape \",\n+                                        out_backprop.shape().DebugString(),\n+                                        \", row_seq_tensor must have at least \",\n+                                        out_rows + 1, \" elements, but got \",\n+                                        row_seq_tensor.NumElements()));\n+    OP_REQUIRES(context, col_seq_tensor.NumElements() > out_cols,\n+                errors::InvalidArgument(\"Given out_backprop shape \",\n+                                        out_backprop.shape().DebugString(),\n+                                        \", col_seq_tensor must have at least \",\n+                                        out_cols + 1, \" elements, but got \",\n+                                        col_seq_tensor.NumElements()));\n+\n     auto row_seq_tensor_flat = row_seq_tensor.flat<int64>();\n     auto col_seq_tensor_flat = col_seq_tensor.flat<int64>();\n     auto orig_input_tensor_shape_flat = orig_input_tensor_shape.flat<int64>();",
      "code_diff":"@@ -250,6 +250,19 @@ class FractionalAvgPoolGradOp : public OpKernel {\n     const int64 out_cols = out_backprop.dim_size(2);\n     const int64 out_depth = out_backprop.dim_size(3);\n \n+    OP_REQUIRES(context, row_seq_tensor.NumElements() > out_rows,\n+                errors::InvalidArgument(\"Given out_backprop shape \",\n+                                        out_backprop.shape().DebugString(),\n+                                        \", row_seq_tensor must have at least \",\n+                                        out_rows + 1, \" elements, but got \",\n+                                        row_seq_tensor.NumElements()));\n+    OP_REQUIRES(context, col_seq_tensor.NumElements() > out_cols,\n+                errors::InvalidArgument(\"Given out_backprop shape \",\n+                                        out_backprop.shape().DebugString(),\n+                                        \", col_seq_tensor must have at least \",\n+                                        out_cols + 1, \" elements, but got \",\n+                                        col_seq_tensor.NumElements()));\n+\n     auto row_seq_tensor_flat = row_seq_tensor.flat<int64>();\n     auto col_seq_tensor_flat = col_seq_tensor.flat<int64>();\n     auto orig_input_tensor_shape_flat = orig_input_tensor_shape.flat<int64>();"
    },
    {
      "index":14,
      "vuln_id":"GHSA-m3f9-w3p3-p669",
      "cwe_id":"{'CWE-787', 'CWE-131'}",
      "score":2.5,
      "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/efea03b38fb8d3b81762237dc85e579cc5fc6e87'}",
      "dataset":"osv",
      "summary":"Heap buffer overflow in `QuantizedMul` ### Impact\nAn attacker can cause a heap buffer overflow in `QuantizedMul` by passing in invalid thresholds for the quantization:\n\n```python\nimport tensorflow as tf\n\nx = tf.constant([256, 328], shape=[1, 2], dtype=tf.quint8)\ny = tf.constant([256, 328], shape=[1, 2], dtype=tf.quint8)\nmin_x = tf.constant([], dtype=tf.float32)\nmax_x = tf.constant([], dtype=tf.float32)\nmin_y = tf.constant([], dtype=tf.float32)\nmax_y = tf.constant([], dtype=tf.float32)\n\ntf.raw_ops.QuantizedMul(x=x, y=y, min_x=min_x, max_x=max_x, min_y=min_y, max_y=max_y)\n```\n\nThis is because the [implementation](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/87cf4d3ea9949051e50ca3f071fc909538a51cd0\/tensorflow\/core\/kernels\/quantized_mul_op.cc#L287-L290) assumes that the 4 arguments are always valid scalars and tries to access the numeric value directly:\n\n```cc \nconst float min_x = context->input(2).flat<float>()(0);\nconst float max_x = context->input(3).flat<float>()(0);\nconst float min_y = context->input(4).flat<float>()(0);\nconst float max_y = context->input(5).flat<float>()(0);\n```\n\nHowever, if any of these tensors is empty, then `.flat<T>()` is an empty buffer and accessing the element at position 0 results in overflow.\n\n### Patches\nWe have patched the issue in GitHub commit [efea03b38fb8d3b81762237dc85e579cc5fc6e87](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/efea03b38fb8d3b81762237dc85e579cc5fc6e87).\n\nThe fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by Ying Wang and Yakun Zhang of Baidu X-Team.",
      "published_date":"2021-05-21",
      "chain_len":1,
      "project":"https:\/\/github.com\/tensorflow\/tensorflow",
      "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/efea03b38fb8d3b81762237dc85e579cc5fc6e87",
      "commit_sha":"efea03b38fb8d3b81762237dc85e579cc5fc6e87",
      "patch":"SINGLE",
      "chain_ord":"['efea03b38fb8d3b81762237dc85e579cc5fc6e87']",
      "before_first_fix_commit":"{'87cf4d3ea9949051e50ca3f071fc909538a51cd0'}",
      "last_fix_commit":"efea03b38fb8d3b81762237dc85e579cc5fc6e87",
      "chain_ord_pos":1.0,
      "commit_datetime":"04\/21\/2021, 23:15:46",
      "message":"Validate inputs to `QuantizedMul`\n\nPiperOrigin-RevId: 369756982\nChange-Id: I00d960cc3b9316fd7a86bd37a44e341c96e17624",
      "author":"Mihai Maruseac",
      "comments":null,
      "stats":"{'additions': 16, 'deletions': 4, 'total': 20}",
      "files":"{'tensorflow\/core\/kernels\/quantized_mul_op.cc': {'additions': 16, 'deletions': 4, 'changes': 20, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/efea03b38fb8d3b81762237dc85e579cc5fc6e87\/tensorflow%2Fcore%2Fkernels%2Fquantized_mul_op.cc', 'patch': '@@ -284,10 +284,22 @@ class QuantizedMulOp : public OpKernel {\\n   void Compute(OpKernelContext* context) override {\\n     const Tensor& x = context->input(0);\\n     const Tensor& y = context->input(1);\\n-    const float min_x = context->input(2).flat<float>()(0);\\n-    const float max_x = context->input(3).flat<float>()(0);\\n-    const float min_y = context->input(4).flat<float>()(0);\\n-    const float max_y = context->input(5).flat<float>()(0);\\n+    auto& min_x_tensor = context->input(2);\\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(min_x_tensor.shape()),\\n+                errors::InvalidArgument(\"min_x must be a scalar\"));\\n+    const float min_x = min_x_tensor.flat<float>()(0);\\n+    auto& max_x_tensor = context->input(3);\\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(max_x_tensor.shape()),\\n+                errors::InvalidArgument(\"max_x must be a scalar\"));\\n+    const float max_x = max_x_tensor.flat<float>()(0);\\n+    auto& min_y_tensor = context->input(4);\\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(min_y_tensor.shape()),\\n+                errors::InvalidArgument(\"min_y must be a scalar\"));\\n+    const float min_y = min_y_tensor.flat<float>()(0);\\n+    auto& max_y_tensor = context->input(5);\\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(max_y_tensor.shape()),\\n+                errors::InvalidArgument(\"max_y must be a scalar\"));\\n+    const float max_y = max_y_tensor.flat<float>()(0);\\n \\n     BCast bcast(BCast::FromShape(x.shape()), BCast::FromShape(y.shape()));\\n     if (!bcast.IsValid()) {'}}",
      "message_norm":"validate inputs to `quantizedmul`\n\npiperorigin-revid: 369756982\nchange-id: i00d960cc3b9316fd7a86bd37a44e341c96e17624",
      "language":"it",
      "entities":"[('validate', 'ACTION', ''), ('369756982', 'SHA', 'generic_sha')]",
      "classification_level_1":null,
      "classification_level_2":null,
      "list_files":"dict_keys(['tensorflow\/core\/kernels\/quantized_mul_op.cc'])",
      "num_files":1.0,
      "patch_content":"From efea03b38fb8d3b81762237dc85e579cc5fc6e87 Mon Sep 17 00:00:00 2001\nFrom: Mihai Maruseac <mihaimaruseac@google.com>\nDate: Wed, 21 Apr 2021 16:15:46 -0700\nSubject: [PATCH] Validate inputs to `QuantizedMul`\n\nPiperOrigin-RevId: 369756982\nChange-Id: I00d960cc3b9316fd7a86bd37a44e341c96e17624\n---\n tensorflow\/core\/kernels\/quantized_mul_op.cc | 20 ++++++++++++++++----\n 1 file changed, 16 insertions(+), 4 deletions(-)\n\ndiff --git a\/tensorflow\/core\/kernels\/quantized_mul_op.cc b\/tensorflow\/core\/kernels\/quantized_mul_op.cc\nindex fb56f68bf14dbb..22cff8939449a6 100644\n--- a\/tensorflow\/core\/kernels\/quantized_mul_op.cc\n+++ b\/tensorflow\/core\/kernels\/quantized_mul_op.cc\n@@ -284,10 +284,22 @@ class QuantizedMulOp : public OpKernel {\n   void Compute(OpKernelContext* context) override {\n     const Tensor& x = context->input(0);\n     const Tensor& y = context->input(1);\n-    const float min_x = context->input(2).flat<float>()(0);\n-    const float max_x = context->input(3).flat<float>()(0);\n-    const float min_y = context->input(4).flat<float>()(0);\n-    const float max_y = context->input(5).flat<float>()(0);\n+    auto& min_x_tensor = context->input(2);\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(min_x_tensor.shape()),\n+                errors::InvalidArgument(\"min_x must be a scalar\"));\n+    const float min_x = min_x_tensor.flat<float>()(0);\n+    auto& max_x_tensor = context->input(3);\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(max_x_tensor.shape()),\n+                errors::InvalidArgument(\"max_x must be a scalar\"));\n+    const float max_x = max_x_tensor.flat<float>()(0);\n+    auto& min_y_tensor = context->input(4);\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(min_y_tensor.shape()),\n+                errors::InvalidArgument(\"min_y must be a scalar\"));\n+    const float min_y = min_y_tensor.flat<float>()(0);\n+    auto& max_y_tensor = context->input(5);\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(max_y_tensor.shape()),\n+                errors::InvalidArgument(\"max_y must be a scalar\"));\n+    const float max_y = max_y_tensor.flat<float>()(0);\n \n     BCast bcast(BCast::FromShape(x.shape()), BCast::FromShape(y.shape()));\n     if (!bcast.IsValid()) {",
      "code_diff":"@@ -284,10 +284,22 @@ class QuantizedMulOp : public OpKernel {\n   void Compute(OpKernelContext* context) override {\n     const Tensor& x = context->input(0);\n     const Tensor& y = context->input(1);\n-    const float min_x = context->input(2).flat<float>()(0);\n-    const float max_x = context->input(3).flat<float>()(0);\n-    const float min_y = context->input(4).flat<float>()(0);\n-    const float max_y = context->input(5).flat<float>()(0);\n+    auto& min_x_tensor = context->input(2);\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(min_x_tensor.shape()),\n+                errors::InvalidArgument(\"min_x must be a scalar\"));\n+    const float min_x = min_x_tensor.flat<float>()(0);\n+    auto& max_x_tensor = context->input(3);\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(max_x_tensor.shape()),\n+                errors::InvalidArgument(\"max_x must be a scalar\"));\n+    const float max_x = max_x_tensor.flat<float>()(0);\n+    auto& min_y_tensor = context->input(4);\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(min_y_tensor.shape()),\n+                errors::InvalidArgument(\"min_y must be a scalar\"));\n+    const float min_y = min_y_tensor.flat<float>()(0);\n+    auto& max_y_tensor = context->input(5);\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(max_y_tensor.shape()),\n+                errors::InvalidArgument(\"max_y must be a scalar\"));\n+    const float max_y = max_y_tensor.flat<float>()(0);\n \n     BCast bcast(BCast::FromShape(x.shape()), BCast::FromShape(y.shape()));\n     if (!bcast.IsValid()) {"
    },
    {
      "index":15,
      "vuln_id":"GHSA-2rfj-2mwp-787v",
      "cwe_id":"{'CWE-787'}",
      "score":7.5,
      "chain":"{'https:\/\/github.com\/chakra-core\/ChakraCore\/commit\/3d6226cc2d1077537220361c82e34a362c6c76ee', 'https:\/\/github.com\/chakra-core\/ChakraCore\/commit\/36644ee0d4cc2bc97a3cd49c3540e6eea193182a'}",
      "dataset":"osv",
      "summary":"Out-of-bounds write A remote code execution vulnerability exists in the way that the Chakra scripting engine handles objects in memory in Microsoft Edge, aka 'Chakra Scripting Engine Memory Corruption Vulnerability'. This CVE ID is unique from CVE-2019-0989, CVE-2019-0991, CVE-2019-0992, CVE-2019-1002, CVE-2019-1003, CVE-2019-1024, CVE-2019-1051, CVE-2019-1052.",
      "published_date":"2021-03-29",
      "chain_len":2,
      "project":"https:\/\/github.com\/chakra-core\/ChakraCore",
      "commit_href":"https:\/\/github.com\/chakra-core\/ChakraCore\/commit\/36644ee0d4cc2bc97a3cd49c3540e6eea193182a",
      "commit_sha":"36644ee0d4cc2bc97a3cd49c3540e6eea193182a",
      "patch":"MULTI",
      "chain_ord":"['36644ee0d4cc2bc97a3cd49c3540e6eea193182a', '3d6226cc2d1077537220361c82e34a362c6c76ee']",
      "before_first_fix_commit":"{'d797e3f00e34c12c8c0ae52f56344325439dccd7', 'eabf77ad17010f220639e5261798da9ac14e43e3'}",
      "last_fix_commit":"3d6226cc2d1077537220361c82e34a362c6c76ee",
      "chain_ord_pos":1.0,
      "commit_datetime":"05\/15\/2019, 23:54:48",
      "message":"CVE-2019-0993",
      "author":"Paul Leathers",
      "comments":null,
      "stats":"{'additions': 4, 'deletions': 0, 'total': 4}",
      "files":"{'lib\/Runtime\/Language\/JavascriptOperators.cpp': {'additions': 4, 'deletions': 0, 'changes': 4, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/chakra-core\/ChakraCore\/raw\/36644ee0d4cc2bc97a3cd49c3540e6eea193182a\/lib%2FRuntime%2FLanguage%2FJavascriptOperators.cpp', 'patch': \"@@ -9655,6 +9655,10 @@ using namespace Js;\\n             Var result = CALL_ENTRYPOINT(threadContext, marshalledFunction->GetEntryPoint(), function, CallInfo(flags, 1), thisVar);\\n             result = CrossSite::MarshalVar(requestContext, result);\\n \\n+            \/\/ Set implicit call flags so we bail out if we're trying to propagate the value forward, e.g., from a compare. Subsequent calls\\n+            \/\/ to the getter may produce different results.\\n+            threadContext->AddImplicitCallFlags(ImplicitCall_Accessor);\\n+\\n             return result;\\n         });\\n     }\"}}",
      "message_norm":"cve-2019-0993",
      "language":"ro",
      "entities":"[('cve-2019-0993', 'VULNID', 'CVE')]",
      "classification_level_1":null,
      "classification_level_2":null,
      "list_files":"dict_keys(['lib\/Runtime\/Language\/JavascriptOperators.cpp'])",
      "num_files":1.0,
      "patch_content":"From 36644ee0d4cc2bc97a3cd49c3540e6eea193182a Mon Sep 17 00:00:00 2001\nFrom: Paul Leathers <pleath@microsoft.com>\nDate: Wed, 15 May 2019 16:54:48 -0700\nSubject: [PATCH] CVE-2019-0993\n\n---\n lib\/Runtime\/Language\/JavascriptOperators.cpp | 4 ++++\n 1 file changed, 4 insertions(+)\n\ndiff --git a\/lib\/Runtime\/Language\/JavascriptOperators.cpp b\/lib\/Runtime\/Language\/JavascriptOperators.cpp\nindex ecfd7d86f4e..4d67ce2a0b7 100644\n--- a\/lib\/Runtime\/Language\/JavascriptOperators.cpp\n+++ b\/lib\/Runtime\/Language\/JavascriptOperators.cpp\n@@ -9655,6 +9655,10 @@ using namespace Js;\n             Var result = CALL_ENTRYPOINT(threadContext, marshalledFunction->GetEntryPoint(), function, CallInfo(flags, 1), thisVar);\n             result = CrossSite::MarshalVar(requestContext, result);\n \n+            \/\/ Set implicit call flags so we bail out if we're trying to propagate the value forward, e.g., from a compare. Subsequent calls\n+            \/\/ to the getter may produce different results.\n+            threadContext->AddImplicitCallFlags(ImplicitCall_Accessor);\n+\n             return result;\n         });\n     }",
      "code_diff":"@@ -9655,6 +9655,10 @@ using namespace Js;\n             Var result = CALL_ENTRYPOINT(threadContext, marshalledFunction->GetEntryPoint(), function, CallInfo(flags, 1), thisVar);\n             result = CrossSite::MarshalVar(requestContext, result);\n \n+            \/\/ Set implicit call flags so we bail out if we're trying to propagate the value forward, e.g., from a compare. Subsequent calls\n+            \/\/ to the getter may produce different results.\n+            threadContext->AddImplicitCallFlags(ImplicitCall_Accessor);\n+\n             return result;\n         });\n     }"
    },
    {
      "index":16,
      "vuln_id":"GHSA-2gfx-95x2-5v3x",
      "cwe_id":"{'CWE-787'}",
      "score":2.5,
      "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/a324ac84e573fba362a5e53d4e74d5de6729933e'}",
      "dataset":"osv",
      "summary":"Heap buffer overflow in `QuantizedReshape` ### Impact\nAn attacker can cause a heap buffer overflow in `QuantizedReshape` by passing in invalid thresholds for the quantization:\n\n```python\nimport tensorflow as tf\n\ntensor = tf.constant([], dtype=tf.qint32)\nshape = tf.constant([], dtype=tf.int32)\ninput_min = tf.constant([], dtype=tf.float32)\ninput_max = tf.constant([], dtype=tf.float32)\n\ntf.raw_ops.QuantizedReshape(tensor=tensor, shape=shape, input_min=input_min, input_max=input_max)\n```\n\nThis is because the [implementation](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/a324ac84e573fba362a5e53d4e74d5de6729933e\/tensorflow\/core\/kernels\/quantized_reshape_op.cc#L38-L55) assumes that the 2 arguments are always valid scalars and tries to access the numeric value directly:\n\n```cc\nconst auto& input_min_float_tensor = ctx->input(2);\n...\nconst float input_min_float = input_min_float_tensor.flat<float>()(0);\nconst auto& input_max_float_tensor = ctx->input(3);\n...\nconst float input_max_float = input_max_float_tensor.flat<float>()(0);\n```\n\nHowever, if any of these tensors is empty, then `.flat<T>()` is an empty buffer and accessing the element at position 0 results in overflow.\n\n### Patches\nWe have patched the issue in GitHub commit [a324ac84e573fba362a5e53d4e74d5de6729933e](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/a324ac84e573fba362a5e53d4e74d5de6729933e).\n\nThe fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by Ying Wang and Yakun Zhang of Baidu X-Team.",
      "published_date":"2021-05-21",
      "chain_len":1,
      "project":"https:\/\/github.com\/tensorflow\/tensorflow",
      "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/a324ac84e573fba362a5e53d4e74d5de6729933e",
      "commit_sha":"a324ac84e573fba362a5e53d4e74d5de6729933e",
      "patch":"SINGLE",
      "chain_ord":"['a324ac84e573fba362a5e53d4e74d5de6729933e']",
      "before_first_fix_commit":"{'2ec2ce48365486311e56b3503bb75ab9e72a813d'}",
      "last_fix_commit":"a324ac84e573fba362a5e53d4e74d5de6729933e",
      "chain_ord_pos":1.0,
      "commit_datetime":"04\/22\/2021, 01:11:15",
      "message":"Validate arguments to `QuantizedReshape`.\n\nEnsure that validations from `Reshape` also terminate `QuantizedReshape` on failure.\n\nPiperOrigin-RevId: 369775421\nChange-Id: If8c5342267aceea65b7cb83a4b183304886f1ce8",
      "author":"Mihai Maruseac",
      "comments":null,
      "stats":"{'additions': 23, 'deletions': 2, 'total': 25}",
      "files":"{'tensorflow\/core\/kernels\/quantized_reshape_op.cc': {'additions': 23, 'deletions': 2, 'changes': 25, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/a324ac84e573fba362a5e53d4e74d5de6729933e\/tensorflow%2Fcore%2Fkernels%2Fquantized_reshape_op.cc', 'patch': '@@ -17,6 +17,7 @@ limitations under the License.\\n \\n #include \"tensorflow\/core\/framework\/op_kernel.h\"\\n #include \"tensorflow\/core\/framework\/register_types.h\"\\n+#include \"tensorflow\/core\/framework\/tensor_shape.h\"\\n #include \"tensorflow\/core\/framework\/tensor_types.h\"\\n #include \"tensorflow\/core\/framework\/types.h\"\\n #include \"tensorflow\/core\/kernels\/reshape_op.h\"\\n@@ -30,9 +31,29 @@ class QuantizedReshapeOp : public ReshapeOp {\\n   void Compute(OpKernelContext* ctx) override {\\n     \/\/ This call processes inputs 1 and 2 to write output 0.\\n     ReshapeOp::Compute(ctx);\\n+    if (!ctx->status().ok()) {\\n+      return;\\n+    }\\n+\\n+    const auto& input_min_float_tensor = ctx->input(2);\\n+    const auto& input_min_float_shape = input_min_float_tensor.shape();\\n+    OP_REQUIRES(ctx,\\n+                TensorShapeUtils::IsScalar(input_min_float_shape) ||\\n+                    (TensorShapeUtils::IsVector(input_min_float_shape) &&\\n+                     (input_min_float_shape.dim_size(0) == 1)),\\n+                errors::InvalidArgument(\\n+                    \"input_min must be a scalar or a vector of 1 element\"));\\n+    const float input_min_float = input_min_float_tensor.flat<float>()(0);\\n+    const auto& input_max_float_tensor = ctx->input(3);\\n+    const auto& input_max_float_shape = input_max_float_tensor.shape();\\n+    OP_REQUIRES(ctx,\\n+                TensorShapeUtils::IsScalar(input_max_float_shape) ||\\n+                    (TensorShapeUtils::IsVector(input_max_float_shape) &&\\n+                     (input_max_float_shape.dim_size(0) == 1)),\\n+                errors::InvalidArgument(\\n+                    \"input_max must be a scalar or a vector of 1 element\"));\\n+    const float input_max_float = input_max_float_tensor.flat<float>()(0);\\n \\n-    const float input_min_float = ctx->input(2).flat<float>()(0);\\n-    const float input_max_float = ctx->input(3).flat<float>()(0);\\n     Tensor* output_min = nullptr;\\n     OP_REQUIRES_OK(ctx, ctx->allocate_output(1, TensorShape({}), &output_min));\\n     output_min->flat<float>()(0) = input_min_float;'}}",
      "message_norm":"validate arguments to `quantizedreshape`.\n\nensure that validations from `reshape` also terminate `quantizedreshape` on failure.\n\npiperorigin-revid: 369775421\nchange-id: if8c5342267aceea65b7cb83a4b183304886f1ce8",
      "language":"en",
      "entities":"[('validate', 'ACTION', ''), ('ensure', 'ACTION', ''), ('369775421', 'SHA', 'generic_sha')]",
      "classification_level_1":null,
      "classification_level_2":null,
      "list_files":"dict_keys(['tensorflow\/core\/kernels\/quantized_reshape_op.cc'])",
      "num_files":1.0,
      "patch_content":"From a324ac84e573fba362a5e53d4e74d5de6729933e Mon Sep 17 00:00:00 2001\nFrom: Mihai Maruseac <mihaimaruseac@google.com>\nDate: Wed, 21 Apr 2021 18:11:15 -0700\nSubject: [PATCH] Validate arguments to `QuantizedReshape`.\n\nEnsure that validations from `Reshape` also terminate `QuantizedReshape` on failure.\n\nPiperOrigin-RevId: 369775421\nChange-Id: If8c5342267aceea65b7cb83a4b183304886f1ce8\n---\n ...\/core\/kernels\/quantized_reshape_op.cc      | 25 +++++++++++++++++--\n 1 file changed, 23 insertions(+), 2 deletions(-)\n\ndiff --git a\/tensorflow\/core\/kernels\/quantized_reshape_op.cc b\/tensorflow\/core\/kernels\/quantized_reshape_op.cc\nindex bd76c94edeea7a..682f4aaa1f79e7 100644\n--- a\/tensorflow\/core\/kernels\/quantized_reshape_op.cc\n+++ b\/tensorflow\/core\/kernels\/quantized_reshape_op.cc\n@@ -17,6 +17,7 @@ limitations under the License.\n \n #include \"tensorflow\/core\/framework\/op_kernel.h\"\n #include \"tensorflow\/core\/framework\/register_types.h\"\n+#include \"tensorflow\/core\/framework\/tensor_shape.h\"\n #include \"tensorflow\/core\/framework\/tensor_types.h\"\n #include \"tensorflow\/core\/framework\/types.h\"\n #include \"tensorflow\/core\/kernels\/reshape_op.h\"\n@@ -30,9 +31,29 @@ class QuantizedReshapeOp : public ReshapeOp {\n   void Compute(OpKernelContext* ctx) override {\n     \/\/ This call processes inputs 1 and 2 to write output 0.\n     ReshapeOp::Compute(ctx);\n+    if (!ctx->status().ok()) {\n+      return;\n+    }\n+\n+    const auto& input_min_float_tensor = ctx->input(2);\n+    const auto& input_min_float_shape = input_min_float_tensor.shape();\n+    OP_REQUIRES(ctx,\n+                TensorShapeUtils::IsScalar(input_min_float_shape) ||\n+                    (TensorShapeUtils::IsVector(input_min_float_shape) &&\n+                     (input_min_float_shape.dim_size(0) == 1)),\n+                errors::InvalidArgument(\n+                    \"input_min must be a scalar or a vector of 1 element\"));\n+    const float input_min_float = input_min_float_tensor.flat<float>()(0);\n+    const auto& input_max_float_tensor = ctx->input(3);\n+    const auto& input_max_float_shape = input_max_float_tensor.shape();\n+    OP_REQUIRES(ctx,\n+                TensorShapeUtils::IsScalar(input_max_float_shape) ||\n+                    (TensorShapeUtils::IsVector(input_max_float_shape) &&\n+                     (input_max_float_shape.dim_size(0) == 1)),\n+                errors::InvalidArgument(\n+                    \"input_max must be a scalar or a vector of 1 element\"));\n+    const float input_max_float = input_max_float_tensor.flat<float>()(0);\n \n-    const float input_min_float = ctx->input(2).flat<float>()(0);\n-    const float input_max_float = ctx->input(3).flat<float>()(0);\n     Tensor* output_min = nullptr;\n     OP_REQUIRES_OK(ctx, ctx->allocate_output(1, TensorShape({}), &output_min));\n     output_min->flat<float>()(0) = input_min_float;",
      "code_diff":"@@ -17,6 +17,7 @@ limitations under the License.\n \n #include \"tensorflow\/core\/framework\/op_kernel.h\"\n #include \"tensorflow\/core\/framework\/register_types.h\"\n+#include \"tensorflow\/core\/framework\/tensor_shape.h\"\n #include \"tensorflow\/core\/framework\/tensor_types.h\"\n #include \"tensorflow\/core\/framework\/types.h\"\n #include \"tensorflow\/core\/kernels\/reshape_op.h\"\n@@ -30,9 +31,29 @@ class QuantizedReshapeOp : public ReshapeOp {\n   void Compute(OpKernelContext* ctx) override {\n     \/\/ This call processes inputs 1 and 2 to write output 0.\n     ReshapeOp::Compute(ctx);\n+    if (!ctx->status().ok()) {\n+      return;\n+    }\n+\n+    const auto& input_min_float_tensor = ctx->input(2);\n+    const auto& input_min_float_shape = input_min_float_tensor.shape();\n+    OP_REQUIRES(ctx,\n+                TensorShapeUtils::IsScalar(input_min_float_shape) ||\n+                    (TensorShapeUtils::IsVector(input_min_float_shape) &&\n+                     (input_min_float_shape.dim_size(0) == 1)),\n+                errors::InvalidArgument(\n+                    \"input_min must be a scalar or a vector of 1 element\"));\n+    const float input_min_float = input_min_float_tensor.flat<float>()(0);\n+    const auto& input_max_float_tensor = ctx->input(3);\n+    const auto& input_max_float_shape = input_max_float_tensor.shape();\n+    OP_REQUIRES(ctx,\n+                TensorShapeUtils::IsScalar(input_max_float_shape) ||\n+                    (TensorShapeUtils::IsVector(input_max_float_shape) &&\n+                     (input_max_float_shape.dim_size(0) == 1)),\n+                errors::InvalidArgument(\n+                    \"input_max must be a scalar or a vector of 1 element\"));\n+    const float input_max_float = input_max_float_tensor.flat<float>()(0);\n \n-    const float input_min_float = ctx->input(2).flat<float>()(0);\n-    const float input_max_float = ctx->input(3).flat<float>()(0);\n     Tensor* output_min = nullptr;\n     OP_REQUIRES_OK(ctx, ctx->allocate_output(1, TensorShape({}), &output_min));\n     output_min->flat<float>()(0) = input_min_float;"
    },
    {
      "index":17,
      "vuln_id":"GHSA-5rcr-q3rx-j7vr",
      "cwe_id":"{'CWE-787'}",
      "score":7.5,
      "chain":"{'https:\/\/github.com\/chakra-core\/ChakraCore\/commit\/75162b7f2d8ac2b37d17564e9c979ba1bae707e8', 'https:\/\/github.com\/chakra-core\/ChakraCore\/commit\/214dec9461f9acb9a4b9004368d2a81e0c125652'}",
      "dataset":"osv",
      "summary":"Out-of-bounds write A remote code execution vulnerability exists in the way that the Chakra scripting engine handles objects in memory in Microsoft Edge, aka 'Chakra Scripting Engine Memory Corruption Vulnerability'. This CVE ID is unique from CVE-2019-1062, CVE-2019-1092, CVE-2019-1103, CVE-2019-1106.",
      "published_date":"2021-03-29",
      "chain_len":2,
      "project":"https:\/\/github.com\/chakra-core\/ChakraCore",
      "commit_href":"https:\/\/github.com\/chakra-core\/ChakraCore\/commit\/214dec9461f9acb9a4b9004368d2a81e0c125652",
      "commit_sha":"214dec9461f9acb9a4b9004368d2a81e0c125652",
      "patch":"MULTI",
      "chain_ord":"['214dec9461f9acb9a4b9004368d2a81e0c125652', '75162b7f2d8ac2b37d17564e9c979ba1bae707e8']",
      "before_first_fix_commit":"{'12c31f0e83ddc511e57b9aa1e78533899199eb32', 'ba1f4455f921ce5f12091ff8a11c8028c6a64b17'}",
      "last_fix_commit":"75162b7f2d8ac2b37d17564e9c979ba1bae707e8",
      "chain_ord_pos":1.0,
      "commit_datetime":"06\/06\/2019, 19:58:34",
      "message":"[CVE-2019-1107] Chakra JIT Type Confusion FinishOptPropOp",
      "author":"Paul Leathers",
      "comments":null,
      "stats":"{'additions': 8, 'deletions': 0, 'total': 8}",
      "files":"{'lib\/Backend\/GlobOptFields.cpp': {'additions': 8, 'deletions': 0, 'changes': 8, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/chakra-core\/ChakraCore\/raw\/214dec9461f9acb9a4b9004368d2a81e0c125652\/lib%2FBackend%2FGlobOptFields.cpp', 'patch': '@@ -410,6 +410,14 @@ GlobOpt::ProcessFieldKills(IR::Instr *instr, BVSparse<JitArenaAllocator> *bv, bo\\n         if (inGlobOpt)\\n         {\\n             KillObjectHeaderInlinedTypeSyms(this->currentBlock, false);\\n+            if (this->objectTypeSyms)\\n+            {\\n+                if (this->currentBlock->globOptData.maybeWrittenTypeSyms == nullptr)\\n+                {\\n+                    this->currentBlock->globOptData.maybeWrittenTypeSyms = JitAnew(this->alloc, BVSparse<JitArenaAllocator>, this->alloc);\\n+                }\\n+                this->currentBlock->globOptData.maybeWrittenTypeSyms->Or(this->objectTypeSyms);\\n+            }\\n         }\\n \\n         \/\/ fall through'}}",
      "message_norm":"[cve-2019-1107] chakra jit type confusion finishoptpropop",
      "language":"en",
      "entities":"[('cve-2019-1107', 'VULNID', 'CVE'), ('type confusion', 'SECWORD', '')]",
      "classification_level_1":null,
      "classification_level_2":null,
      "list_files":"dict_keys(['lib\/Backend\/GlobOptFields.cpp'])",
      "num_files":1.0,
      "patch_content":"From 214dec9461f9acb9a4b9004368d2a81e0c125652 Mon Sep 17 00:00:00 2001\nFrom: Paul Leathers <pleath@microsoft.com>\nDate: Thu, 6 Jun 2019 12:58:34 -0700\nSubject: [PATCH] [CVE-2019-1107] Chakra JIT Type Confusion FinishOptPropOp\n\n---\n lib\/Backend\/GlobOptFields.cpp | 8 ++++++++\n 1 file changed, 8 insertions(+)\n\ndiff --git a\/lib\/Backend\/GlobOptFields.cpp b\/lib\/Backend\/GlobOptFields.cpp\nindex 92a7e9ec108..59e19b71f6d 100644\n--- a\/lib\/Backend\/GlobOptFields.cpp\n+++ b\/lib\/Backend\/GlobOptFields.cpp\n@@ -410,6 +410,14 @@ GlobOpt::ProcessFieldKills(IR::Instr *instr, BVSparse<JitArenaAllocator> *bv, bo\n         if (inGlobOpt)\n         {\n             KillObjectHeaderInlinedTypeSyms(this->currentBlock, false);\n+            if (this->objectTypeSyms)\n+            {\n+                if (this->currentBlock->globOptData.maybeWrittenTypeSyms == nullptr)\n+                {\n+                    this->currentBlock->globOptData.maybeWrittenTypeSyms = JitAnew(this->alloc, BVSparse<JitArenaAllocator>, this->alloc);\n+                }\n+                this->currentBlock->globOptData.maybeWrittenTypeSyms->Or(this->objectTypeSyms);\n+            }\n         }\n \n         \/\/ fall through",
      "code_diff":"@@ -410,6 +410,14 @@ GlobOpt::ProcessFieldKills(IR::Instr *instr, BVSparse<JitArenaAllocator> *bv, bo\n         if (inGlobOpt)\n         {\n             KillObjectHeaderInlinedTypeSyms(this->currentBlock, false);\n+            if (this->objectTypeSyms)\n+            {\n+                if (this->currentBlock->globOptData.maybeWrittenTypeSyms == nullptr)\n+                {\n+                    this->currentBlock->globOptData.maybeWrittenTypeSyms = JitAnew(this->alloc, BVSparse<JitArenaAllocator>, this->alloc);\n+                }\n+                this->currentBlock->globOptData.maybeWrittenTypeSyms->Or(this->objectTypeSyms);\n+            }\n         }\n \n         \/\/ fall through"
    },
    {
      "index":18,
      "vuln_id":"GHSA-cvpc-8phh-8f45",
      "cwe_id":"{'CWE-787', 'CWE-125'}",
      "score":4.8,
      "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/00302787b788c5ff04cb6f62aed5a74d936e86c0', 'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/e11f55585f614645b360563072ffeb5c3eeff162', 'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/46d5b0852528ddfd614ded79bccc75589f801bd9', 'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/cd31fd0ce0449a9e0f83dcad08d6ed7f1d6bef3f', 'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/1970c2158b1ffa416d159d03c3370b9a462aee35', 'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/fff2c8326280c07733828f990548979bdc893859'}",
      "dataset":"osv",
      "summary":"Out of bounds access in tensorflow-lite ### Impact\nIn TensorFlow Lite, saved models in the flatbuffer format use a double indexing scheme: a model has a set of subgraphs, each subgraph has a set of operators and each operator has a set of input\/output tensors. The flatbuffer format uses indices for the tensors, indexing into an array of tensors that is owned by the subgraph. This results in a pattern of double array indexing when trying to get the data of each tensor:https:\/\/github.com\/tensorflow\/tensorflow\/blob\/0e68f4d3295eb0281a517c3662f6698992b7b2cf\/tensorflow\/lite\/kernels\/kernel_util.cc#L36\n\nHowever, some operators can have some tensors be optional. To handle this scenario, the flatbuffer model uses a negative `-1` value as index for these tensors:\nhttps:\/\/github.com\/tensorflow\/tensorflow\/blob\/0e68f4d3295eb0281a517c3662f6698992b7b2cf\/tensorflow\/lite\/c\/common.h#L82\n\nThis results in special casing during validation at model loading time: https:\/\/github.com\/tensorflow\/tensorflow\/blob\/0e68f4d3295eb0281a517c3662f6698992b7b2cf\/tensorflow\/lite\/core\/subgraph.cc#L566-L580\n\nUnfortunately, this means that the `-1` index is a valid tensor index for any operator, including those that don't expect optional inputs and including for output tensors. Thus, this allows writing and reading from outside the bounds of heap allocated arrays, although only at a specific offset from the start of these arrays.\n\nThis results in both read and write gadgets, albeit very limited in scope.\n\n### Patches\nWe have patched the issue in several commits (46d5b0852, 00302787b7, e11f5558, cd31fd0ce, 1970c21, and fff2c83). We will release patch releases for all versions between 1.15 and 2.3.\n\nWe recommend users to upgrade to TensorFlow 1.15.4, 2.0.3, 2.1.2, 2.2.1, or 2.3.1.\n\n### Workarounds\nA potential workaround would be to add a custom `Verifier` to the model loading code to ensure that only operators which accept optional inputs use the `-1` special value and only for the tensors that they expect to be optional. Since this allow-list type approach is erro-prone, we advise upgrading to the patched code.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by members of the Aivul Team from Qihoo 360.",
      "published_date":"2020-09-25",
      "chain_len":6,
      "project":"https:\/\/github.com\/tensorflow\/tensorflow",
      "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/cd31fd0ce0449a9e0f83dcad08d6ed7f1d6bef3f",
      "commit_sha":"cd31fd0ce0449a9e0f83dcad08d6ed7f1d6bef3f",
      "patch":"MULTI",
      "chain_ord":"['46d5b0852528ddfd614ded79bccc75589f801bd9', '00302787b788c5ff04cb6f62aed5a74d936e86c0', 'e11f55585f614645b360563072ffeb5c3eeff162', 'cd31fd0ce0449a9e0f83dcad08d6ed7f1d6bef3f', 'fff2c8326280c07733828f990548979bdc893859', '1970c2158b1ffa416d159d03c3370b9a462aee35']",
      "before_first_fix_commit":"{'fff2c8326280c07733828f990548979bdc893859'}",
      "last_fix_commit":"1970c2158b1ffa416d159d03c3370b9a462aee35",
      "chain_ord_pos":4.0,
      "commit_datetime":"09\/18\/2020, 20:44:32",
      "message":"[tflite]: Insert `nullptr` checks when obtaining tensors.\n\nAs part of ongoing refactoring, `tflite::GetInput`, `tflite::GetOutput`, `tflite::GetTemporary` and `tflite::GetIntermediates` will return `nullptr` in some cases. Hence, we insert the `nullptr` checks on all usages.\n\nWe also insert `nullptr` checks on usages of `tflite::GetVariableInput` and `tflite::GetOptionalInputTensor` but only in the cases where there is no obvious check that `nullptr` is acceptable (that is, we only insert the check for the output of these two functions if the tensor is accessed as if it is always not `nullptr`).\n\nPiperOrigin-RevId: 332518902\nChange-Id: I92eb164a6101ac3cca66090061a9b56a97288236",
      "author":"Mihai Maruseac",
      "comments":null,
      "stats":"{'additions': 16, 'deletions': 7, 'total': 23}",
      "files":"{'tensorflow\/lite\/micro\/test_helpers.cc': {'additions': 16, 'deletions': 7, 'changes': 23, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/cd31fd0ce0449a9e0f83dcad08d6ed7f1d6bef3f\/tensorflow%2Flite%2Fmicro%2Ftest_helpers.cc', 'patch': '@@ -601,7 +601,8 @@ TfLiteStatus SimpleStatefulOp::Prepare(TfLiteContext* context,\\n   OpData* data = reinterpret_cast<OpData*>(node->user_data);\\n \\n   \/\/ Make sure that the input is in uint8_t with at least 1 data entry.\\n-  const TfLiteTensor* input = tflite::GetInput(context, node, kInputTensor);\\n+  const TfLiteTensor* input;\\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputTensor, &input));\\n   if (input->type != kTfLiteUInt8) return kTfLiteError;\\n   if (NumElements(input->dims) == 0) return kTfLiteError;\\n \\n@@ -622,7 +623,8 @@ TfLiteStatus SimpleStatefulOp::Invoke(TfLiteContext* context,\\n   OpData* data = reinterpret_cast<OpData*>(node->user_data);\\n   *data->invoke_count += 1;\\n \\n-  const TfLiteTensor* input = GetInput(context, node, kInputTensor);\\n+  const TfLiteTensor* input;\\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputTensor, &input));\\n   const uint8_t* input_data = GetTensorData<uint8_t>(input);\\n   int size = NumElements(input->dims);\\n \\n@@ -641,9 +643,13 @@ TfLiteStatus SimpleStatefulOp::Invoke(TfLiteContext* context,\\n     }\\n   }\\n \\n-  TfLiteTensor* median = GetOutput(context, node, kMedianTensor);\\n+  TfLiteTensor* median;\\n+  TF_LITE_ENSURE_OK(context,\\n+                    GetOutputSafe(context, node, kMedianTensor, &median));\\n   uint8_t* median_data = GetTensorData<uint8_t>(median);\\n-  TfLiteTensor* invoke_count = GetOutput(context, node, kInvokeCount);\\n+  TfLiteTensor* invoke_count;\\n+  TF_LITE_ENSURE_OK(context,\\n+                    GetOutputSafe(context, node, kInvokeCount, &invoke_count));\\n   int32_t* invoke_count_data = GetTensorData<int32_t>(invoke_count);\\n \\n   median_data[0] = sorting_buffer[size \/ 2];\\n@@ -681,11 +687,14 @@ TfLiteStatus MockCustom::Prepare(TfLiteContext* context, TfLiteNode* node) {\\n }\\n \\n TfLiteStatus MockCustom::Invoke(TfLiteContext* context, TfLiteNode* node) {\\n-  const TfLiteTensor* input = tflite::GetInput(context, node, 0);\\n+  const TfLiteTensor* input;\\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));\\n   const int32_t* input_data = input->data.i32;\\n-  const TfLiteTensor* weight = tflite::GetInput(context, node, 1);\\n+  const TfLiteTensor* weight;\\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 1, &weight));\\n   const uint8_t* weight_data = weight->data.uint8;\\n-  TfLiteTensor* output = GetOutput(context, node, 0);\\n+  TfLiteTensor* output;\\n+  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output));\\n   int32_t* output_data = output->data.i32;\\n   output_data[0] =\\n       0;  \/\/ Catch output tensor sharing memory with an input tensor'}}",
      "message_norm":"[tflite]: insert `nullptr` checks when obtaining tensors.\n\nas part of ongoing refactoring, `tflite::getinput`, `tflite::getoutput`, `tflite::gettemporary` and `tflite::getintermediates` will return `nullptr` in some cases. hence, we insert the `nullptr` checks on all usages.\n\nwe also insert `nullptr` checks on usages of `tflite::getvariableinput` and `tflite::getoptionalinputtensor` but only in the cases where there is no obvious check that `nullptr` is acceptable (that is, we only insert the check for the output of these two functions if the tensor is accessed as if it is always not `nullptr`).\n\npiperorigin-revid: 332518902\nchange-id: i92eb164a6101ac3cca66090061a9b56a97288236",
      "language":"en",
      "entities":"[('nullptr', 'SECWORD', ''), ('nullptr', 'SECWORD', ''), ('nullptr', 'SECWORD', ''), ('nullptr', 'SECWORD', ''), ('nullptr', 'SECWORD', ''), ('nullptr', 'SECWORD', ''), ('332518902', 'SHA', 'generic_sha')]",
      "classification_level_1":null,
      "classification_level_2":null,
      "list_files":"dict_keys(['tensorflow\/lite\/micro\/test_helpers.cc'])",
      "num_files":1.0,
      "patch_content":"From cd31fd0ce0449a9e0f83dcad08d6ed7f1d6bef3f Mon Sep 17 00:00:00 2001\nFrom: Mihai Maruseac <mihaimaruseac@google.com>\nDate: Fri, 18 Sep 2020 13:44:32 -0700\nSubject: [PATCH] [tflite]: Insert `nullptr` checks when obtaining tensors.\n\nAs part of ongoing refactoring, `tflite::GetInput`, `tflite::GetOutput`, `tflite::GetTemporary` and `tflite::GetIntermediates` will return `nullptr` in some cases. Hence, we insert the `nullptr` checks on all usages.\n\nWe also insert `nullptr` checks on usages of `tflite::GetVariableInput` and `tflite::GetOptionalInputTensor` but only in the cases where there is no obvious check that `nullptr` is acceptable (that is, we only insert the check for the output of these two functions if the tensor is accessed as if it is always not `nullptr`).\n\nPiperOrigin-RevId: 332518902\nChange-Id: I92eb164a6101ac3cca66090061a9b56a97288236\n---\n tensorflow\/lite\/micro\/test_helpers.cc | 23 ++++++++++++++++-------\n 1 file changed, 16 insertions(+), 7 deletions(-)\n\ndiff --git a\/tensorflow\/lite\/micro\/test_helpers.cc b\/tensorflow\/lite\/micro\/test_helpers.cc\nindex dd5e996ac26aa6..26575a4d98da9f 100644\n--- a\/tensorflow\/lite\/micro\/test_helpers.cc\n+++ b\/tensorflow\/lite\/micro\/test_helpers.cc\n@@ -601,7 +601,8 @@ TfLiteStatus SimpleStatefulOp::Prepare(TfLiteContext* context,\n   OpData* data = reinterpret_cast<OpData*>(node->user_data);\n \n   \/\/ Make sure that the input is in uint8_t with at least 1 data entry.\n-  const TfLiteTensor* input = tflite::GetInput(context, node, kInputTensor);\n+  const TfLiteTensor* input;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputTensor, &input));\n   if (input->type != kTfLiteUInt8) return kTfLiteError;\n   if (NumElements(input->dims) == 0) return kTfLiteError;\n \n@@ -622,7 +623,8 @@ TfLiteStatus SimpleStatefulOp::Invoke(TfLiteContext* context,\n   OpData* data = reinterpret_cast<OpData*>(node->user_data);\n   *data->invoke_count += 1;\n \n-  const TfLiteTensor* input = GetInput(context, node, kInputTensor);\n+  const TfLiteTensor* input;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputTensor, &input));\n   const uint8_t* input_data = GetTensorData<uint8_t>(input);\n   int size = NumElements(input->dims);\n \n@@ -641,9 +643,13 @@ TfLiteStatus SimpleStatefulOp::Invoke(TfLiteContext* context,\n     }\n   }\n \n-  TfLiteTensor* median = GetOutput(context, node, kMedianTensor);\n+  TfLiteTensor* median;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetOutputSafe(context, node, kMedianTensor, &median));\n   uint8_t* median_data = GetTensorData<uint8_t>(median);\n-  TfLiteTensor* invoke_count = GetOutput(context, node, kInvokeCount);\n+  TfLiteTensor* invoke_count;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetOutputSafe(context, node, kInvokeCount, &invoke_count));\n   int32_t* invoke_count_data = GetTensorData<int32_t>(invoke_count);\n \n   median_data[0] = sorting_buffer[size \/ 2];\n@@ -681,11 +687,14 @@ TfLiteStatus MockCustom::Prepare(TfLiteContext* context, TfLiteNode* node) {\n }\n \n TfLiteStatus MockCustom::Invoke(TfLiteContext* context, TfLiteNode* node) {\n-  const TfLiteTensor* input = tflite::GetInput(context, node, 0);\n+  const TfLiteTensor* input;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));\n   const int32_t* input_data = input->data.i32;\n-  const TfLiteTensor* weight = tflite::GetInput(context, node, 1);\n+  const TfLiteTensor* weight;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 1, &weight));\n   const uint8_t* weight_data = weight->data.uint8;\n-  TfLiteTensor* output = GetOutput(context, node, 0);\n+  TfLiteTensor* output;\n+  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output));\n   int32_t* output_data = output->data.i32;\n   output_data[0] =\n       0;  \/\/ Catch output tensor sharing memory with an input tensor",
      "code_diff":"@@ -601,7 +601,8 @@ TfLiteStatus SimpleStatefulOp::Prepare(TfLiteContext* context,\n   OpData* data = reinterpret_cast<OpData*>(node->user_data);\n \n   \/\/ Make sure that the input is in uint8_t with at least 1 data entry.\n-  const TfLiteTensor* input = tflite::GetInput(context, node, kInputTensor);\n+  const TfLiteTensor* input;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputTensor, &input));\n   if (input->type != kTfLiteUInt8) return kTfLiteError;\n   if (NumElements(input->dims) == 0) return kTfLiteError;\n \n@@ -622,7 +623,8 @@ TfLiteStatus SimpleStatefulOp::Invoke(TfLiteContext* context,\n   OpData* data = reinterpret_cast<OpData*>(node->user_data);\n   *data->invoke_count += 1;\n \n-  const TfLiteTensor* input = GetInput(context, node, kInputTensor);\n+  const TfLiteTensor* input;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputTensor, &input));\n   const uint8_t* input_data = GetTensorData<uint8_t>(input);\n   int size = NumElements(input->dims);\n \n@@ -641,9 +643,13 @@ TfLiteStatus SimpleStatefulOp::Invoke(TfLiteContext* context,\n     }\n   }\n \n-  TfLiteTensor* median = GetOutput(context, node, kMedianTensor);\n+  TfLiteTensor* median;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetOutputSafe(context, node, kMedianTensor, &median));\n   uint8_t* median_data = GetTensorData<uint8_t>(median);\n-  TfLiteTensor* invoke_count = GetOutput(context, node, kInvokeCount);\n+  TfLiteTensor* invoke_count;\n+  TF_LITE_ENSURE_OK(context,\n+                    GetOutputSafe(context, node, kInvokeCount, &invoke_count));\n   int32_t* invoke_count_data = GetTensorData<int32_t>(invoke_count);\n \n   median_data[0] = sorting_buffer[size \/ 2];\n@@ -681,11 +687,14 @@ TfLiteStatus MockCustom::Prepare(TfLiteContext* context, TfLiteNode* node) {\n }\n \n TfLiteStatus MockCustom::Invoke(TfLiteContext* context, TfLiteNode* node) {\n-  const TfLiteTensor* input = tflite::GetInput(context, node, 0);\n+  const TfLiteTensor* input;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));\n   const int32_t* input_data = input->data.i32;\n-  const TfLiteTensor* weight = tflite::GetInput(context, node, 1);\n+  const TfLiteTensor* weight;\n+  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 1, &weight));\n   const uint8_t* weight_data = weight->data.uint8;\n-  TfLiteTensor* output = GetOutput(context, node, 0);\n+  TfLiteTensor* output;\n+  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output));\n   int32_t* output_data = output->data.i32;\n   output_data[0] =\n       0;  \/\/ Catch output tensor sharing memory with an input tensor"
    },
    {
      "index":19,
      "vuln_id":"GHSA-q99r-j969-6jwr",
      "cwe_id":"{'CWE-787'}",
      "score":7.5,
      "chain":"{'https:\/\/github.com\/chakra-core\/ChakraCore\/commit\/7e9a2ee60baa95ceb4f48f522f823c812ca90c80', 'https:\/\/github.com\/chakra-core\/ChakraCore\/commit\/31f2588c7ba5b446bccf2769e9ecf4d444b73045'}",
      "dataset":"osv",
      "summary":"Out-of-bounds write A remote code execution vulnerability exists in the way that the Chakra scripting engine handles objects in memory in Microsoft Edge, aka 'Chakra Scripting Engine Memory Corruption Vulnerability'. This CVE ID is unique from CVE-2019-1138, CVE-2019-1217, CVE-2019-1298, CVE-2019-1300.",
      "published_date":"2021-03-29",
      "chain_len":2,
      "project":"https:\/\/github.com\/chakra-core\/ChakraCore",
      "commit_href":"https:\/\/github.com\/chakra-core\/ChakraCore\/commit\/31f2588c7ba5b446bccf2769e9ecf4d444b73045",
      "commit_sha":"31f2588c7ba5b446bccf2769e9ecf4d444b73045",
      "patch":"MULTI",
      "chain_ord":"['31f2588c7ba5b446bccf2769e9ecf4d444b73045', '7e9a2ee60baa95ceb4f48f522f823c812ca90c80']",
      "before_first_fix_commit":"{'edf5eeef49168bbcc30dac82f57048ad46988295', 'c5297b86536fbf1a02d27cec28fea3c516e6ab84'}",
      "last_fix_commit":"7e9a2ee60baa95ceb4f48f522f823c812ca90c80",
      "chain_ord_pos":1.0,
      "commit_datetime":"07\/26\/2019, 22:39:34",
      "message":"[CVE-2019-1237]",
      "author":"Michael Holman",
      "comments":null,
      "stats":"{'additions': 6, 'deletions': 0, 'total': 6}",
      "files":"{'lib\/Runtime\/Library\/BoundFunction.cpp': {'additions': 6, 'deletions': 0, 'changes': 6, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/chakra-core\/ChakraCore\/raw\/31f2588c7ba5b446bccf2769e9ecf4d444b73045\/lib%2FRuntime%2FLibrary%2FBoundFunction.cpp', 'patch': \"@@ -354,6 +354,12 @@ namespace Js\\n             Var varLength;\\n             if (targetFunction->GetProperty(targetFunction, PropertyIds::length, &varLength, nullptr, requestContext))\\n             {\\n+                if (!TaggedInt::Is(varLength))\\n+                {\\n+                    \/\/ ToInt32 conversion on non-primitive length can invalidate assumptions made by the JIT,\\n+                    \/\/ so add implicit call flag if length isn't a TaggedInt already\\n+                    requestContext->GetThreadContext()->AddImplicitCallFlags(ImplicitCall_Accessor);\\n+                }\\n                 len = JavascriptConversion::ToInt32(varLength, requestContext);\\n             }\"}}",
      "message_norm":"[cve-2019-1237]",
      "language":"ro",
      "entities":"[('cve-2019-1237', 'VULNID', 'CVE')]",
      "classification_level_1":null,
      "classification_level_2":null,
      "list_files":"dict_keys(['lib\/Runtime\/Library\/BoundFunction.cpp'])",
      "num_files":1.0,
      "patch_content":"From 31f2588c7ba5b446bccf2769e9ecf4d444b73045 Mon Sep 17 00:00:00 2001\nFrom: Michael Holman <michhol@microsoft.com>\nDate: Fri, 26 Jul 2019 15:39:34 -0700\nSubject: [PATCH] [CVE-2019-1237]\n\n---\n lib\/Runtime\/Library\/BoundFunction.cpp | 6 ++++++\n 1 file changed, 6 insertions(+)\n\ndiff --git a\/lib\/Runtime\/Library\/BoundFunction.cpp b\/lib\/Runtime\/Library\/BoundFunction.cpp\nindex 754350e3ae7..ce4a0d5c892 100644\n--- a\/lib\/Runtime\/Library\/BoundFunction.cpp\n+++ b\/lib\/Runtime\/Library\/BoundFunction.cpp\n@@ -354,6 +354,12 @@ namespace Js\n             Var varLength;\n             if (targetFunction->GetProperty(targetFunction, PropertyIds::length, &varLength, nullptr, requestContext))\n             {\n+                if (!TaggedInt::Is(varLength))\n+                {\n+                    \/\/ ToInt32 conversion on non-primitive length can invalidate assumptions made by the JIT,\n+                    \/\/ so add implicit call flag if length isn't a TaggedInt already\n+                    requestContext->GetThreadContext()->AddImplicitCallFlags(ImplicitCall_Accessor);\n+                }\n                 len = JavascriptConversion::ToInt32(varLength, requestContext);\n             }",
      "code_diff":"@@ -354,6 +354,12 @@ namespace Js\n             Var varLength;\n             if (targetFunction->GetProperty(targetFunction, PropertyIds::length, &varLength, nullptr, requestContext))\n             {\n+                if (!TaggedInt::Is(varLength))\n+                {\n+                    \/\/ ToInt32 conversion on non-primitive length can invalidate assumptions made by the JIT,\n+                    \/\/ so add implicit call flag if length isn't a TaggedInt already\n+                    requestContext->GetThreadContext()->AddImplicitCallFlags(ImplicitCall_Accessor);\n+                }\n                 len = JavascriptConversion::ToInt32(varLength, requestContext);\n             }"
    },
    {
      "index":20,
      "vuln_id":"GHSA-grvw-q343-58wh",
      "cwe_id":"{'CWE-787'}",
      "score":7.5,
      "chain":"{'https:\/\/github.com\/chakra-core\/ChakraCore\/commit\/7e9a2ee60baa95ceb4f48f522f823c812ca90c80', 'https:\/\/github.com\/chakra-core\/ChakraCore\/commit\/95b3e3400afb8fa20743657f3a8057fb451e6f69'}",
      "dataset":"osv",
      "summary":"Out-of-bounds write A remote code execution vulnerability exists in the way that the Chakra scripting engine handles objects in memory in Microsoft Edge, aka 'Chakra Scripting Engine Memory Corruption Vulnerability'. This CVE ID is unique from CVE-2019-1138, CVE-2019-1217, CVE-2019-1237, CVE-2019-1298.",
      "published_date":"2021-03-29",
      "chain_len":2,
      "project":"https:\/\/github.com\/chakra-core\/ChakraCore",
      "commit_href":"https:\/\/github.com\/chakra-core\/ChakraCore\/commit\/95b3e3400afb8fa20743657f3a8057fb451e6f69",
      "commit_sha":"95b3e3400afb8fa20743657f3a8057fb451e6f69",
      "patch":"MULTI",
      "chain_ord":"['95b3e3400afb8fa20743657f3a8057fb451e6f69', '7e9a2ee60baa95ceb4f48f522f823c812ca90c80']",
      "before_first_fix_commit":"{'edf5eeef49168bbcc30dac82f57048ad46988295', 'c5297b86536fbf1a02d27cec28fea3c516e6ab84'}",
      "last_fix_commit":"7e9a2ee60baa95ceb4f48f522f823c812ca90c80",
      "chain_ord_pos":1.0,
      "commit_datetime":"07\/08\/2019, 15:54:11",
      "message":"[CVE-2019-1300]",
      "author":"Paul Leathers",
      "comments":null,
      "stats":"{'additions': 43, 'deletions': 31, 'total': 74}",
      "files":"{'lib\/Backend\/GlobOpt.cpp': {'additions': 43, 'deletions': 31, 'changes': 74, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/chakra-core\/ChakraCore\/raw\/95b3e3400afb8fa20743657f3a8057fb451e6f69\/lib%2FBackend%2FGlobOpt.cpp', 'patch': \"@@ -1167,6 +1167,10 @@ void GlobOpt::InsertValueCompensation(\\n     IR::Instr *insertBeforeInstr = predecessor->GetLastInstr();\\n     Func *const func = insertBeforeInstr->m_func;\\n     bool setLastInstrInPredecessor;\\n+    \/\/ If this is a loop back edge, and the successor has been completed, don't attempt to update its block data.\\n+    \/\/ The update is unnecessary, and the data has likely been freed.\\n+    bool updateSuccessorBlockData = !this->isPerformingLoopBackEdgeCompensation || successor->GetDataUseCount() > 0;\\n+\\n     if(insertBeforeInstr->IsBranchInstr() || insertBeforeInstr->m_opcode == Js::OpCode::BailTarget)\\n     {\\n         \/\/ Don't insert code between the branch and the corresponding ByteCodeUses instructions\\n@@ -1257,29 +1261,33 @@ void GlobOpt::InsertValueCompensation(\\n             \/\/ Merge the head segment length value\\n             Assert(predecessorBlockData.liveVarSyms->Test(predecessorHeadSegmentLengthSym->m_id));\\n             predecessorBlockData.liveVarSyms->Set(mergedHeadSegmentLengthSym->m_id);\\n-            successorBlockData.liveVarSyms->Set(mergedHeadSegmentLengthSym->m_id);\\n             Value *const predecessorHeadSegmentLengthValue =\\n                 predecessorBlockData.FindValue(predecessorHeadSegmentLengthSym);\\n             Assert(predecessorHeadSegmentLengthValue);\\n             predecessorBlockData.SetValue(predecessorHeadSegmentLengthValue, mergedHeadSegmentLengthSym);\\n-            Value *const mergedHeadSegmentLengthValue = successorBlockData.FindValue(mergedHeadSegmentLengthSym);\\n-            if(mergedHeadSegmentLengthValue)\\n+\\n+            if (updateSuccessorBlockData)\\n             {\\n-                Assert(mergedHeadSegmentLengthValue->GetValueNumber() != predecessorHeadSegmentLengthValue->GetValueNumber());\\n-                if(predecessorHeadSegmentLengthValue->GetValueInfo() != mergedHeadSegmentLengthValue->GetValueInfo())\\n+                successorBlockData.liveVarSyms->Set(mergedHeadSegmentLengthSym->m_id);\\n+                Value *const mergedHeadSegmentLengthValue = successorBlockData.FindValue(mergedHeadSegmentLengthSym);\\n+                if(mergedHeadSegmentLengthValue)\\n                 {\\n-                    mergedHeadSegmentLengthValue->SetValueInfo(\\n-                        ValueInfo::MergeLikelyIntValueInfo(\\n-                            this->alloc,\\n-                            mergedHeadSegmentLengthValue,\\n-                            predecessorHeadSegmentLengthValue,\\n-                            mergedHeadSegmentLengthValue->GetValueInfo()->Type()\\n-                                .Merge(predecessorHeadSegmentLengthValue->GetValueInfo()->Type())));\\n+                    Assert(mergedHeadSegmentLengthValue->GetValueNumber() != predecessorHeadSegmentLengthValue->GetValueNumber());\\n+                    if(predecessorHeadSegmentLengthValue->GetValueInfo() != mergedHeadSegmentLengthValue->GetValueInfo())\\n+                    {\\n+                        mergedHeadSegmentLengthValue->SetValueInfo(\\n+                            ValueInfo::MergeLikelyIntValueInfo(\\n+                                this->alloc,\\n+                                mergedHeadSegmentLengthValue,\\n+                                predecessorHeadSegmentLengthValue,\\n+                                mergedHeadSegmentLengthValue->GetValueInfo()->Type()\\n+                                    .Merge(predecessorHeadSegmentLengthValue->GetValueInfo()->Type())));\\n+                    }\\n+                }\\n+                else\\n+                {\\n+                    successorBlockData.SetValue(CopyValue(predecessorHeadSegmentLengthValue), mergedHeadSegmentLengthSym);\\n                 }\\n-            }\\n-            else\\n-            {\\n-                successorBlockData.SetValue(CopyValue(predecessorHeadSegmentLengthValue), mergedHeadSegmentLengthSym);\\n             }\\n         }\\n \\n@@ -1300,27 +1308,31 @@ void GlobOpt::InsertValueCompensation(\\n             \/\/ Merge the length value\\n             Assert(predecessorBlockData.liveVarSyms->Test(predecessorLengthSym->m_id));\\n             predecessorBlockData.liveVarSyms->Set(mergedLengthSym->m_id);\\n-            successorBlockData.liveVarSyms->Set(mergedLengthSym->m_id);\\n             Value *const predecessorLengthValue = predecessorBlockData.FindValue(predecessorLengthSym);\\n             Assert(predecessorLengthValue);\\n             predecessorBlockData.SetValue(predecessorLengthValue, mergedLengthSym);\\n-            Value *const mergedLengthValue = successorBlockData.FindValue(mergedLengthSym);\\n-            if(mergedLengthValue)\\n+\\n+            if (updateSuccessorBlockData)\\n             {\\n-                Assert(mergedLengthValue->GetValueNumber() != predecessorLengthValue->GetValueNumber());\\n-                if(predecessorLengthValue->GetValueInfo() != mergedLengthValue->GetValueInfo())\\n+                successorBlockData.liveVarSyms->Set(mergedLengthSym->m_id);\\n+                Value *const mergedLengthValue = successorBlockData.FindValue(mergedLengthSym);\\n+                if(mergedLengthValue)\\n                 {\\n-                    mergedLengthValue->SetValueInfo(\\n-                        ValueInfo::MergeLikelyIntValueInfo(\\n-                            this->alloc,\\n-                            mergedLengthValue,\\n-                            predecessorLengthValue,\\n-                            mergedLengthValue->GetValueInfo()->Type().Merge(predecessorLengthValue->GetValueInfo()->Type())));\\n+                    Assert(mergedLengthValue->GetValueNumber() != predecessorLengthValue->GetValueNumber());\\n+                    if(predecessorLengthValue->GetValueInfo() != mergedLengthValue->GetValueInfo())\\n+                    {\\n+                        mergedLengthValue->SetValueInfo(\\n+                            ValueInfo::MergeLikelyIntValueInfo(\\n+                                this->alloc,\\n+                                mergedLengthValue,\\n+                                predecessorLengthValue,\\n+                                mergedLengthValue->GetValueInfo()->Type().Merge(predecessorLengthValue->GetValueInfo()->Type())));\\n+                    }\\n+                }\\n+                else\\n+                {\\n+                    successorBlockData.SetValue(CopyValue(predecessorLengthValue), mergedLengthSym);\\n                 }\\n-            }\\n-            else\\n-            {\\n-                successorBlockData.SetValue(CopyValue(predecessorLengthValue), mergedLengthSym);\\n             }\\n         }\"}}",
      "message_norm":"[cve-2019-1300]",
      "language":"ro",
      "entities":"[('cve-2019-1300', 'VULNID', 'CVE')]",
      "classification_level_1":null,
      "classification_level_2":null,
      "list_files":"dict_keys(['lib\/Backend\/GlobOpt.cpp'])",
      "num_files":1.0,
      "patch_content":"From 95b3e3400afb8fa20743657f3a8057fb451e6f69 Mon Sep 17 00:00:00 2001\nFrom: Paul Leathers <pleath@microsoft.com>\nDate: Mon, 8 Jul 2019 08:54:11 -0700\nSubject: [PATCH] [CVE-2019-1300]\n\n---\n lib\/Backend\/GlobOpt.cpp | 74 ++++++++++++++++++++++++-----------------\n 1 file changed, 43 insertions(+), 31 deletions(-)\n\ndiff --git a\/lib\/Backend\/GlobOpt.cpp b\/lib\/Backend\/GlobOpt.cpp\nindex d134594903b..e19eed15127 100644\n--- a\/lib\/Backend\/GlobOpt.cpp\n+++ b\/lib\/Backend\/GlobOpt.cpp\n@@ -1167,6 +1167,10 @@ void GlobOpt::InsertValueCompensation(\n     IR::Instr *insertBeforeInstr = predecessor->GetLastInstr();\n     Func *const func = insertBeforeInstr->m_func;\n     bool setLastInstrInPredecessor;\n+    \/\/ If this is a loop back edge, and the successor has been completed, don't attempt to update its block data.\n+    \/\/ The update is unnecessary, and the data has likely been freed.\n+    bool updateSuccessorBlockData = !this->isPerformingLoopBackEdgeCompensation || successor->GetDataUseCount() > 0;\n+\n     if(insertBeforeInstr->IsBranchInstr() || insertBeforeInstr->m_opcode == Js::OpCode::BailTarget)\n     {\n         \/\/ Don't insert code between the branch and the corresponding ByteCodeUses instructions\n@@ -1257,29 +1261,33 @@ void GlobOpt::InsertValueCompensation(\n             \/\/ Merge the head segment length value\n             Assert(predecessorBlockData.liveVarSyms->Test(predecessorHeadSegmentLengthSym->m_id));\n             predecessorBlockData.liveVarSyms->Set(mergedHeadSegmentLengthSym->m_id);\n-            successorBlockData.liveVarSyms->Set(mergedHeadSegmentLengthSym->m_id);\n             Value *const predecessorHeadSegmentLengthValue =\n                 predecessorBlockData.FindValue(predecessorHeadSegmentLengthSym);\n             Assert(predecessorHeadSegmentLengthValue);\n             predecessorBlockData.SetValue(predecessorHeadSegmentLengthValue, mergedHeadSegmentLengthSym);\n-            Value *const mergedHeadSegmentLengthValue = successorBlockData.FindValue(mergedHeadSegmentLengthSym);\n-            if(mergedHeadSegmentLengthValue)\n+\n+            if (updateSuccessorBlockData)\n             {\n-                Assert(mergedHeadSegmentLengthValue->GetValueNumber() != predecessorHeadSegmentLengthValue->GetValueNumber());\n-                if(predecessorHeadSegmentLengthValue->GetValueInfo() != mergedHeadSegmentLengthValue->GetValueInfo())\n+                successorBlockData.liveVarSyms->Set(mergedHeadSegmentLengthSym->m_id);\n+                Value *const mergedHeadSegmentLengthValue = successorBlockData.FindValue(mergedHeadSegmentLengthSym);\n+                if(mergedHeadSegmentLengthValue)\n                 {\n-                    mergedHeadSegmentLengthValue->SetValueInfo(\n-                        ValueInfo::MergeLikelyIntValueInfo(\n-                            this->alloc,\n-                            mergedHeadSegmentLengthValue,\n-                            predecessorHeadSegmentLengthValue,\n-                            mergedHeadSegmentLengthValue->GetValueInfo()->Type()\n-                                .Merge(predecessorHeadSegmentLengthValue->GetValueInfo()->Type())));\n+                    Assert(mergedHeadSegmentLengthValue->GetValueNumber() != predecessorHeadSegmentLengthValue->GetValueNumber());\n+                    if(predecessorHeadSegmentLengthValue->GetValueInfo() != mergedHeadSegmentLengthValue->GetValueInfo())\n+                    {\n+                        mergedHeadSegmentLengthValue->SetValueInfo(\n+                            ValueInfo::MergeLikelyIntValueInfo(\n+                                this->alloc,\n+                                mergedHeadSegmentLengthValue,\n+                                predecessorHeadSegmentLengthValue,\n+                                mergedHeadSegmentLengthValue->GetValueInfo()->Type()\n+                                    .Merge(predecessorHeadSegmentLengthValue->GetValueInfo()->Type())));\n+                    }\n+                }\n+                else\n+                {\n+                    successorBlockData.SetValue(CopyValue(predecessorHeadSegmentLengthValue), mergedHeadSegmentLengthSym);\n                 }\n-            }\n-            else\n-            {\n-                successorBlockData.SetValue(CopyValue(predecessorHeadSegmentLengthValue), mergedHeadSegmentLengthSym);\n             }\n         }\n \n@@ -1300,27 +1308,31 @@ void GlobOpt::InsertValueCompensation(\n             \/\/ Merge the length value\n             Assert(predecessorBlockData.liveVarSyms->Test(predecessorLengthSym->m_id));\n             predecessorBlockData.liveVarSyms->Set(mergedLengthSym->m_id);\n-            successorBlockData.liveVarSyms->Set(mergedLengthSym->m_id);\n             Value *const predecessorLengthValue = predecessorBlockData.FindValue(predecessorLengthSym);\n             Assert(predecessorLengthValue);\n             predecessorBlockData.SetValue(predecessorLengthValue, mergedLengthSym);\n-            Value *const mergedLengthValue = successorBlockData.FindValue(mergedLengthSym);\n-            if(mergedLengthValue)\n+\n+            if (updateSuccessorBlockData)\n             {\n-                Assert(mergedLengthValue->GetValueNumber() != predecessorLengthValue->GetValueNumber());\n-                if(predecessorLengthValue->GetValueInfo() != mergedLengthValue->GetValueInfo())\n+                successorBlockData.liveVarSyms->Set(mergedLengthSym->m_id);\n+                Value *const mergedLengthValue = successorBlockData.FindValue(mergedLengthSym);\n+                if(mergedLengthValue)\n                 {\n-                    mergedLengthValue->SetValueInfo(\n-                        ValueInfo::MergeLikelyIntValueInfo(\n-                            this->alloc,\n-                            mergedLengthValue,\n-                            predecessorLengthValue,\n-                            mergedLengthValue->GetValueInfo()->Type().Merge(predecessorLengthValue->GetValueInfo()->Type())));\n+                    Assert(mergedLengthValue->GetValueNumber() != predecessorLengthValue->GetValueNumber());\n+                    if(predecessorLengthValue->GetValueInfo() != mergedLengthValue->GetValueInfo())\n+                    {\n+                        mergedLengthValue->SetValueInfo(\n+                            ValueInfo::MergeLikelyIntValueInfo(\n+                                this->alloc,\n+                                mergedLengthValue,\n+                                predecessorLengthValue,\n+                                mergedLengthValue->GetValueInfo()->Type().Merge(predecessorLengthValue->GetValueInfo()->Type())));\n+                    }\n+                }\n+                else\n+                {\n+                    successorBlockData.SetValue(CopyValue(predecessorLengthValue), mergedLengthSym);\n                 }\n-            }\n-            else\n-            {\n-                successorBlockData.SetValue(CopyValue(predecessorLengthValue), mergedLengthSym);\n             }\n         }",
      "code_diff":"@@ -1167,6 +1167,10 @@ void GlobOpt::InsertValueCompensation(\n     IR::Instr *insertBeforeInstr = predecessor->GetLastInstr();\n     Func *const func = insertBeforeInstr->m_func;\n     bool setLastInstrInPredecessor;\n+    \/\/ If this is a loop back edge, and the successor has been completed, don't attempt to update its block data.\n+    \/\/ The update is unnecessary, and the data has likely been freed.\n+    bool updateSuccessorBlockData = !this->isPerformingLoopBackEdgeCompensation || successor->GetDataUseCount() > 0;\n+\n     if(insertBeforeInstr->IsBranchInstr() || insertBeforeInstr->m_opcode == Js::OpCode::BailTarget)\n     {\n         \/\/ Don't insert code between the branch and the corresponding ByteCodeUses instructions\n@@ -1257,29 +1261,33 @@ void GlobOpt::InsertValueCompensation(\n             \/\/ Merge the head segment length value\n             Assert(predecessorBlockData.liveVarSyms->Test(predecessorHeadSegmentLengthSym->m_id));\n             predecessorBlockData.liveVarSyms->Set(mergedHeadSegmentLengthSym->m_id);\n-            successorBlockData.liveVarSyms->Set(mergedHeadSegmentLengthSym->m_id);\n             Value *const predecessorHeadSegmentLengthValue =\n                 predecessorBlockData.FindValue(predecessorHeadSegmentLengthSym);\n             Assert(predecessorHeadSegmentLengthValue);\n             predecessorBlockData.SetValue(predecessorHeadSegmentLengthValue, mergedHeadSegmentLengthSym);\n-            Value *const mergedHeadSegmentLengthValue = successorBlockData.FindValue(mergedHeadSegmentLengthSym);\n-            if(mergedHeadSegmentLengthValue)\n+\n+            if (updateSuccessorBlockData)\n             {\n-                Assert(mergedHeadSegmentLengthValue->GetValueNumber() != predecessorHeadSegmentLengthValue->GetValueNumber());\n-                if(predecessorHeadSegmentLengthValue->GetValueInfo() != mergedHeadSegmentLengthValue->GetValueInfo())\n+                successorBlockData.liveVarSyms->Set(mergedHeadSegmentLengthSym->m_id);\n+                Value *const mergedHeadSegmentLengthValue = successorBlockData.FindValue(mergedHeadSegmentLengthSym);\n+                if(mergedHeadSegmentLengthValue)\n                 {\n-                    mergedHeadSegmentLengthValue->SetValueInfo(\n-                        ValueInfo::MergeLikelyIntValueInfo(\n-                            this->alloc,\n-                            mergedHeadSegmentLengthValue,\n-                            predecessorHeadSegmentLengthValue,\n-                            mergedHeadSegmentLengthValue->GetValueInfo()->Type()\n-                                .Merge(predecessorHeadSegmentLengthValue->GetValueInfo()->Type())));\n+                    Assert(mergedHeadSegmentLengthValue->GetValueNumber() != predecessorHeadSegmentLengthValue->GetValueNumber());\n+                    if(predecessorHeadSegmentLengthValue->GetValueInfo() != mergedHeadSegmentLengthValue->GetValueInfo())\n+                    {\n+                        mergedHeadSegmentLengthValue->SetValueInfo(\n+                            ValueInfo::MergeLikelyIntValueInfo(\n+                                this->alloc,\n+                                mergedHeadSegmentLengthValue,\n+                                predecessorHeadSegmentLengthValue,\n+                                mergedHeadSegmentLengthValue->GetValueInfo()->Type()\n+                                    .Merge(predecessorHeadSegmentLengthValue->GetValueInfo()->Type())));\n+                    }\n+                }\n+                else\n+                {\n+                    successorBlockData.SetValue(CopyValue(predecessorHeadSegmentLengthValue), mergedHeadSegmentLengthSym);\n                 }\n-            }\n-            else\n-            {\n-                successorBlockData.SetValue(CopyValue(predecessorHeadSegmentLengthValue), mergedHeadSegmentLengthSym);\n             }\n         }\n \n@@ -1300,27 +1308,31 @@ void GlobOpt::InsertValueCompensation(\n             \/\/ Merge the length value\n             Assert(predecessorBlockData.liveVarSyms->Test(predecessorLengthSym->m_id));\n             predecessorBlockData.liveVarSyms->Set(mergedLengthSym->m_id);\n-            successorBlockData.liveVarSyms->Set(mergedLengthSym->m_id);\n             Value *const predecessorLengthValue = predecessorBlockData.FindValue(predecessorLengthSym);\n             Assert(predecessorLengthValue);\n             predecessorBlockData.SetValue(predecessorLengthValue, mergedLengthSym);\n-            Value *const mergedLengthValue = successorBlockData.FindValue(mergedLengthSym);\n-            if(mergedLengthValue)\n+\n+            if (updateSuccessorBlockData)\n             {\n-                Assert(mergedLengthValue->GetValueNumber() != predecessorLengthValue->GetValueNumber());\n-                if(predecessorLengthValue->GetValueInfo() != mergedLengthValue->GetValueInfo())\n+                successorBlockData.liveVarSyms->Set(mergedLengthSym->m_id);\n+                Value *const mergedLengthValue = successorBlockData.FindValue(mergedLengthSym);\n+                if(mergedLengthValue)\n                 {\n-                    mergedLengthValue->SetValueInfo(\n-                        ValueInfo::MergeLikelyIntValueInfo(\n-                            this->alloc,\n-                            mergedLengthValue,\n-                            predecessorLengthValue,\n-                            mergedLengthValue->GetValueInfo()->Type().Merge(predecessorLengthValue->GetValueInfo()->Type())));\n+                    Assert(mergedLengthValue->GetValueNumber() != predecessorLengthValue->GetValueNumber());\n+                    if(predecessorLengthValue->GetValueInfo() != mergedLengthValue->GetValueInfo())\n+                    {\n+                        mergedLengthValue->SetValueInfo(\n+                            ValueInfo::MergeLikelyIntValueInfo(\n+                                this->alloc,\n+                                mergedLengthValue,\n+                                predecessorLengthValue,\n+                                mergedLengthValue->GetValueInfo()->Type().Merge(predecessorLengthValue->GetValueInfo()->Type())));\n+                    }\n+                }\n+                else\n+                {\n+                    successorBlockData.SetValue(CopyValue(predecessorLengthValue), mergedLengthSym);\n                 }\n-            }\n-            else\n-            {\n-                successorBlockData.SetValue(CopyValue(predecessorLengthValue), mergedLengthSym);\n             }\n         }"
    },
    {
      "index":21,
      "vuln_id":"GHSA-f8h4-7rgh-q2gm",
      "cwe_id":"{'CWE-787', 'CWE-120'}",
      "score":7.8,
      "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/e0b6e58c328059829c3eb968136f17aa72b6c876'}",
      "dataset":"osv",
      "summary":"Segfault and heap buffer overflow in `{Experimental,}DatasetToTFRecord` ### Impact\nThe implementation for `tf.raw_ops.ExperimentalDatasetToTFRecord` and `tf.raw_ops.DatasetToTFRecord` can trigger heap buffer overflow and segmentation fault:\n\n```python\nimport tensorflow as tf\n\ndataset = tf.data.Dataset.range(3)\ndataset = tf.data.experimental.to_variant(dataset)\ntf.raw_ops.ExperimentalDatasetToTFRecord(\n  input_dataset=dataset,\n  filename='\/tmp\/output',\n  compression_type='')\n```\n\nThe [implementation](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/f24faa153ad31a4b51578f8181d3aaab77a1ddeb\/tensorflow\/core\/kernels\/data\/experimental\/to_tf_record_op.cc#L93-L102) assumes that all records in the dataset are of string type. However, there is no check for that, and the example given above uses numeric types.\n\n### Patches\nWe have patched the issue in GitHub commit [e0b6e58c328059829c3eb968136f17aa72b6c876](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/e0b6e58c328059829c3eb968136f17aa72b6c876).\n\nThe fix will be included in TensorFlow 2.6.0. We will also cherrypick this commit on TensorFlow 2.5.1, TensorFlow 2.4.3, and TensorFlow 2.3.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by members of the Aivul Team from Qihoo 360.",
      "published_date":"2021-08-25",
      "chain_len":1,
      "project":"https:\/\/github.com\/tensorflow\/tensorflow",
      "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/e0b6e58c328059829c3eb968136f17aa72b6c876",
      "commit_sha":"e0b6e58c328059829c3eb968136f17aa72b6c876",
      "patch":"SINGLE",
      "chain_ord":"['e0b6e58c328059829c3eb968136f17aa72b6c876']",
      "before_first_fix_commit":"{'b5b9ae94a68215d4498ea2b3d1072dc4b2bf5600'}",
      "last_fix_commit":"e0b6e58c328059829c3eb968136f17aa72b6c876",
      "chain_ord_pos":1.0,
      "commit_datetime":"07\/29\/2021, 21:58:43",
      "message":"Fix segfault\/heap buffer overflow in `{Experimental,}DatasetToTFRecord` where dataset is numeric.\n\nCode assumes only strings inputs and then interprets numbers as valid `tstring`s. Then, when trying to compute the CRC of the record this results in heap buffer overflow.\n\nPiperOrigin-RevId: 387675909\nChange-Id: I7396b9b8afc1ac744112af7c0b1cd7bb41e0f556",
      "author":"Mihai Maruseac",
      "comments":null,
      "stats":"{'additions': 14, 'deletions': 1, 'total': 15}",
      "files":"{'tensorflow\/core\/kernels\/data\/experimental\/to_tf_record_op.cc': {'additions': 14, 'deletions': 1, 'changes': 15, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/e0b6e58c328059829c3eb968136f17aa72b6c876\/tensorflow%2Fcore%2Fkernels%2Fdata%2Fexperimental%2Fto_tf_record_op.cc', 'patch': '@@ -18,6 +18,7 @@ limitations under the License.\\n #include \"tensorflow\/core\/framework\/function_handle_cache.h\"\\n #include \"tensorflow\/core\/framework\/op_kernel.h\"\\n #include \"tensorflow\/core\/framework\/resource_mgr.h\"\\n+#include \"tensorflow\/core\/framework\/types.h\"\\n #include \"tensorflow\/core\/kernels\/ops_util.h\"\\n #include \"tensorflow\/core\/lib\/core\/threadpool.h\"\\n #include \"tensorflow\/core\/lib\/io\/record_writer.h\"\\n@@ -91,8 +92,20 @@ class ToTFRecordOp : public AsyncOpKernel {\\n     TF_RETURN_IF_ERROR(finalized_dataset->MakeIterator(\\n         &iter_ctx, \/*parent=*\/nullptr, \"ToTFRecordOpIterator\", &iterator));\\n \\n+    const int num_output_dtypes = finalized_dataset->output_dtypes().size();\\n+    if (num_output_dtypes != 1) {\\n+      return errors::InvalidArgument(\\n+          \"ToTFRecordOp currently only support datasets of 1 single column, \",\\n+          \"but got \", num_output_dtypes);\\n+    }\\n+    const DataType dt = finalized_dataset->output_dtypes()[0];\\n+    if (dt != DT_STRING) {\\n+      return errors::InvalidArgument(\\n+          \"ToTFRecordOp currently only supports DT_STRING dataypes, but got \",\\n+          DataTypeString(dt));\\n+    }\\n     std::vector<Tensor> components;\\n-    components.reserve(finalized_dataset->output_dtypes().size());\\n+    components.reserve(num_output_dtypes);\\n     bool end_of_sequence;\\n     do {\\n       TF_RETURN_IF_ERROR('}}",
      "message_norm":"fix segfault\/heap buffer overflow in `{experimental,}datasettotfrecord` where dataset is numeric.\n\ncode assumes only strings inputs and then interprets numbers as valid `tstring`s. then, when trying to compute the crc of the record this results in heap buffer overflow.\n\npiperorigin-revid: 387675909\nchange-id: i7396b9b8afc1ac744112af7c0b1cd7bb41e0f556",
      "language":"en",
      "entities":"[('fix', 'ACTION', ''), ('segfault', 'SECWORD', ''), ('buffer overflow', 'SECWORD', ''), ('crc', 'SECWORD', ''), ('buffer overflow', 'SECWORD', ''), ('387675909', 'SHA', 'generic_sha')]",
      "classification_level_1":null,
      "classification_level_2":null,
      "list_files":"dict_keys(['tensorflow\/core\/kernels\/data\/experimental\/to_tf_record_op.cc'])",
      "num_files":1.0,
      "patch_content":"From e0b6e58c328059829c3eb968136f17aa72b6c876 Mon Sep 17 00:00:00 2001\nFrom: Mihai Maruseac <mihaimaruseac@google.com>\nDate: Thu, 29 Jul 2021 14:58:43 -0700\nSubject: [PATCH] Fix segfault\/heap buffer overflow in\n `{Experimental,}DatasetToTFRecord` where dataset is numeric.\n\nCode assumes only strings inputs and then interprets numbers as valid `tstring`s. Then, when trying to compute the CRC of the record this results in heap buffer overflow.\n\nPiperOrigin-RevId: 387675909\nChange-Id: I7396b9b8afc1ac744112af7c0b1cd7bb41e0f556\n---\n ...\/kernels\/data\/experimental\/to_tf_record_op.cc  | 15 ++++++++++++++-\n 1 file changed, 14 insertions(+), 1 deletion(-)\n\ndiff --git a\/tensorflow\/core\/kernels\/data\/experimental\/to_tf_record_op.cc b\/tensorflow\/core\/kernels\/data\/experimental\/to_tf_record_op.cc\nindex 0ba04d0bd94bd1..4f759bede55b61 100644\n--- a\/tensorflow\/core\/kernels\/data\/experimental\/to_tf_record_op.cc\n+++ b\/tensorflow\/core\/kernels\/data\/experimental\/to_tf_record_op.cc\n@@ -18,6 +18,7 @@ limitations under the License.\n #include \"tensorflow\/core\/framework\/function_handle_cache.h\"\n #include \"tensorflow\/core\/framework\/op_kernel.h\"\n #include \"tensorflow\/core\/framework\/resource_mgr.h\"\n+#include \"tensorflow\/core\/framework\/types.h\"\n #include \"tensorflow\/core\/kernels\/ops_util.h\"\n #include \"tensorflow\/core\/lib\/core\/threadpool.h\"\n #include \"tensorflow\/core\/lib\/io\/record_writer.h\"\n@@ -91,8 +92,20 @@ class ToTFRecordOp : public AsyncOpKernel {\n     TF_RETURN_IF_ERROR(finalized_dataset->MakeIterator(\n         &iter_ctx, \/*parent=*\/nullptr, \"ToTFRecordOpIterator\", &iterator));\n \n+    const int num_output_dtypes = finalized_dataset->output_dtypes().size();\n+    if (num_output_dtypes != 1) {\n+      return errors::InvalidArgument(\n+          \"ToTFRecordOp currently only support datasets of 1 single column, \",\n+          \"but got \", num_output_dtypes);\n+    }\n+    const DataType dt = finalized_dataset->output_dtypes()[0];\n+    if (dt != DT_STRING) {\n+      return errors::InvalidArgument(\n+          \"ToTFRecordOp currently only supports DT_STRING dataypes, but got \",\n+          DataTypeString(dt));\n+    }\n     std::vector<Tensor> components;\n-    components.reserve(finalized_dataset->output_dtypes().size());\n+    components.reserve(num_output_dtypes);\n     bool end_of_sequence;\n     do {\n       TF_RETURN_IF_ERROR(",
      "code_diff":"@@ -18,6 +18,7 @@ limitations under the License.\n #include \"tensorflow\/core\/framework\/function_handle_cache.h\"\n #include \"tensorflow\/core\/framework\/op_kernel.h\"\n #include \"tensorflow\/core\/framework\/resource_mgr.h\"\n+#include \"tensorflow\/core\/framework\/types.h\"\n #include \"tensorflow\/core\/kernels\/ops_util.h\"\n #include \"tensorflow\/core\/lib\/core\/threadpool.h\"\n #include \"tensorflow\/core\/lib\/io\/record_writer.h\"\n@@ -91,8 +92,20 @@ class ToTFRecordOp : public AsyncOpKernel {\n     TF_RETURN_IF_ERROR(finalized_dataset->MakeIterator(\n         &iter_ctx, \/*parent=*\/nullptr, \"ToTFRecordOpIterator\", &iterator));\n \n+    const int num_output_dtypes = finalized_dataset->output_dtypes().size();\n+    if (num_output_dtypes != 1) {\n+      return errors::InvalidArgument(\n+          \"ToTFRecordOp currently only support datasets of 1 single column, \",\n+          \"but got \", num_output_dtypes);\n+    }\n+    const DataType dt = finalized_dataset->output_dtypes()[0];\n+    if (dt != DT_STRING) {\n+      return errors::InvalidArgument(\n+          \"ToTFRecordOp currently only supports DT_STRING dataypes, but got \",\n+          DataTypeString(dt));\n+    }\n     std::vector<Tensor> components;\n-    components.reserve(finalized_dataset->output_dtypes().size());\n+    components.reserve(num_output_dtypes);\n     bool end_of_sequence;\n     do {\n       TF_RETURN_IF_ERROR("
    },
    {
      "index":22,
      "vuln_id":"GHSA-4278-2v5v-65r4",
      "cwe_id":"{'CWE-787', 'CWE-120'}",
      "score":2.5,
      "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/eebb96c2830d48597d055d247c0e9aebaea94cd5'}",
      "dataset":"osv",
      "summary":"Heap buffer overflow in `RaggedBinCount` ### Impact\nIf the `splits` argument of `RaggedBincount` does not specify a valid [`SparseTensor`](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/sparse\/SparseTensor), then an attacker can trigger a heap buffer overflow:\n\n```python\nimport tensorflow as tf\ntf.raw_ops.RaggedBincount(splits=[0], values=[1,1,1,1,1], size=5, weights=[1,2,3,4], binary_output=False)\n```\n\nThis will cause a read from outside the bounds of the `splits` tensor buffer in the [implementation of the `RaggedBincount` op](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/8b677d79167799f71c42fd3fa074476e0295413a\/tensorflow\/core\/kernels\/bincount_op.cc#L430-L433):\n\n```cc\n    for (int idx = 0; idx < num_values; ++idx) {\n      while (idx >= splits(batch_idx)) {\n        batch_idx++;\n      }\n      ...\n    }\n```\n\nBefore the `for` loop, `batch_idx` is set to 0. The user controls the `splits` array, making it contain only one element, 0. Thus, the code in the `while` loop would increment `batch_idx` and then try to read `splits(1)`, which is outside of bounds.\n\n### Patches\nWe have patched the issue in GitHub commit [eebb96c2830d48597d055d247c0e9aebaea94cd5](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/eebb96c2830d48597d055d247c0e9aebaea94cd5).\n\nThe fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2 and TensorFlow 2.3.3, as these are also affected.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by members of the Aivul Team from Qihoo 360.",
      "published_date":"2021-05-21",
      "chain_len":1,
      "project":"https:\/\/github.com\/tensorflow\/tensorflow",
      "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/eebb96c2830d48597d055d247c0e9aebaea94cd5",
      "commit_sha":"eebb96c2830d48597d055d247c0e9aebaea94cd5",
      "patch":"SINGLE",
      "chain_ord":"['eebb96c2830d48597d055d247c0e9aebaea94cd5']",
      "before_first_fix_commit":"{'8b677d79167799f71c42fd3fa074476e0295413a'}",
      "last_fix_commit":"eebb96c2830d48597d055d247c0e9aebaea94cd5",
      "chain_ord_pos":1.0,
      "commit_datetime":"04\/13\/2021, 21:18:51",
      "message":"Fix an invalid address vulnerability in `tf.raw_ops.RaggedBincount`.\n\nPiperOrigin-RevId: 368293153\nChange-Id: I4b4e493d3fd05e7dc55a55de3a041a80a4f275c3",
      "author":"Amit Patankar",
      "comments":"{'com_1': {'author': 'Rayyan335', 'datetime': '05\/14\/2021, 19:00:36', 'body': 'tensorflow\/core\/kernels\/bincount_op.cc'}}",
      "stats":"{'additions': 9, 'deletions': 0, 'total': 9}",
      "files":"{'tensorflow\/core\/kernels\/bincount_op.cc': {'additions': 9, 'deletions': 0, 'changes': 9, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/eebb96c2830d48597d055d247c0e9aebaea94cd5\/tensorflow%2Fcore%2Fkernels%2Fbincount_op.cc', 'patch': '@@ -420,6 +420,15 @@ class RaggedBincountOp : public OpKernel {\\n     int num_values = values.size();\\n     int batch_idx = 0;\\n \\n+    OP_REQUIRES(ctx, splits(0) == 0,\\n+                errors::InvalidArgument(\"Splits must start with 0, not with \",\\n+                                        splits(0)));\\n+\\n+    OP_REQUIRES(ctx, splits(num_rows) == num_values,\\n+                errors::InvalidArgument(\\n+                    \"Splits must end with the number of values, got \",\\n+                    splits(num_rows), \" instead of \", num_values));\\n+\\n     Tensor* out_t;\\n     OP_REQUIRES_OK(\\n         ctx, ctx->allocate_output(0, TensorShape({num_rows, size}), &out_t));'}}",
      "message_norm":"fix an invalid address vulnerability in `tf.raw_ops.raggedbincount`.\n\npiperorigin-revid: 368293153\nchange-id: i4b4e493d3fd05e7dc55a55de3a041a80a4f275c3",
      "language":"en",
      "entities":"[('fix', 'ACTION', ''), ('vulnerability', 'SECWORD', ''), ('368293153', 'SHA', 'generic_sha')]",
      "classification_level_1":null,
      "classification_level_2":null,
      "list_files":"dict_keys(['tensorflow\/core\/kernels\/bincount_op.cc'])",
      "num_files":1.0,
      "patch_content":"From eebb96c2830d48597d055d247c0e9aebaea94cd5 Mon Sep 17 00:00:00 2001\nFrom: Amit Patankar <amitpatankar@google.com>\nDate: Tue, 13 Apr 2021 14:18:51 -0700\nSubject: [PATCH] Fix an invalid address vulnerability in\n `tf.raw_ops.RaggedBincount`.\n\nPiperOrigin-RevId: 368293153\nChange-Id: I4b4e493d3fd05e7dc55a55de3a041a80a4f275c3\n---\n tensorflow\/core\/kernels\/bincount_op.cc | 9 +++++++++\n 1 file changed, 9 insertions(+)\n\ndiff --git a\/tensorflow\/core\/kernels\/bincount_op.cc b\/tensorflow\/core\/kernels\/bincount_op.cc\nindex 35911ee5d5540a..258266ab29d33f 100644\n--- a\/tensorflow\/core\/kernels\/bincount_op.cc\n+++ b\/tensorflow\/core\/kernels\/bincount_op.cc\n@@ -420,6 +420,15 @@ class RaggedBincountOp : public OpKernel {\n     int num_values = values.size();\n     int batch_idx = 0;\n \n+    OP_REQUIRES(ctx, splits(0) == 0,\n+                errors::InvalidArgument(\"Splits must start with 0, not with \",\n+                                        splits(0)));\n+\n+    OP_REQUIRES(ctx, splits(num_rows) == num_values,\n+                errors::InvalidArgument(\n+                    \"Splits must end with the number of values, got \",\n+                    splits(num_rows), \" instead of \", num_values));\n+\n     Tensor* out_t;\n     OP_REQUIRES_OK(\n         ctx, ctx->allocate_output(0, TensorShape({num_rows, size}), &out_t));",
      "code_diff":"@@ -420,6 +420,15 @@ class RaggedBincountOp : public OpKernel {\n     int num_values = values.size();\n     int batch_idx = 0;\n \n+    OP_REQUIRES(ctx, splits(0) == 0,\n+                errors::InvalidArgument(\"Splits must start with 0, not with \",\n+                                        splits(0)));\n+\n+    OP_REQUIRES(ctx, splits(num_rows) == num_values,\n+                errors::InvalidArgument(\n+                    \"Splits must end with the number of values, got \",\n+                    splits(num_rows), \" instead of \", num_values));\n+\n     Tensor* out_t;\n     OP_REQUIRES_OK(\n         ctx, ctx->allocate_output(0, TensorShape({num_rows, size}), &out_t));"
    },
    {
      "index":23,
      "vuln_id":"GHSA-q4qf-3fc6-8x34",
      "cwe_id":"{'CWE-787', 'CWE-119'}",
      "score":8.7,
      "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/2d88f470dea2671b430884260f3626b1fe99830a'}",
      "dataset":"osv",
      "summary":"Segfault and data corruption in tensorflow-lite ### Impact\nTo mimic Python's indexing with negative values, TFLite uses `ResolveAxis` to convert negative values to positive indices. However, the only check that the converted index is now valid is only present in debug builds:\nhttps:\/\/github.com\/tensorflow\/tensorflow\/blob\/0e68f4d3295eb0281a517c3662f6698992b7b2cf\/tensorflow\/lite\/kernels\/internal\/reference\/reduce.h#L68-L72\n\nIf the `DCHECK` does not trigger, then code execution moves ahead with a negative index. This, in turn, results in accessing data out of bounds which results in segfaults and\/or data corruption.\n### Patches\nWe have patched the issue in 2d88f470dea2671b430884260f3626b1fe99830a and will release patch releases for all versions between 1.15 and 2.3.\n\nWe recommend users to upgrade to TensorFlow 1.15.4, 2.0.3, 2.1.2, 2.2.1, or 2.3.1.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by members of the Aivul Team from Qihoo 360.",
      "published_date":"2020-09-25",
      "chain_len":1,
      "project":"https:\/\/github.com\/tensorflow\/tensorflow",
      "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/2d88f470dea2671b430884260f3626b1fe99830a",
      "commit_sha":"2d88f470dea2671b430884260f3626b1fe99830a",
      "patch":"SINGLE",
      "chain_ord":"['2d88f470dea2671b430884260f3626b1fe99830a']",
      "before_first_fix_commit":"{'d487b8c4ca7d77d650fb8dca94b073aad8ff4b5e'}",
      "last_fix_commit":"2d88f470dea2671b430884260f3626b1fe99830a",
      "chain_ord_pos":1.0,
      "commit_datetime":"09\/18\/2020, 21:43:00",
      "message":"[tflite] Ensure `ResolveAxis` properly handles negative inputs.\n\nIn Python, a list `l` of length `n` allows indexing with negative indices, `l[i]`. The only constraint is that `n + i` becomes positive. Code in `ResolveAxis` assumes the constraints and only checks it using a `DCHECK`. But the macro is a no-op in non-debug builds and that can result in reading from negative offsets (buffer underflows).\n\nPiperOrigin-RevId: 332530683\nChange-Id: I464e073fee618054ae3719a3679739007bb3f3bc",
      "author":"Mihai Maruseac",
      "comments":null,
      "stats":"{'additions': 3, 'deletions': 0, 'total': 3}",
      "files":"{'tensorflow\/lite\/kernels\/internal\/reference\/reduce.h': {'additions': 3, 'deletions': 0, 'changes': 3, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/2d88f470dea2671b430884260f3626b1fe99830a\/tensorflow%2Flite%2Fkernels%2Finternal%2Freference%2Freduce.h', 'patch': '@@ -70,6 +70,9 @@ inline bool ResolveAxis(const int num_dims, const int* axis,\\n     \/\/ eg: For num_dims=3, [0, 1, 2] is the same as [-3, -2, -1]  *\/\\n     int current = axis[idx] < 0 ? (axis[idx] + num_dims) : axis[idx];\\n     TFLITE_DCHECK(current >= 0 && current < num_dims);\\n+    if (current < 0 || current >= num_dims) {\\n+      return false;\\n+    }\\n     bool is_dup = false;\\n     for (int j = 0; j < *out_num_axis; ++j) {\\n       if (out_axis[j] == current) {'}}",
      "message_norm":"[tflite] ensure `resolveaxis` properly handles negative inputs.\n\nin python, a list `l` of length `n` allows indexing with negative indices, `l[i]`. the only constraint is that `n + i` becomes positive. code in `resolveaxis` assumes the constraints and only checks it using a `dcheck`. but the macro is a no-op in non-debug builds and that can result in reading from negative offsets (buffer underflows).\n\npiperorigin-revid: 332530683\nchange-id: i464e073fee618054ae3719a3679739007bb3f3bc",
      "language":"en",
      "entities":"[('ensure', 'ACTION', ''), ('buffer underflows', 'SECWORD', ''), ('332530683', 'SHA', 'generic_sha')]",
      "classification_level_1":null,
      "classification_level_2":null,
      "list_files":"dict_keys(['tensorflow\/lite\/kernels\/internal\/reference\/reduce.h'])",
      "num_files":1.0,
      "patch_content":"From 2d88f470dea2671b430884260f3626b1fe99830a Mon Sep 17 00:00:00 2001\nFrom: Mihai Maruseac <mihaimaruseac@google.com>\nDate: Fri, 18 Sep 2020 14:43:00 -0700\nSubject: [PATCH] [tflite] Ensure `ResolveAxis` properly handles negative\n inputs.\n\nIn Python, a list `l` of length `n` allows indexing with negative indices, `l[i]`. The only constraint is that `n + i` becomes positive. Code in `ResolveAxis` assumes the constraints and only checks it using a `DCHECK`. But the macro is a no-op in non-debug builds and that can result in reading from negative offsets (buffer underflows).\n\nPiperOrigin-RevId: 332530683\nChange-Id: I464e073fee618054ae3719a3679739007bb3f3bc\n---\n tensorflow\/lite\/kernels\/internal\/reference\/reduce.h | 3 +++\n 1 file changed, 3 insertions(+)\n\ndiff --git a\/tensorflow\/lite\/kernels\/internal\/reference\/reduce.h b\/tensorflow\/lite\/kernels\/internal\/reference\/reduce.h\nindex 7953b4347c6c20..994baa0b254b3b 100644\n--- a\/tensorflow\/lite\/kernels\/internal\/reference\/reduce.h\n+++ b\/tensorflow\/lite\/kernels\/internal\/reference\/reduce.h\n@@ -70,6 +70,9 @@ inline bool ResolveAxis(const int num_dims, const int* axis,\n     \/\/ eg: For num_dims=3, [0, 1, 2] is the same as [-3, -2, -1]  *\/\n     int current = axis[idx] < 0 ? (axis[idx] + num_dims) : axis[idx];\n     TFLITE_DCHECK(current >= 0 && current < num_dims);\n+    if (current < 0 || current >= num_dims) {\n+      return false;\n+    }\n     bool is_dup = false;\n     for (int j = 0; j < *out_num_axis; ++j) {\n       if (out_axis[j] == current) {",
      "code_diff":"@@ -70,6 +70,9 @@ inline bool ResolveAxis(const int num_dims, const int* axis,\n     \/\/ eg: For num_dims=3, [0, 1, 2] is the same as [-3, -2, -1]  *\/\n     int current = axis[idx] < 0 ? (axis[idx] + num_dims) : axis[idx];\n     TFLITE_DCHECK(current >= 0 && current < num_dims);\n+    if (current < 0 || current >= num_dims) {\n+      return false;\n+    }\n     bool is_dup = false;\n     for (int j = 0; j < *out_num_axis; ++j) {\n       if (out_axis[j] == current) {"
    },
    {
      "index":24,
      "vuln_id":"GHSA-8gvg-8vhf-h26g",
      "cwe_id":"{'CWE-787'}",
      "score":7.5,
      "chain":"{'https:\/\/github.com\/chakra-core\/ChakraCore\/commit\/d797e3f00e34c12c8c0ae52f56344325439dccd7', 'https:\/\/github.com\/chakra-core\/ChakraCore\/commit\/7827e117753052d479fabe19a25cfece88059bca'}",
      "dataset":"osv",
      "summary":"Out-of-bounds write A remote code execution vulnerability exists in the way that the Chakra scripting engine handles objects in memory in Microsoft Edge, aka 'Chakra Scripting Engine Memory Corruption Vulnerability'. This CVE ID is unique from CVE-2019-0912, CVE-2019-0913, CVE-2019-0914, CVE-2019-0915, CVE-2019-0916, CVE-2019-0917, CVE-2019-0922, CVE-2019-0923, CVE-2019-0924, CVE-2019-0925, CVE-2019-0927, CVE-2019-0933.",
      "published_date":"2021-03-29",
      "chain_len":2,
      "project":"https:\/\/github.com\/chakra-core\/ChakraCore",
      "commit_href":"https:\/\/github.com\/chakra-core\/ChakraCore\/commit\/7827e117753052d479fabe19a25cfece88059bca",
      "commit_sha":"7827e117753052d479fabe19a25cfece88059bca",
      "patch":"MULTI",
      "chain_ord":"['7827e117753052d479fabe19a25cfece88059bca', 'd797e3f00e34c12c8c0ae52f56344325439dccd7']",
      "before_first_fix_commit":"{'ea0491305137183603bf43844b5584d4cc972e28', '4594e340bc9ca9f857010a68e8b562d65b46eed6'}",
      "last_fix_commit":"d797e3f00e34c12c8c0ae52f56344325439dccd7",
      "chain_ord_pos":1.0,
      "commit_datetime":"04\/17\/2019, 22:42:35",
      "message":"[CVE-2019-0937]",
      "author":"Paul Leathers",
      "comments":null,
      "stats":"{'additions': 10, 'deletions': 0, 'total': 10}",
      "files":"{'lib\/Runtime\/ByteCode\/ByteCodeEmitter.cpp': {'additions': 10, 'deletions': 0, 'changes': 10, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/chakra-core\/ChakraCore\/raw\/7827e117753052d479fabe19a25cfece88059bca\/lib%2FRuntime%2FByteCode%2FByteCodeEmitter.cpp', 'patch': '@@ -4006,6 +4006,11 @@ void ByteCodeGenerator::StartEmitCatch(ParseNodeCatch *pnodeCatch)\\n                 sym->SetIsGlobalCatch(true);\\n             }\\n \\n+            if (sym->NeedsScopeObject())\\n+            {\\n+                scope->SetIsObject();\\n+            }\\n+\\n             Assert(sym->GetScopeSlot() == Js::Constants::NoProperty);\\n             if (sym->NeedsSlotAlloc(this, funcInfo))\\n             {\\n@@ -4029,6 +4034,11 @@ void ByteCodeGenerator::StartEmitCatch(ParseNodeCatch *pnodeCatch)\\n             sym->SetIsGlobalCatch(true);\\n         }\\n \\n+        if (sym->NeedsScopeObject())\\n+        {\\n+            scope->SetIsObject();\\n+        }\\n+\\n         if (scope->GetMustInstantiate())\\n         {\\n             if (sym->IsInSlot(this, funcInfo))'}}",
      "message_norm":"[cve-2019-0937]",
      "language":"ro",
      "entities":"[('cve-2019-0937', 'VULNID', 'CVE')]",
      "classification_level_1":null,
      "classification_level_2":null,
      "list_files":"dict_keys(['lib\/Runtime\/ByteCode\/ByteCodeEmitter.cpp'])",
      "num_files":1.0,
      "patch_content":"From 7827e117753052d479fabe19a25cfece88059bca Mon Sep 17 00:00:00 2001\nFrom: Paul Leathers <pleath@microsoft.com>\nDate: Wed, 17 Apr 2019 15:42:35 -0700\nSubject: [PATCH] [CVE-2019-0937]\n\n---\n lib\/Runtime\/ByteCode\/ByteCodeEmitter.cpp | 10 ++++++++++\n 1 file changed, 10 insertions(+)\n\ndiff --git a\/lib\/Runtime\/ByteCode\/ByteCodeEmitter.cpp b\/lib\/Runtime\/ByteCode\/ByteCodeEmitter.cpp\nindex 5c07982b92b..933a21d385d 100644\n--- a\/lib\/Runtime\/ByteCode\/ByteCodeEmitter.cpp\n+++ b\/lib\/Runtime\/ByteCode\/ByteCodeEmitter.cpp\n@@ -4006,6 +4006,11 @@ void ByteCodeGenerator::StartEmitCatch(ParseNodeCatch *pnodeCatch)\n                 sym->SetIsGlobalCatch(true);\n             }\n \n+            if (sym->NeedsScopeObject())\n+            {\n+                scope->SetIsObject();\n+            }\n+\n             Assert(sym->GetScopeSlot() == Js::Constants::NoProperty);\n             if (sym->NeedsSlotAlloc(this, funcInfo))\n             {\n@@ -4029,6 +4034,11 @@ void ByteCodeGenerator::StartEmitCatch(ParseNodeCatch *pnodeCatch)\n             sym->SetIsGlobalCatch(true);\n         }\n \n+        if (sym->NeedsScopeObject())\n+        {\n+            scope->SetIsObject();\n+        }\n+\n         if (scope->GetMustInstantiate())\n         {\n             if (sym->IsInSlot(this, funcInfo))",
      "code_diff":"@@ -4006,6 +4006,11 @@ void ByteCodeGenerator::StartEmitCatch(ParseNodeCatch *pnodeCatch)\n                 sym->SetIsGlobalCatch(true);\n             }\n \n+            if (sym->NeedsScopeObject())\n+            {\n+                scope->SetIsObject();\n+            }\n+\n             Assert(sym->GetScopeSlot() == Js::Constants::NoProperty);\n             if (sym->NeedsSlotAlloc(this, funcInfo))\n             {\n@@ -4029,6 +4034,11 @@ void ByteCodeGenerator::StartEmitCatch(ParseNodeCatch *pnodeCatch)\n             sym->SetIsGlobalCatch(true);\n         }\n \n+        if (sym->NeedsScopeObject())\n+        {\n+            scope->SetIsObject();\n+        }\n+\n         if (scope->GetMustInstantiate())\n         {\n             if (sym->IsInSlot(this, funcInfo))"
    },
    {
      "index":25,
      "vuln_id":"GHSA-hrmm-f4j8-8vxc",
      "cwe_id":"{'CWE-787'}",
      "score":7.5,
      "chain":"{'https:\/\/github.com\/chakra-core\/ChakraCore\/commit\/d797e3f00e34c12c8c0ae52f56344325439dccd7', 'https:\/\/github.com\/chakra-core\/ChakraCore\/commit\/a9ab1aae31078e80593b9227db11d316c2239ef3'}",
      "dataset":"osv",
      "summary":"Out-of-bounds write A remote code execution vulnerability exists in the way that the Chakra scripting engine handles objects in memory in Microsoft Edge, aka 'Chakra Scripting Engine Memory Corruption Vulnerability'. This CVE ID is unique from CVE-2019-0912, CVE-2019-0913, CVE-2019-0914, CVE-2019-0915, CVE-2019-0916, CVE-2019-0917, CVE-2019-0923, CVE-2019-0924, CVE-2019-0925, CVE-2019-0927, CVE-2019-0933, CVE-2019-0937.",
      "published_date":"2021-03-29",
      "chain_len":2,
      "project":"https:\/\/github.com\/chakra-core\/ChakraCore",
      "commit_href":"https:\/\/github.com\/chakra-core\/ChakraCore\/commit\/a9ab1aae31078e80593b9227db11d316c2239ef3",
      "commit_sha":"a9ab1aae31078e80593b9227db11d316c2239ef3",
      "patch":"MULTI",
      "chain_ord":"['a9ab1aae31078e80593b9227db11d316c2239ef3', 'd797e3f00e34c12c8c0ae52f56344325439dccd7']",
      "before_first_fix_commit":"{'ea0491305137183603bf43844b5584d4cc972e28', '4594e340bc9ca9f857010a68e8b562d65b46eed6'}",
      "last_fix_commit":"d797e3f00e34c12c8c0ae52f56344325439dccd7",
      "chain_ord_pos":1.0,
      "commit_datetime":"04\/12\/2019, 23:15:50",
      "message":"[CVE-2019-0922] Type confusion in Edge - Individual",
      "author":"Meghana Gupta",
      "comments":null,
      "stats":"{'additions': 8, 'deletions': 1, 'total': 9}",
      "files":"{'lib\/Backend\/GlobOptArrays.cpp': {'additions': 8, 'deletions': 1, 'changes': 9, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/chakra-core\/ChakraCore\/raw\/a9ab1aae31078e80593b9227db11d316c2239ef3\/lib%2FBackend%2FGlobOptArrays.cpp', 'patch': '@@ -1736,7 +1736,14 @@ void GlobOpt::ArraySrcOpt::Optimize()\\n     {\\n         if (newBaseValueType != baseValueType)\\n         {\\n-            UpdateValue(nullptr, nullptr, nullptr);\\n+            if (globOpt->IsSafeToTransferInPrePass(baseOpnd, baseValue))\\n+            {\\n+                UpdateValue(nullptr, nullptr, nullptr);\\n+            }\\n+            else if (globOpt->IsOperationThatLikelyKillsJsArraysWithNoMissingValues(instr) && baseValueInfo->HasNoMissingValues())\\n+            {\\n+                globOpt->ChangeValueType(nullptr, baseValue, baseValueInfo->Type().SetHasNoMissingValues(false), true);\\n+            }\\n         }\\n \\n         \/\/ For javascript arrays and objects with javascript arrays:'}}",
      "message_norm":"[cve-2019-0922] type confusion in edge - individual",
      "language":"en",
      "entities":"[('cve-2019-0922', 'VULNID', 'CVE'), ('type confusion', 'SECWORD', '')]",
      "classification_level_1":null,
      "classification_level_2":null,
      "list_files":"dict_keys(['lib\/Backend\/GlobOptArrays.cpp'])",
      "num_files":1.0,
      "patch_content":"From a9ab1aae31078e80593b9227db11d316c2239ef3 Mon Sep 17 00:00:00 2001\nFrom: Meghana Gupta <megupta@microsoft.com>\nDate: Fri, 12 Apr 2019 16:15:50 -0700\nSubject: [PATCH] [CVE-2019-0922] Type confusion in Edge - Individual\n\n---\n lib\/Backend\/GlobOptArrays.cpp | 9 ++++++++-\n 1 file changed, 8 insertions(+), 1 deletion(-)\n\ndiff --git a\/lib\/Backend\/GlobOptArrays.cpp b\/lib\/Backend\/GlobOptArrays.cpp\nindex 135fcf5c108..9ac28c59c12 100644\n--- a\/lib\/Backend\/GlobOptArrays.cpp\n+++ b\/lib\/Backend\/GlobOptArrays.cpp\n@@ -1736,7 +1736,14 @@ void GlobOpt::ArraySrcOpt::Optimize()\n     {\n         if (newBaseValueType != baseValueType)\n         {\n-            UpdateValue(nullptr, nullptr, nullptr);\n+            if (globOpt->IsSafeToTransferInPrePass(baseOpnd, baseValue))\n+            {\n+                UpdateValue(nullptr, nullptr, nullptr);\n+            }\n+            else if (globOpt->IsOperationThatLikelyKillsJsArraysWithNoMissingValues(instr) && baseValueInfo->HasNoMissingValues())\n+            {\n+                globOpt->ChangeValueType(nullptr, baseValue, baseValueInfo->Type().SetHasNoMissingValues(false), true);\n+            }\n         }\n \n         \/\/ For javascript arrays and objects with javascript arrays:",
      "code_diff":"@@ -1736,7 +1736,14 @@ void GlobOpt::ArraySrcOpt::Optimize()\n     {\n         if (newBaseValueType != baseValueType)\n         {\n-            UpdateValue(nullptr, nullptr, nullptr);\n+            if (globOpt->IsSafeToTransferInPrePass(baseOpnd, baseValue))\n+            {\n+                UpdateValue(nullptr, nullptr, nullptr);\n+            }\n+            else if (globOpt->IsOperationThatLikelyKillsJsArraysWithNoMissingValues(instr) && baseValueInfo->HasNoMissingValues())\n+            {\n+                globOpt->ChangeValueType(nullptr, baseValue, baseValueInfo->Type().SetHasNoMissingValues(false), true);\n+            }\n         }\n \n         \/\/ For javascript arrays and objects with javascript arrays:"
    },
    {
      "index":26,
      "vuln_id":"GHSA-79fv-9865-4qcv",
      "cwe_id":"{'CWE-787', 'CWE-119'}",
      "score":2.5,
      "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/a74768f8e4efbda4def9f16ee7e13cf3922ac5f7'}",
      "dataset":"osv",
      "summary":"Heap buffer overflow in `MaxPoolGrad` ### Impact\nThe implementation of `tf.raw_ops.MaxPoolGrad` is vulnerable to a heap buffer overflow:\n  \n```python\nimport tensorflow as tf\n\norig_input = tf.constant([0.0], shape=[1, 1, 1, 1], dtype=tf.float32)\norig_output = tf.constant([0.0], shape=[1, 1, 1, 1], dtype=tf.float32)\ngrad = tf.constant([], shape=[0, 0, 0, 0], dtype=tf.float32)\nksize = [1, 1, 1, 1] \nstrides = [1, 1, 1, 1]\npadding = \"SAME\"\n\ntf.raw_ops.MaxPoolGrad(\n  orig_input=orig_input, orig_output=orig_output, grad=grad, ksize=ksize,\n  strides=strides, padding=padding, explicit_paddings=[])\n```\n\nThe [implementation](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/ab1e644b48c82cb71493f4362b4dd38f4577a1cf\/tensorflow\/core\/kernels\/maxpooling_op.cc#L194-L203) fails to validate that indices used to access elements of input\/output arrays are valid:\n\n```cc\nfor (int index = out_start; index < out_end; ++index) {\n  int input_backprop_index = out_arg_max_flat(index);\n  FastBoundsCheck(input_backprop_index - in_start, in_end - in_start);\n  input_backprop_flat(input_backprop_index) += out_backprop_flat(index);\n}\n```\n\nWhereas accesses to `input_backprop_flat` are guarded by `FastBoundsCheck`, the indexing in `out_backprop_flat` can result in OOB access.\n\n### Patches\nWe have patched the issue in GitHub commit [a74768f8e4efbda4def9f16ee7e13cf3922ac5f7](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/a74768f8e4efbda4def9f16ee7e13cf3922ac5f7).\n\nThe fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by Ying Wang and Yakun Zhang of Baidu X-Team.",
      "published_date":"2021-05-21",
      "chain_len":1,
      "project":"https:\/\/github.com\/tensorflow\/tensorflow",
      "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/a74768f8e4efbda4def9f16ee7e13cf3922ac5f7",
      "commit_sha":"a74768f8e4efbda4def9f16ee7e13cf3922ac5f7",
      "patch":"SINGLE",
      "chain_ord":"['a74768f8e4efbda4def9f16ee7e13cf3922ac5f7']",
      "before_first_fix_commit":"{'ab1e644b48c82cb71493f4362b4dd38f4577a1cf'}",
      "last_fix_commit":"a74768f8e4efbda4def9f16ee7e13cf3922ac5f7",
      "chain_ord_pos":1.0,
      "commit_datetime":"05\/06\/2021, 21:24:09",
      "message":"Prevent heap OOB error in `MaxPoolGrad`\n\nPiperOrigin-RevId: 372424854\nChange-Id: Idac0f23867ad8b0601cafbaaa52d5e64269e63a7",
      "author":"Mihai Maruseac",
      "comments":null,
      "stats":"{'additions': 3, 'deletions': 1, 'total': 4}",
      "files":"{'tensorflow\/core\/kernels\/maxpooling_op.cc': {'additions': 3, 'deletions': 1, 'changes': 4, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/a74768f8e4efbda4def9f16ee7e13cf3922ac5f7\/tensorflow%2Fcore%2Fkernels%2Fmaxpooling_op.cc', 'patch': '@@ -199,7 +199,9 @@ static void SpatialMaxPoolWithArgMaxHelper(\\n         \/\/ CHECK(input_backprop_index >= in_start && input_backprop_index <\\n         \/\/ in_end)\\n         FastBoundsCheck(input_backprop_index - in_start, in_end - in_start);\\n-        input_backprop_flat(input_backprop_index) += out_backprop_flat(index);\\n+        if (index < out_backprop.NumElements()) {\\n+          input_backprop_flat(input_backprop_index) += out_backprop_flat(index);\\n+        }\\n       }\\n     }\\n   };'}}",
      "message_norm":"prevent heap oob error in `maxpoolgrad`\n\npiperorigin-revid: 372424854\nchange-id: idac0f23867ad8b0601cafbaaa52d5e64269e63a7",
      "language":"nl",
      "entities":"[('prevent', 'ACTION', ''), ('heap oob', 'SECWORD', ''), ('error', 'FLAW', ''), ('372424854', 'SHA', 'generic_sha')]",
      "classification_level_1":null,
      "classification_level_2":null,
      "list_files":"dict_keys(['tensorflow\/core\/kernels\/maxpooling_op.cc'])",
      "num_files":1.0,
      "patch_content":"From a74768f8e4efbda4def9f16ee7e13cf3922ac5f7 Mon Sep 17 00:00:00 2001\nFrom: Mihai Maruseac <mihaimaruseac@google.com>\nDate: Thu, 6 May 2021 14:24:09 -0700\nSubject: [PATCH] Prevent heap OOB error in `MaxPoolGrad`\n\nPiperOrigin-RevId: 372424854\nChange-Id: Idac0f23867ad8b0601cafbaaa52d5e64269e63a7\n---\n tensorflow\/core\/kernels\/maxpooling_op.cc | 4 +++-\n 1 file changed, 3 insertions(+), 1 deletion(-)\n\ndiff --git a\/tensorflow\/core\/kernels\/maxpooling_op.cc b\/tensorflow\/core\/kernels\/maxpooling_op.cc\nindex ceb6694ed665d8..01f303eb26980f 100644\n--- a\/tensorflow\/core\/kernels\/maxpooling_op.cc\n+++ b\/tensorflow\/core\/kernels\/maxpooling_op.cc\n@@ -199,7 +199,9 @@ static void SpatialMaxPoolWithArgMaxHelper(\n         \/\/ CHECK(input_backprop_index >= in_start && input_backprop_index <\n         \/\/ in_end)\n         FastBoundsCheck(input_backprop_index - in_start, in_end - in_start);\n-        input_backprop_flat(input_backprop_index) += out_backprop_flat(index);\n+        if (index < out_backprop.NumElements()) {\n+          input_backprop_flat(input_backprop_index) += out_backprop_flat(index);\n+        }\n       }\n     }\n   };",
      "code_diff":"@@ -199,7 +199,9 @@ static void SpatialMaxPoolWithArgMaxHelper(\n         \/\/ CHECK(input_backprop_index >= in_start && input_backprop_index <\n         \/\/ in_end)\n         FastBoundsCheck(input_backprop_index - in_start, in_end - in_start);\n-        input_backprop_flat(input_backprop_index) += out_backprop_flat(index);\n+        if (index < out_backprop.NumElements()) {\n+          input_backprop_flat(input_backprop_index) += out_backprop_flat(index);\n+        }\n       }\n     }\n   };"
    },
    {
      "index":27,
      "vuln_id":"GHSA-cjc7-49v2-jp64",
      "cwe_id":"{'CWE-787', 'CWE-665'}",
      "score":5.3,
      "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/6fd02f44810754ae7481838b6a67c5df7f909ca3', 'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/41727ff06111117bdf86b37db198217fd7a143cc'}",
      "dataset":"osv",
      "summary":"Incomplete validation in `SparseAdd` ### Impact\nIncomplete validation in `SparseAdd` results in allowing attackers to exploit undefined behavior (dereferencing null pointers) as well as write outside of bounds of heap allocated data:\n\n```python\nimport tensorflow as tf\n\na_indices = tf.zeros([10, 97], dtype=tf.int64)\na_values = tf.zeros([10], dtype=tf.int64)\na_shape = tf.zeros([0], dtype=tf.int64)\n\nb_indices = tf.zeros([0, 0], dtype=tf.int64)\nb_values = tf.zeros([0], dtype=tf.int64)\nb_shape = tf.zeros([0], dtype=tf.int64)\n  \nthresh = 0\n\ntf.raw_ops.SparseAdd(a_indices=a_indices,\n                    a_values=a_values,\n                    a_shape=a_shape,\n                    b_indices=b_indices,\n                    b_values=b_values,\n                    b_shape=b_shape,\n                    thresh=thresh)\n```\n\nThe [implementation](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/656e7673b14acd7835dc778867f84916c6d1cac2\/tensorflow\/core\/kernels\/sparse_add_op.cc) has a large set of validation for the two sparse tensor inputs (6 tensors in total), but does not validate that the tensors are not empty or that the second dimension of `*_indices` matches the size of corresponding `*_shape`. This allows attackers to send tensor triples that represent invalid sparse tensors to abuse code assumptions that are not protected by validation.\n\n### Patches\nWe have patched the issue in GitHub commit [6fd02f44810754ae7481838b6a67c5df7f909ca3](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/6fd02f44810754ae7481838b6a67c5df7f909ca3) followed by GitHub commit  [41727ff06111117bdf86b37db198217fd7a143cc](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/41727ff06111117bdf86b37db198217fd7a143cc).\n\nThe fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by Yakun Zhang and Ying Wang of Baidu X-Team.",
      "published_date":"2021-05-21",
      "chain_len":2,
      "project":"https:\/\/github.com\/tensorflow\/tensorflow",
      "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/6fd02f44810754ae7481838b6a67c5df7f909ca3",
      "commit_sha":"6fd02f44810754ae7481838b6a67c5df7f909ca3",
      "patch":"MULTI",
      "chain_ord":"['6fd02f44810754ae7481838b6a67c5df7f909ca3', '41727ff06111117bdf86b37db198217fd7a143cc']",
      "before_first_fix_commit":"{'6f432d6334edc93fd5af0070170def56b0413e8a'}",
      "last_fix_commit":"41727ff06111117bdf86b37db198217fd7a143cc",
      "chain_ord_pos":1.0,
      "commit_datetime":"04\/26\/2021, 23:40:49",
      "message":"Fix `tf.raw_ops.SparseAdd ` invalid memory access failure.\n\nPiperOrigin-RevId: 370568774\nChange-Id: I5f73b31c865f2948a1c8dfb7ebd22b3cfb6405bf",
      "author":"Amit Patankar",
      "comments":null,
      "stats":"{'additions': 5, 'deletions': 0, 'total': 5}",
      "files":"{'tensorflow\/core\/kernels\/sparse_add_op.cc': {'additions': 5, 'deletions': 0, 'changes': 5, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/6fd02f44810754ae7481838b6a67c5df7f909ca3\/tensorflow%2Fcore%2Fkernels%2Fsparse_add_op.cc', 'patch': '@@ -14,6 +14,7 @@ limitations under the License.\\n ==============================================================================*\/\\n \\n #include \"tensorflow\/core\/framework\/op_kernel.h\"\\n+#include \"tensorflow\/core\/framework\/op_requires.h\"\\n #include \"tensorflow\/core\/framework\/register_types.h\"\\n #include \"tensorflow\/core\/framework\/tensor.h\"\\n #include \"tensorflow\/core\/framework\/tensor_util.h\"\\n@@ -101,6 +102,10 @@ class SparseAddOp : public OpKernel {\\n     std::vector<T> out_values;\\n     const int num_dims = a_shape->dim_size(0);\\n \\n+    OP_REQUIRES(ctx, num_dims > 0,\\n+                errors::InvalidArgument(\"Invalid input_a shape. Received: \",\\n+                                        a_shape->DebugString()));\\n+\\n     \/\/ The input and output sparse tensors are assumed to be ordered along\\n     \/\/ increasing dimension number.\\n     int64 i = 0, j = 0;'}}",
      "message_norm":"fix `tf.raw_ops.sparseadd ` invalid memory access failure.\n\npiperorigin-revid: 370568774\nchange-id: i5f73b31c865f2948a1c8dfb7ebd22b3cfb6405bf",
      "language":"en",
      "entities":"[('fix', 'ACTION', ''), ('invalid memory access', 'SECWORD', ''), ('370568774', 'SHA', 'generic_sha')]",
      "classification_level_1":null,
      "classification_level_2":null,
      "list_files":"dict_keys(['tensorflow\/core\/kernels\/sparse_add_op.cc'])",
      "num_files":1.0,
      "patch_content":"From 6fd02f44810754ae7481838b6a67c5df7f909ca3 Mon Sep 17 00:00:00 2001\nFrom: Amit Patankar <amitpatankar@google.com>\nDate: Mon, 26 Apr 2021 16:40:49 -0700\nSubject: [PATCH] Fix `tf.raw_ops.SparseAdd ` invalid memory access failure.\n\nPiperOrigin-RevId: 370568774\nChange-Id: I5f73b31c865f2948a1c8dfb7ebd22b3cfb6405bf\n---\n tensorflow\/core\/kernels\/sparse_add_op.cc | 5 +++++\n 1 file changed, 5 insertions(+)\n\ndiff --git a\/tensorflow\/core\/kernels\/sparse_add_op.cc b\/tensorflow\/core\/kernels\/sparse_add_op.cc\nindex 0cf40a709a39a7..346206365af8d5 100644\n--- a\/tensorflow\/core\/kernels\/sparse_add_op.cc\n+++ b\/tensorflow\/core\/kernels\/sparse_add_op.cc\n@@ -14,6 +14,7 @@ limitations under the License.\n ==============================================================================*\/\n \n #include \"tensorflow\/core\/framework\/op_kernel.h\"\n+#include \"tensorflow\/core\/framework\/op_requires.h\"\n #include \"tensorflow\/core\/framework\/register_types.h\"\n #include \"tensorflow\/core\/framework\/tensor.h\"\n #include \"tensorflow\/core\/framework\/tensor_util.h\"\n@@ -101,6 +102,10 @@ class SparseAddOp : public OpKernel {\n     std::vector<T> out_values;\n     const int num_dims = a_shape->dim_size(0);\n \n+    OP_REQUIRES(ctx, num_dims > 0,\n+                errors::InvalidArgument(\"Invalid input_a shape. Received: \",\n+                                        a_shape->DebugString()));\n+\n     \/\/ The input and output sparse tensors are assumed to be ordered along\n     \/\/ increasing dimension number.\n     int64 i = 0, j = 0;",
      "code_diff":"@@ -14,6 +14,7 @@ limitations under the License.\n ==============================================================================*\/\n \n #include \"tensorflow\/core\/framework\/op_kernel.h\"\n+#include \"tensorflow\/core\/framework\/op_requires.h\"\n #include \"tensorflow\/core\/framework\/register_types.h\"\n #include \"tensorflow\/core\/framework\/tensor.h\"\n #include \"tensorflow\/core\/framework\/tensor_util.h\"\n@@ -101,6 +102,10 @@ class SparseAddOp : public OpKernel {\n     std::vector<T> out_values;\n     const int num_dims = a_shape->dim_size(0);\n \n+    OP_REQUIRES(ctx, num_dims > 0,\n+                errors::InvalidArgument(\"Invalid input_a shape. Received: \",\n+                                        a_shape->DebugString()));\n+\n     \/\/ The input and output sparse tensors are assumed to be ordered along\n     \/\/ increasing dimension number.\n     int64 i = 0, j = 0;"
    },
    {
      "index":28,
      "vuln_id":"GHSA-5qw5-89mw-wcg2",
      "cwe_id":"{'CWE-787'}",
      "score":8.8,
      "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/97282c6d0d34476b6ba033f961590b783fa184cd'}",
      "dataset":"osv",
      "summary":"Out of bounds write in Tensorflow ### Impact\nTensorFlow is vulnerable to a heap OOB write in [Grappler](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/a1320ec1eac186da1d03f033109191f715b2b130\/tensorflow\/core\/grappler\/costs\/graph_properties.cc#L1132-L1141):\n\n```cc\nStatus SetUnknownShape(const NodeDef* node, int output_port) {\n  shape_inference::ShapeHandle shape = \n      GetUnknownOutputShape(node, output_port);\n  InferenceContext* ctx = GetContext(node);\n  if (ctx == nullptr) {\n    return errors::InvalidArgument(\"Missing context\");\n  }\n  ctx->set_output(output_port, shape);\n  return Status::OK();\n}\n```\n\nThe [`set_output`](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/a1320ec1eac186da1d03f033109191f715b2b130\/tensorflow\/core\/framework\/shape_inference.h#L394) function writes to an array at the specified index:\n\n```cc\nvoid set_output(int idx, ShapeHandle shape) { outputs_.at(idx) = shape; }\n```\n\nHence, this gives a malicious user a write primitive.\n\n### Patches\nWe have patched the issue in GitHub commit [97282c6d0d34476b6ba033f961590b783fa184cd](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/97282c6d0d34476b6ba033f961590b783fa184cd).\n\nThe fix will be included in TensorFlow 2.8.0. We will also cherrypick this commit on TensorFlow 2.7.1, TensorFlow 2.6.3, and TensorFlow 2.5.3, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.",
      "published_date":"2022-02-09",
      "chain_len":1,
      "project":"https:\/\/github.com\/tensorflow\/tensorflow",
      "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/97282c6d0d34476b6ba033f961590b783fa184cd",
      "commit_sha":"97282c6d0d34476b6ba033f961590b783fa184cd",
      "patch":"SINGLE",
      "chain_ord":"['97282c6d0d34476b6ba033f961590b783fa184cd']",
      "before_first_fix_commit":"{'557a09455bc98108bc29b3b78e818f9d7dca920f'}",
      "last_fix_commit":"97282c6d0d34476b6ba033f961590b783fa184cd",
      "chain_ord_pos":1.0,
      "commit_datetime":"11\/08\/2021, 13:48:40",
      "message":"Prevent a crash due to heap OOB write in grappler.\n\nPiperOrigin-RevId: 408318417\nChange-Id: If095feb8c001e3a8ac4a85b7387b81e8309df47d",
      "author":"Mihai Maruseac",
      "comments":null,
      "stats":"{'additions': 6, 'deletions': 1, 'total': 7}",
      "files":"{'tensorflow\/core\/grappler\/costs\/graph_properties.cc': {'additions': 6, 'deletions': 1, 'changes': 7, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/97282c6d0d34476b6ba033f961590b783fa184cd\/tensorflow%2Fcore%2Fgrappler%2Fcosts%2Fgraph_properties.cc', 'patch': '@@ -1134,7 +1134,12 @@ class SymbolicShapeRefiner {\\n         GetUnknownOutputShape(node, output_port);\\n     InferenceContext* ctx = GetContext(node);\\n     if (ctx == nullptr) {\\n-      return errors::InvalidArgument(\"Missing context\");\\n+      return errors::InvalidArgument(\"SetUnknownShape: Missing context\");\\n+    }\\n+    if (output_port < 0 || output_port >= ctx->num_outputs()) {\\n+      return errors::InvalidArgument(\\n+          \"SetUnknownShape: output_port must be in [0, \", ctx->num_outputs(),\\n+          \") but was \", output_port);\\n     }\\n     ctx->set_output(output_port, shape);\\n     return Status::OK();'}}",
      "message_norm":"prevent a crash due to heap oob write in grappler.\n\npiperorigin-revid: 408318417\nchange-id: if095feb8c001e3a8ac4a85b7387b81e8309df47d",
      "language":"en",
      "entities":"[('prevent', 'ACTION', ''), ('heap oob', 'SECWORD', ''), ('408318417', 'SHA', 'generic_sha')]",
      "classification_level_1":null,
      "classification_level_2":null,
      "list_files":"dict_keys(['tensorflow\/core\/grappler\/costs\/graph_properties.cc'])",
      "num_files":1.0,
      "patch_content":"From 97282c6d0d34476b6ba033f961590b783fa184cd Mon Sep 17 00:00:00 2001\nFrom: Mihai Maruseac <mihaimaruseac@google.com>\nDate: Mon, 8 Nov 2021 05:48:40 -0800\nSubject: [PATCH] Prevent a crash due to heap OOB write in grappler.\n\nPiperOrigin-RevId: 408318417\nChange-Id: If095feb8c001e3a8ac4a85b7387b81e8309df47d\n---\n tensorflow\/core\/grappler\/costs\/graph_properties.cc | 7 ++++++-\n 1 file changed, 6 insertions(+), 1 deletion(-)\n\ndiff --git a\/tensorflow\/core\/grappler\/costs\/graph_properties.cc b\/tensorflow\/core\/grappler\/costs\/graph_properties.cc\nindex 51a2cd080c5445..3cc12173ba10c5 100644\n--- a\/tensorflow\/core\/grappler\/costs\/graph_properties.cc\n+++ b\/tensorflow\/core\/grappler\/costs\/graph_properties.cc\n@@ -1134,7 +1134,12 @@ class SymbolicShapeRefiner {\n         GetUnknownOutputShape(node, output_port);\n     InferenceContext* ctx = GetContext(node);\n     if (ctx == nullptr) {\n-      return errors::InvalidArgument(\"Missing context\");\n+      return errors::InvalidArgument(\"SetUnknownShape: Missing context\");\n+    }\n+    if (output_port < 0 || output_port >= ctx->num_outputs()) {\n+      return errors::InvalidArgument(\n+          \"SetUnknownShape: output_port must be in [0, \", ctx->num_outputs(),\n+          \") but was \", output_port);\n     }\n     ctx->set_output(output_port, shape);\n     return Status::OK();",
      "code_diff":"@@ -1134,7 +1134,12 @@ class SymbolicShapeRefiner {\n         GetUnknownOutputShape(node, output_port);\n     InferenceContext* ctx = GetContext(node);\n     if (ctx == nullptr) {\n-      return errors::InvalidArgument(\"Missing context\");\n+      return errors::InvalidArgument(\"SetUnknownShape: Missing context\");\n+    }\n+    if (output_port < 0 || output_port >= ctx->num_outputs()) {\n+      return errors::InvalidArgument(\n+          \"SetUnknownShape: output_port must be in [0, \", ctx->num_outputs(),\n+          \") but was \", output_port);\n     }\n     ctx->set_output(output_port, shape);\n     return Status::OK();"
    },
    {
      "index":29,
      "vuln_id":"GHSA-w32p-76xr-88pc",
      "cwe_id":"{'CWE-787'}",
      "score":7.5,
      "chain":"{'https:\/\/github.com\/chakra-core\/ChakraCore\/commit\/90f67afac6362828c750f3bccbcc1c360caf29e4', 'https:\/\/github.com\/chakra-core\/ChakraCore\/commit\/3d6226cc2d1077537220361c82e34a362c6c76ee'}",
      "dataset":"osv",
      "summary":"Out-of-bounds write A remote code execution vulnerability exists in the way that the Chakra scripting engine handles objects in memory in Microsoft Edge, aka 'Chakra Scripting Engine Memory Corruption Vulnerability'. This CVE ID is unique from CVE-2019-0989, CVE-2019-0991, CVE-2019-0992, CVE-2019-0993, CVE-2019-1002, CVE-2019-1024, CVE-2019-1051, CVE-2019-1052.",
      "published_date":"2021-03-29",
      "chain_len":2,
      "project":"https:\/\/github.com\/chakra-core\/ChakraCore",
      "commit_href":"https:\/\/github.com\/chakra-core\/ChakraCore\/commit\/90f67afac6362828c750f3bccbcc1c360caf29e4",
      "commit_sha":"90f67afac6362828c750f3bccbcc1c360caf29e4",
      "patch":"MULTI",
      "chain_ord":"['90f67afac6362828c750f3bccbcc1c360caf29e4', '3d6226cc2d1077537220361c82e34a362c6c76ee']",
      "before_first_fix_commit":"{'d797e3f00e34c12c8c0ae52f56344325439dccd7', 'eabf77ad17010f220639e5261798da9ac14e43e3'}",
      "last_fix_commit":"3d6226cc2d1077537220361c82e34a362c6c76ee",
      "chain_ord_pos":1.0,
      "commit_datetime":"05\/15\/2019, 22:37:38",
      "message":"CVE-2019-1003",
      "author":"Paul Leathers",
      "comments":null,
      "stats":"{'additions': 5, 'deletions': 0, 'total': 5}",
      "files":"{'lib\/Runtime\/Library\/JavascriptProxy.cpp': {'additions': 5, 'deletions': 0, 'changes': 5, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/chakra-core\/ChakraCore\/raw\/90f67afac6362828c750f3bccbcc1c360caf29e4\/lib%2FRuntime%2FLibrary%2FJavascriptProxy.cpp', 'patch': '@@ -392,6 +392,8 @@ namespace Js\\n         }\\n         propertyDescriptor->SetValue(getGetResult);\\n \\n+        threadContext->AddImplicitCallFlags(Js::ImplicitCall_External);\\n+\\n         return TRUE;\\n     }\\n \\n@@ -1907,6 +1909,9 @@ namespace Js\\n                 }\\n             }\\n         }\\n+\\n+        threadContext->AddImplicitCallFlags(Js::ImplicitCall_External);\\n+\\n         return TRUE;\\n \\n     }'}}",
      "message_norm":"cve-2019-1003",
      "language":"ro",
      "entities":"[('cve-2019-1003', 'VULNID', 'CVE')]",
      "classification_level_1":null,
      "classification_level_2":null,
      "list_files":"dict_keys(['lib\/Runtime\/Library\/JavascriptProxy.cpp'])",
      "num_files":1.0,
      "patch_content":"From 90f67afac6362828c750f3bccbcc1c360caf29e4 Mon Sep 17 00:00:00 2001\nFrom: Paul Leathers <pleath@microsoft.com>\nDate: Wed, 15 May 2019 15:37:38 -0700\nSubject: [PATCH] CVE-2019-1003\n\n---\n lib\/Runtime\/Library\/JavascriptProxy.cpp | 5 +++++\n 1 file changed, 5 insertions(+)\n\ndiff --git a\/lib\/Runtime\/Library\/JavascriptProxy.cpp b\/lib\/Runtime\/Library\/JavascriptProxy.cpp\nindex 6e026109735..ed817269a82 100644\n--- a\/lib\/Runtime\/Library\/JavascriptProxy.cpp\n+++ b\/lib\/Runtime\/Library\/JavascriptProxy.cpp\n@@ -392,6 +392,8 @@ namespace Js\n         }\n         propertyDescriptor->SetValue(getGetResult);\n \n+        threadContext->AddImplicitCallFlags(Js::ImplicitCall_External);\n+\n         return TRUE;\n     }\n \n@@ -1907,6 +1909,9 @@ namespace Js\n                 }\n             }\n         }\n+\n+        threadContext->AddImplicitCallFlags(Js::ImplicitCall_External);\n+\n         return TRUE;\n \n     }",
      "code_diff":"@@ -392,6 +392,8 @@ namespace Js\n         }\n         propertyDescriptor->SetValue(getGetResult);\n \n+        threadContext->AddImplicitCallFlags(Js::ImplicitCall_External);\n+\n         return TRUE;\n     }\n \n@@ -1907,6 +1909,9 @@ namespace Js\n                 }\n             }\n         }\n+\n+        threadContext->AddImplicitCallFlags(Js::ImplicitCall_External);\n+\n         return TRUE;\n \n     }"
    },
    {
      "index":30,
      "vuln_id":"GHSA-6j89-jhpr-849f",
      "cwe_id":"{'CWE-787'}",
      "score":7.5,
      "chain":"{'https:\/\/github.com\/chakra-core\/ChakraCore\/commit\/3fe5e24694729966a157dc613f5586a6c75f1e9b', 'https:\/\/github.com\/chakra-core\/ChakraCore\/commit\/cc871514deeaeaedb5b757c2ca8cd4ab9abccb5d'}",
      "dataset":"osv",
      "summary":"Out-of-bounds write A remote code execution vulnerability exists in the way that the Chakra scripting engine handles objects in memory in Microsoft Edge, aka 'Chakra Scripting Engine Memory Corruption Vulnerability'. This CVE ID is unique from CVE-2019-1308, CVE-2019-1335, CVE-2019-1366.",
      "published_date":"2021-03-29",
      "chain_len":2,
      "project":"https:\/\/github.com\/chakra-core\/ChakraCore",
      "commit_href":"https:\/\/github.com\/chakra-core\/ChakraCore\/commit\/3fe5e24694729966a157dc613f5586a6c75f1e9b",
      "commit_sha":"3fe5e24694729966a157dc613f5586a6c75f1e9b",
      "patch":"MULTI",
      "chain_ord":"['3fe5e24694729966a157dc613f5586a6c75f1e9b', 'cc871514deeaeaedb5b757c2ca8cd4ab9abccb5d']",
      "before_first_fix_commit":"{'7e9a2ee60baa95ceb4f48f522f823c812ca90c80', '5989c6e038d80f92dcd8e10d725cdf45396201bb'}",
      "last_fix_commit":"cc871514deeaeaedb5b757c2ca8cd4ab9abccb5d",
      "chain_ord_pos":1.0,
      "commit_datetime":"09\/03\/2019, 19:26:32",
      "message":"CVE-2019-1307",
      "author":"Paul Leathers",
      "comments":null,
      "stats":"{'additions': 1, 'deletions': 0, 'total': 1}",
      "files":"{'lib\/Backend\/GlobOpt.h': {'additions': 1, 'deletions': 0, 'changes': 1, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/chakra-core\/ChakraCore\/raw\/3fe5e24694729966a157dc613f5586a6c75f1e9b\/lib%2FBackend%2FGlobOpt.h', 'patch': '@@ -370,6 +370,7 @@ class JsArrayKills\\n             (valueType.IsArrayOrObjectWithArray() &&\\n              (\\n               (killsArraysWithNoMissingValues && valueType.HasNoMissingValues()) ||\\n+              (killsObjectArraysWithNoMissingValues && !valueType.IsArray() && valueType.HasNoMissingValues()) ||\\n               (killsNativeArrays && !valueType.HasVarElements())\\n              )\\n             );'}}",
      "message_norm":"cve-2019-1307",
      "language":"ro",
      "entities":"[('cve-2019-1307', 'VULNID', 'CVE')]",
      "classification_level_1":null,
      "classification_level_2":null,
      "list_files":"dict_keys(['lib\/Backend\/GlobOpt.h'])",
      "num_files":1.0,
      "patch_content":"From 3fe5e24694729966a157dc613f5586a6c75f1e9b Mon Sep 17 00:00:00 2001\nFrom: Paul Leathers <pleath@microsoft.com>\nDate: Tue, 3 Sep 2019 12:26:32 -0700\nSubject: [PATCH] CVE-2019-1307\n\n---\n lib\/Backend\/GlobOpt.h | 1 +\n 1 file changed, 1 insertion(+)\n\ndiff --git a\/lib\/Backend\/GlobOpt.h b\/lib\/Backend\/GlobOpt.h\nindex e141d2c84db..725ac0ea975 100644\n--- a\/lib\/Backend\/GlobOpt.h\n+++ b\/lib\/Backend\/GlobOpt.h\n@@ -370,6 +370,7 @@ class JsArrayKills\n             (valueType.IsArrayOrObjectWithArray() &&\n              (\n               (killsArraysWithNoMissingValues && valueType.HasNoMissingValues()) ||\n+              (killsObjectArraysWithNoMissingValues && !valueType.IsArray() && valueType.HasNoMissingValues()) ||\n               (killsNativeArrays && !valueType.HasVarElements())\n              )\n             );",
      "code_diff":"@@ -370,6 +370,7 @@ class JsArrayKills\n             (valueType.IsArrayOrObjectWithArray() &&\n              (\n               (killsArraysWithNoMissingValues && valueType.HasNoMissingValues()) ||\n+              (killsObjectArraysWithNoMissingValues && !valueType.IsArray() && valueType.HasNoMissingValues()) ||\n               (killsNativeArrays && !valueType.HasVarElements())\n              )\n             );"
    },
    {
      "index":31,
      "vuln_id":"GHSA-mw7r-3g6w-85qg",
      "cwe_id":"{'CWE-787'}",
      "score":7.5,
      "chain":"{'https:\/\/github.com\/chakra-core\/ChakraCore\/commit\/6b1250b6ffea7006226dd937e52cf5b353fcfc15', 'https:\/\/github.com\/chakra-core\/ChakraCore\/commit\/242c59ea40c0428e1ced7366bf2c28bfbdda1999'}",
      "dataset":"osv",
      "summary":"Out-of-bounds write A remote code execution vulnerability exists in the way that the Chakra scripting engine handles objects in memory in Microsoft Edge, aka 'Chakra Scripting Engine Memory Corruption Vulnerability'. This CVE ID is unique from CVE-2019-1139, CVE-2019-1140, CVE-2019-1141, CVE-2019-1195, CVE-2019-1196, CVE-2019-1197.",
      "published_date":"2021-03-29",
      "chain_len":2,
      "project":"https:\/\/github.com\/chakra-core\/ChakraCore",
      "commit_href":"https:\/\/github.com\/chakra-core\/ChakraCore\/commit\/242c59ea40c0428e1ced7366bf2c28bfbdda1999",
      "commit_sha":"242c59ea40c0428e1ced7366bf2c28bfbdda1999",
      "patch":"MULTI",
      "chain_ord":"['242c59ea40c0428e1ced7366bf2c28bfbdda1999', '6b1250b6ffea7006226dd937e52cf5b353fcfc15']",
      "before_first_fix_commit":"{'75162b7f2d8ac2b37d17564e9c979ba1bae707e8', '450a349fda1b153d758a9e01698b977e60870e4c'}",
      "last_fix_commit":"6b1250b6ffea7006226dd937e52cf5b353fcfc15",
      "chain_ord_pos":1.0,
      "commit_datetime":"07\/02\/2019, 20:12:05",
      "message":"[CVE-2019-1131] Chakra Type confusion",
      "author":"Taylor Woll",
      "comments":null,
      "stats":"{'additions': 1, 'deletions': 4, 'total': 5}",
      "files":"{'lib\/Parser\/Parse.cpp': {'additions': 1, 'deletions': 4, 'changes': 5, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/chakra-core\/ChakraCore\/raw\/242c59ea40c0428e1ced7366bf2c28bfbdda1999\/lib%2FParser%2FParse.cpp', 'patch': '@@ -4664,10 +4664,7 @@ ParseNodePtr Parser::ParseMemberList(LPCOLESTR pNameHint, uint32* pNameHintLengt\\n                     }\\n                 }\\n \\n-                if (buildAST)\\n-                {\\n-                    CheckArgumentsUse(pidHint, GetCurrentFunctionNode());\\n-                }\\n+                CheckArgumentsUse(pidHint, GetCurrentFunctionNode());\\n \\n                 bool couldBeObjectPattern = !isObjectPattern && m_token.tk == tkAsg;\\n                 \/\/ Saving the current state as we may change the isObjectPattern down below.'}}",
      "message_norm":"[cve-2019-1131] chakra type confusion",
      "language":"en",
      "entities":"[('cve-2019-1131', 'VULNID', 'CVE'), ('type confusion', 'SECWORD', '')]",
      "classification_level_1":null,
      "classification_level_2":null,
      "list_files":"dict_keys(['lib\/Parser\/Parse.cpp'])",
      "num_files":1.0,
      "patch_content":"From 242c59ea40c0428e1ced7366bf2c28bfbdda1999 Mon Sep 17 00:00:00 2001\nFrom: Taylor Woll <tawoll@ntdev.microsoft.com>\nDate: Tue, 2 Jul 2019 13:12:05 -0700\nSubject: [PATCH] [CVE-2019-1131] Chakra Type confusion\n\n---\n lib\/Parser\/Parse.cpp | 5 +----\n 1 file changed, 1 insertion(+), 4 deletions(-)\n\ndiff --git a\/lib\/Parser\/Parse.cpp b\/lib\/Parser\/Parse.cpp\nindex b49d0feec31..518457b8e6d 100644\n--- a\/lib\/Parser\/Parse.cpp\n+++ b\/lib\/Parser\/Parse.cpp\n@@ -4664,10 +4664,7 @@ ParseNodePtr Parser::ParseMemberList(LPCOLESTR pNameHint, uint32* pNameHintLengt\n                     }\n                 }\n \n-                if (buildAST)\n-                {\n-                    CheckArgumentsUse(pidHint, GetCurrentFunctionNode());\n-                }\n+                CheckArgumentsUse(pidHint, GetCurrentFunctionNode());\n \n                 bool couldBeObjectPattern = !isObjectPattern && m_token.tk == tkAsg;\n                 \/\/ Saving the current state as we may change the isObjectPattern down below.",
      "code_diff":"@@ -4664,10 +4664,7 @@ ParseNodePtr Parser::ParseMemberList(LPCOLESTR pNameHint, uint32* pNameHintLengt\n                     }\n                 }\n \n-                if (buildAST)\n-                {\n-                    CheckArgumentsUse(pidHint, GetCurrentFunctionNode());\n-                }\n+                CheckArgumentsUse(pidHint, GetCurrentFunctionNode());\n \n                 bool couldBeObjectPattern = !isObjectPattern && m_token.tk == tkAsg;\n                 \/\/ Saving the current state as we may change the isObjectPattern down below."
    },
    {
      "index":32,
      "vuln_id":"GHSA-9g8h-pjm4-q92p",
      "cwe_id":"{'CWE-787'}",
      "score":5.5,
      "chain":"{'https:\/\/github.com\/opencv\/opencv\/pull\/10566\/commits\/435a3e337bd9d4e11af61cf8b8afca067bf1a8aa'}",
      "dataset":"osv",
      "summary":"Out-of-bounds Write in OpenCV. In OpenCV 3.3.1 (corresponding with OpenCV-Python 3.3.1.11), a heap-based buffer overflow happens in cv::Jpeg2KDecoder::readComponent8u in modules\/imgcodecs\/src\/grfmt_jpeg2000.cpp when parsing a crafted image file.",
      "published_date":"2021-10-12",
      "chain_len":1,
      "project":"https:\/\/github.com\/opencv\/opencv",
      "commit_href":"https:\/\/github.com\/opencv\/opencv\/pull\/10566\/commits\/435a3e337bd9d4e11af61cf8b8afca067bf1a8aa",
      "commit_sha":"435a3e337bd9d4e11af61cf8b8afca067bf1a8aa",
      "patch":"SINGLE",
      "chain_ord":"['435a3e337bd9d4e11af61cf8b8afca067bf1a8aa']",
      "before_first_fix_commit":"{'f34a0a874a029a6201df0acbf46eeeaab8686e4d'}",
      "last_fix_commit":"435a3e337bd9d4e11af61cf8b8afca067bf1a8aa",
      "chain_ord_pos":1.0,
      "commit_datetime":"01\/09\/2018, 14:36:57",
      "message":"imgcodecs: add more Jasper checks for supported and tested cases",
      "author":"Alexander Alekhin",
      "comments":null,
      "stats":"{'additions': 39, 'deletions': 7, 'total': 46}",
      "files":"{'modules\/imgcodecs\/src\/grfmt_jpeg2000.cpp': {'additions': 39, 'deletions': 7, 'changes': 46, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/opencv\/opencv\/raw\/435a3e337bd9d4e11af61cf8b8afca067bf1a8aa\/modules%2Fimgcodecs%2Fsrc%2Fgrfmt_jpeg2000.cpp', 'patch': '@@ -77,7 +77,8 @@ static JasperInitializer initialize_jasper;\\n \\n Jpeg2KDecoder::Jpeg2KDecoder()\\n {\\n-    m_signature = \\'\\\\0\\' + String() + \\'\\\\0\\' + String() + \\'\\\\0\\' + String(\"\\\\x0cjP  \\\\r\\\\n\\\\x87\\\\n\");\\n+    static const unsigned char signature_[12] = { 0, 0, 0, 0x0c, \\'j\\', \\'P\\', \\' \\', \\' \\', 13, 10, 0x87, 10};\\n+    m_signature = String((const char*)signature_, (const char*)signature_ + sizeof(signature_));\\n     m_stream = 0;\\n     m_image = 0;\\n }\\n@@ -121,6 +122,8 @@ bool  Jpeg2KDecoder::readHeader()\\n         jas_image_t* image = jas_image_decode( stream, -1, 0 );\\n         m_image = image;\\n         if( image ) {\\n+            CV_Assert(0 == (jas_image_tlx(image)) && \"not supported\");\\n+            CV_Assert(0 == (jas_image_tly(image)) && \"not supported\");\\n             m_width = jas_image_width( image );\\n             m_height = jas_image_height( image );\\n \\n@@ -130,14 +133,31 @@ bool  Jpeg2KDecoder::readHeader()\\n             for( int i = 0; i < numcmpts; i++ )\\n             {\\n                 int depth_i = jas_image_cmptprec( image, i );\\n+                CV_Assert(depth == 0 || depth == depth_i); \/\/ component data type mismatch\\n                 depth = MAX(depth, depth_i);\\n                 if( jas_image_cmpttype( image, i ) > 2 )\\n                     continue;\\n+                int sgnd = jas_image_cmptsgnd(image, i);\\n+                int xstart = jas_image_cmpttlx(image, i);\\n+                int xend = jas_image_cmptbrx(image, i);\\n+                int xstep = jas_image_cmpthstep(image, i);\\n+                int ystart = jas_image_cmpttly(image, i);\\n+                int yend = jas_image_cmptbry(image, i);\\n+                int ystep = jas_image_cmptvstep(image, i);\\n+                CV_Assert(sgnd == 0 && \"not supported\");\\n+                CV_Assert(xstart == 0 && \"not supported\");\\n+                CV_Assert(ystart == 0 && \"not supported\");\\n+                CV_Assert(xstep == 1 && \"not supported\");\\n+                CV_Assert(ystep == 1 && \"not supported\");\\n+                CV_Assert(xend == m_width);\\n+                CV_Assert(yend == m_height);\\n                 cntcmpts++;\\n             }\\n \\n             if( cntcmpts )\\n             {\\n+                CV_Assert(depth == 8 || depth == 16);\\n+                CV_Assert(cntcmpts == 1 || cntcmpts == 3);\\n                 m_type = CV_MAKETYPE(depth <= 8 ? CV_8U : CV_16U, cntcmpts > 1 ? 3 : 1);\\n                 result = true;\\n             }\\n@@ -150,9 +170,14 @@ bool  Jpeg2KDecoder::readHeader()\\n     return result;\\n }\\n \\n+static void Jpeg2KDecoder_close(Jpeg2KDecoder* ptr)\\n+{\\n+    ptr->close();\\n+}\\n \\n bool  Jpeg2KDecoder::readData( Mat& img )\\n {\\n+    Ptr<Jpeg2KDecoder> close_this(this, Jpeg2KDecoder_close);\\n     bool result = false;\\n     int color = img.channels() > 1;\\n     uchar* data = img.ptr();\\n@@ -204,11 +229,16 @@ bool  Jpeg2KDecoder::readData( Mat& img )\\n                     result = true;\\n                 }\\n                 else\\n-                    fprintf(stderr, \"JPEG 2000 LOADER ERROR: cannot convert colorspace\\\\n\");\\n+                {\\n+                    jas_cmprof_destroy(clrprof);\\n+                    CV_Error(Error::StsError, \"JPEG 2000 LOADER ERROR: cannot convert colorspace\");\\n+                }\\n                 jas_cmprof_destroy( clrprof );\\n             }\\n             else\\n-                fprintf(stderr, \"JPEG 2000 LOADER ERROR: unable to create colorspace\\\\n\");\\n+            {\\n+                CV_Error(Error::StsError, \"JPEG 2000 LOADER ERROR: unable to create colorspace\");\\n+            }\\n         }\\n         else\\n             result = true;\\n@@ -257,8 +287,8 @@ bool  Jpeg2KDecoder::readData( Mat& img )\\n                                 result = readComponent16u( ((unsigned short *)data) + i, buffer, validateToInt(step \/ 2), cmptlut[i], maxval, offset, ncmpts );\\n                             if( !result )\\n                             {\\n-                                i = ncmpts;\\n-                                result = false;\\n+                                jas_matrix_destroy( buffer );\\n+                                CV_Error(Error::StsError, \"JPEG2000 LOADER ERROR: failed to read component\");\\n                             }\\n                         }\\n                         jas_matrix_destroy( buffer );\\n@@ -267,10 +297,12 @@ bool  Jpeg2KDecoder::readData( Mat& img )\\n             }\\n         }\\n         else\\n-            fprintf(stderr, \"JPEG2000 LOADER ERROR: colorspace conversion failed\\\\n\" );\\n+        {\\n+            CV_Error(Error::StsError, \"JPEG2000 LOADER ERROR: colorspace conversion failed\");\\n+        }\\n     }\\n \\n-    close();\\n+    CV_Assert(result == true);\\n \\n #ifndef _WIN32\\n     if (!clr.empty())'}}",
      "message_norm":"imgcodecs: add more jasper checks for supported and tested cases",
      "language":"en",
      "entities":"[('add', 'ACTION', '')]",
      "classification_level_1":null,
      "classification_level_2":null,
      "list_files":"dict_keys(['modules\/imgcodecs\/src\/grfmt_jpeg2000.cpp'])",
      "num_files":1.0,
      "patch_content":"From 435a3e337bd9d4e11af61cf8b8afca067bf1a8aa Mon Sep 17 00:00:00 2001\nFrom: Alexander Alekhin <alexander.alekhin@intel.com>\nDate: Tue, 9 Jan 2018 17:36:57 +0300\nSubject: [PATCH] imgcodecs: add more Jasper checks for supported and tested\n cases\n\n---\n modules\/imgcodecs\/src\/grfmt_jpeg2000.cpp | 46 ++++++++++++++++++++----\n 1 file changed, 39 insertions(+), 7 deletions(-)\n\ndiff --git a\/modules\/imgcodecs\/src\/grfmt_jpeg2000.cpp b\/modules\/imgcodecs\/src\/grfmt_jpeg2000.cpp\nindex 24dfb38bb9d6..b8b70fee85b8 100644\n--- a\/modules\/imgcodecs\/src\/grfmt_jpeg2000.cpp\n+++ b\/modules\/imgcodecs\/src\/grfmt_jpeg2000.cpp\n@@ -77,7 +77,8 @@ static JasperInitializer initialize_jasper;\n \n Jpeg2KDecoder::Jpeg2KDecoder()\n {\n-    m_signature = '\\0' + String() + '\\0' + String() + '\\0' + String(\"\\x0cjP  \\r\\n\\x87\\n\");\n+    static const unsigned char signature_[12] = { 0, 0, 0, 0x0c, 'j', 'P', ' ', ' ', 13, 10, 0x87, 10};\n+    m_signature = String((const char*)signature_, (const char*)signature_ + sizeof(signature_));\n     m_stream = 0;\n     m_image = 0;\n }\n@@ -121,6 +122,8 @@ bool  Jpeg2KDecoder::readHeader()\n         jas_image_t* image = jas_image_decode( stream, -1, 0 );\n         m_image = image;\n         if( image ) {\n+            CV_Assert(0 == (jas_image_tlx(image)) && \"not supported\");\n+            CV_Assert(0 == (jas_image_tly(image)) && \"not supported\");\n             m_width = jas_image_width( image );\n             m_height = jas_image_height( image );\n \n@@ -130,14 +133,31 @@ bool  Jpeg2KDecoder::readHeader()\n             for( int i = 0; i < numcmpts; i++ )\n             {\n                 int depth_i = jas_image_cmptprec( image, i );\n+                CV_Assert(depth == 0 || depth == depth_i); \/\/ component data type mismatch\n                 depth = MAX(depth, depth_i);\n                 if( jas_image_cmpttype( image, i ) > 2 )\n                     continue;\n+                int sgnd = jas_image_cmptsgnd(image, i);\n+                int xstart = jas_image_cmpttlx(image, i);\n+                int xend = jas_image_cmptbrx(image, i);\n+                int xstep = jas_image_cmpthstep(image, i);\n+                int ystart = jas_image_cmpttly(image, i);\n+                int yend = jas_image_cmptbry(image, i);\n+                int ystep = jas_image_cmptvstep(image, i);\n+                CV_Assert(sgnd == 0 && \"not supported\");\n+                CV_Assert(xstart == 0 && \"not supported\");\n+                CV_Assert(ystart == 0 && \"not supported\");\n+                CV_Assert(xstep == 1 && \"not supported\");\n+                CV_Assert(ystep == 1 && \"not supported\");\n+                CV_Assert(xend == m_width);\n+                CV_Assert(yend == m_height);\n                 cntcmpts++;\n             }\n \n             if( cntcmpts )\n             {\n+                CV_Assert(depth == 8 || depth == 16);\n+                CV_Assert(cntcmpts == 1 || cntcmpts == 3);\n                 m_type = CV_MAKETYPE(depth <= 8 ? CV_8U : CV_16U, cntcmpts > 1 ? 3 : 1);\n                 result = true;\n             }\n@@ -150,9 +170,14 @@ bool  Jpeg2KDecoder::readHeader()\n     return result;\n }\n \n+static void Jpeg2KDecoder_close(Jpeg2KDecoder* ptr)\n+{\n+    ptr->close();\n+}\n \n bool  Jpeg2KDecoder::readData( Mat& img )\n {\n+    Ptr<Jpeg2KDecoder> close_this(this, Jpeg2KDecoder_close);\n     bool result = false;\n     int color = img.channels() > 1;\n     uchar* data = img.ptr();\n@@ -204,11 +229,16 @@ bool  Jpeg2KDecoder::readData( Mat& img )\n                     result = true;\n                 }\n                 else\n-                    fprintf(stderr, \"JPEG 2000 LOADER ERROR: cannot convert colorspace\\n\");\n+                {\n+                    jas_cmprof_destroy(clrprof);\n+                    CV_Error(Error::StsError, \"JPEG 2000 LOADER ERROR: cannot convert colorspace\");\n+                }\n                 jas_cmprof_destroy( clrprof );\n             }\n             else\n-                fprintf(stderr, \"JPEG 2000 LOADER ERROR: unable to create colorspace\\n\");\n+            {\n+                CV_Error(Error::StsError, \"JPEG 2000 LOADER ERROR: unable to create colorspace\");\n+            }\n         }\n         else\n             result = true;\n@@ -257,8 +287,8 @@ bool  Jpeg2KDecoder::readData( Mat& img )\n                                 result = readComponent16u( ((unsigned short *)data) + i, buffer, validateToInt(step \/ 2), cmptlut[i], maxval, offset, ncmpts );\n                             if( !result )\n                             {\n-                                i = ncmpts;\n-                                result = false;\n+                                jas_matrix_destroy( buffer );\n+                                CV_Error(Error::StsError, \"JPEG2000 LOADER ERROR: failed to read component\");\n                             }\n                         }\n                         jas_matrix_destroy( buffer );\n@@ -267,10 +297,12 @@ bool  Jpeg2KDecoder::readData( Mat& img )\n             }\n         }\n         else\n-            fprintf(stderr, \"JPEG2000 LOADER ERROR: colorspace conversion failed\\n\" );\n+        {\n+            CV_Error(Error::StsError, \"JPEG2000 LOADER ERROR: colorspace conversion failed\");\n+        }\n     }\n \n-    close();\n+    CV_Assert(result == true);\n \n #ifndef _WIN32\n     if (!clr.empty())",
      "code_diff":"@@ -77,7 +77,8 @@ static JasperInitializer initialize_jasper;\n \n Jpeg2KDecoder::Jpeg2KDecoder()\n {\n-    m_signature = '\\0' + String() + '\\0' + String() + '\\0' + String(\"\\x0cjP  \\r\\n\\x87\\n\");\n+    static const unsigned char signature_[12] = { 0, 0, 0, 0x0c, 'j', 'P', ' ', ' ', 13, 10, 0x87, 10};\n+    m_signature = String((const char*)signature_, (const char*)signature_ + sizeof(signature_));\n     m_stream = 0;\n     m_image = 0;\n }\n@@ -121,6 +122,8 @@ bool  Jpeg2KDecoder::readHeader()\n         jas_image_t* image = jas_image_decode( stream, -1, 0 );\n         m_image = image;\n         if( image ) {\n+            CV_Assert(0 == (jas_image_tlx(image)) && \"not supported\");\n+            CV_Assert(0 == (jas_image_tly(image)) && \"not supported\");\n             m_width = jas_image_width( image );\n             m_height = jas_image_height( image );\n \n@@ -130,14 +133,31 @@ bool  Jpeg2KDecoder::readHeader()\n             for( int i = 0; i < numcmpts; i++ )\n             {\n                 int depth_i = jas_image_cmptprec( image, i );\n+                CV_Assert(depth == 0 || depth == depth_i); \/\/ component data type mismatch\n                 depth = MAX(depth, depth_i);\n                 if( jas_image_cmpttype( image, i ) > 2 )\n                     continue;\n+                int sgnd = jas_image_cmptsgnd(image, i);\n+                int xstart = jas_image_cmpttlx(image, i);\n+                int xend = jas_image_cmptbrx(image, i);\n+                int xstep = jas_image_cmpthstep(image, i);\n+                int ystart = jas_image_cmpttly(image, i);\n+                int yend = jas_image_cmptbry(image, i);\n+                int ystep = jas_image_cmptvstep(image, i);\n+                CV_Assert(sgnd == 0 && \"not supported\");\n+                CV_Assert(xstart == 0 && \"not supported\");\n+                CV_Assert(ystart == 0 && \"not supported\");\n+                CV_Assert(xstep == 1 && \"not supported\");\n+                CV_Assert(ystep == 1 && \"not supported\");\n+                CV_Assert(xend == m_width);\n+                CV_Assert(yend == m_height);\n                 cntcmpts++;\n             }\n \n             if( cntcmpts )\n             {\n+                CV_Assert(depth == 8 || depth == 16);\n+                CV_Assert(cntcmpts == 1 || cntcmpts == 3);\n                 m_type = CV_MAKETYPE(depth <= 8 ? CV_8U : CV_16U, cntcmpts > 1 ? 3 : 1);\n                 result = true;\n             }\n@@ -150,9 +170,14 @@ bool  Jpeg2KDecoder::readHeader()\n     return result;\n }\n \n+static void Jpeg2KDecoder_close(Jpeg2KDecoder* ptr)\n+{\n+    ptr->close();\n+}\n \n bool  Jpeg2KDecoder::readData( Mat& img )\n {\n+    Ptr<Jpeg2KDecoder> close_this(this, Jpeg2KDecoder_close);\n     bool result = false;\n     int color = img.channels() > 1;\n     uchar* data = img.ptr();\n@@ -204,11 +229,16 @@ bool  Jpeg2KDecoder::readData( Mat& img )\n                     result = true;\n                 }\n                 else\n-                    fprintf(stderr, \"JPEG 2000 LOADER ERROR: cannot convert colorspace\\n\");\n+                {\n+                    jas_cmprof_destroy(clrprof);\n+                    CV_Error(Error::StsError, \"JPEG 2000 LOADER ERROR: cannot convert colorspace\");\n+                }\n                 jas_cmprof_destroy( clrprof );\n             }\n             else\n-                fprintf(stderr, \"JPEG 2000 LOADER ERROR: unable to create colorspace\\n\");\n+            {\n+                CV_Error(Error::StsError, \"JPEG 2000 LOADER ERROR: unable to create colorspace\");\n+            }\n         }\n         else\n             result = true;\n@@ -257,8 +287,8 @@ bool  Jpeg2KDecoder::readData( Mat& img )\n                                 result = readComponent16u( ((unsigned short *)data) + i, buffer, validateToInt(step \/ 2), cmptlut[i], maxval, offset, ncmpts );\n                             if( !result )\n                             {\n-                                i = ncmpts;\n-                                result = false;\n+                                jas_matrix_destroy( buffer );\n+                                CV_Error(Error::StsError, \"JPEG2000 LOADER ERROR: failed to read component\");\n                             }\n                         }\n                         jas_matrix_destroy( buffer );\n@@ -267,10 +297,12 @@ bool  Jpeg2KDecoder::readData( Mat& img )\n             }\n         }\n         else\n-            fprintf(stderr, \"JPEG2000 LOADER ERROR: colorspace conversion failed\\n\" );\n+        {\n+            CV_Error(Error::StsError, \"JPEG2000 LOADER ERROR: colorspace conversion failed\");\n+        }\n     }\n \n-    close();\n+    CV_Assert(result == true);\n \n #ifndef _WIN32\n     if (!clr.empty())"
    },
    {
      "index":33,
      "vuln_id":"GHSA-88cw-3m6x-49f7",
      "cwe_id":"{'CWE-787'}",
      "score":7.5,
      "chain":"{'https:\/\/github.com\/chakra-core\/ChakraCore\/pull\/6528\/commits\/e81e8a51ec7ba3d0dfb6089254f166c2733216e1'}",
      "dataset":"osv",
      "summary":"Out-of-bounds Write in ChakraCore Chakra Scripting Engine Memory Corruption Vulnerability This CVE ID is unique from CVE-2020-17048.",
      "published_date":"2021-08-02",
      "chain_len":1,
      "project":"https:\/\/github.com\/chakra-core\/ChakraCore",
      "commit_href":"https:\/\/github.com\/chakra-core\/ChakraCore\/pull\/6528\/commits\/e81e8a51ec7ba3d0dfb6089254f166c2733216e1",
      "commit_sha":"e81e8a51ec7ba3d0dfb6089254f166c2733216e1",
      "patch":"SINGLE",
      "chain_ord":"['e81e8a51ec7ba3d0dfb6089254f166c2733216e1']",
      "before_first_fix_commit":"{'90e222e9a9ba64bd808666f44e6a0913d6318f78'}",
      "last_fix_commit":"e81e8a51ec7ba3d0dfb6089254f166c2733216e1",
      "chain_ord_pos":1.0,
      "commit_datetime":"09\/30\/2020, 22:00:01",
      "message":"[CVE-2020-17054]",
      "author":"Paul Leathers",
      "comments":null,
      "stats":"{'additions': 5, 'deletions': 2, 'total': 7}",
      "files":"{'lib\/Backend\/Lower.cpp': {'additions': 5, 'deletions': 2, 'changes': 7, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/chakra-core\/ChakraCore\/raw\/e81e8a51ec7ba3d0dfb6089254f166c2733216e1\/lib%2FBackend%2FLower.cpp', 'patch': '@@ -27152,8 +27152,11 @@ void Lowerer::LowerLdFrameDisplay(IR::Instr *instr, bool doStackFrameDisplay)\\n         if (instr->m_func != this->m_func && this->m_func->DoStackFrameDisplay())\\n         {\\n             StackSym * inlineeFrameDisplaySym = instr->m_func->GetLocalFrameDisplaySym();\\n-            Assert(inlineeFrameDisplaySym->IsAllocated());\\n-            InsertMove(IR::SymOpnd::New(inlineeFrameDisplaySym, TyMachReg, m_func), dstOpnd, instr);\\n+            Assert((inlineeFrameDisplaySym && inlineeFrameDisplaySym->IsAllocated()) || this->m_func->IsLoopBody());\\n+            if (inlineeFrameDisplaySym && inlineeFrameDisplaySym->IsAllocated())\\n+            {\\n+                InsertMove(IR::SymOpnd::New(inlineeFrameDisplaySym, TyMachReg, m_func), dstOpnd, instr);\\n+            }\\n         }\\n     }'}}",
      "message_norm":"[cve-2020-17054]",
      "language":"ro",
      "entities":"[('cve-2020-17054', 'VULNID', 'CVE')]",
      "classification_level_1":null,
      "classification_level_2":null,
      "list_files":"dict_keys(['lib\/Backend\/Lower.cpp'])",
      "num_files":1.0,
      "patch_content":"From e81e8a51ec7ba3d0dfb6089254f166c2733216e1 Mon Sep 17 00:00:00 2001\nFrom: Paul Leathers <pleath@microsoft.com>\nDate: Wed, 30 Sep 2020 15:00:01 -0700\nSubject: [PATCH] [CVE-2020-17054]\n\n---\n lib\/Backend\/Lower.cpp | 7 +++++--\n 1 file changed, 5 insertions(+), 2 deletions(-)\n\ndiff --git a\/lib\/Backend\/Lower.cpp b\/lib\/Backend\/Lower.cpp\nindex 813227fe3e0..7543a787040 100644\n--- a\/lib\/Backend\/Lower.cpp\n+++ b\/lib\/Backend\/Lower.cpp\n@@ -27152,8 +27152,11 @@ void Lowerer::LowerLdFrameDisplay(IR::Instr *instr, bool doStackFrameDisplay)\n         if (instr->m_func != this->m_func && this->m_func->DoStackFrameDisplay())\n         {\n             StackSym * inlineeFrameDisplaySym = instr->m_func->GetLocalFrameDisplaySym();\n-            Assert(inlineeFrameDisplaySym->IsAllocated());\n-            InsertMove(IR::SymOpnd::New(inlineeFrameDisplaySym, TyMachReg, m_func), dstOpnd, instr);\n+            Assert((inlineeFrameDisplaySym && inlineeFrameDisplaySym->IsAllocated()) || this->m_func->IsLoopBody());\n+            if (inlineeFrameDisplaySym && inlineeFrameDisplaySym->IsAllocated())\n+            {\n+                InsertMove(IR::SymOpnd::New(inlineeFrameDisplaySym, TyMachReg, m_func), dstOpnd, instr);\n+            }\n         }\n     }",
      "code_diff":"@@ -27152,8 +27152,11 @@ void Lowerer::LowerLdFrameDisplay(IR::Instr *instr, bool doStackFrameDisplay)\n         if (instr->m_func != this->m_func && this->m_func->DoStackFrameDisplay())\n         {\n             StackSym * inlineeFrameDisplaySym = instr->m_func->GetLocalFrameDisplaySym();\n-            Assert(inlineeFrameDisplaySym->IsAllocated());\n-            InsertMove(IR::SymOpnd::New(inlineeFrameDisplaySym, TyMachReg, m_func), dstOpnd, instr);\n+            Assert((inlineeFrameDisplaySym && inlineeFrameDisplaySym->IsAllocated()) || this->m_func->IsLoopBody());\n+            if (inlineeFrameDisplaySym && inlineeFrameDisplaySym->IsAllocated())\n+            {\n+                InsertMove(IR::SymOpnd::New(inlineeFrameDisplaySym, TyMachReg, m_func), dstOpnd, instr);\n+            }\n         }\n     }"
    },
    {
      "index":34,
      "vuln_id":"GHSA-9c78-vcq7-7vxq",
      "cwe_id":"{'CWE-787'}",
      "score":8.8,
      "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/6c0b2b70eeee588591680f5b7d5d38175fd7cdf6'}",
      "dataset":"osv",
      "summary":"Out of bounds write in TFLite ### Impact \nAn attacker can craft a TFLite model that would cause a write outside of bounds of an array in TFLite. In fact, the attacker can override the linked list used by the memory allocator. This can be leveraged for an arbitrary write primitive under certain conditions.\n\n### Patches\nWe have patched the issue in GitHub commit [6c0b2b70eeee588591680f5b7d5d38175fd7cdf6](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/6c0b2b70eeee588591680f5b7d5d38175fd7cdf6).\n  \nThe fix will be included in TensorFlow 2.8.0. We will also cherrypick this commit on TensorFlow 2.7.1, TensorFlow 2.6.3, and TensorFlow 2.5.3, as these are also affected and still in supported range.\n    \n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n  \n### Attribution\nThis vulnerability has been reported by Wang Xuan of Qihoo 360 AIVul Team.",
      "published_date":"2022-02-09",
      "chain_len":1,
      "project":"https:\/\/github.com\/tensorflow\/tensorflow",
      "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/6c0b2b70eeee588591680f5b7d5d38175fd7cdf6",
      "commit_sha":"6c0b2b70eeee588591680f5b7d5d38175fd7cdf6",
      "patch":"SINGLE",
      "chain_ord":"['6c0b2b70eeee588591680f5b7d5d38175fd7cdf6']",
      "before_first_fix_commit":"{'1de49725a5fc4e48f1a3b902ec3599ee99283043'}",
      "last_fix_commit":"6c0b2b70eeee588591680f5b7d5d38175fd7cdf6",
      "chain_ord_pos":1.0,
      "commit_datetime":"12\/21\/2021, 16:50:37",
      "message":"[lite] add validation check for sparse fully connected\n\nPiperOrigin-RevId: 417629354\nChange-Id: If96171c4bd4f5fdb01d6368d6deab19d1c9beca7",
      "author":"Karim Nosir",
      "comments":null,
      "stats":"{'additions': 48, 'deletions': 10, 'total': 58}",
      "files":"{'tensorflow\/lite\/kernels\/fully_connected.cc': {'additions': 48, 'deletions': 10, 'changes': 58, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/6c0b2b70eeee588591680f5b7d5d38175fd7cdf6\/tensorflow%2Flite%2Fkernels%2Ffully_connected.cc', 'patch': '@@ -928,6 +928,36 @@ TfLiteStatus EvalShuffledQuantized(TfLiteContext* context, TfLiteNode* node,\\n   return kTfLiteOk;\\n }\\n \\n+\/\/ Verifies that sparsity values are valid given input\/weight\/output.\\n+bool VerifySparsity(const RuntimeShape& weights_shape,\\n+                    const RuntimeShape& input_shape,\\n+                    const RuntimeShape& output_shape,\\n+                    const TfLiteSparsity* sparsity) {\\n+  const int weights_dims_count = weights_shape.DimensionsCount();\\n+  const int output_dims_count = output_shape.DimensionsCount();\\n+  const int w0_size = sparsity->dim_metadata[0].dense_size;\\n+  const int accum_depth = weights_shape.Dims(weights_dims_count - 1);\\n+  const int output_elements = output_shape.FlatSize();\\n+  const int input_elements = input_shape.FlatSize();\\n+  const int batches = FlatSizeSkipDim(output_shape, output_dims_count - 1);\\n+  const int output_depth = MatchingDim(weights_shape, weights_dims_count - 2,\\n+                                       output_shape, output_dims_count - 1);\\n+  const int max_batch_index = batches - 1;\\n+  const int max_output = max_batch_index * output_depth + w0_size;\\n+  const int max_batch_depth = accum_depth * max_batch_index;\\n+\\n+  \/\/ Verify output size is enough.\\n+  if (output_elements < max_output) return false;\\n+\\n+  \/\/ Verify index from sparse in input is valid.\\n+  for (int i = 0; i < sparsity->dim_metadata[1].array_indices->size; ++i) {\\n+    if (input_elements <=\\n+        max_batch_depth + sparsity->dim_metadata[1].array_indices->data[i])\\n+      return false;\\n+  }\\n+  return true;\\n+}\\n+\\n template <KernelType kernel_type>\\n TfLiteStatus EvalFloat(TfLiteContext* context, TfLiteNode* node,\\n                        TfLiteFullyConnectedParams* params, OpData* data,\\n@@ -968,24 +998,32 @@ TfLiteStatus EvalFloat(TfLiteContext* context, TfLiteNode* node,\\n                            \"Unsupported sparse fully-connected weight format.\");\\n         return kTfLiteError;\\n       }\\n+      const auto& input_shape = GetTensorShape(input);\\n+      const auto& filter_shape = GetTensorShape(filter);\\n+      const auto& output_shape = GetTensorShape(output);\\n+      const auto& bias_shape = GetTensorShape(bias);\\n+      if (!VerifySparsity(filter_shape, input_shape, output_shape, &sparsity)) {\\n+        TF_LITE_KERNEL_LOG(context, \"Invalid sparse fully-connected format.\");\\n+        return kTfLiteError;\\n+      }\\n \\n       if (sparsity.dim_metadata_size == kDimMetadataSizeRandomSparse) {\\n         \/\/ Random sparse.\\n         optimized_ops::FullyConnectedSparseWeight(\\n-            sparsity, op_params, GetTensorShape(input),\\n-            GetTensorData<float>(input), GetTensorShape(filter),\\n-            GetTensorData<float>(filter), GetTensorShape(bias),\\n-            GetTensorData<float>(bias), GetTensorShape(output),\\n-            GetTensorData<float>(output));\\n+            sparsity, op_params,                         \/\/ Disable formatting\\n+            input_shape, GetTensorData<float>(input),    \/\/ Disable formatting\\n+            filter_shape, GetTensorData<float>(filter),  \/\/ Disable formatting\\n+            bias_shape, GetTensorData<float>(bias),      \/\/ Disable formatting\\n+            output_shape, GetTensorData<float>(output));\\n       } else if (sparsity.dim_metadata_size == kDimMetadataSizeBlockSparse &&\\n                  sparsity.dim_metadata[2].dense_size == 4) {\\n         \/\/ Block sparse with block size of 1x4.\\n         optimized_ops::FullyConnectedSparseWeight1x4(\\n-            sparsity, op_params, GetTensorShape(input),\\n-            GetTensorData<float>(input), GetTensorShape(filter),\\n-            GetTensorData<float>(filter), GetTensorShape(bias),\\n-            GetTensorData<float>(bias), GetTensorShape(output),\\n-            GetTensorData<float>(output),\\n+            sparsity, op_params,                         \/\/ Disable formatting\\n+            input_shape, GetTensorData<float>(input),    \/\/ Disable formatting\\n+            filter_shape, GetTensorData<float>(filter),  \/\/ Disable formatting\\n+            bias_shape, GetTensorData<float>(bias),      \/\/ Disable formatting\\n+            output_shape, GetTensorData<float>(output),\\n             CpuBackendContext::GetFromContext(context));\\n       } else {\\n         TF_LITE_KERNEL_LOG(context,'}}",
      "message_norm":"[lite] add validation check for sparse fully connected\n\npiperorigin-revid: 417629354\nchange-id: if96171c4bd4f5fdb01d6368d6deab19d1c9beca7",
      "language":"en",
      "entities":"[('add', 'ACTION', ''), ('417629354', 'SHA', 'generic_sha')]",
      "classification_level_1":null,
      "classification_level_2":null,
      "list_files":"dict_keys(['tensorflow\/lite\/kernels\/fully_connected.cc'])",
      "num_files":1.0,
      "patch_content":"From 6c0b2b70eeee588591680f5b7d5d38175fd7cdf6 Mon Sep 17 00:00:00 2001\nFrom: Karim Nosir <karimnosseir@google.com>\nDate: Tue, 21 Dec 2021 08:50:37 -0800\nSubject: [PATCH] [lite] add validation check for sparse fully connected\n\nPiperOrigin-RevId: 417629354\nChange-Id: If96171c4bd4f5fdb01d6368d6deab19d1c9beca7\n---\n tensorflow\/lite\/kernels\/fully_connected.cc | 58 ++++++++++++++++++----\n 1 file changed, 48 insertions(+), 10 deletions(-)\n\ndiff --git a\/tensorflow\/lite\/kernels\/fully_connected.cc b\/tensorflow\/lite\/kernels\/fully_connected.cc\nindex 8d59a2cbd5a692..8b9f74f623ec68 100644\n--- a\/tensorflow\/lite\/kernels\/fully_connected.cc\n+++ b\/tensorflow\/lite\/kernels\/fully_connected.cc\n@@ -928,6 +928,36 @@ TfLiteStatus EvalShuffledQuantized(TfLiteContext* context, TfLiteNode* node,\n   return kTfLiteOk;\n }\n \n+\/\/ Verifies that sparsity values are valid given input\/weight\/output.\n+bool VerifySparsity(const RuntimeShape& weights_shape,\n+                    const RuntimeShape& input_shape,\n+                    const RuntimeShape& output_shape,\n+                    const TfLiteSparsity* sparsity) {\n+  const int weights_dims_count = weights_shape.DimensionsCount();\n+  const int output_dims_count = output_shape.DimensionsCount();\n+  const int w0_size = sparsity->dim_metadata[0].dense_size;\n+  const int accum_depth = weights_shape.Dims(weights_dims_count - 1);\n+  const int output_elements = output_shape.FlatSize();\n+  const int input_elements = input_shape.FlatSize();\n+  const int batches = FlatSizeSkipDim(output_shape, output_dims_count - 1);\n+  const int output_depth = MatchingDim(weights_shape, weights_dims_count - 2,\n+                                       output_shape, output_dims_count - 1);\n+  const int max_batch_index = batches - 1;\n+  const int max_output = max_batch_index * output_depth + w0_size;\n+  const int max_batch_depth = accum_depth * max_batch_index;\n+\n+  \/\/ Verify output size is enough.\n+  if (output_elements < max_output) return false;\n+\n+  \/\/ Verify index from sparse in input is valid.\n+  for (int i = 0; i < sparsity->dim_metadata[1].array_indices->size; ++i) {\n+    if (input_elements <=\n+        max_batch_depth + sparsity->dim_metadata[1].array_indices->data[i])\n+      return false;\n+  }\n+  return true;\n+}\n+\n template <KernelType kernel_type>\n TfLiteStatus EvalFloat(TfLiteContext* context, TfLiteNode* node,\n                        TfLiteFullyConnectedParams* params, OpData* data,\n@@ -968,24 +998,32 @@ TfLiteStatus EvalFloat(TfLiteContext* context, TfLiteNode* node,\n                            \"Unsupported sparse fully-connected weight format.\");\n         return kTfLiteError;\n       }\n+      const auto& input_shape = GetTensorShape(input);\n+      const auto& filter_shape = GetTensorShape(filter);\n+      const auto& output_shape = GetTensorShape(output);\n+      const auto& bias_shape = GetTensorShape(bias);\n+      if (!VerifySparsity(filter_shape, input_shape, output_shape, &sparsity)) {\n+        TF_LITE_KERNEL_LOG(context, \"Invalid sparse fully-connected format.\");\n+        return kTfLiteError;\n+      }\n \n       if (sparsity.dim_metadata_size == kDimMetadataSizeRandomSparse) {\n         \/\/ Random sparse.\n         optimized_ops::FullyConnectedSparseWeight(\n-            sparsity, op_params, GetTensorShape(input),\n-            GetTensorData<float>(input), GetTensorShape(filter),\n-            GetTensorData<float>(filter), GetTensorShape(bias),\n-            GetTensorData<float>(bias), GetTensorShape(output),\n-            GetTensorData<float>(output));\n+            sparsity, op_params,                         \/\/ Disable formatting\n+            input_shape, GetTensorData<float>(input),    \/\/ Disable formatting\n+            filter_shape, GetTensorData<float>(filter),  \/\/ Disable formatting\n+            bias_shape, GetTensorData<float>(bias),      \/\/ Disable formatting\n+            output_shape, GetTensorData<float>(output));\n       } else if (sparsity.dim_metadata_size == kDimMetadataSizeBlockSparse &&\n                  sparsity.dim_metadata[2].dense_size == 4) {\n         \/\/ Block sparse with block size of 1x4.\n         optimized_ops::FullyConnectedSparseWeight1x4(\n-            sparsity, op_params, GetTensorShape(input),\n-            GetTensorData<float>(input), GetTensorShape(filter),\n-            GetTensorData<float>(filter), GetTensorShape(bias),\n-            GetTensorData<float>(bias), GetTensorShape(output),\n-            GetTensorData<float>(output),\n+            sparsity, op_params,                         \/\/ Disable formatting\n+            input_shape, GetTensorData<float>(input),    \/\/ Disable formatting\n+            filter_shape, GetTensorData<float>(filter),  \/\/ Disable formatting\n+            bias_shape, GetTensorData<float>(bias),      \/\/ Disable formatting\n+            output_shape, GetTensorData<float>(output),\n             CpuBackendContext::GetFromContext(context));\n       } else {\n         TF_LITE_KERNEL_LOG(context,",
      "code_diff":"@@ -928,6 +928,36 @@ TfLiteStatus EvalShuffledQuantized(TfLiteContext* context, TfLiteNode* node,\n   return kTfLiteOk;\n }\n \n+\/\/ Verifies that sparsity values are valid given input\/weight\/output.\n+bool VerifySparsity(const RuntimeShape& weights_shape,\n+                    const RuntimeShape& input_shape,\n+                    const RuntimeShape& output_shape,\n+                    const TfLiteSparsity* sparsity) {\n+  const int weights_dims_count = weights_shape.DimensionsCount();\n+  const int output_dims_count = output_shape.DimensionsCount();\n+  const int w0_size = sparsity->dim_metadata[0].dense_size;\n+  const int accum_depth = weights_shape.Dims(weights_dims_count - 1);\n+  const int output_elements = output_shape.FlatSize();\n+  const int input_elements = input_shape.FlatSize();\n+  const int batches = FlatSizeSkipDim(output_shape, output_dims_count - 1);\n+  const int output_depth = MatchingDim(weights_shape, weights_dims_count - 2,\n+                                       output_shape, output_dims_count - 1);\n+  const int max_batch_index = batches - 1;\n+  const int max_output = max_batch_index * output_depth + w0_size;\n+  const int max_batch_depth = accum_depth * max_batch_index;\n+\n+  \/\/ Verify output size is enough.\n+  if (output_elements < max_output) return false;\n+\n+  \/\/ Verify index from sparse in input is valid.\n+  for (int i = 0; i < sparsity->dim_metadata[1].array_indices->size; ++i) {\n+    if (input_elements <=\n+        max_batch_depth + sparsity->dim_metadata[1].array_indices->data[i])\n+      return false;\n+  }\n+  return true;\n+}\n+\n template <KernelType kernel_type>\n TfLiteStatus EvalFloat(TfLiteContext* context, TfLiteNode* node,\n                        TfLiteFullyConnectedParams* params, OpData* data,\n@@ -968,24 +998,32 @@ TfLiteStatus EvalFloat(TfLiteContext* context, TfLiteNode* node,\n                            \"Unsupported sparse fully-connected weight format.\");\n         return kTfLiteError;\n       }\n+      const auto& input_shape = GetTensorShape(input);\n+      const auto& filter_shape = GetTensorShape(filter);\n+      const auto& output_shape = GetTensorShape(output);\n+      const auto& bias_shape = GetTensorShape(bias);\n+      if (!VerifySparsity(filter_shape, input_shape, output_shape, &sparsity)) {\n+        TF_LITE_KERNEL_LOG(context, \"Invalid sparse fully-connected format.\");\n+        return kTfLiteError;\n+      }\n \n       if (sparsity.dim_metadata_size == kDimMetadataSizeRandomSparse) {\n         \/\/ Random sparse.\n         optimized_ops::FullyConnectedSparseWeight(\n-            sparsity, op_params, GetTensorShape(input),\n-            GetTensorData<float>(input), GetTensorShape(filter),\n-            GetTensorData<float>(filter), GetTensorShape(bias),\n-            GetTensorData<float>(bias), GetTensorShape(output),\n-            GetTensorData<float>(output));\n+            sparsity, op_params,                         \/\/ Disable formatting\n+            input_shape, GetTensorData<float>(input),    \/\/ Disable formatting\n+            filter_shape, GetTensorData<float>(filter),  \/\/ Disable formatting\n+            bias_shape, GetTensorData<float>(bias),      \/\/ Disable formatting\n+            output_shape, GetTensorData<float>(output));\n       } else if (sparsity.dim_metadata_size == kDimMetadataSizeBlockSparse &&\n                  sparsity.dim_metadata[2].dense_size == 4) {\n         \/\/ Block sparse with block size of 1x4.\n         optimized_ops::FullyConnectedSparseWeight1x4(\n-            sparsity, op_params, GetTensorShape(input),\n-            GetTensorData<float>(input), GetTensorShape(filter),\n-            GetTensorData<float>(filter), GetTensorShape(bias),\n-            GetTensorData<float>(bias), GetTensorShape(output),\n-            GetTensorData<float>(output),\n+            sparsity, op_params,                         \/\/ Disable formatting\n+            input_shape, GetTensorData<float>(input),    \/\/ Disable formatting\n+            filter_shape, GetTensorData<float>(filter),  \/\/ Disable formatting\n+            bias_shape, GetTensorData<float>(bias),      \/\/ Disable formatting\n+            output_shape, GetTensorData<float>(output),\n             CpuBackendContext::GetFromContext(context));\n       } else {\n         TF_LITE_KERNEL_LOG(context,"
    },
    {
      "index":35,
      "vuln_id":"GHSA-cjc7-49v2-jp64",
      "cwe_id":"{'CWE-787', 'CWE-665'}",
      "score":5.3,
      "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/6fd02f44810754ae7481838b6a67c5df7f909ca3', 'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/41727ff06111117bdf86b37db198217fd7a143cc'}",
      "dataset":"osv",
      "summary":"Incomplete validation in `SparseAdd` ### Impact\nIncomplete validation in `SparseAdd` results in allowing attackers to exploit undefined behavior (dereferencing null pointers) as well as write outside of bounds of heap allocated data:\n\n```python\nimport tensorflow as tf\n\na_indices = tf.zeros([10, 97], dtype=tf.int64)\na_values = tf.zeros([10], dtype=tf.int64)\na_shape = tf.zeros([0], dtype=tf.int64)\n\nb_indices = tf.zeros([0, 0], dtype=tf.int64)\nb_values = tf.zeros([0], dtype=tf.int64)\nb_shape = tf.zeros([0], dtype=tf.int64)\n  \nthresh = 0\n\ntf.raw_ops.SparseAdd(a_indices=a_indices,\n                    a_values=a_values,\n                    a_shape=a_shape,\n                    b_indices=b_indices,\n                    b_values=b_values,\n                    b_shape=b_shape,\n                    thresh=thresh)\n```\n\nThe [implementation](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/656e7673b14acd7835dc778867f84916c6d1cac2\/tensorflow\/core\/kernels\/sparse_add_op.cc) has a large set of validation for the two sparse tensor inputs (6 tensors in total), but does not validate that the tensors are not empty or that the second dimension of `*_indices` matches the size of corresponding `*_shape`. This allows attackers to send tensor triples that represent invalid sparse tensors to abuse code assumptions that are not protected by validation.\n\n### Patches\nWe have patched the issue in GitHub commit [6fd02f44810754ae7481838b6a67c5df7f909ca3](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/6fd02f44810754ae7481838b6a67c5df7f909ca3) followed by GitHub commit  [41727ff06111117bdf86b37db198217fd7a143cc](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/41727ff06111117bdf86b37db198217fd7a143cc).\n\nThe fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by Yakun Zhang and Ying Wang of Baidu X-Team.",
      "published_date":"2021-05-21",
      "chain_len":2,
      "project":"https:\/\/github.com\/tensorflow\/tensorflow",
      "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/41727ff06111117bdf86b37db198217fd7a143cc",
      "commit_sha":"41727ff06111117bdf86b37db198217fd7a143cc",
      "patch":"MULTI",
      "chain_ord":"['6fd02f44810754ae7481838b6a67c5df7f909ca3', '41727ff06111117bdf86b37db198217fd7a143cc']",
      "before_first_fix_commit":"{'6f432d6334edc93fd5af0070170def56b0413e8a'}",
      "last_fix_commit":"41727ff06111117bdf86b37db198217fd7a143cc",
      "chain_ord_pos":2.0,
      "commit_datetime":"05\/11\/2021, 22:41:51",
      "message":"Validate that a and b are proper sparse tensors\n\nPiperOrigin-RevId: 373248068\nChange-Id: I0a2041a0747901b3f00387a6a3bce9bca6b0b3b1",
      "author":"Mihai Maruseac",
      "comments":null,
      "stats":"{'additions': 12, 'deletions': 5, 'total': 17}",
      "files":"{'tensorflow\/core\/kernels\/sparse_add_op.cc': {'additions': 12, 'deletions': 5, 'changes': 17, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/41727ff06111117bdf86b37db198217fd7a143cc\/tensorflow%2Fcore%2Fkernels%2Fsparse_add_op.cc', 'patch': '@@ -44,6 +44,11 @@ class SparseAddOp : public OpKernel {\\n                     b_indices->shape().DebugString()));\\n     const int64 a_nnz = a_indices->dim_size(0);\\n     const int64 b_nnz = b_indices->dim_size(0);\\n+    const int num_dims = a_indices->dim_size(1);\\n+    OP_REQUIRES(ctx, b_indices->dim_size(1) == num_dims,\\n+                errors::InvalidArgument(\\n+                    \"Input indices must have the same dimension, got \",\\n+                    num_dims, \" and \", b_indices->dim_size(1)));\\n \\n     OP_REQUIRES_OK(ctx, ctx->input(\"a_values\", &a_values_t));\\n     OP_REQUIRES_OK(ctx, ctx->input(\"b_values\", &b_values_t));\\n@@ -72,6 +77,13 @@ class SparseAddOp : public OpKernel {\\n                     \"Input shapes should be a vector but received shapes \",\\n                     a_shape->shape().DebugString(), \" and \",\\n                     b_shape->shape().DebugString()));\\n+    OP_REQUIRES(\\n+        ctx, a_shape->NumElements() == num_dims,\\n+        errors::InvalidArgument(\"Second dimension of a_indices and length of \"\\n+                                \"a_shape must match, got \",\\n+                                num_dims, \" and \", a_shape->NumElements()));\\n+    OP_REQUIRES(ctx, num_dims > 0,\\n+                errors::InvalidArgument(\"Tesors must not be empty\"));\\n     OP_REQUIRES(\\n         ctx, a_shape->IsSameSize(*b_shape),\\n         errors::InvalidArgument(\\n@@ -100,11 +112,6 @@ class SparseAddOp : public OpKernel {\\n     std::vector<std::pair<bool, int64>> entries_to_copy;  \/\/ from_a?, idx\\n     entries_to_copy.reserve(a_nnz + b_nnz);\\n     std::vector<T> out_values;\\n-    const int num_dims = a_shape->dim_size(0);\\n-\\n-    OP_REQUIRES(ctx, num_dims > 0,\\n-                errors::InvalidArgument(\"Invalid input_a shape. Received: \",\\n-                                        a_shape->DebugString()));\\n \\n     \/\/ The input and output sparse tensors are assumed to be ordered along\\n     \/\/ increasing dimension number.'}}",
      "message_norm":"validate that a and b are proper sparse tensors\n\npiperorigin-revid: 373248068\nchange-id: i0a2041a0747901b3f00387a6a3bce9bca6b0b3b1",
      "language":"en",
      "entities":"[('validate', 'ACTION', ''), ('373248068', 'SHA', 'generic_sha')]",
      "classification_level_1":null,
      "classification_level_2":null,
      "list_files":"dict_keys(['tensorflow\/core\/kernels\/sparse_add_op.cc'])",
      "num_files":1.0,
      "patch_content":"From 41727ff06111117bdf86b37db198217fd7a143cc Mon Sep 17 00:00:00 2001\nFrom: Mihai Maruseac <mihaimaruseac@google.com>\nDate: Tue, 11 May 2021 15:41:51 -0700\nSubject: [PATCH] Validate that a and b are proper sparse tensors\n\nPiperOrigin-RevId: 373248068\nChange-Id: I0a2041a0747901b3f00387a6a3bce9bca6b0b3b1\n---\n tensorflow\/core\/kernels\/sparse_add_op.cc | 17 ++++++++++++-----\n 1 file changed, 12 insertions(+), 5 deletions(-)\n\ndiff --git a\/tensorflow\/core\/kernels\/sparse_add_op.cc b\/tensorflow\/core\/kernels\/sparse_add_op.cc\nindex 346206365af8d5..2bd05fa41adc26 100644\n--- a\/tensorflow\/core\/kernels\/sparse_add_op.cc\n+++ b\/tensorflow\/core\/kernels\/sparse_add_op.cc\n@@ -44,6 +44,11 @@ class SparseAddOp : public OpKernel {\n                     b_indices->shape().DebugString()));\n     const int64 a_nnz = a_indices->dim_size(0);\n     const int64 b_nnz = b_indices->dim_size(0);\n+    const int num_dims = a_indices->dim_size(1);\n+    OP_REQUIRES(ctx, b_indices->dim_size(1) == num_dims,\n+                errors::InvalidArgument(\n+                    \"Input indices must have the same dimension, got \",\n+                    num_dims, \" and \", b_indices->dim_size(1)));\n \n     OP_REQUIRES_OK(ctx, ctx->input(\"a_values\", &a_values_t));\n     OP_REQUIRES_OK(ctx, ctx->input(\"b_values\", &b_values_t));\n@@ -72,6 +77,13 @@ class SparseAddOp : public OpKernel {\n                     \"Input shapes should be a vector but received shapes \",\n                     a_shape->shape().DebugString(), \" and \",\n                     b_shape->shape().DebugString()));\n+    OP_REQUIRES(\n+        ctx, a_shape->NumElements() == num_dims,\n+        errors::InvalidArgument(\"Second dimension of a_indices and length of \"\n+                                \"a_shape must match, got \",\n+                                num_dims, \" and \", a_shape->NumElements()));\n+    OP_REQUIRES(ctx, num_dims > 0,\n+                errors::InvalidArgument(\"Tesors must not be empty\"));\n     OP_REQUIRES(\n         ctx, a_shape->IsSameSize(*b_shape),\n         errors::InvalidArgument(\n@@ -100,11 +112,6 @@ class SparseAddOp : public OpKernel {\n     std::vector<std::pair<bool, int64>> entries_to_copy;  \/\/ from_a?, idx\n     entries_to_copy.reserve(a_nnz + b_nnz);\n     std::vector<T> out_values;\n-    const int num_dims = a_shape->dim_size(0);\n-\n-    OP_REQUIRES(ctx, num_dims > 0,\n-                errors::InvalidArgument(\"Invalid input_a shape. Received: \",\n-                                        a_shape->DebugString()));\n \n     \/\/ The input and output sparse tensors are assumed to be ordered along\n     \/\/ increasing dimension number.",
      "code_diff":"@@ -44,6 +44,11 @@ class SparseAddOp : public OpKernel {\n                     b_indices->shape().DebugString()));\n     const int64 a_nnz = a_indices->dim_size(0);\n     const int64 b_nnz = b_indices->dim_size(0);\n+    const int num_dims = a_indices->dim_size(1);\n+    OP_REQUIRES(ctx, b_indices->dim_size(1) == num_dims,\n+                errors::InvalidArgument(\n+                    \"Input indices must have the same dimension, got \",\n+                    num_dims, \" and \", b_indices->dim_size(1)));\n \n     OP_REQUIRES_OK(ctx, ctx->input(\"a_values\", &a_values_t));\n     OP_REQUIRES_OK(ctx, ctx->input(\"b_values\", &b_values_t));\n@@ -72,6 +77,13 @@ class SparseAddOp : public OpKernel {\n                     \"Input shapes should be a vector but received shapes \",\n                     a_shape->shape().DebugString(), \" and \",\n                     b_shape->shape().DebugString()));\n+    OP_REQUIRES(\n+        ctx, a_shape->NumElements() == num_dims,\n+        errors::InvalidArgument(\"Second dimension of a_indices and length of \"\n+                                \"a_shape must match, got \",\n+                                num_dims, \" and \", a_shape->NumElements()));\n+    OP_REQUIRES(ctx, num_dims > 0,\n+                errors::InvalidArgument(\"Tesors must not be empty\"));\n     OP_REQUIRES(\n         ctx, a_shape->IsSameSize(*b_shape),\n         errors::InvalidArgument(\n@@ -100,11 +112,6 @@ class SparseAddOp : public OpKernel {\n     std::vector<std::pair<bool, int64>> entries_to_copy;  \/\/ from_a?, idx\n     entries_to_copy.reserve(a_nnz + b_nnz);\n     std::vector<T> out_values;\n-    const int num_dims = a_shape->dim_size(0);\n-\n-    OP_REQUIRES(ctx, num_dims > 0,\n-                errors::InvalidArgument(\"Invalid input_a shape. Received: \",\n-                                        a_shape->DebugString()));\n \n     \/\/ The input and output sparse tensors are assumed to be ordered along\n     \/\/ increasing dimension number."
    },
    {
      "index":36,
      "vuln_id":"GHSA-cvpc-8phh-8f45",
      "cwe_id":"{'CWE-787', 'CWE-125'}",
      "score":4.8,
      "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/00302787b788c5ff04cb6f62aed5a74d936e86c0', 'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/e11f55585f614645b360563072ffeb5c3eeff162', 'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/46d5b0852528ddfd614ded79bccc75589f801bd9', 'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/cd31fd0ce0449a9e0f83dcad08d6ed7f1d6bef3f', 'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/1970c2158b1ffa416d159d03c3370b9a462aee35', 'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/fff2c8326280c07733828f990548979bdc893859'}",
      "dataset":"osv",
      "summary":"Out of bounds access in tensorflow-lite ### Impact\nIn TensorFlow Lite, saved models in the flatbuffer format use a double indexing scheme: a model has a set of subgraphs, each subgraph has a set of operators and each operator has a set of input\/output tensors. The flatbuffer format uses indices for the tensors, indexing into an array of tensors that is owned by the subgraph. This results in a pattern of double array indexing when trying to get the data of each tensor:https:\/\/github.com\/tensorflow\/tensorflow\/blob\/0e68f4d3295eb0281a517c3662f6698992b7b2cf\/tensorflow\/lite\/kernels\/kernel_util.cc#L36\n\nHowever, some operators can have some tensors be optional. To handle this scenario, the flatbuffer model uses a negative `-1` value as index for these tensors:\nhttps:\/\/github.com\/tensorflow\/tensorflow\/blob\/0e68f4d3295eb0281a517c3662f6698992b7b2cf\/tensorflow\/lite\/c\/common.h#L82\n\nThis results in special casing during validation at model loading time: https:\/\/github.com\/tensorflow\/tensorflow\/blob\/0e68f4d3295eb0281a517c3662f6698992b7b2cf\/tensorflow\/lite\/core\/subgraph.cc#L566-L580\n\nUnfortunately, this means that the `-1` index is a valid tensor index for any operator, including those that don't expect optional inputs and including for output tensors. Thus, this allows writing and reading from outside the bounds of heap allocated arrays, although only at a specific offset from the start of these arrays.\n\nThis results in both read and write gadgets, albeit very limited in scope.\n\n### Patches\nWe have patched the issue in several commits (46d5b0852, 00302787b7, e11f5558, cd31fd0ce, 1970c21, and fff2c83). We will release patch releases for all versions between 1.15 and 2.3.\n\nWe recommend users to upgrade to TensorFlow 1.15.4, 2.0.3, 2.1.2, 2.2.1, or 2.3.1.\n\n### Workarounds\nA potential workaround would be to add a custom `Verifier` to the model loading code to ensure that only operators which accept optional inputs use the `-1` special value and only for the tensors that they expect to be optional. Since this allow-list type approach is erro-prone, we advise upgrading to the patched code.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by members of the Aivul Team from Qihoo 360.",
      "published_date":"2020-09-25",
      "chain_len":6,
      "project":"https:\/\/github.com\/tensorflow\/tensorflow",
      "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/00302787b788c5ff04cb6f62aed5a74d936e86c0",
      "commit_sha":"00302787b788c5ff04cb6f62aed5a74d936e86c0",
      "patch":"MULTI",
      "chain_ord":"['46d5b0852528ddfd614ded79bccc75589f801bd9', '00302787b788c5ff04cb6f62aed5a74d936e86c0', 'e11f55585f614645b360563072ffeb5c3eeff162', 'cd31fd0ce0449a9e0f83dcad08d6ed7f1d6bef3f', 'fff2c8326280c07733828f990548979bdc893859', '1970c2158b1ffa416d159d03c3370b9a462aee35']",
      "before_first_fix_commit":"{'fff2c8326280c07733828f990548979bdc893859'}",
      "last_fix_commit":"1970c2158b1ffa416d159d03c3370b9a462aee35",
      "chain_ord_pos":2.0,
      "commit_datetime":"09\/18\/2020, 20:16:53",
      "message":"[tflite] Make `GetOptionalInputTensor` the same as `GetInput`.\n\nWith the previous change, there is no more need for two separate APIs. We would deprecate `GetOptionalInputTensor` in the future.\n\nPiperOrigin-RevId: 332513386\nChange-Id: Id7110271c25ebd6126ad8c82a493e37e0e0756b3",
      "author":"Mihai Maruseac",
      "comments":null,
      "stats":"{'additions': 1, 'deletions': 6, 'total': 7}",
      "files":"{'tensorflow\/lite\/kernels\/kernel_util.cc': {'additions': 1, 'deletions': 6, 'changes': 7, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/00302787b788c5ff04cb6f62aed5a74d936e86c0\/tensorflow%2Flite%2Fkernels%2Fkernel_util.cc', 'patch': '@@ -75,12 +75,7 @@ TfLiteTensor* GetOutput(TfLiteContext* context, const TfLiteNode* node,\\n \\n const TfLiteTensor* GetOptionalInputTensor(const TfLiteContext* context,\\n                                            const TfLiteNode* node, int index) {\\n-  const bool use_tensor = index < node->inputs->size &&\\n-                          node->inputs->data[index] != kTfLiteOptionalTensor;\\n-  if (use_tensor) {\\n-    return GetMutableInput(context, node, index);\\n-  }\\n-  return nullptr;\\n+  return GetInput(context, node, index);\\n }\\n \\n \/\/ Per-axis'}}",
      "message_norm":"[tflite] make `getoptionalinputtensor` the same as `getinput`.\n\nwith the previous change, there is no more need for two separate apis. we would deprecate `getoptionalinputtensor` in the future.\n\npiperorigin-revid: 332513386\nchange-id: id7110271c25ebd6126ad8c82a493e37e0e0756b3",
      "language":"en",
      "entities":"[('332513386', 'SHA', 'generic_sha')]",
      "classification_level_1":null,
      "classification_level_2":null,
      "list_files":"dict_keys(['tensorflow\/lite\/kernels\/kernel_util.cc'])",
      "num_files":1.0,
      "patch_content":"From 00302787b788c5ff04cb6f62aed5a74d936e86c0 Mon Sep 17 00:00:00 2001\nFrom: Mihai Maruseac <mihaimaruseac@google.com>\nDate: Fri, 18 Sep 2020 13:16:53 -0700\nSubject: [PATCH] [tflite] Make `GetOptionalInputTensor` the same as\n `GetInput`.\n\nWith the previous change, there is no more need for two separate APIs. We would deprecate `GetOptionalInputTensor` in the future.\n\nPiperOrigin-RevId: 332513386\nChange-Id: Id7110271c25ebd6126ad8c82a493e37e0e0756b3\n---\n tensorflow\/lite\/kernels\/kernel_util.cc | 7 +------\n 1 file changed, 1 insertion(+), 6 deletions(-)\n\ndiff --git a\/tensorflow\/lite\/kernels\/kernel_util.cc b\/tensorflow\/lite\/kernels\/kernel_util.cc\nindex fab884bc90c9a0..cd243335c9ca15 100644\n--- a\/tensorflow\/lite\/kernels\/kernel_util.cc\n+++ b\/tensorflow\/lite\/kernels\/kernel_util.cc\n@@ -75,12 +75,7 @@ TfLiteTensor* GetOutput(TfLiteContext* context, const TfLiteNode* node,\n \n const TfLiteTensor* GetOptionalInputTensor(const TfLiteContext* context,\n                                            const TfLiteNode* node, int index) {\n-  const bool use_tensor = index < node->inputs->size &&\n-                          node->inputs->data[index] != kTfLiteOptionalTensor;\n-  if (use_tensor) {\n-    return GetMutableInput(context, node, index);\n-  }\n-  return nullptr;\n+  return GetInput(context, node, index);\n }\n \n \/\/ Per-axis",
      "code_diff":"@@ -75,12 +75,7 @@ TfLiteTensor* GetOutput(TfLiteContext* context, const TfLiteNode* node,\n \n const TfLiteTensor* GetOptionalInputTensor(const TfLiteContext* context,\n                                            const TfLiteNode* node, int index) {\n-  const bool use_tensor = index < node->inputs->size &&\n-                          node->inputs->data[index] != kTfLiteOptionalTensor;\n-  if (use_tensor) {\n-    return GetMutableInput(context, node, index);\n-  }\n-  return nullptr;\n+  return GetInput(context, node, index);\n }\n \n \/\/ Per-axis"
    },
    {
      "index":37,
      "vuln_id":"GHSA-8gv3-57p6-g35r",
      "cwe_id":"{'CWE-787', 'CWE-125'}",
      "score":2.5,
      "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/a84358aa12f0b1518e606095ab9cfddbf597c121'}",
      "dataset":"osv",
      "summary":"Heap buffer overflow in `RaggedTensorToTensor` ### Impact\nAn attacker can cause a heap buffer overflow in `tf.raw_ops.RaggedTensorToTensor`:\n\n```python\nimport tensorflow as tf\n\nshape = tf.constant([10, 10], shape=[2], dtype=tf.int64)\nvalues = tf.constant(0, shape=[1], dtype=tf.int64)\ndefault_value = tf.constant(0, dtype=tf.int64)\nl = [849, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nrow = tf.constant(l, shape=[5, 43], dtype=tf.int64)\nrows = [row]\ntypes = ['ROW_SPLITS']\n\ntf.raw_ops.RaggedTensorToTensor(\n    shape=shape, values=values, default_value=default_value,\n    row_partition_tensors=rows, row_partition_types=types) \n```\n\nThis is because the [implementation](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/d94227d43aa125ad8b54115c03cece54f6a1977b\/tensorflow\/core\/kernels\/ragged_tensor_to_tensor_op.cc#L219-L222) uses the same index to access two arrays in parallel:\n\n```cc\nfor (INDEX_TYPE i = 0; i < row_split_size - 1; ++i) {\n  INDEX_TYPE row_length = row_split(i + 1) - row_split(i);\n  INDEX_TYPE real_length = std::min(output_size, row_length);\n  INDEX_TYPE parent_output_index_current = parent_output_index[i];\n  ...\n}\n```\n\nSince the user controls the shape of the input arguments, an attacker could trigger a heap OOB access when `parent_output_index` is shorter than `row_split`.\n\n### Patches\nWe have patched the issue in GitHub commit [a84358aa12f0b1518e606095ab9cfddbf597c121](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/a84358aa12f0b1518e606095ab9cfddbf597c121).\n\nThe fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by Ying Wang and Yakun Zhang of Baidu X-Team.",
      "published_date":"2021-05-21",
      "chain_len":1,
      "project":"https:\/\/github.com\/tensorflow\/tensorflow",
      "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/a84358aa12f0b1518e606095ab9cfddbf597c121",
      "commit_sha":"a84358aa12f0b1518e606095ab9cfddbf597c121",
      "patch":"SINGLE",
      "chain_ord":"['a84358aa12f0b1518e606095ab9cfddbf597c121']",
      "before_first_fix_commit":"{'d94227d43aa125ad8b54115c03cece54f6a1977b'}",
      "last_fix_commit":"a84358aa12f0b1518e606095ab9cfddbf597c121",
      "chain_ord_pos":1.0,
      "commit_datetime":"05\/04\/2021, 20:45:57",
      "message":"Fix heap-buffer-overflow issue with `tf.raw_ops.RaggedTensorToTensor`.\n\nPiperOrigin-RevId: 371986929\nChange-Id: I79ab962a22c5867f36f7f45b780a1ac881b1dbdd",
      "author":"Amit Patankar",
      "comments":null,
      "stats":"{'additions': 6, 'deletions': 0, 'total': 6}",
      "files":"{'tensorflow\/core\/kernels\/ragged_tensor_to_tensor_op.cc': {'additions': 6, 'deletions': 0, 'changes': 6, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/a84358aa12f0b1518e606095ab9cfddbf597c121\/tensorflow%2Fcore%2Fkernels%2Fragged_tensor_to_tensor_op.cc', 'patch': '@@ -313,6 +313,12 @@ class RaggedTensorToTensorBaseOp : public OpKernel {\\n             output_index_multiplier, output_size, result);\\n         return tensorflow::Status::OK();\\n       case RowPartitionType::ROW_SPLITS:\\n+        if (row_partition_tensor.size() - 1 > parent_output_index.size()) {\\n+          return errors::InvalidArgument(\\n+              \"Row partition size is greater than output size: \",\\n+              row_partition_tensor.size() - 1, \" > \",\\n+              parent_output_index.size());\\n+        }\\n         CalculateOutputIndexRowSplit(\\n             context, row_partition_tensor, parent_output_index,\\n             output_index_multiplier, output_size, result);'}}",
      "message_norm":"fix heap-buffer-overflow issue with `tf.raw_ops.raggedtensortotensor`.\n\npiperorigin-revid: 371986929\nchange-id: i79ab962a22c5867f36f7f45b780a1ac881b1dbdd",
      "language":"en",
      "entities":"[('fix', 'ACTION', ''), ('overflow', 'SECWORD', ''), ('issue', 'FLAW', ''), ('371986929', 'SHA', 'generic_sha')]",
      "classification_level_1":null,
      "classification_level_2":null,
      "list_files":"dict_keys(['tensorflow\/core\/kernels\/ragged_tensor_to_tensor_op.cc'])",
      "num_files":1.0,
      "patch_content":"From a84358aa12f0b1518e606095ab9cfddbf597c121 Mon Sep 17 00:00:00 2001\nFrom: Amit Patankar <amitpatankar@google.com>\nDate: Tue, 4 May 2021 13:45:57 -0700\nSubject: [PATCH] Fix heap-buffer-overflow issue with\n `tf.raw_ops.RaggedTensorToTensor`.\n\nPiperOrigin-RevId: 371986929\nChange-Id: I79ab962a22c5867f36f7f45b780a1ac881b1dbdd\n---\n tensorflow\/core\/kernels\/ragged_tensor_to_tensor_op.cc | 6 ++++++\n 1 file changed, 6 insertions(+)\n\ndiff --git a\/tensorflow\/core\/kernels\/ragged_tensor_to_tensor_op.cc b\/tensorflow\/core\/kernels\/ragged_tensor_to_tensor_op.cc\nindex 434c853b63daa4..376d55945d2ce8 100644\n--- a\/tensorflow\/core\/kernels\/ragged_tensor_to_tensor_op.cc\n+++ b\/tensorflow\/core\/kernels\/ragged_tensor_to_tensor_op.cc\n@@ -313,6 +313,12 @@ class RaggedTensorToTensorBaseOp : public OpKernel {\n             output_index_multiplier, output_size, result);\n         return tensorflow::Status::OK();\n       case RowPartitionType::ROW_SPLITS:\n+        if (row_partition_tensor.size() - 1 > parent_output_index.size()) {\n+          return errors::InvalidArgument(\n+              \"Row partition size is greater than output size: \",\n+              row_partition_tensor.size() - 1, \" > \",\n+              parent_output_index.size());\n+        }\n         CalculateOutputIndexRowSplit(\n             context, row_partition_tensor, parent_output_index,\n             output_index_multiplier, output_size, result);",
      "code_diff":"@@ -313,6 +313,12 @@ class RaggedTensorToTensorBaseOp : public OpKernel {\n             output_index_multiplier, output_size, result);\n         return tensorflow::Status::OK();\n       case RowPartitionType::ROW_SPLITS:\n+        if (row_partition_tensor.size() - 1 > parent_output_index.size()) {\n+          return errors::InvalidArgument(\n+              \"Row partition size is greater than output size: \",\n+              row_partition_tensor.size() - 1, \" > \",\n+              parent_output_index.size());\n+        }\n         CalculateOutputIndexRowSplit(\n             context, row_partition_tensor, parent_output_index,\n             output_index_multiplier, output_size, result);"
    },
    {
      "index":38,
      "vuln_id":"GHSA-hpv4-7p9c-mvfr",
      "cwe_id":"{'CWE-787', 'CWE-125'}",
      "score":7.1,
      "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/0f931751fb20f565c4e94aa6df58d54a003cdb30'}",
      "dataset":"osv",
      "summary":"Heap buffer overflow in `FractionalAvgPoolGrad` ### Impact\nThe implementation for `tf.raw_ops.FractionalAvgPoolGrad` can be tricked into accessing data outside of bounds of heap allocated buffers:\n\n```python\nimport tensorflow as tf\n\ntf.raw_ops.FractionalAvgPoolGrad(\n  orig_input_tensor_shape=[0,1,2,3],\n  out_backprop = np.array([[[[541],[541]],[[541],[541]]]]),\n  row_pooling_sequence=[0, 0, 0, 0, 0],\n  col_pooling_sequence=[-2, 0, 0, 2, 0],\n  overlapping=True)\n```\n\nThe [implementation](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/f24faa153ad31a4b51578f8181d3aaab77a1ddeb\/tensorflow\/core\/kernels\/fractional_avg_pool_op.cc#L205) does not validate that the input tensor is non-empty. Thus, code constructs an empty `EigenDoubleMatrixMap` and then accesses this buffer with indices that are outside of the empty area.\n\n### Patches\nWe have patched the issue in GitHub commit [0f931751fb20f565c4e94aa6df58d54a003cdb30](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/0f931751fb20f565c4e94aa6df58d54a003cdb30).\n\nThe fix will be included in TensorFlow 2.6.0. We will also cherrypick this commit on TensorFlow 2.5.1, TensorFlow 2.4.3, and TensorFlow 2.3.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by members of the Aivul Team from Qihoo 360.",
      "published_date":"2021-08-25",
      "chain_len":1,
      "project":"https:\/\/github.com\/tensorflow\/tensorflow",
      "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/0f931751fb20f565c4e94aa6df58d54a003cdb30",
      "commit_sha":"0f931751fb20f565c4e94aa6df58d54a003cdb30",
      "patch":"SINGLE",
      "chain_ord":"['0f931751fb20f565c4e94aa6df58d54a003cdb30']",
      "before_first_fix_commit":"{'55e763ffe2b348a61ab1c2fcfedc7bdf05c91990'}",
      "last_fix_commit":"0f931751fb20f565c4e94aa6df58d54a003cdb30",
      "chain_ord_pos":1.0,
      "commit_datetime":"08\/02\/2021, 20:03:44",
      "message":"Validate dimensions of input tensor in `FractionalAvgPoolGrad`\n\nPiperOrigin-RevId: 388286227\nChange-Id: Ieb7566155e92acc8993a2212c76deacadc0edc8a",
      "author":"Mihai Maruseac",
      "comments":null,
      "stats":"{'additions': 12, 'deletions': 0, 'total': 12}",
      "files":"{'tensorflow\/core\/kernels\/fractional_avg_pool_op.cc': {'additions': 12, 'deletions': 0, 'changes': 12, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/0f931751fb20f565c4e94aa6df58d54a003cdb30\/tensorflow%2Fcore%2Fkernels%2Ffractional_avg_pool_op.cc', 'patch': '@@ -271,6 +271,18 @@ class FractionalAvgPoolGradOp : public OpKernel {\\n     const int64_t in_rows = orig_input_tensor_shape_flat(1);\\n     const int64_t in_cols = orig_input_tensor_shape_flat(2);\\n     const int64_t in_depth = orig_input_tensor_shape_flat(3);\\n+    OP_REQUIRES(\\n+        context, in_batch != 0,\\n+        errors::InvalidArgument(\"Batch dimension of input must not be 0\"));\\n+    OP_REQUIRES(\\n+        context, in_rows != 0,\\n+        errors::InvalidArgument(\"Rows dimension of input must not be 0\"));\\n+    OP_REQUIRES(\\n+        context, in_cols != 0,\\n+        errors::InvalidArgument(\"Columns dimension of input must not be 0\"));\\n+    OP_REQUIRES(\\n+        context, in_depth != 0,\\n+        errors::InvalidArgument(\"Depth dimension of input must not be 0\"));\\n \\n     constexpr int tensor_in_and_out_dims = 4;\\n     \/\/ Transform orig_input_tensor_shape into TensorShape'}}",
      "message_norm":"validate dimensions of input tensor in `fractionalavgpoolgrad`\n\npiperorigin-revid: 388286227\nchange-id: ieb7566155e92acc8993a2212c76deacadc0edc8a",
      "language":"en",
      "entities":"[('validate', 'ACTION', ''), ('388286227', 'SHA', 'generic_sha')]",
      "classification_level_1":null,
      "classification_level_2":null,
      "list_files":"dict_keys(['tensorflow\/core\/kernels\/fractional_avg_pool_op.cc'])",
      "num_files":1.0,
      "patch_content":"From 0f931751fb20f565c4e94aa6df58d54a003cdb30 Mon Sep 17 00:00:00 2001\nFrom: Mihai Maruseac <mihaimaruseac@google.com>\nDate: Mon, 2 Aug 2021 13:03:44 -0700\nSubject: [PATCH] Validate dimensions of input tensor in\n `FractionalAvgPoolGrad`\n\nPiperOrigin-RevId: 388286227\nChange-Id: Ieb7566155e92acc8993a2212c76deacadc0edc8a\n---\n tensorflow\/core\/kernels\/fractional_avg_pool_op.cc | 12 ++++++++++++\n 1 file changed, 12 insertions(+)\n\ndiff --git a\/tensorflow\/core\/kernels\/fractional_avg_pool_op.cc b\/tensorflow\/core\/kernels\/fractional_avg_pool_op.cc\nindex 3c80e87bcf76dc..63f8d67d93cc47 100644\n--- a\/tensorflow\/core\/kernels\/fractional_avg_pool_op.cc\n+++ b\/tensorflow\/core\/kernels\/fractional_avg_pool_op.cc\n@@ -271,6 +271,18 @@ class FractionalAvgPoolGradOp : public OpKernel {\n     const int64_t in_rows = orig_input_tensor_shape_flat(1);\n     const int64_t in_cols = orig_input_tensor_shape_flat(2);\n     const int64_t in_depth = orig_input_tensor_shape_flat(3);\n+    OP_REQUIRES(\n+        context, in_batch != 0,\n+        errors::InvalidArgument(\"Batch dimension of input must not be 0\"));\n+    OP_REQUIRES(\n+        context, in_rows != 0,\n+        errors::InvalidArgument(\"Rows dimension of input must not be 0\"));\n+    OP_REQUIRES(\n+        context, in_cols != 0,\n+        errors::InvalidArgument(\"Columns dimension of input must not be 0\"));\n+    OP_REQUIRES(\n+        context, in_depth != 0,\n+        errors::InvalidArgument(\"Depth dimension of input must not be 0\"));\n \n     constexpr int tensor_in_and_out_dims = 4;\n     \/\/ Transform orig_input_tensor_shape into TensorShape",
      "code_diff":"@@ -271,6 +271,18 @@ class FractionalAvgPoolGradOp : public OpKernel {\n     const int64_t in_rows = orig_input_tensor_shape_flat(1);\n     const int64_t in_cols = orig_input_tensor_shape_flat(2);\n     const int64_t in_depth = orig_input_tensor_shape_flat(3);\n+    OP_REQUIRES(\n+        context, in_batch != 0,\n+        errors::InvalidArgument(\"Batch dimension of input must not be 0\"));\n+    OP_REQUIRES(\n+        context, in_rows != 0,\n+        errors::InvalidArgument(\"Rows dimension of input must not be 0\"));\n+    OP_REQUIRES(\n+        context, in_cols != 0,\n+        errors::InvalidArgument(\"Columns dimension of input must not be 0\"));\n+    OP_REQUIRES(\n+        context, in_depth != 0,\n+        errors::InvalidArgument(\"Depth dimension of input must not be 0\"));\n \n     constexpr int tensor_in_and_out_dims = 4;\n     \/\/ Transform orig_input_tensor_shape into TensorShape"
    },
    {
      "index":39,
      "vuln_id":"GHSA-7cqx-92hp-x6wh",
      "cwe_id":"{'CWE-787', 'CWE-119'}",
      "score":2.5,
      "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/63c6a29d0f2d692b247f7bf81f8732d6442fad09'}",
      "dataset":"osv",
      "summary":"Heap buffer overflow in `MaxPool3DGradGrad` ### Impact\nThe implementation of `tf.raw_ops.MaxPool3DGradGrad` is vulnerable to a heap buffer overflow: \n\n```python\nimport tensorflow as tf\n\nvalues = [0.01] * 11\norig_input = tf.constant(values, shape=[11, 1, 1, 1, 1], dtype=tf.float32)\norig_output = tf.constant([0.01], shape=[1, 1, 1, 1, 1], dtype=tf.float32)\ngrad = tf.constant([0.01], shape=[1, 1, 1, 1, 1], dtype=tf.float32)\nksize = [1, 1, 1, 1, 1]\nstrides = [1, 1, 1, 1, 1]\npadding = \"SAME\"\n\ntf.raw_ops.MaxPool3DGradGrad(\n    orig_input=orig_input, orig_output=orig_output, grad=grad, ksize=ksize,\n    strides=strides, padding=padding)\n```\n\nThe [implementation](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/596c05a159b6fbb9e39ca10b3f7753b7244fa1e9\/tensorflow\/core\/kernels\/pooling_ops_3d.cc#L694-L696) does not check that the initialization of `Pool3dParameters` completes successfully:\n\n```cc\nPool3dParameters params{context,  ksize_,       stride_,\n                        padding_, data_format_, tensor_in.shape()};\n```\n\nSince [the constructor](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/596c05a159b6fbb9e39ca10b3f7753b7244fa1e9\/tensorflow\/core\/kernels\/pooling_ops_3d.cc#L48-L88) uses `OP_REQUIRES` to validate conditions, the first assertion that fails interrupts the initialization of `params`, making it contain invalid data. In turn, this might cause a heap buffer overflow, depending on default initialized values.\n\n### Patches\nWe have patched the issue in GitHub commit [63c6a29d0f2d692b247f7bf81f8732d6442fad09](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/63c6a29d0f2d692b247f7bf81f8732d6442fad09).\n\nThe fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by Ying Wang and Yakun Zhang of Baidu X-Team.",
      "published_date":"2021-05-21",
      "chain_len":1,
      "project":"https:\/\/github.com\/tensorflow\/tensorflow",
      "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/63c6a29d0f2d692b247f7bf81f8732d6442fad09",
      "commit_sha":"63c6a29d0f2d692b247f7bf81f8732d6442fad09",
      "patch":"SINGLE",
      "chain_ord":"['63c6a29d0f2d692b247f7bf81f8732d6442fad09']",
      "before_first_fix_commit":"{'596c05a159b6fbb9e39ca10b3f7753b7244fa1e9'}",
      "last_fix_commit":"63c6a29d0f2d692b247f7bf81f8732d6442fad09",
      "chain_ord_pos":1.0,
      "commit_datetime":"05\/06\/2021, 01:07:02",
      "message":"Add missing validation, prevent heap OOB\n\nPiperOrigin-RevId: 372246723\nChange-Id: I1a454a643810e77d7d14821b342098c56a09fbbf",
      "author":"Mihai Maruseac",
      "comments":null,
      "stats":"{'additions': 12, 'deletions': 0, 'total': 12}",
      "files":"{'tensorflow\/core\/kernels\/pooling_ops_3d.cc': {'additions': 12, 'deletions': 0, 'changes': 12, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/63c6a29d0f2d692b247f7bf81f8732d6442fad09\/tensorflow%2Fcore%2Fkernels%2Fpooling_ops_3d.cc', 'patch': '@@ -693,6 +693,7 @@ class MaxPooling3dGradGradOp : public OpKernel {\\n \\n     Pool3dParameters params{context,  ksize_,       stride_,\\n                             padding_, data_format_, tensor_in.shape()};\\n+    if (!context->status().ok()) return;  \/\/ params is invalid\\n \\n     Tensor* output = nullptr;\\n     OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\\n@@ -710,6 +711,17 @@ class MaxPooling3dGradGradOp : public OpKernel {\\n         context, out_grad_backprop.NumElements() > 0,\\n         errors::InvalidArgument(\"received empty tensor out_grad_backprop: \",\\n                                 out_grad_backprop.DebugString()));\\n+    OP_REQUIRES(context,\\n+                tensor_in.NumElements() == out_grad_backprop.NumElements(),\\n+                errors::InvalidArgument(\"tensor_in and out_grad_backprop must \"\\n+                                        \"have same number of elements, got <\",\\n+                                        tensor_in.DebugString(), \"> and <\",\\n+                                        out_grad_backprop.DebugString(), \">\"));\\n+    OP_REQUIRES(\\n+        context, tensor_out.NumElements() == output->NumElements(),\\n+        errors::InvalidArgument(\\n+            \"tensor_out and output must have same number of elements, got <\",\\n+            tensor_out.DebugString(), \"> and <\", output->DebugString(), \">\"));\\n \\n     LaunchMaxPooling3dGradGradOp<Device, T>::launch(\\n         context, params, tensor_in, tensor_out, out_grad_backprop, output);'}}",
      "message_norm":"add missing validation, prevent heap oob\n\npiperorigin-revid: 372246723\nchange-id: i1a454a643810e77d7d14821b342098c56a09fbbf",
      "language":"en",
      "entities":"[('add', 'ACTION', ''), ('missing validation', 'SECWORD', ''), ('prevent', 'ACTION', ''), ('heap oob', 'SECWORD', ''), ('372246723', 'SHA', 'generic_sha')]",
      "classification_level_1":null,
      "classification_level_2":null,
      "list_files":"dict_keys(['tensorflow\/core\/kernels\/pooling_ops_3d.cc'])",
      "num_files":1.0,
      "patch_content":"From 63c6a29d0f2d692b247f7bf81f8732d6442fad09 Mon Sep 17 00:00:00 2001\nFrom: Mihai Maruseac <mihaimaruseac@google.com>\nDate: Wed, 5 May 2021 18:07:02 -0700\nSubject: [PATCH] Add missing validation, prevent heap OOB\n\nPiperOrigin-RevId: 372246723\nChange-Id: I1a454a643810e77d7d14821b342098c56a09fbbf\n---\n tensorflow\/core\/kernels\/pooling_ops_3d.cc | 12 ++++++++++++\n 1 file changed, 12 insertions(+)\n\ndiff --git a\/tensorflow\/core\/kernels\/pooling_ops_3d.cc b\/tensorflow\/core\/kernels\/pooling_ops_3d.cc\nindex 7d133b66a1ebdf..9da2d62b0a21d3 100644\n--- a\/tensorflow\/core\/kernels\/pooling_ops_3d.cc\n+++ b\/tensorflow\/core\/kernels\/pooling_ops_3d.cc\n@@ -693,6 +693,7 @@ class MaxPooling3dGradGradOp : public OpKernel {\n \n     Pool3dParameters params{context,  ksize_,       stride_,\n                             padding_, data_format_, tensor_in.shape()};\n+    if (!context->status().ok()) return;  \/\/ params is invalid\n \n     Tensor* output = nullptr;\n     OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n@@ -710,6 +711,17 @@ class MaxPooling3dGradGradOp : public OpKernel {\n         context, out_grad_backprop.NumElements() > 0,\n         errors::InvalidArgument(\"received empty tensor out_grad_backprop: \",\n                                 out_grad_backprop.DebugString()));\n+    OP_REQUIRES(context,\n+                tensor_in.NumElements() == out_grad_backprop.NumElements(),\n+                errors::InvalidArgument(\"tensor_in and out_grad_backprop must \"\n+                                        \"have same number of elements, got <\",\n+                                        tensor_in.DebugString(), \"> and <\",\n+                                        out_grad_backprop.DebugString(), \">\"));\n+    OP_REQUIRES(\n+        context, tensor_out.NumElements() == output->NumElements(),\n+        errors::InvalidArgument(\n+            \"tensor_out and output must have same number of elements, got <\",\n+            tensor_out.DebugString(), \"> and <\", output->DebugString(), \">\"));\n \n     LaunchMaxPooling3dGradGradOp<Device, T>::launch(\n         context, params, tensor_in, tensor_out, out_grad_backprop, output);",
      "code_diff":"@@ -693,6 +693,7 @@ class MaxPooling3dGradGradOp : public OpKernel {\n \n     Pool3dParameters params{context,  ksize_,       stride_,\n                             padding_, data_format_, tensor_in.shape()};\n+    if (!context->status().ok()) return;  \/\/ params is invalid\n \n     Tensor* output = nullptr;\n     OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n@@ -710,6 +711,17 @@ class MaxPooling3dGradGradOp : public OpKernel {\n         context, out_grad_backprop.NumElements() > 0,\n         errors::InvalidArgument(\"received empty tensor out_grad_backprop: \",\n                                 out_grad_backprop.DebugString()));\n+    OP_REQUIRES(context,\n+                tensor_in.NumElements() == out_grad_backprop.NumElements(),\n+                errors::InvalidArgument(\"tensor_in and out_grad_backprop must \"\n+                                        \"have same number of elements, got <\",\n+                                        tensor_in.DebugString(), \"> and <\",\n+                                        out_grad_backprop.DebugString(), \">\"));\n+    OP_REQUIRES(\n+        context, tensor_out.NumElements() == output->NumElements(),\n+        errors::InvalidArgument(\n+            \"tensor_out and output must have same number of elements, got <\",\n+            tensor_out.DebugString(), \"> and <\", output->DebugString(), \">\"));\n \n     LaunchMaxPooling3dGradGradOp<Device, T>::launch(\n         context, params, tensor_in, tensor_out, out_grad_backprop, output);"
    },
    {
      "index":40,
      "vuln_id":"GHSA-w89r-qch4-8jv5",
      "cwe_id":"{'CWE-787'}",
      "score":7.5,
      "chain":"{'https:\/\/github.com\/chakra-core\/ChakraCore\/commit\/d797e3f00e34c12c8c0ae52f56344325439dccd7', 'https:\/\/github.com\/chakra-core\/ChakraCore\/commit\/936a5af1c07e0fdec9aab85c05339dabe4aaeeb1'}",
      "dataset":"osv",
      "summary":"Out-of-bounds write A remote code execution vulnerability exists in the way that the Chakra scripting engine handles objects in memory in Microsoft Edge, aka 'Chakra Scripting Engine Memory Corruption Vulnerability'. This CVE ID is unique from CVE-2019-0913, CVE-2019-0914, CVE-2019-0915, CVE-2019-0916, CVE-2019-0917, CVE-2019-0922, CVE-2019-0923, CVE-2019-0924, CVE-2019-0925, CVE-2019-0927, CVE-2019-0933, CVE-2019-0937.",
      "published_date":"2021-03-29",
      "chain_len":2,
      "project":"https:\/\/github.com\/chakra-core\/ChakraCore",
      "commit_href":"https:\/\/github.com\/chakra-core\/ChakraCore\/commit\/936a5af1c07e0fdec9aab85c05339dabe4aaeeb1",
      "commit_sha":"936a5af1c07e0fdec9aab85c05339dabe4aaeeb1",
      "patch":"MULTI",
      "chain_ord":"['936a5af1c07e0fdec9aab85c05339dabe4aaeeb1', 'd797e3f00e34c12c8c0ae52f56344325439dccd7']",
      "before_first_fix_commit":"{'ea0491305137183603bf43844b5584d4cc972e28', '4594e340bc9ca9f857010a68e8b562d65b46eed6'}",
      "last_fix_commit":"d797e3f00e34c12c8c0ae52f56344325439dccd7",
      "chain_ord_pos":1.0,
      "commit_datetime":"04\/17\/2019, 17:18:03",
      "message":"[CVE-2019-0912]",
      "author":"Michael Holman",
      "comments":null,
      "stats":"{'additions': 24, 'deletions': 7, 'total': 31}",
      "files":"{'lib\/Backend\/GlobOptFields.cpp': {'additions': 24, 'deletions': 7, 'changes': 31, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/chakra-core\/ChakraCore\/raw\/936a5af1c07e0fdec9aab85c05339dabe4aaeeb1\/lib%2FBackend%2FGlobOptFields.cpp', 'patch': '@@ -415,11 +415,19 @@ GlobOpt::ProcessFieldKills(IR::Instr *instr, BVSparse<JitArenaAllocator> *bv, bo\\n \\n     case Js::OpCode::InlineArrayPush:\\n     case Js::OpCode::InlineArrayPop:\\n-        KillLiveFields(this->lengthEquivBv, bv);\\n-        if (inGlobOpt)\\n+        if(instr->m_func->GetThisOrParentInlinerHasArguments())\\n         {\\n-            \/\/ Deleting an item, or pushing a property to a non-array, may change object layout\\n-            KillAllObjectTypes(bv);\\n+            this->KillAllFields(bv);\\n+            this->SetAnyPropertyMayBeWrittenTo();\\n+        }\\n+        else\\n+        {\\n+            KillLiveFields(this->lengthEquivBv, bv);\\n+            if (inGlobOpt)\\n+            {\\n+                \/\/ Deleting an item, or pushing a property to a non-array, may change object layout\\n+                KillAllObjectTypes(bv);\\n+            }\\n         }\\n         break;\\n \\n@@ -444,14 +452,23 @@ GlobOpt::ProcessFieldKills(IR::Instr *instr, BVSparse<JitArenaAllocator> *bv, bo\\n                 \/\/ Kill length field for built-ins that can update it.\\n                 if (nullptr != this->lengthEquivBv)\\n                 {\\n-                    KillLiveFields(this->lengthEquivBv, bv);\\n+                    \/\/ If has arguments, all fields are killed in fall through\\n+                    if (!instr->m_func->GetThisOrParentInlinerHasArguments())\\n+                    {\\n+                        KillLiveFields(this->lengthEquivBv, bv);\\n+                    }\\n                 }\\n                 \/\/ fall through\\n \\n             case IR::JnHelperMethod::HelperArray_Reverse:\\n-                \/\/ Deleting an item may change object layout\\n-                if (inGlobOpt)\\n+                if (instr->m_func->GetThisOrParentInlinerHasArguments())\\n+                {\\n+                    this->KillAllFields(bv);\\n+                    this->SetAnyPropertyMayBeWrittenTo();\\n+                }\\n+                else if (inGlobOpt)\\n                 {\\n+                    \/\/ Deleting an item may change object layout\\n                     KillAllObjectTypes(bv);\\n                 }\\n                 break;'}}",
      "message_norm":"[cve-2019-0912]",
      "language":"ro",
      "entities":"[('cve-2019-0912', 'VULNID', 'CVE')]",
      "classification_level_1":null,
      "classification_level_2":null,
      "list_files":"dict_keys(['lib\/Backend\/GlobOptFields.cpp'])",
      "num_files":1.0,
      "patch_content":"From 936a5af1c07e0fdec9aab85c05339dabe4aaeeb1 Mon Sep 17 00:00:00 2001\nFrom: Michael Holman <michhol@microsoft.com>\nDate: Wed, 17 Apr 2019 10:18:03 -0700\nSubject: [PATCH] [CVE-2019-0912]\n\n---\n lib\/Backend\/GlobOptFields.cpp | 31 ++++++++++++++++++++++++-------\n 1 file changed, 24 insertions(+), 7 deletions(-)\n\ndiff --git a\/lib\/Backend\/GlobOptFields.cpp b\/lib\/Backend\/GlobOptFields.cpp\nindex c355563523d..2eff8b7c61f 100644\n--- a\/lib\/Backend\/GlobOptFields.cpp\n+++ b\/lib\/Backend\/GlobOptFields.cpp\n@@ -415,11 +415,19 @@ GlobOpt::ProcessFieldKills(IR::Instr *instr, BVSparse<JitArenaAllocator> *bv, bo\n \n     case Js::OpCode::InlineArrayPush:\n     case Js::OpCode::InlineArrayPop:\n-        KillLiveFields(this->lengthEquivBv, bv);\n-        if (inGlobOpt)\n+        if(instr->m_func->GetThisOrParentInlinerHasArguments())\n         {\n-            \/\/ Deleting an item, or pushing a property to a non-array, may change object layout\n-            KillAllObjectTypes(bv);\n+            this->KillAllFields(bv);\n+            this->SetAnyPropertyMayBeWrittenTo();\n+        }\n+        else\n+        {\n+            KillLiveFields(this->lengthEquivBv, bv);\n+            if (inGlobOpt)\n+            {\n+                \/\/ Deleting an item, or pushing a property to a non-array, may change object layout\n+                KillAllObjectTypes(bv);\n+            }\n         }\n         break;\n \n@@ -444,14 +452,23 @@ GlobOpt::ProcessFieldKills(IR::Instr *instr, BVSparse<JitArenaAllocator> *bv, bo\n                 \/\/ Kill length field for built-ins that can update it.\n                 if (nullptr != this->lengthEquivBv)\n                 {\n-                    KillLiveFields(this->lengthEquivBv, bv);\n+                    \/\/ If has arguments, all fields are killed in fall through\n+                    if (!instr->m_func->GetThisOrParentInlinerHasArguments())\n+                    {\n+                        KillLiveFields(this->lengthEquivBv, bv);\n+                    }\n                 }\n                 \/\/ fall through\n \n             case IR::JnHelperMethod::HelperArray_Reverse:\n-                \/\/ Deleting an item may change object layout\n-                if (inGlobOpt)\n+                if (instr->m_func->GetThisOrParentInlinerHasArguments())\n+                {\n+                    this->KillAllFields(bv);\n+                    this->SetAnyPropertyMayBeWrittenTo();\n+                }\n+                else if (inGlobOpt)\n                 {\n+                    \/\/ Deleting an item may change object layout\n                     KillAllObjectTypes(bv);\n                 }\n                 break;",
      "code_diff":"@@ -415,11 +415,19 @@ GlobOpt::ProcessFieldKills(IR::Instr *instr, BVSparse<JitArenaAllocator> *bv, bo\n \n     case Js::OpCode::InlineArrayPush:\n     case Js::OpCode::InlineArrayPop:\n-        KillLiveFields(this->lengthEquivBv, bv);\n-        if (inGlobOpt)\n+        if(instr->m_func->GetThisOrParentInlinerHasArguments())\n         {\n-            \/\/ Deleting an item, or pushing a property to a non-array, may change object layout\n-            KillAllObjectTypes(bv);\n+            this->KillAllFields(bv);\n+            this->SetAnyPropertyMayBeWrittenTo();\n+        }\n+        else\n+        {\n+            KillLiveFields(this->lengthEquivBv, bv);\n+            if (inGlobOpt)\n+            {\n+                \/\/ Deleting an item, or pushing a property to a non-array, may change object layout\n+                KillAllObjectTypes(bv);\n+            }\n         }\n         break;\n \n@@ -444,14 +452,23 @@ GlobOpt::ProcessFieldKills(IR::Instr *instr, BVSparse<JitArenaAllocator> *bv, bo\n                 \/\/ Kill length field for built-ins that can update it.\n                 if (nullptr != this->lengthEquivBv)\n                 {\n-                    KillLiveFields(this->lengthEquivBv, bv);\n+                    \/\/ If has arguments, all fields are killed in fall through\n+                    if (!instr->m_func->GetThisOrParentInlinerHasArguments())\n+                    {\n+                        KillLiveFields(this->lengthEquivBv, bv);\n+                    }\n                 }\n                 \/\/ fall through\n \n             case IR::JnHelperMethod::HelperArray_Reverse:\n-                \/\/ Deleting an item may change object layout\n-                if (inGlobOpt)\n+                if (instr->m_func->GetThisOrParentInlinerHasArguments())\n+                {\n+                    this->KillAllFields(bv);\n+                    this->SetAnyPropertyMayBeWrittenTo();\n+                }\n+                else if (inGlobOpt)\n                 {\n+                    \/\/ Deleting an item may change object layout\n                     KillAllObjectTypes(bv);\n                 }\n                 break;"
    },
    {
      "index":41,
      "vuln_id":"GHSA-8c89-2vwr-chcq",
      "cwe_id":"{'CWE-787', 'CWE-131'}",
      "score":2.5,
      "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/f6c40f0c6cbf00d46c7717a26419f2062f2f8694'}",
      "dataset":"osv",
      "summary":"Heap buffer overflow in `QuantizedResizeBilinear` ### Impact\nAn attacker can cause a heap buffer overflow in `QuantizedResizeBilinear` by passing in invalid thresholds for the quantization:\n\n```python\nimport tensorflow as tf\n\nimages = tf.constant([], shape=[0], dtype=tf.qint32)\nsize = tf.constant([], shape=[0], dtype=tf.int32) \nmin = tf.constant([], dtype=tf.float32)\nmax = tf.constant([], dtype=tf.float32)\n\ntf.raw_ops.QuantizedResizeBilinear(images=images, size=size, min=min, max=max, align_corners=False, half_pixel_centers=False)\n```\n\nThis is because the [implementation](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/50711818d2e61ccce012591eeb4fdf93a8496726\/tensorflow\/core\/kernels\/quantized_resize_bilinear_op.cc#L705-L706) assumes that the 2 arguments are always valid scalars and tries to access the numeric value directly:\n\n```cc\nconst float in_min = context->input(2).flat<float>()(0);\nconst float in_max = context->input(3).flat<float>()(0);\n```\n\nHowever, if any of these tensors is empty, then `.flat<T>()` is an empty buffer and accessing the element at position 0 results in overflow.\n\n### Patches \nWe have patched the issue in GitHub commit [f6c40f0c6cbf00d46c7717a26419f2062f2f8694](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/f6c40f0c6cbf00d46c7717a26419f2062f2f8694).\n\nThe fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by Ying Wang and Yakun Zhang of Baidu X-Team.",
      "published_date":"2021-05-21",
      "chain_len":1,
      "project":"https:\/\/github.com\/tensorflow\/tensorflow",
      "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/f6c40f0c6cbf00d46c7717a26419f2062f2f8694",
      "commit_sha":"f6c40f0c6cbf00d46c7717a26419f2062f2f8694",
      "patch":"SINGLE",
      "chain_ord":"['f6c40f0c6cbf00d46c7717a26419f2062f2f8694']",
      "before_first_fix_commit":"{'50711818d2e61ccce012591eeb4fdf93a8496726'}",
      "last_fix_commit":"f6c40f0c6cbf00d46c7717a26419f2062f2f8694",
      "chain_ord_pos":1.0,
      "commit_datetime":"04\/22\/2021, 00:00:39",
      "message":"Validate min and max arguments to `QuantizedResizeBilinear`.\n\nPiperOrigin-RevId: 369765091\nChange-Id: I33be8b78273ab7d08b97541692fe05cb7f94963a",
      "author":"Mihai Maruseac",
      "comments":null,
      "stats":"{'additions': 8, 'deletions': 2, 'total': 10}",
      "files":"{'tensorflow\/core\/kernels\/quantized_resize_bilinear_op.cc': {'additions': 8, 'deletions': 2, 'changes': 10, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/f6c40f0c6cbf00d46c7717a26419f2062f2f8694\/tensorflow%2Fcore%2Fkernels%2Fquantized_resize_bilinear_op.cc', 'patch': '@@ -702,8 +702,14 @@ class QuantizedResizeBilinearOp : public OpKernel {\\n   }\\n \\n   void Compute(OpKernelContext* context) override {\\n-    const float in_min = context->input(2).flat<float>()(0);\\n-    const float in_max = context->input(3).flat<float>()(0);\\n+    const auto& in_min_tensor = context->input(2);\\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(in_min_tensor.shape()),\\n+                errors::InvalidArgument(\"min must be a scalar\"));\\n+    const float in_min = in_min_tensor.flat<float>()(0);\\n+    const auto& in_max_tensor = context->input(3);\\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(in_max_tensor.shape()),\\n+                errors::InvalidArgument(\"max must be a scalar\"));\\n+    const float in_max = in_max_tensor.flat<float>()(0);\\n \\n     ImageResizerState st(align_corners_, false);\\n     st.ValidateAndCreateOutput(context);'}}",
      "message_norm":"validate min and max arguments to `quantizedresizebilinear`.\n\npiperorigin-revid: 369765091\nchange-id: i33be8b78273ab7d08b97541692fe05cb7f94963a",
      "language":"en",
      "entities":"[('validate', 'ACTION', ''), ('369765091', 'SHA', 'generic_sha')]",
      "classification_level_1":null,
      "classification_level_2":null,
      "list_files":"dict_keys(['tensorflow\/core\/kernels\/quantized_resize_bilinear_op.cc'])",
      "num_files":1.0,
      "patch_content":"From f6c40f0c6cbf00d46c7717a26419f2062f2f8694 Mon Sep 17 00:00:00 2001\nFrom: Mihai Maruseac <mihaimaruseac@google.com>\nDate: Wed, 21 Apr 2021 17:00:39 -0700\nSubject: [PATCH] Validate min and max arguments to `QuantizedResizeBilinear`.\n\nPiperOrigin-RevId: 369765091\nChange-Id: I33be8b78273ab7d08b97541692fe05cb7f94963a\n---\n ...\/core\/kernels\/quantized_resize_bilinear_op.cc       | 10 ++++++++--\n 1 file changed, 8 insertions(+), 2 deletions(-)\n\ndiff --git a\/tensorflow\/core\/kernels\/quantized_resize_bilinear_op.cc b\/tensorflow\/core\/kernels\/quantized_resize_bilinear_op.cc\nindex 2fd807f6df9614..a110944dda5844 100644\n--- a\/tensorflow\/core\/kernels\/quantized_resize_bilinear_op.cc\n+++ b\/tensorflow\/core\/kernels\/quantized_resize_bilinear_op.cc\n@@ -702,8 +702,14 @@ class QuantizedResizeBilinearOp : public OpKernel {\n   }\n \n   void Compute(OpKernelContext* context) override {\n-    const float in_min = context->input(2).flat<float>()(0);\n-    const float in_max = context->input(3).flat<float>()(0);\n+    const auto& in_min_tensor = context->input(2);\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(in_min_tensor.shape()),\n+                errors::InvalidArgument(\"min must be a scalar\"));\n+    const float in_min = in_min_tensor.flat<float>()(0);\n+    const auto& in_max_tensor = context->input(3);\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(in_max_tensor.shape()),\n+                errors::InvalidArgument(\"max must be a scalar\"));\n+    const float in_max = in_max_tensor.flat<float>()(0);\n \n     ImageResizerState st(align_corners_, false);\n     st.ValidateAndCreateOutput(context);",
      "code_diff":"@@ -702,8 +702,14 @@ class QuantizedResizeBilinearOp : public OpKernel {\n   }\n \n   void Compute(OpKernelContext* context) override {\n-    const float in_min = context->input(2).flat<float>()(0);\n-    const float in_max = context->input(3).flat<float>()(0);\n+    const auto& in_min_tensor = context->input(2);\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(in_min_tensor.shape()),\n+                errors::InvalidArgument(\"min must be a scalar\"));\n+    const float in_min = in_min_tensor.flat<float>()(0);\n+    const auto& in_max_tensor = context->input(3);\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(in_max_tensor.shape()),\n+                errors::InvalidArgument(\"max must be a scalar\"));\n+    const float in_max = in_max_tensor.flat<float>()(0);\n \n     ImageResizerState st(align_corners_, false);\n     st.ValidateAndCreateOutput(context);"
    },
    {
      "index":42,
      "vuln_id":"GHSA-44qp-9wwf-734r",
      "cwe_id":"{'CWE-787', 'CWE-120'}",
      "score":7.6,
      "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/2b7100d6cdff36aa21010a82269bc05a6d1cc74a', 'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/adbbabdb0d3abb3cdeac69e38a96de1d678b24b3'}",
      "dataset":"osv",
      "summary":"Heap overflow in Tensorflow ### Impact \nThe [implementation of `SparseCountSparseOutput`](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/5100e359aef5c8021f2e71c7b986420b85ce7b3d\/tensorflow\/core\/kernels\/count_ops.cc#L168-L273) is vulnerable to a heap overflow:\n\n```python\nimport tensorflow as tf\nimport numpy as np\n\ntf.raw_ops.SparseCountSparseOutput(\n  indices=[[-1,-1]],\n  values=[2],\n  dense_shape=[1, 1],\n  weights=[1],\n  binary_output=True,\n  minlength=-1,\n  maxlength=-1,\n  name=None)\n```\n\n### Patches\nWe have patched the issue in GitHub commits [2b7100d6cdff36aa21010a82269bc05a6d1cc74a](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/2b7100d6cdff36aa21010a82269bc05a6d1cc74a) and [adbbabdb0d3abb3cdeac69e38a96de1d678b24b3](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/adbbabdb0d3abb3cdeac69e38a96de1d678b24b3).\n\nThe fix will be included in TensorFlow 2.8.0. We will also cherrypick this commit on TensorFlow 2.7.1, TensorFlow 2.6.3, and TensorFlow 2.5.3, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by Faysal Hossain Shezan from University of Virginia.",
      "published_date":"2022-02-09",
      "chain_len":2,
      "project":"https:\/\/github.com\/tensorflow\/tensorflow",
      "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/adbbabdb0d3abb3cdeac69e38a96de1d678b24b3",
      "commit_sha":"adbbabdb0d3abb3cdeac69e38a96de1d678b24b3",
      "patch":"MULTI",
      "chain_ord":"['2b7100d6cdff36aa21010a82269bc05a6d1cc74a', 'adbbabdb0d3abb3cdeac69e38a96de1d678b24b3']",
      "before_first_fix_commit":"{'2b7100d6cdff36aa21010a82269bc05a6d1cc74a'}",
      "last_fix_commit":"adbbabdb0d3abb3cdeac69e38a96de1d678b24b3",
      "chain_ord_pos":2.0,
      "commit_datetime":"12\/08\/2021, 03:44:33",
      "message":"Further validate sparse tensor for `SparseCount`: indices must be valid within dense shape.\n\nPiperOrigin-RevId: 414888122\nChange-Id: I4552bd74c135ecd4bcb5448acc0a3ce9402d8286",
      "author":"Mihai Maruseac",
      "comments":null,
      "stats":"{'additions': 17, 'deletions': 3, 'total': 20}",
      "files":"{'tensorflow\/core\/kernels\/count_ops.cc': {'additions': 17, 'deletions': 3, 'changes': 20, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/adbbabdb0d3abb3cdeac69e38a96de1d678b24b3\/tensorflow%2Fcore%2Fkernels%2Fcount_ops.cc', 'patch': '@@ -206,6 +206,23 @@ class SparseCount : public OpKernel {\\n     OP_REQUIRES(context, shape.NumElements() > 0,\\n                 errors::InvalidArgument(\\n                     \"The shape argument requires at least one element.\"));\\n+    \/\/ Validate indices: each index must be valid for the corresponding\\n+    \/\/ dimension. This could be possibly done better.\\n+    const auto indices_values = indices.matrix<int64_t>();\\n+    const auto shape_vector = shape.vec<int64_t>();\\n+    int num_values = values.NumElements();  \/\/ same as first dim of indices\\n+    int rank = indices.shape().dim_size(1);\\n+    for (int i = 0; i < num_values; ++i) {\\n+      for (int j = 0; j < rank; ++j) {\\n+        OP_REQUIRES(\\n+            context,\\n+            indices_values(i, j) >= 0 && indices_values(i, j) < shape_vector(j),\\n+            errors::InvalidArgument(\\n+                \"Invalid index value at \", i, \": dimension \", j, \" has value \",\\n+                indices_values(i, j), \" which is not in [0, \", shape_vector(j),\\n+                \") (as given by dense shape \", shape.DebugString()));\\n+      }\\n+    }\\n \\n     if (use_weights) {\\n       OP_REQUIRES(\\n@@ -217,11 +234,8 @@ class SparseCount : public OpKernel {\\n     }\\n \\n     bool is_1d = shape.NumElements() == 1;\\n-    auto shape_vector = shape.flat<int64_t>();\\n     int num_batches = is_1d ? 1 : shape_vector(0);\\n-    int num_values = values.NumElements();\\n \\n-    const auto indices_values = indices.matrix<int64_t>();\\n     const auto values_values = values.flat<T>();\\n     const auto weight_values = weights.flat<W>();'}}",
      "message_norm":"further validate sparse tensor for `sparsecount`: indices must be valid within dense shape.\n\npiperorigin-revid: 414888122\nchange-id: i4552bd74c135ecd4bcb5448acc0a3ce9402d8286",
      "language":"en",
      "entities":"[('validate', 'ACTION', ''), ('414888122', 'SHA', 'generic_sha')]",
      "classification_level_1":null,
      "classification_level_2":null,
      "list_files":"dict_keys(['tensorflow\/core\/kernels\/count_ops.cc'])",
      "num_files":1.0,
      "patch_content":"From adbbabdb0d3abb3cdeac69e38a96de1d678b24b3 Mon Sep 17 00:00:00 2001\nFrom: Mihai Maruseac <mihaimaruseac@google.com>\nDate: Tue, 7 Dec 2021 19:44:33 -0800\nSubject: [PATCH] Further validate sparse tensor for `SparseCount`: indices\n must be valid within dense shape.\n\nPiperOrigin-RevId: 414888122\nChange-Id: I4552bd74c135ecd4bcb5448acc0a3ce9402d8286\n---\n tensorflow\/core\/kernels\/count_ops.cc | 20 +++++++++++++++++---\n 1 file changed, 17 insertions(+), 3 deletions(-)\n\ndiff --git a\/tensorflow\/core\/kernels\/count_ops.cc b\/tensorflow\/core\/kernels\/count_ops.cc\nindex 1f99e0783e26f6..cc101b66f81403 100644\n--- a\/tensorflow\/core\/kernels\/count_ops.cc\n+++ b\/tensorflow\/core\/kernels\/count_ops.cc\n@@ -206,6 +206,23 @@ class SparseCount : public OpKernel {\n     OP_REQUIRES(context, shape.NumElements() > 0,\n                 errors::InvalidArgument(\n                     \"The shape argument requires at least one element.\"));\n+    \/\/ Validate indices: each index must be valid for the corresponding\n+    \/\/ dimension. This could be possibly done better.\n+    const auto indices_values = indices.matrix<int64_t>();\n+    const auto shape_vector = shape.vec<int64_t>();\n+    int num_values = values.NumElements();  \/\/ same as first dim of indices\n+    int rank = indices.shape().dim_size(1);\n+    for (int i = 0; i < num_values; ++i) {\n+      for (int j = 0; j < rank; ++j) {\n+        OP_REQUIRES(\n+            context,\n+            indices_values(i, j) >= 0 && indices_values(i, j) < shape_vector(j),\n+            errors::InvalidArgument(\n+                \"Invalid index value at \", i, \": dimension \", j, \" has value \",\n+                indices_values(i, j), \" which is not in [0, \", shape_vector(j),\n+                \") (as given by dense shape \", shape.DebugString()));\n+      }\n+    }\n \n     if (use_weights) {\n       OP_REQUIRES(\n@@ -217,11 +234,8 @@ class SparseCount : public OpKernel {\n     }\n \n     bool is_1d = shape.NumElements() == 1;\n-    auto shape_vector = shape.flat<int64_t>();\n     int num_batches = is_1d ? 1 : shape_vector(0);\n-    int num_values = values.NumElements();\n \n-    const auto indices_values = indices.matrix<int64_t>();\n     const auto values_values = values.flat<T>();\n     const auto weight_values = weights.flat<W>();",
      "code_diff":"@@ -206,6 +206,23 @@ class SparseCount : public OpKernel {\n     OP_REQUIRES(context, shape.NumElements() > 0,\n                 errors::InvalidArgument(\n                     \"The shape argument requires at least one element.\"));\n+    \/\/ Validate indices: each index must be valid for the corresponding\n+    \/\/ dimension. This could be possibly done better.\n+    const auto indices_values = indices.matrix<int64_t>();\n+    const auto shape_vector = shape.vec<int64_t>();\n+    int num_values = values.NumElements();  \/\/ same as first dim of indices\n+    int rank = indices.shape().dim_size(1);\n+    for (int i = 0; i < num_values; ++i) {\n+      for (int j = 0; j < rank; ++j) {\n+        OP_REQUIRES(\n+            context,\n+            indices_values(i, j) >= 0 && indices_values(i, j) < shape_vector(j),\n+            errors::InvalidArgument(\n+                \"Invalid index value at \", i, \": dimension \", j, \" has value \",\n+                indices_values(i, j), \" which is not in [0, \", shape_vector(j),\n+                \") (as given by dense shape \", shape.DebugString()));\n+      }\n+    }\n \n     if (use_weights) {\n       OP_REQUIRES(\n@@ -217,11 +234,8 @@ class SparseCount : public OpKernel {\n     }\n \n     bool is_1d = shape.NumElements() == 1;\n-    auto shape_vector = shape.flat<int64_t>();\n     int num_batches = is_1d ? 1 : shape_vector(0);\n-    int num_values = values.NumElements();\n \n-    const auto indices_values = indices.matrix<int64_t>();\n     const auto values_values = values.flat<T>();\n     const auto weight_values = weights.flat<W>();"
    },
    {
      "index":43,
      "vuln_id":"GHSA-mqh2-9wrp-vx84",
      "cwe_id":"{'CWE-787'}",
      "score":2.5,
      "chain":"{'https:\/\/github.com\/tensorflow\/tensorflow\/commit\/8ba6fa29cd8bf9cef9b718dc31c78c73081f5b31'}",
      "dataset":"osv",
      "summary":"Heap buffer overflow in `SparseSplit` ### Impact\nAn attacker can cause a heap buffer overflow in `tf.raw_ops.SparseSplit`:\n\n```python\nimport tensorflow as tf\n\nshape_dims = tf.constant(0, dtype=tf.int64)\nindices = tf.ones([1, 1], dtype=tf.int64)\nvalues = tf.ones([1], dtype=tf.int64)\nshape = tf.ones([1], dtype=tf.int64)\n\ntf.raw_ops.SparseSplit(\n    split_dim=shape_dims, indices=indices, values=values,\n    shape=shape, num_split=1)\n```\n\nThis is because the [implementation](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/699bff5d961f0abfde8fa3f876e6d241681fbef8\/tensorflow\/core\/util\/sparse\/sparse_tensor.h#L528-L530) accesses an array element based on a user controlled offset:\n\n```cc\nconst int dim = input_tensor.indices().matrix<int64>()(i, split_dim);\nint slice_index = GetSliceIndex(dim, split_size, residual);\nnum_values[slice_index]++;\n```\n\nThis results in overriding values on the heap.\n\n### Patches\nWe have patched the issue in GitHub commit [8ba6fa29cd8bf9cef9b718dc31c78c73081f5b31](https:\/\/github.com\/tensorflow\/tensorflow\/commit\/8ba6fa29cd8bf9cef9b718dc31c78c73081f5b31).\n\nThe fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.\n\n### For more information\nPlease consult [our security guide](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.\n\n### Attribution\nThis vulnerability has been reported by Ying Wang and Yakun Zhang of Baidu X-Team.",
      "published_date":"2021-05-21",
      "chain_len":1,
      "project":"https:\/\/github.com\/tensorflow\/tensorflow",
      "commit_href":"https:\/\/github.com\/tensorflow\/tensorflow\/commit\/8ba6fa29cd8bf9cef9b718dc31c78c73081f5b31",
      "commit_sha":"8ba6fa29cd8bf9cef9b718dc31c78c73081f5b31",
      "patch":"SINGLE",
      "chain_ord":"['8ba6fa29cd8bf9cef9b718dc31c78c73081f5b31']",
      "before_first_fix_commit":"{'699bff5d961f0abfde8fa3f876e6d241681fbef8'}",
      "last_fix_commit":"8ba6fa29cd8bf9cef9b718dc31c78c73081f5b31",
      "chain_ord_pos":1.0,
      "commit_datetime":"04\/30\/2021, 00:58:08",
      "message":"Fix heap-buffer-overflow issue with `tf.raw_ops.SparseSplit`.\n\nPiperOrigin-RevId: 371242872\nChange-Id: I482bb3d12602c7c3cc9446f97fb9f584bb98e9a4",
      "author":"Amit Patankar",
      "comments":null,
      "stats":"{'additions': 4, 'deletions': 0, 'total': 4}",
      "files":"{'tensorflow\/core\/util\/sparse\/sparse_tensor.h': {'additions': 4, 'deletions': 0, 'changes': 4, 'status': 'modified', 'raw_url': 'https:\/\/github.com\/tensorflow\/tensorflow\/raw\/8ba6fa29cd8bf9cef9b718dc31c78c73081f5b31\/tensorflow%2Fcore%2Futil%2Fsparse%2Fsparse_tensor.h', 'patch': '@@ -527,6 +527,10 @@ inline Status SparseTensor::Split(const SparseTensor& input_tensor,\\n   for (int i = 0; i < input_tensor.indices().dim_size(0); ++i) {\\n     const int dim = input_tensor.indices().matrix<int64>()(i, split_dim);\\n     int slice_index = GetSliceIndex(dim, split_size, residual);\\n+    if (slice_index >= num_values.size()) {\\n+      return errors::InvalidArgument(\"Slice index \", slice_index,\\n+                                     \" is larger than num_split.\");\\n+    }\\n     num_values[slice_index]++;\\n   }'}}",
      "message_norm":"fix heap-buffer-overflow issue with `tf.raw_ops.sparsesplit`.\n\npiperorigin-revid: 371242872\nchange-id: i482bb3d12602c7c3cc9446f97fb9f584bb98e9a4",
      "language":"en",
      "entities":"[('fix', 'ACTION', ''), ('overflow', 'SECWORD', ''), ('issue', 'FLAW', ''), ('371242872', 'SHA', 'generic_sha')]",
      "classification_level_1":null,
      "classification_level_2":null,
      "list_files":"dict_keys(['tensorflow\/core\/util\/sparse\/sparse_tensor.h'])",
      "num_files":1.0,
      "patch_content":"From 8ba6fa29cd8bf9cef9b718dc31c78c73081f5b31 Mon Sep 17 00:00:00 2001\nFrom: Amit Patankar <amitpatankar@google.com>\nDate: Thu, 29 Apr 2021 17:58:08 -0700\nSubject: [PATCH] Fix heap-buffer-overflow issue with `tf.raw_ops.SparseSplit`.\n\nPiperOrigin-RevId: 371242872\nChange-Id: I482bb3d12602c7c3cc9446f97fb9f584bb98e9a4\n---\n tensorflow\/core\/util\/sparse\/sparse_tensor.h | 4 ++++\n 1 file changed, 4 insertions(+)\n\ndiff --git a\/tensorflow\/core\/util\/sparse\/sparse_tensor.h b\/tensorflow\/core\/util\/sparse\/sparse_tensor.h\nindex 062226d7699bc8..341290dbbc6982 100644\n--- a\/tensorflow\/core\/util\/sparse\/sparse_tensor.h\n+++ b\/tensorflow\/core\/util\/sparse\/sparse_tensor.h\n@@ -527,6 +527,10 @@ inline Status SparseTensor::Split(const SparseTensor& input_tensor,\n   for (int i = 0; i < input_tensor.indices().dim_size(0); ++i) {\n     const int dim = input_tensor.indices().matrix<int64>()(i, split_dim);\n     int slice_index = GetSliceIndex(dim, split_size, residual);\n+    if (slice_index >= num_values.size()) {\n+      return errors::InvalidArgument(\"Slice index \", slice_index,\n+                                     \" is larger than num_split.\");\n+    }\n     num_values[slice_index]++;\n   }",
      "code_diff":"@@ -527,6 +527,10 @@ inline Status SparseTensor::Split(const SparseTensor& input_tensor,\n   for (int i = 0; i < input_tensor.indices().dim_size(0); ++i) {\n     const int dim = input_tensor.indices().matrix<int64>()(i, split_dim);\n     int slice_index = GetSliceIndex(dim, split_size, residual);\n+    if (slice_index >= num_values.size()) {\n+      return errors::InvalidArgument(\"Slice index \", slice_index,\n+                                     \" is larger than num_split.\");\n+    }\n     num_values[slice_index]++;\n   }"
    }
  ]
}